{
  "name": "machine-learning-lounge",
  "messages": [
    {
      "type": "message",
      "user": "U03EBH4MA8Y",
      "text": "This content can't be displayed.",
      "ts": "1654551021.190969",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 18,
          "users": [
            "U03HZ40P6LV",
            "U03K3TPTFPA",
            "U03HMBFH687",
            "U03JKNC8XK6",
            "U03JV6L2ZPA",
            "U03JARQCZDL",
            "U03J4EK1H1A",
            "U03J8B88VE1",
            "U03JVFS8VLH",
            "U03J88L7YJG",
            "U03J4D2D8FN",
            "U03JTF09KU1",
            "U03K0UFCSNM",
            "U03J4BNTQSG",
            "U03J6N75163",
            "U03K3D9G6TA",
            "U03JN7USRDY",
            "U03K2TG7ZDY"
          ]
        },
        {
          "name": "heart",
          "count": 16,
          "users": [
            "U03JB85MK35",
            "U03HMBFH687",
            "U03JKNC8XK6",
            "U03JELDBMT3",
            "U03J1UVFPK5",
            "U03J4EK1H1A",
            "U03JE7D37QU",
            "U03J8B88VE1",
            "U03HMCBRYQP",
            "U03JVEMS83B",
            "U03JTF09KU1",
            "U03K0G0C01X",
            "U03K0UFCSNM",
            "U03K3D9G6TA",
            "U03JMUP9D9B",
            "U03JELZ8JTT"
          ]
        },
        {
          "name": "green_heart",
          "count": 6,
          "users": [
            "U03JKNC8XK6",
            "U03J4C51BM2",
            "U03JE7D37QU",
            "U03J8B88VE1",
            "U03K0UFCSNM",
            "U03K0FTKXLY"
          ]
        },
        {
          "name": "thinking_face",
          "count": 3,
          "users": [
            "U03JKNC8XK6",
            "U03J8B88VE1",
            "U03K0UFCSNM"
          ]
        },
        {
          "name": "zany_face",
          "count": 4,
          "users": [
            "U03JKNC8XK6",
            "U03J8B88VE1",
            "U03JEFFP47N",
            "U03K0UFCSNM"
          ]
        },
        {
          "name": "simple_smile",
          "count": 2,
          "users": [
            "U03JKNC8XK6",
            "U03J8B88VE1"
          ]
        },
        {
          "name": "+1",
          "count": 3,
          "users": [
            "U03JEUXG8H1",
            "U03J8B88VE1",
            "U03K0UFCSNM"
          ]
        },
        {
          "name": "flag-pt",
          "count": 3,
          "users": [
            "U03J8B88VE1",
            "U03JKGPS36F",
            "U03K0UFCSNM"
          ]
        },
        {
          "name": "brain",
          "count": 3,
          "users": [
            "U03K67T11G8",
            "U03JELQLESV",
            "U03K0UFCSNM"
          ]
        },
        {
          "name": "smiling_face_with_3_hearts",
          "count": 1,
          "users": [
            "U03K0UFCSNM"
          ]
        },
        {
          "name": "wave",
          "count": 1,
          "users": [
            "U03JG9VGEEA"
          ]
        },
        {
          "name": "es",
          "count": 1,
          "users": [
            "U03J9UWUV37"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "header",
          "text": {
            "type": "plain_text",
            "text": "Welcome to the Machine Learning Digital Lounge!",
            "emoji": true
          },
          "block_id": "t2y9"
        },
        {
          "type": "image",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/wwdc22/slack/ML%20%26%20Vision%20-%20regular%20hero%403x.png",
          "alt_text": "A multi-colored eye on a black background ",
          "block_id": "mCfq"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "Welcome to the WWDC22 Machine Learning Digital Lounge! We’ve got an exciting week planned for you. You can find the full schedule of events in the \u003chttps://developer.apple.com/news/?id=8sf5pvt9|  Apple Developer app and on developer.apple.com\u003e, and activities for all lounges kick off on June 7."
          },
          "block_id": "Z3QmG"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "If you haven't already, please take a moment to familiarize yourself with \u003chttps://developer.apple.com/news/?id=735utu4s|how the Digital Lounges will work\u003e."
          },
          "block_id": "GIYUd"
        },
        {
          "type": "header",
          "text": {
            "type": "plain_text",
            "text": "WWDC22 Interactive Events Policy",
            "emoji": true
          },
          "block_id": "hlD+"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "We want to make sure these spaces are helpful and welcoming for everyone — developers and Apple employees alike. Please review and follow the \u003chttps://developer.apple.com/wwdc22/policies/interactive-events/1/Apple-Developer-WWDC22-Interactive-Events-Attendance-Policy.pdf| WWDC22 Interactive Events Policy\u003e."
          },
          "block_id": "C/I1u"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "Have a great WWDC!"
          },
          "block_id": "nfJ"
        }
      ]
    },
    {
      "client_msg_id": "7c4b223b-aab7-4b77-9f4f-217e8ee92c27",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*This lounge is about to go live!* Join us here, in 5 minutes for _*Q\u0026amp;A: Create ML*_. We have a great group of engineers from the Machine Learning team ready to answer all of your questions about model creation, model evaluation, and everything new in Create ML app and framework. And honestly, most any question you may have about bring machine learning to your app. It's going to be a fun, lively conversation. Can't wait!",
      "ts": "1654631908.083859",
      "edited": {
        "user": "U03DJTBMHFF",
        "ts": "1654631929.000000"
      },
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "brain",
          "count": 2,
          "users": [
            "U03JELQLESV",
            "U03HVCSF7B8"
          ]
        },
        {
          "name": "raised_hands",
          "count": 4,
          "users": [
            "U03JRP87THN",
            "U03JGTHPP7D",
            "U03HVCSF7B8",
            "U03HZ4BPHPX"
          ]
        },
        {
          "name": "wwdc22",
          "count": 2,
          "users": [
            "U03J4CASP0R",
            "U03J4CARJQ1"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "fzQ40",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "This lounge is about to go live!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " Join us here, in 5 minutes for "
                },
                {
                  "type": "text",
                  "text": "Q\u0026A: Create ML",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ". We have a great group of engineers from the Machine Learning team ready to answer all of your questions about model creation, model evaluation, and everything new in Create ML app and framework. And honestly, most any question you may have about bring machine learning to your app. It's going to be a fun, lively conversation. Can't wait!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03DJTBMHFF\u003e added a workflow to this channel: *Machine Learning - Idea Submission*.",
      "ts": "1654631989.086229",
      "subtype": "bot_message",
      "bot_id": "B03HBPSE4R3",
      "username": "Machine Learning - Idea Submission",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "type": "message",
      "text": "\u003c@U03DJTBMHFF\u003e added a workflow to this channel: *Machine Learning - Ask a Question*.",
      "ts": "1654632009.217909",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "client_msg_id": "8f17ddb8-b350-460a-a164-67882bffff42",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*Alright! Let's roll.* Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows.",
      "ts": "1654632089.696679",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "9Fdo",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Alright! Let's roll. ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "53546a68-337f-4c0b-96cc-6f6103d4acea",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*Open discussion:* To get things going, who has already watched the _\"What's new in Create ML_\" and _\"Get to know Create ML Components\"_ sessions? What are you most excited about? Reply in the thread… :thread:",
      "ts": "1654632409.082239",
      "thread_ts": "1654632409.082239",
      "reply_count": 9,
      "latest_reply": "1654633300.023319",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "6/K5A",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Open discussion: ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "To get things going, who has already watched the "
                },
                {
                  "type": "text",
                  "text": "\"What's new in Create ML",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "\" and "
                },
                {
                  "type": "text",
                  "text": "\"Get to know Create ML Components\" ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "sessions? What are you most excited about? Reply in the thread… "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "03f13753-dc46-4368-a5c2-a693de5c396a",
          "type": "message",
          "user": "U03JA6H3Z38",
          "text": "I am super excited about the live preview feature for quick testing of models",
          "ts": "1654632567.239049",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "100",
              "count": 1,
              "users": [
                "U03J4CASP0R"
              ]
            },
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB5Z1HQF",
                "U03JELZ8JTT"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mom",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I am super excited about the live preview feature for quick testing of models"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "993948e9-087e-4085-a966-217141d69f27",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Repetition Counting!  (where is the sample code?)",
          "ts": "1654632616.215339",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 3,
              "users": [
                "U03J4CASP0R",
                "U03HB5Z1HQF",
                "U03HRNBHZEX"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "n3k0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Repetition Counting!  (where is the sample code?)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "C3DB16C0-BC5D-4A23-8F83-E2E55A04B0F2",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "I really like the live testing with continuity camera",
          "ts": "1654632688.272659",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 3,
              "users": [
                "U03HB5Z1HQF",
                "U03J7UASVEU",
                "U03J4CASP0R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "BooM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "really"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "like"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "live"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "testing"
                    },
                    {
                      "type": "text",
                      "text": " with "
                    },
                    {
                      "type": "text",
                      "text": "continuity"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "camera"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0c6ec948-474c-4223-9a67-8d12b373a54a",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "In What’s New, near the end, sample code for Repetition Counting is mentioned, but when searching the WWDC22 sample code, not finding it.  Is this really available?  if so, where?",
          "ts": "1654632858.145149",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tokDB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In What’s New, near the end, sample code for Repetition Counting is mentioned, but when searching the WWDC22 sample code, not finding it.  Is this really available?  if so, where?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ab5ffb6e-937e-4ee8-9b02-956971efe992",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "\u003c@U03HVD99L14\u003e Yeah, it should be available, and the sample app is called “CountMyActions”",
          "ts": "1654633023.058459",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "V7Kz/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HVD99L14"
                    },
                    {
                      "type": "text",
                      "text": " Yeah, it should be available, and the sample app is called “CountMyActions”"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "20fbabf7-4415-41c7-878e-7e5fa0ca6249",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "If not today, it may come together with another CreateML session tomorrow (called: _Compose advanced models with Create ML Components)_",
          "ts": "1654633085.293709",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Gl2y",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If not today, it may come together with another CreateML session tomorrow (called: "
                    },
                    {
                      "type": "text",
                      "text": "Compose advanced models with Create ML Components)",
                      "style": {
                        "italic": true
                      }
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9fd1765a-e48f-40b5-9141-e4a83dba74e9",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Hi Yuxin,  \u003chttps://developer.apple.com/sample-code/wwdc/2022/?q=CountMyActions\u003e shows no results",
          "ts": "1654633110.080789",
          "thread_ts": "1654632409.082239",
          "attachments": [
            {
              "fallback": "Sample Code - WWDC22 - Apple Developer",
              "id": 1,
              "title": "Sample Code - WWDC22 - Apple Developer",
              "title_link": "https://developer.apple.com/sample-code/wwdc/2022/?q=CountMyActions",
              "text": "View sample code to see how the latest Apple technologies are implemented.",
              "service_name": "developer.apple.com",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/sample-code/wwdc/2022/?q=CountMyActions",
              "original_url": "https://developer.apple.com/sample-code/wwdc/2022/?q=CountMyActions",
              "blocks": null
            }
          ],
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "O5j+j",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Yuxin,  "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/sample-code/wwdc/2022/?q=CountMyActions",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " shows no results"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4b5077e5-b56f-494e-b45e-0522ddaab399",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "\u003c@U03HVD99L14\u003e Ah, I see. Let us look into this.",
          "ts": "1654633277.232839",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YW6F",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HVD99L14"
                    },
                    {
                      "type": "text",
                      "text": " Ah, I see. Let us look into this."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "616a35be-6943-4f5a-ba28-ad7f9515da9c",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Thanks Yuxin!",
          "ts": "1654633300.023319",
          "thread_ts": "1654632409.082239",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "jSI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks Yuxin!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JA6H3Z38\u003e asked\n\u0026gt; When training an Object Detection Model in Create ML, is there a way to take automatic snapshots to ensure that there is a record of how the model is performing throughout iterations as opposed to having to take manual snapshots to do so?",
      "ts": "1654632748.466379",
      "thread_ts": "1654632748.466379",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654632878.418499",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "j0A",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JA6H3Z38"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"When training an Object Detection Model in Create ML, is there a way to take automatic snapshots to ensure that there is a record of how the model is performing throughout iterations as opposed to having to take manual snapshots to do so?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "45c27a81-7fe8-4817-8f33-63d11ffa29c6",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "In the Create ML app this is a manual process. You can also set options on the project to automatically snapshot on pause or train more. Having an automatic option is a great feature request. Please consider filing a request using \u003chttp://feedbackassistant.apple.com|feedback assistant\u003e.\n\nYou do have the option to use the Create ML framework directly in Swift and set your own cadence of checkpointing.  \u003chttps://developer.apple.com/documentation/createml/mlobjectdetector\u003e",
          "ts": "1654632834.031409",
          "thread_ts": "1654632748.466379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JA6H3Z38"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "NL67",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In the Create ML app this is a manual process. You can also set options on the project to automatically snapshot on pause or train more. Having an automatic option is a great feature request. Please consider filing a request using "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": "feedback assistant"
                    },
                    {
                      "type": "text",
                      "text": ".\n\nYou do have the option to use the Create ML framework directly in Swift and set your own cadence of checkpointing.  "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createml/mlobjectdetector",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5120168c-f3b3-4471-beb7-05d79e208a63",
          "type": "message",
          "user": "U03JA6H3Z38",
          "text": "Thanks for the info!",
          "ts": "1654632878.418499",
          "thread_ts": "1654632748.466379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KorBZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for the info!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "565f0592-45a3-4a9f-924d-cfeb4f2b765e",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*Luca* asked:\n\u0026gt;  \"Can I train a tabular classifier model on iOS?",
      "ts": "1654632781.334529",
      "thread_ts": "1654632781.334529",
      "reply_count": 2,
      "latest_reply": "1654632882.521079",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "+1",
          "count": 2,
          "users": [
            "U03HX9ZTNQ7",
            "U03J9R7MVJ7"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "aKNH",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Luca",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " ",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "asked:\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\" \\\"Can I train a tabular classifier model on iOS?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "31b95eca-057f-4d5d-8fb8-aac2064eb00f",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "You absolutely can! Have a look at last year's session on building dynamic apps: \u003chttps://developer.apple.com/videos/play/wwdc2021/10037/\u003e",
          "ts": "1654632841.736089",
          "thread_ts": "1654632781.334529",
          "attachments": [
            {
              "fallback": "Apple Developer: Build dynamic iOS apps with the Create ML framework - WWDC21 - Videos - Apple Developer",
              "id": 1,
              "title": "Build dynamic iOS apps with the Create ML framework - WWDC21 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2021/10037/",
              "text": "Discover how your app can train Core ML models fully on device with the Create ML framework, enabling adaptive and customized app...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/119/4927/4927_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2021/10037/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2021/10037/",
              "blocks": null
            }
          ],
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0gm9P",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You absolutely can! Have a look at last year's session on building dynamic apps: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2021/10037/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "358348e2-7032-44f9-9d07-b5e1c6b2d834",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "Also, be sure to checkout the _Get to know Create ML Components_ session that dropped today. There, Alejandro walks through building a tabular regressor all in Swift.",
          "ts": "1654632882.521079",
          "thread_ts": "1654632781.334529",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03J9R7MVJ7",
                "U03J4CASP0R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0=PK9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also, be sure to checkout the "
                    },
                    {
                      "type": "text",
                      "text": "Get to know Create ML Components",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " session that dropped today. There, Alejandro walks through building a tabular regressor all in Swift."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JELQLESV\u003e asked\n\u0026gt; When training a video activity classifier model in Create ML, is it best to have only one person's poses in the frame (and crop out others)?",
      "ts": "1654632868.224379",
      "thread_ts": "1654632868.224379",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654633313.318229",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "KxW4",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JELQLESV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"When training a video activity classifier model in Create ML, is it best to have only one person's poses in the frame (and crop out others)?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "3a01817a-e4f2-417a-be01-2b71e4722c8a",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "Yes",
          "ts": "1654632902.192109",
          "thread_ts": "1654632868.224379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "WICX+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "589c70e4-d200-492d-b1b1-add0fcf9a2f5",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "If you have multiple people in the screen. Try to keep other people consistently smaller than the main person. Then it will still work (automatically select the maximum bounding box person)",
          "ts": "1654632945.131359",
          "thread_ts": "1654632868.224379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "CPcQ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you have multiple people in the screen. Try to keep other people consistently smaller than the main person. Then it will still work (automatically select the maximum bounding box person)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a364ea73-2872-4ceb-bb13-32509371be9e",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "check out the video from WWDC 2020 :\n\n\u003chttps://developer.apple.com/wwdc20/10043?time=1461\u003e\n\n\u0026gt; When it comes to using the model in your applications, make sure to only select a single person. Your app may remind users to keep only one person in view when multiple people are detected, or you can implement your own selection logic to choose a person based on their size or location within the frame, and this can be achieved by using the coordinates from pose landmarks.\n",
          "ts": "1654633137.914199",
          "thread_ts": "1654632868.224379",
          "attachments": [
            {
              "fallback": "Apple Developer: Build an Action Classifier with Create ML - WWDC20 - Videos - Apple Developer",
              "id": 1,
              "title": "Build an Action Classifier with Create ML - WWDC20 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/wwdc20/10043?time=1461",
              "text": "Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/49/3438/3438_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/wwdc20/10043?time=1461",
              "original_url": "https://developer.apple.com/wwdc20/10043?time=1461",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "khSV",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "check out the video from WWDC 2020 :\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/wwdc20/10043?time=1461",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"When it comes to using the model in your applications, make sure to only select a single person. Your app may remind users to keep only one person in view when multiple people are detected, or you can implement your own selection logic to choose a person based on their size or location within the frame, and this can be achieved by using the coordinates from pose landmarks.\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": []
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cb9a30f2-2ba3-45f9-83f3-e173abfd814a",
          "type": "message",
          "user": "U03JELQLESV",
          "text": "Thanks for the answers, and for including the relevant part of the session!",
          "ts": "1654633313.318229",
          "thread_ts": "1654632868.224379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HRNBHZEX"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "dDV",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for the answers, and for including the relevant part of the session!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JDNSUW78\u003e asked\n\u0026gt; This may be a more general Machine Learning question. I am interested in the ability to extract text from images and video.\n\u0026gt; \n\u0026gt; The Vision framework does a great job of extracting the text.\n\u0026gt; \n\u0026gt; One thing I would like to do is determine whether each piece of text is in a particular category of typeface, largely looking to tell a source code / monospace font from a sans serif font.\n\u0026gt; \n\u0026gt; Which machine learning technologies available on Apple platforms would be best suited for that? And a high level of how you might approach that?",
      "ts": "1654632974.722859",
      "thread_ts": "1654632974.722859",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654634780.246899",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "hHg",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JDNSUW78"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"This may be a more general Machine Learning question. I am interested in the ability to extract text from images and video.\\n\\nThe Vision framework does a great job of extracting the text.\\n\\nOne thing I would like to do is determine whether each piece of text is in a particular category of typeface, largely looking to tell a source code \\/ monospace font from a sans serif font.\\n\\nWhich machine learning technologies available on Apple platforms would be best suited for that? And a high level of how you might approach that?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "cd1dbc86-8b45-47cb-9d3c-32e470820476",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "So the Vision framework, where you extract the text, tells you the region in the image where the text is; the first thing to do would be to crop the text out of the image.\n\nIf you have a binary image classifier (sans serif vs serif, or “looks like source code” vs “doesn’t look like source code”, it’s worth experimenting with what definition works best – and you’d need to collect samples of each for your training set!), you can then throw that crop to this classifier to work out whether it’s source code or not.\n\nSo at a high level, what I’d do is;\n\n• train a binary classifier to distinguish source-code from not-source-code\n• using Vision, crop out the region of the image with detected text in\n• use your classifier to determine whether it’s source code or not\nand go from there!",
          "ts": "1654633154.012919",
          "thread_ts": "1654632974.722859",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6h0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So the Vision framework, where you extract the text, tells you the region in the image where the text is; the first thing to do would be to crop the text out of the image.\n\nIf you have a binary image classifier (sans serif vs serif, or “looks like source code” vs “doesn’t look like source code”, it’s worth experimenting with what definition works best – and you’d need to collect samples of each for your training set!), you can then throw that crop to this classifier to work out whether it’s source code or not.\n\nSo at a high level, what I’d do is;\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"train a binary classifier to distinguish source-code from not-source-code\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"using Vision, crop out the region of the image with detected text in\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"use your classifier to determine whether it\\u2019s source code or not\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "\nand go from there!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4514f7f3-fea0-4963-9351-c63cb9b3b0c0",
          "type": "message",
          "user": "U03HNRF5MB7",
          "text": "Also you can try out the Live Text API this year, it's able to extract text of out images and videos. However it does not provide font-related information of the text yet. You can file a bug tracking this issue to us if needed.",
          "ts": "1654633334.093099",
          "thread_ts": "1654632974.722859",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Jim",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also you can try out the Live Text API this year, it's able to extract text of out images and videos. However it does not provide font-related information of the text yet. You can file a bug tracking this issue to us if needed."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a2dcf432-e47c-4532-8624-58257d3baecc",
          "type": "message",
          "user": "U03JENH943V",
          "text": "James, I created a serif/sans-serif model with CreateML. You can find it here: \u003chttps://github.com/jmousseau/Mimeo\u003e",
          "ts": "1654634432.751299",
          "thread_ts": "1654632974.722859",
          "attachments": [
            {
              "fallback": "GitHub: GitHub - jmousseau/Mimeo: Copy text with your camera.",
              "id": 1,
              "title": "GitHub - jmousseau/Mimeo: Copy text with your camera.",
              "title_link": "https://github.com/jmousseau/Mimeo",
              "text": "Copy text with your camera. Contribute to jmousseau/Mimeo development by creating an account on GitHub.",
              "image_url": "https://opengraph.githubassets.com/dc89a48eb1a2a65b7b49c771f459a7e7d0adb1a21126c2bfe919042f4c4ad38a/jmousseau/Mimeo",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/jmousseau/Mimeo",
              "original_url": "https://github.com/jmousseau/Mimeo",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "E+RU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "James, I created a serif/sans-serif model with CreateML. You can find it here: "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/jmousseau/Mimeo",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "de0d444c-0ca1-4c4a-98a6-0431bd7b2f40",
          "type": "message",
          "user": "U03JDNSUW78",
          "text": "Thank you! This is all very helpful! And thank you \u003c@U03JENH943V\u003e I will take a look at that model. It may be just what I need!",
          "ts": "1654634780.246899",
          "thread_ts": "1654632974.722859",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "U+sX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you! This is all very helpful! And thank you "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JENH943V"
                    },
                    {
                      "type": "text",
                      "text": " I will take a look at that model. It may be just what I need!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVD99L14\u003e asked\n\u0026gt; In the \"What's new in Create ML\" talk, near the end Repetition Counting was mentioned, and a reference to the \"linked article and sample code\", yet the WWDC22 Sample Code list does not include this, nor does the documentation, I believe.  Can you point me to the Sample Repetition Count code and Documentation?",
      "ts": "1654633149.119139",
      "thread_ts": "1654633149.119139",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 16,
      "latest_reply": "1654707133.005009",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "X6yd",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVD99L14"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"In the \\\"What's new in Create ML\\\" talk, near the end Repetition Counting was mentioned, and a reference to the \\\"linked article and sample code\\\", yet the WWDC22 Sample Code list does not include this, nor does the documentation, I believe.  Can you point me to the Sample Repetition Count code and Documentation?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "03d6fa14-43c1-4bb0-827a-8a77a1bd380d",
          "type": "message",
          "user": "U03JELQLESV",
          "text": "Probably just taking some time for it to be ready, I had a similar experience with WWDC21 and the relevant sample code was put up after I filed feedback.",
          "ts": "1654633243.393759",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=6hWk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Probably just taking some time for it to be ready, I had a similar experience with WWDC21 and the relevant sample code was put up after I filed feedback."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5aeefec2-1866-48f3-b75a-f54b2a4f3c13",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "Hi! We're currently taking a look, thanks for your patience.",
          "ts": "1654633298.482679",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "CO9ri",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi! We're currently taking a look, thanks for your patience."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ce99b773-924c-4a75-a07f-dbdc7d18d6e4",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Can you give us an update on this?  WWDC time is short, and availability of sample code earlier rather than later improves early use and feedback.",
          "ts": "1654704422.652829",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vY5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Can you give us an update on this?  WWDC time is short, and availability of sample code earlier rather than later improves early use and feedback."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dd311658-727a-4c48-99a9-1af58f05316c",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "Hi Glen, thanks for waiting. Try this link: \u003chttps://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed\u003e",
          "ts": "1654705043.082739",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ia3R",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Glen, thanks for waiting. Try this link: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5cada10a-b63f-47e8-8524-5c3f9500c910",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Hurray!  Thanks!",
          "ts": "1654705129.330569",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zxMt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hurray!  Thanks!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d30c88be-fbf1-4d7d-9116-71b2a06f33f2",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "(The beta version build fails,…)",
          "ts": "1654706292.597619",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Hj/k",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "(The beta version build fails,…)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d60a13f1-2ecc-4b9e-a4c5-e60302b7da14",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "what is the build error?",
          "ts": "1654706361.812759",
          "thread_ts": "1654633149.119139",
          "edited": {
            "user": "U03HRMABBDZ",
            "ts": "1654706368.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "r2K",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "what is the build error?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b3194391-ec92-49ac-a2cd-2c2770e5ed2d",
          "type": "message",
          "user": "U03HVD99L14",
          "ts": "1654706439.153639",
          "thread_ts": "1654633149.119139",
          "files": [
            {
              "id": "F03KHRCC0U8",
              "created": 1654706435,
              "timestamp": 1654706435,
              "name": "Screen Shot 2022-06-08 at 9.39.46 AM.jpeg",
              "title": "Screen Shot 2022-06-08 at 9.39.46 AM.jpeg",
              "mimetype": "image/jpeg",
              "image_exif_rotation": 0,
              "filetype": "jpg",
              "pretty_type": "JPEG",
              "user": "U03HVD99L14",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 89695,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03KHRCC0U8-Screen Shot 2022-06-08 at 9.39.46 AM.jpeg",
              "url_private_download": "C03H4A911EH/F03KHRCC0U8-Screen Shot 2022-06-08 at 9.39.46 AM.jpeg",
              "original_h": 420,
              "original_w": 508,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03KHRCC0U8-6e9f96c2aa/screen_shot_2022-06-08_at_9.39.46_am_64.jpg",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03KHRCC0U8-6e9f96c2aa/screen_shot_2022-06-08_at_9.39.46_am_80.jpg",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03KHRCC0U8-6e9f96c2aa/screen_shot_2022-06-08_at_9.39.46_am_160.jpg",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03KHRCC0U8-6e9f96c2aa/screen_shot_2022-06-08_at_9.39.46_am_360.jpg",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 298,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03KHRCC0U8-6e9f96c2aa/screen_shot_2022-06-08_at_9.39.46_am_480.jpg",
              "thumb_480_w": 480,
              "thumb_480_h": 397,
              "thumb_720": "",
              "thumb_720_w": 0,
              "thumb_720_h": 0,
              "thumb_960": "",
              "thumb_960_w": 0,
              "thumb_960_h": 0,
              "thumb_1024": "",
              "thumb_1024_w": 0,
              "thumb_1024_h": 0,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03HVD99L14/F03KHRCC0U8/screen_shot_2022-06-08_at_9.39.46_am.jpeg",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03KHRCC0U8-ec6ab41447",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": null
        },
        {
          "client_msg_id": "b3950228-4e94-4189-b2f2-58d1b018a1f2",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "Xcode needs to download the new Swift Async-Algorithm swift package, can you check if the download is successful",
          "ts": "1654706578.895149",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0hyh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Xcode needs to download the new Swift Async-Algorithm swift package, can you check if the download is successful"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "22df491e-7315-452a-bb26-f4608bd9f970",
          "type": "message",
          "user": "U03HRMABBDZ",
          "ts": "1654706601.608779",
          "thread_ts": "1654633149.119139",
          "files": [
            {
              "id": "F03K6NQLNBT",
              "created": 1654706597,
              "timestamp": 1654706597,
              "name": "Screen Shot 2022-06-08 at 9.43.13 AM.png",
              "title": "Screen Shot 2022-06-08 at 9.43.13 AM.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03HRMABBDZ",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 68430,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03K6NQLNBT-Screen Shot 2022-06-08 at 9.43.13 AM.png",
              "url_private_download": "C03H4A911EH/F03K6NQLNBT-Screen Shot 2022-06-08 at 9.43.13 AM.png",
              "original_h": 494,
              "original_w": 418,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6NQLNBT-91f4ea1968/screen_shot_2022-06-08_at_9.43.13_am_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6NQLNBT-91f4ea1968/screen_shot_2022-06-08_at_9.43.13_am_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6NQLNBT-91f4ea1968/screen_shot_2022-06-08_at_9.43.13_am_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6NQLNBT-91f4ea1968/screen_shot_2022-06-08_at_9.43.13_am_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 305,
              "thumb_360_h": 360,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6NQLNBT-91f4ea1968/screen_shot_2022-06-08_at_9.43.13_am_480.png",
              "thumb_480_w": 406,
              "thumb_480_h": 480,
              "thumb_720": "",
              "thumb_720_w": 0,
              "thumb_720_h": 0,
              "thumb_960": "",
              "thumb_960_w": 0,
              "thumb_960_h": 0,
              "thumb_1024": "",
              "thumb_1024_w": 0,
              "thumb_1024_h": 0,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03HRMABBDZ/F03K6NQLNBT/screen_shot_2022-06-08_at_9.43.13_am.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03K6NQLNBT-8db48be5ac",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": null
        },
        {
          "client_msg_id": "97294654-5cfe-4233-833c-d2e1536105c6",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "I just got it working…",
          "ts": "1654706676.219929",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Jdk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I just got it working…"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "054c9da8-d621-4edb-96b9-cf070650c7e4",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Although Xcode still shows the error",
          "ts": "1654706800.749209",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EAcyd",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Although Xcode still shows the error"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "984252cf-04a6-4867-9509-013ee4e3288a",
          "type": "message",
          "user": "U03HVD99L14",
          "ts": "1654706811.540649",
          "thread_ts": "1654633149.119139",
          "files": [
            {
              "id": "F03JWFS7NJG",
              "created": 1654706807,
              "timestamp": 1654706807,
              "name": "Screen Shot 2022-06-08 at 9.45.57 AM.jpeg",
              "title": "Screen Shot 2022-06-08 at 9.45.57 AM.jpeg",
              "mimetype": "image/jpeg",
              "image_exif_rotation": 0,
              "filetype": "jpg",
              "pretty_type": "JPEG",
              "user": "U03HVD99L14",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 460666,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03JWFS7NJG-Screen Shot 2022-06-08 at 9.45.57 AM.jpeg",
              "url_private_download": "C03H4A911EH/F03JWFS7NJG-Screen Shot 2022-06-08 at 9.45.57 AM.jpeg",
              "original_h": 1678,
              "original_w": 2256,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_64.jpg",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_80.jpg",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_160.jpg",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_360.jpg",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 268,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_480.jpg",
              "thumb_480_w": 480,
              "thumb_480_h": 357,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_720.jpg",
              "thumb_720_w": 720,
              "thumb_720_h": 536,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_960.jpg",
              "thumb_960_w": 960,
              "thumb_960_h": 714,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JWFS7NJG-3cc6129e37/screen_shot_2022-06-08_at_9.45.57_am_1024.jpg",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 762,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03HVD99L14/F03JWFS7NJG/screen_shot_2022-06-08_at_9.45.57_am.jpeg",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03JWFS7NJG-faa1dfa8ca",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": null
        },
        {
          "client_msg_id": "1036507e-d433-43c0-b774-4523e5738e7f",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Restarting Xcode, and All is Well!",
          "ts": "1654706930.349539",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "WNM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Restarting Xcode, and All is Well!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f38d514e-4e3b-4ecc-9002-222194176689",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "Great!",
          "ts": "1654706971.566499",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bugI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Great!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "70bad3c3-4c91-4177-be80-24aaf74fddf9",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Thanks for your help!",
          "ts": "1654707133.005009",
          "thread_ts": "1654633149.119139",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PaZ5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for your help!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JELQLESV\u003e asked\n\u0026gt; When making activity classification predictions on video frames, are there ways or considerations for improving speed aside from skipping frames (cadence)? Is prediction window size a factor?",
      "ts": "1654633602.775309",
      "thread_ts": "1654633602.775309",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 9,
      "latest_reply": "1654634536.743489",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "RKl9j",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JELQLESV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"When making activity classification predictions on video frames, are there ways or considerations for improving speed aside from skipping frames (cadence)? Is prediction window size a factor?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "04468cd7-f2c7-440f-bbcb-021cde92b4c9",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "The window size is fixed once a model is trained. At prediction time, you may adjust something called `stride` , i.e., how often (in terms of number of frames) you create a new prediction window. The smaller the `stride` , the faster you make a next prediction. This can make the results to refresh faster",
          "ts": "1654633698.701669",
          "thread_ts": "1654633602.775309",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vSFFb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The window size is fixed once a model is trained. At prediction time, you may adjust something called "
                    },
                    {
                      "type": "text",
                      "text": "stride",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " , i.e., how often (in terms of number of frames) you create a new prediction window. The smaller the "
                    },
                    {
                      "type": "text",
                      "text": "stride",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " , the faster you make a next prediction. This can make the results to refresh faster"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "525ad970-92c7-402d-916b-b0239e2f9cba",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "There was a sample code from last year, called “\u003chttps://www.google.com/url?sa=t\u0026amp;rct=j\u0026amp;q=\u0026amp;esrc=s\u0026amp;source=web\u0026amp;cd=\u0026amp;ved=2ahUKEwiThdeDl5z4AhUpiI4IHYOUAAcQFnoECAgQAw\u0026amp;url=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fcreateml%2Fdetecting_human_actions_in_a_live_video_feed\u0026amp;usg=AOvVaw1ufSKqW6Qz_al-8HXVsvWK|Detecting Human Actions in a Live Video Feed\u003e”, you may check this as a reference",
          "ts": "1654633820.561999",
          "thread_ts": "1654633602.775309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7nlA",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There was a sample code from last year, called “"
                    },
                    {
                      "type": "link",
                      "url": "https://www.google.com/url?sa=t\u0026rct=j\u0026q=\u0026esrc=s\u0026source=web\u0026cd=\u0026ved=2ahUKEwiThdeDl5z4AhUpiI4IHYOUAAcQFnoECAgQAw\u0026url=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fcreateml%2Fdetecting_human_actions_in_a_live_video_feed\u0026usg=AOvVaw1ufSKqW6Qz_al-8HXVsvWK",
                      "text": "Detecting Human Actions in a Live Video Feed"
                    },
                    {
                      "type": "text",
                      "text": "”, you may check this as a reference"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "bb6a5c9b-35d6-457f-9dad-f126e6d64c6b",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Your app - and the model - need to balance three concerns:  Accuracy, Latency, and CPU Usage.\n\n• :window: A shorter window size (selected at training time) may reduce run-time latency!  But if the window is shorter than the longest event you are trying to recognise, then accuracy may be reduced\n• :man-walking:A shorter stride can also reduce latency, but results in more frequent calls to the model, which can increase energy use.\n• :kangaroo: A longer stride reduces CPU usage, and allows other functions in your app such as drawing functions more time to work - which might help reduce any performance related bugs in your app, but will increase latency.",
          "ts": "1654633957.196739",
          "thread_ts": "1654633602.775309",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "100",
              "count": 2,
              "users": [
                "U03J4CASP0R",
                "U03HB5Z2MHV"
              ]
            },
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7xEl",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Your app - and the model - need to balance three concerns:  Accuracy, Latency, and CPU Usage.\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"emoji\",\"name\":\"window\"},{\"type\":\"text\",\"text\":\" A shorter window size (selected at training time) may reduce run-time latency!  But if the window is shorter than the longest event you are trying to recognise, then accuracy may be reduced\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"emoji\",\"name\":\"man-walking\"},{\"type\":\"text\",\"text\":\"A shorter stride can also reduce latency, but results in more frequent calls to the model, which can increase energy use.\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"emoji\",\"name\":\"kangaroo\"},{\"type\":\"text\",\"text\":\" A longer stride reduces CPU usage, and allows other functions in your app such as drawing functions more time to work - which might help reduce any performance related bugs in your app, but will increase latency.\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b4869f6b-b3b3-4f65-a723-702b72e62712",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "Also, try not to skip frames (unless you did the same for your training data), otherwise, the effective action speed captured in the window will change, and this may mess up the prediction result (accuracy)",
          "ts": "1654634137.740679",
          "thread_ts": "1654633602.775309",
          "edited": {
            "user": "U03HRNBHZEX",
            "ts": "1654634148.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bRg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also, try not to skip frames (unless you did the same for your training data), otherwise, the effective action speed captured in the window will change, and this may mess up the prediction result (accuracy)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ac60cdd2-9c82-4576-b911-5d5c2981d4dc",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "*Latency*: is the time between when the action happens in the real world to when the app detects it and tells the user.  This will be experienced as 'snappiness'.  For action classifications, latency is at least as long as the windows size, plus a bit.  It's an emergent parameter caused by other choices you make.\n\n*Stride*: is the time the model waits in between two detections.  This can be shorter OR longer than the window length.  It's a parameter used in the inference pipeline in your app.",
          "ts": "1654634197.713429",
          "thread_ts": "1654633602.775309",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HRNBHZEX",
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+Ja5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Latency",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ": is the time between when the action happens in the real world to when the app detects it and tells the user.  This will be experienced as 'snappiness'.  For action classifications, latency is at least as long as the windows size, plus a bit.  It's an emergent parameter caused by other choices you make.\n\n"
                    },
                    {
                      "type": "text",
                      "text": "Stride",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ": is the time the model waits in between two detections.  This can be shorter OR longer than the window length.  It's a parameter used in the inference pipeline in your app."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e02f6d80-e3b1-4006-aa94-c7b2236d32a7",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "Good summary :+1:. Latency can be slightly shorter than the window size, cause the model can predict the action when it sees the partial action. That’s why a shorter stride can help to refresh the results faster",
          "ts": "1654634309.431139",
          "thread_ts": "1654633602.775309",
          "edited": {
            "user": "U03HRNBHZEX",
            "ts": "1654634336.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nLkuq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Good summary "
                    },
                    {
                      "type": "emoji",
                      "name": "+1",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": ". Latency can be slightly shorter than the window size, cause the model can predict the action when it sees the partial action. That’s why a shorter stride can help to refresh the results faster"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0464bcfd-a0c3-424f-b253-0b6a67e36462",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "You may also be able to reduce energy, memory and CPU usage by setting the camera to a lower resolution.  If your app is very busy, making this change might improve app performance.\n\nThis change should not reduce model accuracy, but your user's view of the scene will not be so sharp.",
          "ts": "1654634429.610979",
          "thread_ts": "1654633602.775309",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "triangular_ruler",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PGd7y",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You may also be able to reduce energy, memory and CPU usage by setting the camera to a lower resolution.  If your app is very busy, making this change might improve app performance.\n\nThis change should not reduce model accuracy, but your user's view of the scene will not be so sharp."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "982fd0e2-93e8-456a-8d82-c723672e761c",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "Another inference speed factor might be your device. A newer device with neural engine will help. A fully charged device, and operating in good lighting condition, etc. can also help to reduce the latency (improve the Vision pose detection speed, and camera frame rate)",
          "ts": "1654634450.139799",
          "thread_ts": "1654633602.775309",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/HthJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Another inference speed factor might be your device. A newer device with neural engine will help. A fully charged device, and operating in good lighting condition, etc. can also help to reduce the latency (improve the Vision pose detection speed, and camera frame rate)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f771e7e6-3b55-4315-8765-955d7f636221",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "We have a great session on understanding the performance of apps with machine learning models this year!  Check it out:\n\n\u003chttps://developer.apple.com/videos/play/wwdc2022/10027/\u003e",
          "ts": "1654634536.743489",
          "thread_ts": "1654633602.775309",
          "attachments": [
            {
              "fallback": "Apple Developer: Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "text": "Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6520/6520_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "bar_chart",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            },
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U03JELQLESV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QiC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We have a great session on understanding the performance of apps with machine learning models this year!  Check it out:\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JV9KH3G9\u003e asked\n\u0026gt; How would you recommend to approach the classification of various fine-grained subclasses of the same class? Specifically talking about different types of something made of paper. \n\u0026gt; For example: \"a postcard with something written on it\", vs \"an empty postcard\" vs \"just some piece of paper\" vs \"another object\"?\n\u0026gt; \n\u0026gt; With classifier model we were able to obtain very accurate results to distinguish \"paper vs some other object\". However we couldn't get accurate enough results (I think ~60% accuracy) regarding the more fine-grained decisions: \"postcard vs some piece of paper\" and \"postcard with text vs empty postcard\". The mistakes were usually into false-positive side (identifying some piece of paper as a postcard in my example). \n\u0026gt; \n\u0026gt; So how would you setup the training samples for this sort of goal? Or are we looking in the wrong option, and should be considering some other method, or a combination of methods instead?\n\u0026gt; \n\u0026gt; Thanks.",
      "ts": "1654633857.386789",
      "thread_ts": "1654633857.386789",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654634510.370439",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "eJt",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JV9KH3G9"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"How would you recommend to approach the classification of various fine-grained subclasses of the same class? Specifically talking about different types of something made of paper. \\nFor example: \\\"a postcard with something written on it\\\", vs \\\"an empty postcard\\\" vs \\\"just some piece of paper\\\" vs \\\"another object\\\"?\\n\\nWith classifier model we were able to obtain very accurate results to distinguish \\\"paper vs some other object\\\". However we couldn't get accurate enough results (I think ~60% accuracy) regarding the more fine-grained decisions: \\\"postcard vs some piece of paper\\\" and \\\"postcard with text vs empty postcard\\\". The mistakes were usually into false-positive side (identifying some piece of paper as a postcard in my example). \\n\\nSo how would you setup the training samples for this sort of goal? Or are we looking in the wrong option, and should be considering some other method, or a combination of methods instead?\\n\\nThanks.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "61691683-a9e6-4a5d-8054-615e8c7e4900",
          "type": "message",
          "user": "U03HK3KNMDL",
          "text": "Are you using Create ML for this or a different training framework?",
          "ts": "1654633906.444759",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "pML",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Are you using Create ML for this or a different training framework?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "856e9415-f3cc-4b8c-bf88-a15d0ed0d439",
          "type": "message",
          "user": "U03JV9KH3G9",
          "text": "Currently had to settle on Vision (rectangle + text detection), but would prefer Create ML solution (if I can find a proper training for it - hence this question)",
          "ts": "1654634013.586039",
          "thread_ts": "1654633857.386789",
          "edited": {
            "user": "U03JV9KH3G9",
            "ts": "1654634031.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ShK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Currently had to settle on Vision (rectangle + text detection), but would prefer Create ML solution (if I can find a proper training for it - hence this question)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "124567a9-eeb2-40c4-9ae9-c58021dcf764",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Something you could try is doing a hierarchical approach where you first detect the overall class and then crop and do a more precise classification.",
          "ts": "1654634169.785419",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2k6cN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Something you could try is doing a hierarchical approach where you first detect the overall class and then crop and do a more precise classification."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9536b6e4-8cdf-476f-90eb-6ce6b790faa1",
          "type": "message",
          "user": "U03JV9KH3G9",
          "text": "Thanks. Do you think classification is feasible at all in this case? Or I should really concentrate on trying to get the text in such case, rather than trying to classify an object?",
          "ts": "1654634313.971199",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vxR2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks. Do you think classification is feasible at all in this case? Or I should really concentrate on trying to get the text in such case, rather than trying to classify an object?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e847ec75-8c01-42f6-bf91-00bac877fbb1",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "In addition to tuning the training, you can also tune the data you're training on like you mention. Adding more examples (width breadth to encompass you expect to see in practice) where you're getting the false positives might help iron out those edge cases. There's some data augmentation options in both the CreateML App and CreateML Components to that could help grow your sample size and add some diversity to it without collecting monumentally more data.",
          "ts": "1654634399.490939",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JV9KH3G9"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "m++",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In addition to tuning the training, you can also tune the data you're training on like you mention. Adding more examples (width breadth to encompass you expect to see in practice) where you're getting the false positives might help iron out those edge cases. There's some data augmentation options in both the CreateML App and CreateML Components to that could help grow your sample size and add some diversity to it without collecting monumentally more data."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "79f4b8c9-964e-423f-b7fe-fff7aad8e0e3",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Hard to say if it's feasible. But getting text definitely sounds better suited if your sub-classes are always text based.",
          "ts": "1654634442.593459",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JV9KH3G9"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bj5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hard to say if it's feasible. But getting text definitely sounds better suited if your sub-classes are always text based."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "794383e2-65d7-4a85-8f08-0e5ca33a5f04",
          "type": "message",
          "user": "U03JV9KH3G9",
          "text": "Thank you, guys",
          "ts": "1654634498.536719",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "A4XH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you, guys"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8a7795fb-da04-45a2-a97e-41150bb20c88",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "Thank you!",
          "ts": "1654634510.370439",
          "thread_ts": "1654633857.386789",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "93SM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVD5TM7G\u003e asked\n\u0026gt; Does an `MLModel` need to be adapted in any way to support predicting into buffers given with `MLPredictionOptions.outputBackings`?",
      "ts": "1654634062.499719",
      "thread_ts": "1654634062.499719",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654634136.820839",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "BkAQf",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVD5TM7G"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Does an `MLModel` need to be adapted in any way to support predicting into buffers given with `MLPredictionOptions.outputBackings`?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "ea7a6943-47b6-42c3-a3d3-23cb54c92eee",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "`MLModel` does *not* need to be adapted in any way to accept output backings. `MLPredictionOptions.outputBackings` can be used to provide either a `CVPixelBuffer` or `MLMultiArray` depending on the output feature value type. When a `CVPixelBuffer` is provided as output backing for prediction, please ensure that the base address is not locked for read / write. Please check out \u003chttps://developer.apple.com/videos/play/wwdc2022/10027/|Optimize your Core ML usage\u003e session tomorrow. If you have more questions, please bring your questions to the Q\u0026amp;A Core ML session at 9am on 6/8.",
          "ts": "1654634136.820839",
          "thread_ts": "1654634062.499719",
          "attachments": [
            {
              "fallback": "Apple Developer: Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "text": "Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6520/6520_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "pYA=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MLModel",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " does "
                    },
                    {
                      "type": "text",
                      "text": "not",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " need to be adapted in any way to accept output backings. "
                    },
                    {
                      "type": "text",
                      "text": "MLPredictionOptions.outputBackings",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " can be used to provide either a "
                    },
                    {
                      "type": "text",
                      "text": "CVPixelBuffer",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " or "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " depending on the output feature value type. When a "
                    },
                    {
                      "type": "text",
                      "text": "CVPixelBuffer",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is provided as output backing for prediction, please ensure that the base address is not locked for read / write. Please check out "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
                      "text": "Optimize your Core ML usage"
                    },
                    {
                      "type": "text",
                      "text": " session tomorrow. If you have more questions, please bring your questions to the Q\u0026A Core ML session at 9am on 6/8."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "b6787103-b3e4-41b0-938e-715060c91573",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*There are no bad questions!* Anyone completely new to Create ML, Core ML, or machine learning in general? Don't hesitate to ask _where do I begin?_ or _What's the best task for this?_ kind of questions. We're here to help!",
      "ts": "1654634419.706609",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "NHhff",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "There are no bad questions! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "Anyone completely new to Create ML, Core ML, or machine learning in general? Don't hesitate to ask "
                },
                {
                  "type": "text",
                  "text": "where do I begin? ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "or "
                },
                {
                  "type": "text",
                  "text": "What's the best task for this?",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": " kind of questions. We're here to help!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVD5TM7G\u003e asked\n\u0026gt; The object extraction from images (\"copy the dog out of the image into iMessage\") is very impressive! Can we get access to that feature via some API?",
      "ts": "1654634539.055589",
      "thread_ts": "1654634539.055589",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654654508.317739",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "+n2",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVD5TM7G"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"The object extraction from images (\\\"copy the dog out of the image into iMessage\\\") is very impressive! Can we get access to that feature via some API?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "49e975ba-d37b-4fa2-bc1a-52e7e6c92a96",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "Thanks! This might be a question better suited to the Vision API Q/A on Thursday from 2-3 PM PT.",
          "ts": "1654634574.131879",
          "thread_ts": "1654634539.055589",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZGKbS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks! This might be a question better suited to the Vision API Q/A on Thursday from 2-3 PM PT."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9a4bea14-874e-4686-9406-d6bf62bc438e",
          "type": "message",
          "user": "U03HMD6FP9V",
          "text": "I was able to get around this by copying it into Notes app and saving the image in notes \u003c@U03HVD5TM7G\u003e",
          "ts": "1654654508.317739",
          "thread_ts": "1654634539.055589",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "D5TXn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I was able to get around this by copying it into Notes app and saving the image in notes "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HVD5TM7G"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J9R7MVJ7\u003e asked\n\u0026gt; I‘m trying to get into Machine learning. What’s the best way to get to know all the methods for integrating CreateML into an app?",
      "ts": "1654634599.697119",
      "thread_ts": "1654634599.697119",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 18,
      "latest_reply": "1654635560.417379",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "9F5t2",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J9R7MVJ7"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I\\u2018m trying to get into Machine learning. What\\u2019s the best way to get to know all the methods for integrating CreateML into an app?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "3e39bbc7-4506-4a90-9734-35974b1c4217",
          "type": "message",
          "user": "U03J4CARJQ1",
          "text": "One way is to train a model in CreateML and then, add the resulting model into an app.",
          "ts": "1654634772.758929",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "f8pl",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "One way is to train a model in CreateML and then, add the resulting model into an app."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "40777511-ac80-42d0-b55c-333ebbe44b7e",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "That's exciting, glad you're interested! I can definitely recommend this year's (and previous years') sessions on CreateML. Each help go over part of CreateML and might be useful to you.",
          "ts": "1654634802.987929",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PJwc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That's exciting, glad you're interested! I can definitely recommend this year's (and previous years') sessions on CreateML. Each help go over part of CreateML and might be useful to you."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "290ac45f-9d31-4d4a-933e-c8daa7c3ddb9",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Yes, please watch _Get to know Create ML Components_ :grinning:",
          "ts": "1654634829.903709",
          "thread_ts": "1654634599.697119",
          "edited": {
            "user": "U03J4CASP0R",
            "ts": "1654634839.000000"
          },
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xtNF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, please watch "
                    },
                    {
                      "type": "text",
                      "text": "Get to know Create ML Components ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "emoji",
                      "name": "grinning",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "43412c3b-3298-4e41-95a7-09fa6fb6c779",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "\u003c@U03J9R7MVJ7\u003e I'm curious to hear what kinds of features you are wanting to build. Choosing the right task and understanding the data needs for training are a great place to start.",
          "ts": "1654634869.382429",
          "thread_ts": "1654634599.697119",
          "edited": {
            "user": "U03DJTBMHFF",
            "ts": "1654634885.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "n++X",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J9R7MVJ7"
                    },
                    {
                      "type": "text",
                      "text": " I'm curious to hear what kinds of features you are wanting to build. Choosing the right task and understanding the data needs for training are a great place to start."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9d0339b4-0833-470f-bd08-ef8d67d7a6d9",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "Also, are you looking at off-line training (then deploy) OR on-device training (training on iOS) as the potential use cases?",
          "ts": "1654634923.968659",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZHYH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also, are you looking at off-line training (then deploy) OR on-device training (training on iOS) as the potential use cases?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "497A3731-131B-4A63-98CB-1DFB62659F2B",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "I would like to build an app to predict stock price trends and classify expenses in a bookkeeping / tax management app",
          "ts": "1654634949.454179",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Nja",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "would"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "like"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "build"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "an"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "app"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "predict"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "stock"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "price"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "trends"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "classify"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "expenses"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "in"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "bookkeeping"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "/"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "tax"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "management"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "app"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9379f1fd-dd46-4c96-b484-b191679a181f",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "So for something like that, you might look into Tabular Regressors for predicting future prices based on past price data (and whatever other data you want to use), and Tabular Classifiers to classify expenses into categories you select.",
          "ts": "1654635048.258039",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "DDu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So for something like that, you might look into Tabular Regressors for predicting future prices based on past price data (and whatever other data you want to use), and Tabular Classifiers to classify expenses into categories you select."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2AFB55D8-A4F0-468E-B322-DF514D969DB0",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "And would you use offline training for that?",
          "ts": "1654635085.753189",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vZyD",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "And"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "would"
                    },
                    {
                      "type": "text",
                      "text": " you "
                    },
                    {
                      "type": "text",
                      "text": "use"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "offline"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "training"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "afa0ee08-13ab-4f3c-81f5-8e8dd8cb5bf9",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "yes, like \u003c@U03HB4V9Q5V\u003e suggested, this falls into offline-training (then deploy) use case",
          "ts": "1654635093.243609",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bTX0y",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "yes, like "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4V9Q5V"
                    },
                    {
                      "type": "text",
                      "text": " suggested, this falls into offline-training (then deploy) use case"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f8e6e213-a7ea-41f9-aa4a-d5e5d6a3280f",
          "type": "message",
          "user": "U03HK4YQZ0W",
          "text": "Here are some videos that can help you start from scratch. As others suggested there are a lot more things added to  Create ML in the last couple years.\n* Introducing Create ML - \u003chttps://developer.apple.com/videos/play/wwdc2018/703/\u003e\n* Introducing the Create ML App - \u003chttps://developer.apple.com/videos/play/wwdc2019/430/\u003e\n\nIn this video there’s a nice example project that you can follow through:\n* Build an Action Classifier with Create ML - \u003chttps://developer.apple.com/videos/play/wwdc2020/10043/\u003e",
          "ts": "1654635112.400339",
          "thread_ts": "1654634599.697119",
          "attachments": [
            {
              "fallback": "Apple Developer: Introducing Create ML - WWDC18 - Videos - Apple Developer",
              "id": 1,
              "title": "Introducing Create ML - WWDC18 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2018/703/",
              "text": "Create ML is a new framework designed to help you easily build machine learning models using Swift and Xcode. Designed for Simplicity and...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/42/2081/2081_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2018/703/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2018/703/",
              "blocks": null
            },
            {
              "fallback": "Apple Developer: Introducing the Create ML App - WWDC19 - Videos - Apple Developer",
              "id": 2,
              "title": "Introducing the Create ML App - WWDC19 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2019/430/",
              "text": "Bringing the power of Core ML to your app begins with one challenge. How do you create your model? The new Create ML app provides an...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/48/3189/3189_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2019/430/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2019/430/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PTi",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Here are some videos that can help you start from scratch. As others suggested there are a lot more things added to  Create ML in the last couple years.\n* Introducing Create ML - "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2018/703/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n* Introducing the Create ML App - "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2019/430/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\nIn this video there’s a nice example project that you can follow through:\n* Build an Action Classifier with Create ML - "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2020/10043/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "96e42aa2-0eec-4290-92f5-a8430e1729e2",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Can you tell me more about the \"classify expenses in a bookkeeping / tax management app\" case?  What does an example input look like?",
          "ts": "1654635141.178369",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "3xEo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Can you tell me more about the \"classify expenses in a bookkeeping / tax management app\" case?  What does an example input look like?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "AC2F1ED7-29FA-4236-9CC6-6E3557F6AA08",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "Thanks to all of you for your hard work developing all these amazing technologies and providing this support! :clap::clap:",
          "ts": "1654635195.796459",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 3,
              "users": [
                "U03J4CASP0R",
                "U03HRMABBDZ",
                "U03J4CARJQ1"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "khHKP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "all"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "you"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "your"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "hard"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "work"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "developing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "all"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "these"
                    },
                    {
                      "type": "text",
                      "text": " amazing technologies "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "providing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "this"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "support!"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "emoji",
                      "name": "clap",
                      "skin_tone": 0
                    },
                    {
                      "type": "emoji",
                      "name": "clap",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "015DCB09-2D46-4B06-8268-CCA793F05209",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "I am not that far yet but I was thinking of a combination of a text and other associated values (business/personal expense) and where the payment is going",
          "ts": "1654635306.459369",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6jP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "am"
                    },
                    {
                      "type": "text",
                      "text": " not "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "far"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "yet"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "but"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "thinking"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " combination "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "other"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "associated"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "values"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "(business/personal"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "expense)"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "where"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "payment"
                    },
                    {
                      "type": "text",
                      "text": " is "
                    },
                    {
                      "type": "text",
                      "text": "going"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8347dd95-5636-4ac4-a5ca-4e7ae3ad2569",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "is the text likely to be freeform: \"A box of chocolates from the candy store on main street\", or more like terms: \"lunchtime snack\"?",
          "ts": "1654635374.041959",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "gco0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "is the text likely to be freeform: \"A box of chocolates from the candy store on main street\", or more like terms: \"lunchtime snack\"?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "CEC0E863-01B6-4D36-9AA4-EFB3B8C3C877",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "More likely to be terms",
          "ts": "1654635412.706269",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Kvr",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "More"
                    },
                    {
                      "type": "text",
                      "text": " likely "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "be"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "terms"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9084f0f3-749b-4eaf-ab1b-6035286e5d67",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "You might be particularly interested in training on-device using the Create ML framework for this... given the historical data would be highly personal and continually changing (if I'm understanding the use case). The Tabular Regressor example that Alejandro shows in the _Get to know Create ML Components_ session is almost spot on with the problem you're trying to solve.",
          "ts": "1654635432.740489",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "M5Rw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You might be particularly interested in training on-device using the Create ML framework for this... given the historical data would be highly personal and continually changing (if I'm understanding the use case). The Tabular Regressor example that Alejandro shows in the "
                    },
                    {
                      "type": "text",
                      "text": "Get to know Create ML Components",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " session is almost spot on with the problem you're trying to solve."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e6a6739d-ca42-4007-9b64-92141fd77bef",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "If the textual input is more varied and similar to human language then you might use a *Text Classifier*.\n\nIf it's more like terms and you want to include additional context such as price (numbers), then the *Tabular Regressor* is more suitable.",
          "ts": "1654635465.690879",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            },
            {
              "name": "clap",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iMN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If the textual input is more varied and similar to human language then you might use a "
                    },
                    {
                      "type": "text",
                      "text": "Text Classifier",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ".\n\nIf it's more like terms and you want to include additional context such as price (numbers), then the "
                    },
                    {
                      "type": "text",
                      "text": "Tabular Regressor ",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "is more suitable."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "264BB010-833A-4FF0-9343-97AD4C111658",
          "type": "message",
          "user": "U03J9R7MVJ7",
          "text": "Thanks again for the helpful knowledge!",
          "ts": "1654635560.417379",
          "thread_ts": "1654634599.697119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kd/hA",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "again"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " helpful knowledge"
                    },
                    {
                      "type": "text",
                      "text": "!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JVEMS83B\u003e asked\n\u0026gt; Create ML Components looks amazing!! Have you experimented recreating popular architectures, like GANs and Autoencoders, with Create ML components?",
      "ts": "1654634948.209009",
      "thread_ts": "1654634948.209009",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 5,
      "latest_reply": "1654635872.485589",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "mt4",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JVEMS83B"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Create ML Components looks amazing!! Have you experimented recreating popular architectures, like GANs and Autoencoders, with Create ML components?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "fda031b7-614e-4a42-9ccd-67a9b5b3d604",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "I'm glad you like it. Components allow you to work with a lot of different architectures. Can't wait to see what everyone builds.",
          "ts": "1654635079.171179",
          "thread_ts": "1654634948.209009",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "VeYza",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I'm glad you like it. Components allow you to work with a lot of different architectures. Can't wait to see what everyone builds."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9c85b2e5-a06a-4b4b-9626-34d5bae30a38",
          "type": "message",
          "user": "U03JVEMS83B",
          "text": "Thank you for the answer! Would you consider Create ML Components as an alternative to Tensorflow or PyTorch for iOS Developers?",
          "ts": "1654635490.136259",
          "thread_ts": "1654634948.209009",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "eLR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you for the answer! Would you consider Create ML Components as an alternative to Tensorflow or PyTorch for iOS Developers?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "68ff9c98-69d4-411f-a458-66cf80fc388f",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "You are welcome. No, Create ML Components does not support building or training neural networks.",
          "ts": "1654635587.304579",
          "thread_ts": "1654634948.209009",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Y2bt2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You are welcome. No, Create ML Components does not support building or training neural networks."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fb46b38d-f84f-4669-98dd-5c786a144eb0",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "It's closer to something like scikit-learn",
          "ts": "1654635629.279479",
          "thread_ts": "1654634948.209009",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03K0FTKXLY",
                "U03JRPTG8BS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "WXpmj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It's closer to something like scikit-learn"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ad9450b9-8710-467d-a604-11487f3f9dd0",
          "type": "message",
          "user": "U03JVEMS83B",
          "text": "Great! Thank you for the clarification! For iOS developers who code for ML, it looks way more welcoming than scikit-learn with python. I’m very excited to try it out!",
          "ts": "1654635872.485589",
          "thread_ts": "1654634948.209009",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U03J4CASP0R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Btl8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Great! Thank you for the clarification! For iOS developers who code for ML, it looks way more welcoming than scikit-learn with python. I’m very excited to try it out!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HMCT187R\u003e asked\n\u0026gt; I utilize the drawing classifier from Turi Create in my app which has been working well. I previously tried an image classifier with Create ML but it took very significantly longer to train (about 55 hours vs 3.5 hours). Is Turi Create still the way to go until a drawing classifier gets added to Create ML? :D Bonus q: any Turi Create updates to share?",
      "ts": "1654634996.782019",
      "thread_ts": "1654634996.782019",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 5,
      "latest_reply": "1654635702.178289",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Iu=Fb",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HMCT187R"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I utilize the drawing classifier from Turi Create in my app which has been working well. I previously tried an image classifier with Create ML but it took very significantly longer to train (about 55 hours vs 3.5 hours). Is Turi Create still the way to go until a drawing classifier gets added to Create ML? :D Bonus q: any Turi Create updates to share?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a49333aa-7662-42ae-9f83-27526ff5f644",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Create ML does not have a drawing classifier template or task API. You may want to check out the new Create ML Components framework which will let you construct your own pipeline similar to the one used in Turi Create.",
          "ts": "1654635062.440719",
          "thread_ts": "1654634996.782019",
          "edited": {
            "user": "U03HRMWNP4J",
            "ts": "1654635208.000000"
          },
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HMCT187R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+mq2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Create ML does not have a drawing classifier template or task API. You may want to check out the new Create ML Components framework which will let you construct your own pipeline similar to the one used in Turi Create."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b2509719-8cdd-4e88-901a-8010bd758efd",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Turi Create is still a good option if its working for you",
          "ts": "1654635146.506949",
          "thread_ts": "1654634996.782019",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PPoy/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Turi Create is still a good option if its working for you"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ddcaaa1a-0b04-4a58-960e-99722972eb5a",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "However, Turi Create is no longer under active development",
          "ts": "1654635160.336259",
          "thread_ts": "1654634996.782019",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "broken_heart",
              "count": 1,
              "users": [
                "U03HMCT187R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "f1+5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "However, Turi Create is no longer under active development"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0204a2fd-a5d3-4357-821c-04f97539b91a",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Note: The updatable drawing classifier available on \u003chttps://developer.apple.com/machine-learning/models/\u003e has a pre-trained feature extractor for drawings as the first model in its pipeline.  You could use this sub-model as a custom feature extractor",
          "ts": "1654635565.725419",
          "thread_ts": "1654634996.782019",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U03HMCT187R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "t/y",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Note: The updatable drawing classifier available on "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/machine-learning/models/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " has a pre-trained feature extractor for drawings as the first model in its pipeline.  You could use this sub-model as a custom feature extractor"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3573a6e2-854a-4ad8-95e7-33feb66cdb86",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Please consider filing feature requests or feedback on drawing classifiers via \u003chttps://feedbackassistant.apple.com\u003e",
          "ts": "1654635702.178289",
          "thread_ts": "1654634996.782019",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HMCT187R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "wvp3W",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Please consider filing feature requests or feedback on drawing classifiers via "
                    },
                    {
                      "type": "link",
                      "url": "https://feedbackassistant.apple.com",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRQ9ADA4\u003e asked\n\u0026gt; Hello everybody! I’ll follow your suggestion: what are some ways to apply Create ML and CoreML to everyday tasks? What are the best tasks for CoreML models?",
      "ts": "1654635205.030519",
      "thread_ts": "1654635205.030519",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654636572.705409",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "/2I",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRQ9ADA4"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hello everybody! I\\u2019ll follow your suggestion: what are some ways to apply Create ML and CoreML to everyday tasks? What are the best tasks for CoreML models?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "cbe19237-ccef-40cc-8d75-eb1e8f6ee7f5",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "Really, the limit’s your imagination! But machine learning methods, generally, work well if you have:\n\n• a well-defined objective (in other words, a clear, unambiguous criterion which tells you whether a classification is the right one or how far an estimate is from its true value)\n• enough training data.\nThat’s true of a lot of different problems! Image, audio and text classification are all things which are applicable to a lot of real-world problems – what kind of app are you thinking of building?",
          "ts": "1654635348.693689",
          "thread_ts": "1654635205.030519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=+n",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Really, the limit’s your imagination! But machine learning methods, generally, work well if you have:\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"a well-defined objective (in other words, a clear, unambiguous criterion which tells you whether a classification is the right one or how far an estimate is from its true value)\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"enough training data.\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "\nThat’s true of a lot of different problems! Image, audio and text classification are all things which are applicable to a lot of real-world problems – what kind of app are you thinking of building?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e6a5de90-2bc1-409c-90b4-34816fe93365",
          "type": "message",
          "user": "U03JRQ9ADA4",
          "text": "Thank you a lot, Andrew! To be honest, I’m just exploring and learning new fields - and I’d like to understand as best as possible CoreML and Create ML to be able to incorporate them in future apps. Nevertheless, Image classification really intrigues me: I think I’ll dive deep into it :smile:",
          "ts": "1654635598.865759",
          "thread_ts": "1654635205.030519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "u15D",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you a lot, Andrew! To be honest, I’m just exploring and learning new fields - and I’d like to understand as best as possible CoreML and Create ML to be able to incorporate them in future apps. Nevertheless, Image classification really intrigues me: I think I’ll dive deep into it "
                    },
                    {
                      "type": "emoji",
                      "name": "smile",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7ecafde6-e475-488f-9c83-1f9a359cc084",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "Excellent! Good luck – the key way to take advantate of ML is identifying problems where the _goal_ can be clearly defined but the _method_ is tricky. It’s not worth training a ML model to say whether something is red or blue – the average pixel value tells you that. But determining the class of an object (or reading text out of an image, or…) – these are problems where you can define what a success is unambiguously, but coming up with heuristics for the problem is harder. That’s where you’ll get most “bang for the buck” from ML methods!",
          "ts": "1654635991.230059",
          "thread_ts": "1654635205.030519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "9lvk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Excellent! Good luck – the key way to take advantate of ML is identifying problems where the "
                    },
                    {
                      "type": "text",
                      "text": "goal",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " can be clearly defined but the "
                    },
                    {
                      "type": "text",
                      "text": "method",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is tricky. It’s not worth training a ML model to say whether something is red or blue – the average pixel value tells you that. But determining the class of an object (or reading text out of an image, or…) – these are problems where you can define what a success is unambiguously, but coming up with heuristics for the problem is harder. That’s where you’ll get most “bang for the buck” from ML methods!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2b0c564f-f17f-4088-b9b5-9fe16cff237a",
          "type": "message",
          "user": "U03JRQ9ADA4",
          "text": "Thank you again! That’s an incredible technology - and I’m sure it’ll help me solve challenging problems. Thank you for your precious advices! :smile:",
          "ts": "1654636572.705409",
          "thread_ts": "1654635205.030519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "LM6k7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you again! That’s an incredible technology - and I’m sure it’ll help me solve challenging problems. Thank you for your precious advices! "
                    },
                    {
                      "type": "emoji",
                      "name": "smile",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVD99L14\u003e asked\n\u0026gt; Can a GAN solution be made from ML Components?",
      "ts": "1654635487.172469",
      "thread_ts": "1654635487.172469",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 16,
      "latest_reply": "1654795004.973999",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "1j4Fs",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVD99L14"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Can a GAN solution be made from ML Components?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "d0cf576b-6dca-4a68-bb9f-1119f07ecd6f",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "If you already have a GAN model, then you can use it in Create ML Components using an adaptor.",
          "ts": "1654635610.819659",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZR9+J",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you already have a GAN model, then you can use it in Create ML Components using an adaptor."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5c7b3b9d-a2e4-4129-9f6c-f42aa469e90f",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Any Sample Code?",
          "ts": "1654635642.549059",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "74Pny",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Any Sample Code?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ce4287e6-bc61-49e3-832d-4edb5ed85bc3",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Looking for more of an “Unsharp mask”",
          "ts": "1654635701.830179",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "uoK+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Looking for more of an “Unsharp mask”"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e7aae0f9-7ab9-481f-8b15-ec7e557516fa",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Analog for intelligence",
          "ts": "1654635750.118219",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iSo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Analog for intelligence"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "71a47eef-4c0f-4473-91e8-b9e7e0cffce8",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Classification models to Generative ones",
          "ts": "1654635783.434329",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "9RD9q",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Classification models to Generative ones"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fe58bdaf-d9f8-4483-9470-44b6304b2729",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "like compute blur, create Sharp",
          "ts": "1654635833.676859",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xgKh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "like compute blur, create Sharp"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "bb24ab1a-821e-4704-8f1f-0ff09ea6d560",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "What is your use case in mind? Do you already have a mlmodel for this?",
          "ts": "1654635877.021739",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HF4Fi",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "What is your use case in mind? Do you already have a mlmodel for this?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "658c2ff6-910a-49f8-b5ee-7f179a3e68a2",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Building synthetic views of objects that are well represented in the training data.",
          "ts": "1654635922.501529",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "49Am+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Building synthetic views of objects that are well represented in the training data."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7edaca82-50b7-4b95-83b4-e8b02392dc7a",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Like having 1000 hummingbird pictures, being able to create N additional ones",
          "ts": "1654635951.622549",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fiX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Like having 1000 hummingbird pictures, being able to create N additional ones"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0ada0080-b50a-416d-a84c-dc248281c38b",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "that look real.  Where N is large",
          "ts": "1654635990.120499",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "GvW",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "that look real.  Where N is large"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4d505fc5-be0e-48b0-a121-f4c73b0c1e8f",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "Or creating my 3D avitar based on all the images of me in my photo library",
          "ts": "1654636289.157619",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "jVqVd",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Or creating my 3D avitar based on all the images of me in my photo library"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9155f2a0-ad70-4ab5-a55c-37ecf637fcf3",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "Thanks. CreateML Components does not offer out-of-box support for any *specific* GAN model, but you are able to make a transformer to use a GAN under the hood.",
          "ts": "1654636341.528989",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rwM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks. CreateML Components does not offer out-of-box support for any "
                    },
                    {
                      "type": "text",
                      "text": "specific",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " GAN model, but you are able to make a transformer to use a GAN under the hood."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f7474dc0-e591-46e6-b8b1-b9367fd9f648",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "for the GAN model you are interested, do you already have a corresponding mlmodel?",
          "ts": "1654636370.175899",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "my9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "for the GAN model you are interested, do you already have a corresponding mlmodel?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "562dad18-1c43-45e9-8345-11a1d51b5729",
          "type": "message",
          "user": "U03HVD99L14",
          "text": "I have a large training set of hummingbird images,…",
          "ts": "1654636436.267249",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Cmb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I have a large training set of hummingbird images,…"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7b67b1f3-e60a-4438-ba08-7b7a5039574a",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "I’d suggest starting from checking these models (where they are published) that you are interested, see if they can be converted to mlmodels, if you want to perform inference (deploy) on Apple hardware. CreateML or CreateML Components do not offer out-of-box support for such models, it meant to give *you* the flexibility to do so.",
          "ts": "1654637123.773369",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "VQRU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I’d suggest starting from checking these models (where they are published) that you are interested, see if they can be converted to mlmodels, if you want to perform inference (deploy) on Apple hardware. CreateML or CreateML Components do not offer out-of-box support for such models, it meant to give "
                    },
                    {
                      "type": "text",
                      "text": "you",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " the flexibility to do so."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "918a7799-5909-433e-a558-a534583c65ba",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Did you come to WWDC before, Glen?  I feel like maybe you showed me a whole lot of hummingbird photos in San Jose a few years ago. I love those little guys - we get them in the the Bay Area.",
          "ts": "1654795004.973999",
          "thread_ts": "1654635487.172469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YsK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Did you come to WWDC before, Glen?  I feel like maybe you showed me a whole lot of hummingbird photos in San Jose a few years ago. I love those little guys - we get them in the the Bay Area."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "98e066e8-c7f9-450a-8967-454b53defc0c",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*WOW!* So many great questions rolling in. We're going to keep this going for awhile longer. Keep them coming.",
      "ts": "1654635572.410779",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "heart",
          "count": 5,
          "users": [
            "U03J7UASVEU",
            "U03J4CARJQ1",
            "U03HK3KNMDL",
            "U03J9R7MVJ7",
            "U03JRP87THN"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "rhzB",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "WOW! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "So many great questions rolling in. We're going to keep this going for awhile longer. Keep them coming."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03K7TYHFHN\u003e asked\n\u0026gt; There was a great WWDC video back in 2018 titled Vision with Core ML. The example app used live video capture to feed an image through the model using scene stability. There is sample code out there for UIKit, but always wanted to try and re-make using SwiftUI as a starting point. Any tips or pointers on making an image recognition app with live video capture using SwiftUI as a starting point ?",
      "ts": "1654635850.229399",
      "thread_ts": "1654635850.229399",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 13,
      "latest_reply": "1654645616.686799",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "LTdAR",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03K7TYHFHN"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"There was a great WWDC video back in 2018 titled Vision with Core ML. The example app used live video capture to feed an image through the model using scene stability. There is sample code out there for UIKit, but always wanted to try and re-make using SwiftUI as a starting point. Any tips or pointers on making an image recognition app with live video capture using SwiftUI as a starting point ?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "41194bdb-1ddb-4ce4-824c-e8ca50ccb12a",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "You have a couple choices here.\n\nYou can use the techniques you already know, and use `UIViewRepresentable`  to import the UIViews into your app.  That will still work really well!",
          "ts": "1654635992.678819",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7gE2j",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You have a couple choices here.\n\nYou can use the techniques you already know, and use "
                    },
                    {
                      "type": "text",
                      "text": "UIViewRepresentable",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "  to import the UIViews into your app.  That will still work really well!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "00f02a4e-2f98-4cdd-ab18-f67fa65899a9",
          "type": "message",
          "user": "U03JBEQUEAJ",
          "text": "I've actually done exactly that using `UIViewControllerRepresentable` in the app I'm working on, and used a modified version of the VisionViewController which I think came from that video's sample code. It works perfectly... the entire app is SwiftUI except for the VisionViewController part.",
          "ts": "1654636306.245489",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03K7TYHFHN"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "acK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I've actually done exactly that using "
                    },
                    {
                      "type": "text",
                      "text": "UIViewControllerRepresentable",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " in the app I'm working on, and used a modified version of the VisionViewController which I think came from that video's sample code. It works perfectly... the entire app is SwiftUI except for the VisionViewController part."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cf92ea40-2153-4401-9591-585fe7f6627c",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Alternatively, you can display the camera feed by sending the image from each frame into a swiftUI `Image`.\n\nThe new `CreateMLComponents` feature actually includes a `VideoReader.readCamera` method which provides an asynchronous stream of image buffers, which is a great way to get started with this approach.\n\nAlternatively you can use your existing AVCaptureDevice logic and delegate methods to provide a series of images.\n\nYou can see an example of this approach in the rep counting demo app which will be available soon as part of the WWDC session on CreateML this year \u003chttps://developer.apple.com/wwdc22/110332\u003e",
          "ts": "1654636342.318609",
          "thread_ts": "1654635850.229399",
          "attachments": [
            {
              "fallback": "Apple Developer: What's new in Create ML - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "What's new in Create ML - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/wwdc22/110332",
              "text": "Discover the latest updates to Create ML. We'll share improvements to Create ML's evaluation tools that can help you understand how your...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6684/6684_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/wwdc22/110332",
              "original_url": "https://developer.apple.com/wwdc22/110332",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "W6Cs6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Alternatively, you can display the camera feed by sending the image from each frame into a swiftUI "
                    },
                    {
                      "type": "text",
                      "text": "Image",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ".\n\nThe new "
                    },
                    {
                      "type": "text",
                      "text": "CreateMLComponents",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " feature actually includes a "
                    },
                    {
                      "type": "text",
                      "text": "VideoReader.readCamera",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " method which provides an asynchronous stream of image buffers, which is a great way to get started with this approach.\n\nAlternatively you can use your existing AVCaptureDevice logic and delegate methods to provide a series of images.\n\nYou can see an example of this approach in the rep counting demo app which will be available soon as part of the WWDC session on CreateML this year "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/wwdc22/110332",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c0dc72b4-a36a-4776-a805-6827dcaf207f",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Recently I refactored some UIKit code into SwiftUI and found that the `ViewController` could be relatively easily transformed into an `ObservableObject` class, changing `@IBOulet` s into `@Published` properties.",
          "ts": "1654636535.984519",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "sdeK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Recently I refactored some UIKit code into SwiftUI and found that the "
                    },
                    {
                      "type": "text",
                      "text": "ViewController",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " could be relatively easily transformed into an "
                    },
                    {
                      "type": "text",
                      "text": "ObservableObject",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " class, changing "
                    },
                    {
                      "type": "text",
                      "text": "@IBOulet",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " s into "
                    },
                    {
                      "type": "text",
                      "text": "@Published",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " properties."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2FA225ED-2DF9-4A11-908A-E8392C01A520",
          "type": "message",
          "user": "U03K7TYHFHN",
          "text": "Thanks \u003c@U03HB4T0CA3\u003e  and \u003c@U03JBEQUEAJ\u003e . Lots of useful hints for me to try and figure out. I am excited to try working on this again ",
          "ts": "1654636581.380929",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03JLPLTZ4M"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "TlN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4T0CA3"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JBEQUEAJ"
                    },
                    {
                      "type": "text",
                      "text": " ."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Lots"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "useful"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "hints"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "me"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "try"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "figure"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "out."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "am"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "excited"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "try"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "working"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "this"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "again"
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ca2fa45e-c258-48b5-b96e-965bdb1aceef",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "I've been experimenting with using `AVCaptureVideoPreviewLayer` too, is that what you used \u003c@U03K7TYHFHN\u003e?",
          "ts": "1654636755.011539",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tYrxs",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I've been experimenting with using "
                    },
                    {
                      "type": "text",
                      "text": "AVCaptureVideoPreviewLayer",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " too, is that what you used "
                    },
                    {
                      "type": "user",
                      "user_id": "U03K7TYHFHN"
                    },
                    {
                      "type": "text",
                      "text": "?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8402dc34-b178-4028-9443-410032e0514b",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "\u003c@U03JBEQUEAJ\u003e if you have a link to the VisionViewController I would appreciate it, I may take a look one day and see how that's put together.",
          "ts": "1654636796.851939",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KGOx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JBEQUEAJ"
                    },
                    {
                      "type": "text",
                      "text": " if you have a link to the VisionViewController I would appreciate it, I may take a look one day and see how that's put together."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8512473D-DD56-43DF-B5C6-DAB149363FDE",
          "type": "message",
          "user": "U03K7TYHFHN",
          "text": "Yes was using the preview layer and showed a rectangle box on the screen when scene stability was reached.",
          "ts": "1654636998.753959",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "H4La",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "using"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "preview"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "layer"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "showed"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "rectangle"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "box"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "screen"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "when"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "scene"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "stability"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "reached."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9cd0c5f0-990e-48d8-a7c8-eb2ca952f42e",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Cool, using a preview layer is likely to be a very CPU / memory efficient way to do it.\n\nI don't yet have a clear picture of the efficiency of using SwiftUI views to run the preview directly in the way I proposed above, so while it can be a lot of fun to do it purely in SwiftUI it may not be the best choice just yet.",
          "ts": "1654637128.303879",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "74OX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Cool, using a preview layer is likely to be a very CPU / memory efficient way to do it.\n\nI don't yet have a clear picture of the efficiency of using SwiftUI views to run the preview directly in the way I proposed above, so while it can be a lot of fun to do it purely in SwiftUI it may not be the best choice just yet."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6f3fa97b-cb7a-4598-91d4-0c87ce7d7a39",
          "type": "message",
          "user": "U03JBEQUEAJ",
          "text": "\u003c@U03HB4T0CA3\u003e Here's a link to the original \u003chttps://developer.apple.com/documentation/vision/training_a_create_ml_model_to_classify_flowers|sample code that has the VisionViewController\u003e (click the download button from that page). Unfortunately I don't have a publicly available version that shows where I've used that with `UIViewControllerRepresentable`.",
          "ts": "1654637342.873139",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "UNrbQ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4T0CA3"
                    },
                    {
                      "type": "text",
                      "text": " Here's a link to the original "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/vision/training_a_create_ml_model_to_classify_flowers",
                      "text": "sample code that has the VisionViewController"
                    },
                    {
                      "type": "text",
                      "text": " (click the download button from that page). Unfortunately I don't have a publicly available version that shows where I've used that with "
                    },
                    {
                      "type": "text",
                      "text": "UIViewControllerRepresentable",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c23b099d-79e6-4cc8-88e5-ebfac14034da",
          "type": "message",
          "user": "U03JBEQUEAJ",
          "text": "In case it helps \u003c@U03K7TYHFHN\u003e though (maybe ignore where the Coordinator is a subclass of my ViewModel as that's probably very specific to my use case :sweat_smile:):",
          "ts": "1654637730.389629",
          "thread_ts": "1654635850.229399",
          "files": [
            {
              "id": "F03JRTA4PGC",
              "created": 1654637568,
              "timestamp": 1654637568,
              "name": "Screen Shot 2022-06-07 at 5.30.27 PM.png",
              "title": "Screen Shot 2022-06-07 at 5.30.27 PM.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03JBEQUEAJ",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 363885,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03JRTA4PGC-Screen Shot 2022-06-07 at 5.30.27 PM.png",
              "url_private_download": "C03H4A911EH/F03JRTA4PGC-Screen Shot 2022-06-07 at 5.30.27 PM.png",
              "original_h": 1304,
              "original_w": 1962,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 239,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 319,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_720.png",
              "thumb_720_w": 720,
              "thumb_720_h": 479,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_960.png",
              "thumb_960_w": 960,
              "thumb_960_h": 638,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JRTA4PGC-1c2687b91b/screen_shot_2022-06-07_at_5.30.27_pm_1024.png",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 681,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03JBEQUEAJ/F03JRTA4PGC/screen_shot_2022-06-07_at_5.30.27_pm.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03JRTA4PGC-c5055a8f0b",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7vWQ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In case it helps "
                    },
                    {
                      "type": "user",
                      "user_id": "U03K7TYHFHN"
                    },
                    {
                      "type": "text",
                      "text": " though (maybe ignore where the Coordinator is a subclass of my ViewModel as that's probably very specific to my use case "
                    },
                    {
                      "type": "emoji",
                      "name": "sweat_smile",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": "):"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "76B2BC3E-ADDB-4AB4-956E-0F00597524DA",
          "type": "message",
          "user": "U03K7TYHFHN",
          "text": "\u003c@U03JBEQUEAJ\u003eThanks so much for sharing. New to learning SwiftUI and trying to grasp how to use UIViewcontrollerRepresentable correctly. Helps to see how you approached it . I am going to keep trying as feeding in an image from live capture for image recognition is way more magical vs forcing a user to take a single snapshot . Thanks again ",
          "ts": "1654638389.612069",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "smiley",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "pG/3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JBEQUEAJ"
                    },
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "so"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "much"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "sharing."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "New"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "learning"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "SwiftUI"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "trying"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "grasp"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "how"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "use"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "UIViewcontrollerRepresentable"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "correctly."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Helps"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "see"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "how"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "you"
                    },
                    {
                      "type": "text",
                      "text": " approached "
                    },
                    {
                      "type": "text",
                      "text": "it"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "am"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "going"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "keep"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "trying"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "as"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "feeding"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "in"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "an"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "image"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "from"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "live"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "capture"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "image"
                    },
                    {
                      "type": "text",
                      "text": " recognition "
                    },
                    {
                      "type": "text",
                      "text": "is"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "way"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "more"
                    },
                    {
                      "type": "text",
                      "text": " magical "
                    },
                    {
                      "type": "text",
                      "text": "vs"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "forcing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "user"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "take"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "single"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "snapshot"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "again"
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "795e1350-9826-412e-8bc9-544dab1c6cc8",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "Great Q. - I’ve been grappling with the same question of converting to Swift UI",
          "ts": "1654645616.686799",
          "thread_ts": "1654635850.229399",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ggLj5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Great Q. - I’ve been grappling with the same question of converting to Swift UI"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J9R7MVJ7\u003e asked\n\u0026gt; When will CreateML support Neural Networks?",
      "ts": "1654635927.005219",
      "thread_ts": "1654635927.005219",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654636062.363239",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "x/r3",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J9R7MVJ7"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"When will CreateML support Neural Networks?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "904dc598-9c5c-4c12-b35a-f039974c597d",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Just to clarify, it does support neural networks. For instance `FullyConnectedNetworkClassifier`.",
          "ts": "1654635980.725859",
          "thread_ts": "1654635927.005219",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "w0z",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Just to clarify, it does support neural networks. For instance "
                    },
                    {
                      "type": "text",
                      "text": "FullyConnectedNetworkClassifier",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "85c64087-6417-4b43-a25e-b9b3b39fba41",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "But if you wanted to create a custom network you would need to use Metal or Accelerate.",
          "ts": "1654636062.363239",
          "thread_ts": "1654635927.005219",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1pXY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "But if you wanted to create a custom network you would need to use Metal or Accelerate."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J2255P42\u003e asked\n\u0026gt; We can separate people and background on photo, for example to create stickers, using VNGeneratePersonSegmentationRequest.\n\u0026gt; \n\u0026gt; What about animals, objects and etc like you did it in iOS 16? I mean feature that I can longpress at any object on photo to copy/paste it. Do we have ready API for that?",
      "ts": "1654635936.727689",
      "thread_ts": "1654635936.727689",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654636177.367129",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "H4z",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J2255P42"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"We can separate people and background on photo, for example to create stickers, using VNGeneratePersonSegmentationRequest.\\n\\nWhat about animals, objects and etc like you did it in iOS 16? I mean feature that I can longpress at any object on photo to copy\\/paste it. Do we have ready API for that?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "907b50dc-d3ec-469f-9602-9cd86909b039",
          "type": "message",
          "user": "U03J20RJQ2X",
          "text": "Interested in this as well!",
          "ts": "1654636033.181029",
          "thread_ts": "1654635936.727689",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0m1tp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Interested in this as well!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "71ceed9e-992e-46be-a0f1-962e321c4180",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Yes, you can use VNGeneratePersonSegmentationRequest for people. There currently is no equivalent for animals or other objects via the Vision APIs.\n\nPlease consider filing a feature request or feedback via \u003chttps://feedbackassistant.apple.com\u003e or bringing more questions to the Q\u0026amp;A Vision digital lounge from 2-3pm on Thursday",
          "ts": "1654636177.367129",
          "thread_ts": "1654635936.727689",
          "edited": {
            "user": "U03HRMWNP4J",
            "ts": "1654636244.000000"
          },
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 3,
              "users": [
                "U03J2255P42",
                "U03J9R7MVJ7",
                "U03K2392NFJ"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "y8p",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, you can use VNGeneratePersonSegmentationRequest for people. There currently is no equivalent for animals or other objects via the Vision APIs.\n\nPlease consider filing a feature request or feedback via "
                    },
                    {
                      "type": "link",
                      "url": "https://feedbackassistant.apple.com",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " or bringing more questions to the Q\u0026A Vision digital lounge from 2-3pm on Thursday"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JBEQUEAJ\u003e asked\n\u0026gt; I've created a small image classification model using Create ML with around 350 image labels. However, for the iOS app I'm making that could scale to over 100,000 labels (and likely much more) - each with over a hundred images for training/testing. Is there any way to scale to that level using Create ML? I've started teaching myself TensorFlow and researching various cloud services like Google Colab for the training because I think I'll need to go that route... and then convert that to Core ML. I'd appreciate any thoughts / recommendations.",
      "ts": "1654636129.822469",
      "thread_ts": "1654636129.822469",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654731326.082049",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "AeY",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JBEQUEAJ"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I've created a small image classification model using Create ML with around 350 image labels. However, for the iOS app I'm making that could scale to over 100,000 labels (and likely much more) - each with over a hundred images for training\\/testing. Is there any way to scale to that level using Create ML? I've started teaching myself TensorFlow and researching various cloud services like Google Colab for the training because I think I'll need to go that route... and then convert that to Core ML. I'd appreciate any thoughts \\/ recommendations.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "5922847d-9e14-4e5c-a21b-ef7dfbd4c78e",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "Wow, 100000 classes! That’s a hard problem – in fact, it’s something which would be at the research cutting edge.\n\nIs there any structure to your classes? That might be something you could exploit.",
          "ts": "1654636180.522509",
          "thread_ts": "1654636129.822469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Rxkq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Wow, 100000 classes! That’s a hard problem – in fact, it’s something which would be at the research cutting edge.\n\nIs there any structure to your classes? That might be something you could exploit."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1f5f8d99-af29-49af-af57-96e1bdbab52a",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "It’s definitely worth having a look at the literature here — these tend to be called “extreme classification tasks”",
          "ts": "1654636303.537299",
          "thread_ts": "1654636129.822469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1ntpf",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It’s definitely worth having a look at the literature here — these tend to be called “extreme classification tasks”"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7c2d1cff-99ac-407b-877a-e93c2d181737",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "\u003chttp://manikvarma.org/pubs/bengio19.pdf\u003e",
          "ts": "1654636336.417169",
          "thread_ts": "1654636129.822469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QwgPi",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "http://manikvarma.org/pubs/bengio19.pdf",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fc50a445-d22f-47e2-86ab-685c051f746c",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "This is a review which covers some of the problems in this area. However, if you have a natural hierarchy to your labels, you might consider having a hierarchy of classifiers. Let’s say we’re talking about animals; you might have a classifier from “animal” to “bird”, “mammal”, “reptile”, etc etc etc, then from “bird” to bird species",
          "ts": "1654636419.074749",
          "thread_ts": "1654636129.822469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "a=Nh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This is a review which covers some of the problems in this area. However, if you have a natural hierarchy to your labels, you might consider having a hierarchy of classifiers. Let’s say we’re talking about animals; you might have a classifier from “animal” to “bird”, “mammal”, “reptile”, etc etc etc, then from “bird” to bird species"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5af2398a-0d48-4842-b0df-c03d5e7251c6",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "That way each classifier is only predicting among, say, 1000 classes – which is a more tractable problem",
          "ts": "1654636443.646199",
          "thread_ts": "1654636129.822469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "49wYc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That way each classifier is only predicting among, say, 1000 classes – which is a more tractable problem"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "20cd5020-6383-4b9c-b9f6-442d950eecf2",
          "type": "message",
          "user": "U03JBEQUEAJ",
          "text": "Unfortunately there is no hierarchy. Each image is completely different but easily recognized (usually). This isn't it exactly... but it would be similar to recognizing famous paintings, but where there are hundreds of thousands of them. I appreciate the feedback and will look over that \"extreme classification\" info. Thanks :smile:",
          "ts": "1654636684.520929",
          "thread_ts": "1654636129.822469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2rFx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Unfortunately there is no hierarchy. Each image is completely different but easily recognized (usually). This isn't it exactly... but it would be similar to recognizing famous paintings, but where there are hundreds of thousands of them. I appreciate the feedback and will look over that \"extreme classification\" info. Thanks "
                    },
                    {
                      "type": "emoji",
                      "name": "smile",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "B365B6FA-41D8-4D89-A48A-0567F2D922FF",
          "type": "message",
          "user": "U03HY66772A",
          "text": "You may want to check out algorithms behind similar image search, where you calculate an embedding for an image, then find N nearest embedding from the known images, and derive your class/object from them.",
          "ts": "1654731326.082049",
          "thread_ts": "1654636129.822469",
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654731343.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "E=8x2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You may want to check out algorithms behind similar image search, where you calculate an embedding for an image, then find N nearest embedding from the known images, and derive your class/object from them."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J2004PGT\u003e asked\n\u0026gt; Is there the ability in CreateML to make text generation models, GPT-3 type applications? Haven’t seen it but wanted to double check",
      "ts": "1654636815.011809",
      "thread_ts": "1654636815.011809",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654637483.960749",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "tDgm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J2004PGT"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there the ability in CreateML to make text generation models, GPT-3 type applications? Haven\\u2019t seen it but wanted to double check\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a0310e51-33af-4545-990d-90d86cef2dbe",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "No. I am interested in hearing your use case.",
          "ts": "1654636848.780249",
          "thread_ts": "1654636815.011809",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "JftUo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "No. I am interested in hearing your use case."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "00F6994C-D8CD-4C84-A08C-2B009B6C97CA",
          "type": "message",
          "user": "U03J2004PGT",
          "text": "The things I'm interested in are things like, generating writing prompts, autocompleting text in the style of famous writers (I believe there was a demo of converting a model to CoreML for this), lightweight chatbots, and so forth.",
          "ts": "1654637012.193859",
          "thread_ts": "1654636815.011809",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mXW1l",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The"
                    },
                    {
                      "type": "text",
                      "text": " things I'm "
                    },
                    {
                      "type": "text",
                      "text": "interested"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "in"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "are"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "things"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "like,"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "generating"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "writing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "prompts,"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "autocompleting"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "in"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "style"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " famous "
                    },
                    {
                      "type": "text",
                      "text": "writers"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "("
                    },
                    {
                      "type": "text",
                      "text": "I believe "
                    },
                    {
                      "type": "text",
                      "text": "there"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "demo"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "converting"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "model"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "CoreML"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "this),"
                    },
                    {
                      "type": "text",
                      "text": " lightweight "
                    },
                    {
                      "type": "text",
                      "text": "chatbots,"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "so"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "forth."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "856fe11c-fe8f-4ef4-87e9-dc61cfec4a58",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "Great use cases!  CreateML or CreateMLComponents meant to allow you to create custom ML models fitted to your training data. If you want to use such model, it makes sense to get that converted-to-CoreML model to try it out.",
          "ts": "1654637483.960749",
          "thread_ts": "1654636815.011809",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Tbu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Great use cases!  CreateML or CreateMLComponents meant to allow you to create custom ML models fitted to your training data. If you want to use such model, it makes sense to get that converted-to-CoreML model to try it out."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "4d7d2434-2256-46ce-800d-bdfe452a37eb",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "I love all the great questions, please add any more as we will be wrapping up this lounge in a bit. Thanks everyone!",
      "ts": "1654636973.389959",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "A+T",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "I love all the great questions, please add any more as we will be wrapping up this lounge in a bit. Thanks everyone!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J20RJQ2X\u003e asked\n\u0026gt; I'm new to ML. I would like to implement some sort of color matching with two photos (i.e. when superimposing a person on a different background, adjusting color, contrast, etc. to match the background better). Is something like that suited for CoreML (and if so, do you have any suggestions on how to approach that?), or would a simple algorithm be a better solution for those kinds of tasks?",
      "ts": "1654637032.663479",
      "thread_ts": "1654637032.663479",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654637080.513019",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "nTM3O",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20RJQ2X"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I'm new to ML. I would like to implement some sort of color matching with two photos (i.e. when superimposing a person on a different background, adjusting color, contrast, etc. to match the background better). Is something like that suited for CoreML (and if so, do you have any suggestions on how to approach that?), or would a simple algorithm be a better solution for those kinds of tasks?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "bf9f01b5-6f36-40c7-9def-1a8dc386d911",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Doesn't sound like something you can do with Core ML or Create ML. Try the Vision API Q/A on Thursday from 2-3 PM PT.",
          "ts": "1654637080.513019",
          "thread_ts": "1654637032.663479",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J20RJQ2X"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "oftbo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Doesn't sound like something you can do with Core ML or Create ML. Try the Vision API Q/A on Thursday from 2-3 PM PT."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "46690d9e-1f52-4923-b690-10a583572d4f",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*Thank you, everyone!* It was great to see all the fantastic questions and lively conversation over the past 1.5 hours. We're going to close down this Q\u0026amp;A session now to get ourselves ready for the next activity… _*Meet the Presenter: Get to know Create ML Components*_ where Alejandro and David will be joining you all to take your questions regarding this incredibly flexible new framework. And don't fret, we have another general Q\u0026amp;A session for Core ML tomorrow at 9 AM.",
      "ts": "1654637338.342719",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "TaE9",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Thank you, everyone! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "It was great to see all the fantastic questions and lively conversation over the past 1.5 hours. We're going to close down this Q\u0026A session now to get ourselves ready for the next activity… "
                },
                {
                  "type": "text",
                  "text": "Meet the Presenter: Get to know Create ML Components",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": " ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "where Alejandro and David will be joining you all to take your questions regarding this incredibly flexible new framework. And don't fret, we have another general Q\u0026A session for Core ML tomorrow at 9 AM."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "e4aa67cb-db42-48be-a7a4-af1687eb73c3",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*It's almost showtime!* We'll be hitting play together for the session _*Get to know Create ML Components*_ in about 5 minutes. You'll be joined here by Alejandro, the presenter of this session, as well as David who is presenting a related session tomorrow. We're eager to talk all of your questions and jump into some lively conversation around this great new framework. Get queue up to watch the video with us: \u003chttps://developer.apple.com/videos/play/wwdc2022/10019/\u003e",
      "ts": "1654639350.478159",
      "attachments": [
        {
          "fallback": "Apple Developer: Get to know Create ML Components - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Get to know Create ML Components - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022/10019/",
          "text": "Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6512/6512_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
          "blocks": null
        }
      ],
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "eyes",
          "count": 4,
          "users": [
            "U03J7UASVEU",
            "U03JFGMTU8G",
            "U03KD24FB40",
            "U03J4CASP0R"
          ]
        },
        {
          "name": "heart_eyes",
          "count": 1,
          "users": [
            "U03HZ4BPHPX"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "c/V",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "It's almost showtime! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "We'll be hitting play together for the session "
                },
                {
                  "type": "text",
                  "text": "Get to know Create ML Components",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": " ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "in about 5 minutes. You'll be joined here by Alejandro, the presenter of this session, as well as David who is presenting a related session tomorrow. We're eager to talk all of your questions and jump into some lively conversation around this great new framework. Get queue up to watch the video with us: "
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
                  "text": ""
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "d0350e4b-6715-4ad1-ad83-88ae75ff3b1e",
      "type": "message",
      "user": "U03J4CASP0R",
      "text": "Hello :wave: Enjoy and ask away!",
      "ts": "1654639541.093739",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "yLV",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Hello "
                },
                {
                  "type": "emoji",
                  "name": "wave",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " Enjoy and ask away!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "ce04e717-19c7-422d-8cc5-a5edb2cb4800",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*Everyone ready?* The workflows are open again for you to ask questions while we watch this great session together. Hit the link above and hit play in *3…2…1…GO!*",
      "ts": "1654639614.081459",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "IW6",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Everyone ready?",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " The workflows are open again for you to ask questions while we watch this great session together. Hit the link above and hit play in "
                },
                {
                  "type": "text",
                  "text": "3…2…1…GO!",
                  "style": {
                    "bold": true
                  }
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "7e23404c-c119-455e-9aef-8212ee135eb9",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "Under the hood :thread:",
      "ts": "1654639774.221899",
      "thread_ts": "1654639774.221899",
      "reply_count": 1,
      "latest_reply": "1654639820.770049",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "0nTh6",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Under the hood "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "9558195d-8102-4ef5-a6d1-4621ebff0b4f",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "What kind of ideas come to mind as you dive under the hood to discover the composition of Create ML tasks? What possibilities would you like to explore?",
          "ts": "1654639820.770049",
          "thread_ts": "1654639774.221899",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mwS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "What kind of ideas come to mind as you dive under the hood to discover the composition of Create ML tasks? What possibilities would you like to explore?"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "eaf95d8c-925f-46b0-9475-06a54f1f2499",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "Transformers vs. Estimators :thread:",
      "ts": "1654639891.432769",
      "thread_ts": "1654639891.432769",
      "reply_count": 4,
      "latest_reply": "1654640966.154179",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "R474",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Transformers vs. Estimators "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "1a9e9e0f-9079-4e6c-a8d0-d13b34cfe707",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "Is the difference clear? Any questions around this important concept of the framework?",
          "ts": "1654639913.875149",
          "thread_ts": "1654639891.432769",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zSG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Is the difference clear? Any questions around this important concept of the framework?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "212DB40F-C863-4A6A-965B-CAE59E279B1E",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "Is there a list of transformers and estimators available for use?",
          "ts": "1654640443.257099",
          "thread_ts": "1654639891.432769",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0XyFg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Is there a list of transformers and estimators available for use?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6a6f1d3f-1878-4f66-b708-2a6399ec03c5",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "We have many transformers and estimators. A good place to get the list is the developer documentation.",
          "ts": "1654640616.820349",
          "thread_ts": "1654639891.432769",
          "edited": {
            "user": "U03J4CASP0R",
            "ts": "1654640689.000000"
          },
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QLizF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We have many transformers and estimators. A good place to get the list is the developer documentation."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b2bef536-bafe-4706-8c10-8ae2475feab1",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "\u003chttps://developer.apple.com/documentation/createmlcomponents\u003e",
          "ts": "1654640966.154179",
          "thread_ts": "1654639891.432769",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Dog",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "23e6d9f2-fa1d-4c54-a440-973a51659539",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "Image Regressor: Coding demo :thread:",
      "ts": "1654640360.774349",
      "thread_ts": "1654640360.774349",
      "reply_count": 4,
      "latest_reply": "1654645637.490809",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "+PVY",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Image Regressor: Coding demo "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "c883d95a-d507-4b31-b2e3-6774f2376c13",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "What did you think about the coding demo? Lots of great bits in there!",
          "ts": "1654640386.535979",
          "thread_ts": "1654640360.774349",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "TgF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "What did you think about the coding demo? Lots of great bits in there!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4c053cb1-9898-4c24-857a-ebb783d1dd91",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "The swift api looks interesting. You can use any iOS graphics api for augmentation, right?",
          "ts": "1654644427.539449",
          "thread_ts": "1654640360.774349",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5f9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The swift api looks interesting. You can use any iOS graphics api for augmentation, right?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7dd01c95-2171-4d5c-9f69-8e3d820657b5",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Compared to trying to do custom augmentation in pytorch/opengl it would be great to be able write metal shaders",
          "ts": "1654644633.635009",
          "thread_ts": "1654640360.774349",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "K58i",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Compared to trying to do custom augmentation in pytorch/opengl it would be great to be able write metal shaders"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7867b105-f3dc-46da-beea-3a77bbe19537",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Yes, you are free to use any API. The only requirement is that it produces a CIImage.",
          "ts": "1654645637.490809",
          "thread_ts": "1654640360.774349",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U03JZNY81L0"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "eU=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, you are free to use any API. The only requirement is that it produces a CIImage."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "7efa5e7a-d470-4bff-8ff8-1cbb9aadf153",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "A frequently asked question: *Can someone clarify the difference between a classifier and regressor again for me?* Let's chat about it. :thread:",
      "ts": "1654640563.041639",
      "thread_ts": "1654640563.041639",
      "edited": {
        "user": "U03DJTBMHFF",
        "ts": "1654640591.000000"
      },
      "reply_count": 5,
      "latest_reply": "1654738047.191139",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "D+Jv",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "A frequently asked question: "
                },
                {
                  "type": "text",
                  "text": "Can someone clarify the difference between a classifier and regressor again for me?",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " Let's chat about it. "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "59dca5b0-5415-4e37-888f-bad8dddc861e",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "A classifier is used to predict a discrete categorical value. Think of it as predicting an enum.\n\nA regressor is used to predict a real value. Think of it as predicting a float or double.",
          "ts": "1654640674.392809",
          "thread_ts": "1654640563.041639",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 3,
              "users": [
                "U03J4CASP0R",
                "U03HX9ZTNQ7",
                "U03JN7USRDY"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xWZlK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A classifier is used to predict a discrete categorical value. Think of it as predicting an enum.\n\nA regressor is used to predict a real value. Think of it as predicting a float or double."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "58ae7463-a16b-4728-b98e-8ac25dbda14e",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "In the first demo a regressor was used to predict a ripeness *value*.",
          "ts": "1654641180.482459",
          "thread_ts": "1654640563.041639",
          "edited": {
            "user": "U03HRMWNP4J",
            "ts": "1654641418.000000"
          },
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "t9WO",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In the first demo a regressor was used to predict a ripeness "
                    },
                    {
                      "type": "text",
                      "text": "value",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b0cff9a8-eb22-4f57-a008-bb865687ab1d",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "A classification approach to the same problem would require you to define categories like  “green”, “not ripe”, “ripe”, “over ripe”.  This is an option as well,  but you would not be able to compare the ripeness of two examples that got classified into the same category.",
          "ts": "1654641331.451419",
          "thread_ts": "1654640563.041639",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5Nlt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A classification approach to the same problem would require you to define categories like  “green”, “not ripe”, “ripe”, “over ripe”.  This is an option as well,  but you would not be able to compare the ripeness of two examples that got classified into the same category."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "FC73CCD9-F38A-4236-A1AA-6636A7BE6A3A",
          "type": "message",
          "user": "U03J9GM2ESE",
          "text": "How about run time efficiency between the above mentioned classifier and regressor?",
          "ts": "1654736460.901719",
          "thread_ts": "1654640563.041639",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "WwZ7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "How"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "about"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "run"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "time"
                    },
                    {
                      "type": "text",
                      "text": " efficiency between "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "above"
                    },
                    {
                      "type": "text",
                      "text": " mentioned "
                    },
                    {
                      "type": "text",
                      "text": "classifier"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "regressor?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ed148a3b-e1c2-489d-a7b9-0d4331b4c72b",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "They should have similar prediction compute time if you are using a common feature extractor. For image models that feature extraction step will likely dominate inference compute.  \u003chttps://developer.apple.com/documentation/createmlcomponents/logisticregressionclassifier|LogisticRegressionClassifier\u003e and \u003chttps://developer.apple.com/documentation/createmlcomponents/linearregressor|LinearRegressor\u003e will both be doing a similar size matrix multiplication behind the scenes, particularly if you restrict yourself to a few classes. As the number of classes increase the classifier will become slower.",
          "ts": "1654738047.191139",
          "thread_ts": "1654640563.041639",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1Dq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "They should have similar prediction compute time if you are using a common feature extractor. For image models that feature extraction step will likely dominate inference compute.  "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/logisticregressionclassifier",
                      "text": "LogisticRegressionClassifier"
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/linearregressor",
                      "text": "LinearRegressor"
                    },
                    {
                      "type": "text",
                      "text": " will both be doing a similar size matrix multiplication behind the scenes, particularly if you restrict yourself to a few classes. As the number of classes increase the classifier will become slower."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "ba54cfd0-abae-4210-bab4-400b5cb42a46",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "Tabular Data :thread:",
      "ts": "1654640640.241949",
      "thread_ts": "1654640640.241949",
      "reply_count": 3,
      "latest_reply": "1654640839.102169",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "fz0",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Tabular Data "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "7457ce15-ec57-4ff2-829e-0aab9e3ac579",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "Who's familiar with the TabularData framework already? It's really cool!!",
          "ts": "1654640670.311639",
          "thread_ts": "1654640640.241949",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HMDXJD8X"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rq1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Who's familiar with the TabularData framework already? It's really cool!!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5d286136-a203-4246-aa5c-72345ae29fa0",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "You can find the docs here: \u003chttps://developer.apple.com/documentation/tabulardata\u003e",
          "ts": "1654640766.754049",
          "thread_ts": "1654640640.241949",
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "90YIH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can find the docs here: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/tabulardata",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "891e1452-4563-4e15-882f-f091de45d4ed",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "And the tech talk: \u003chttps://developer.apple.com/videos/play/tech-talks/10100|Explore and manipulate data in Swift with TabularData - Tech Talks - Videos - Apple Developer\u003e",
          "ts": "1654640839.102169",
          "thread_ts": "1654640640.241949",
          "attachments": [
            {
              "fallback": "Apple Developer: Explore and manipulate data in Swift with TabularData - Tech Talks - Videos - Apple Developer",
              "id": 1,
              "title": "Explore and manipulate data in Swift with TabularData - Tech Talks - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/tech-talks/10100",
              "text": "Discover how you can use the TabularData framework to load, explore, and manipulate unstructured data in Swift — whether you need to...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/8/4996/4996_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/tech-talks/10100",
              "original_url": "https://developer.apple.com/videos/play/tech-talks/10100",
              "blocks": null
            }
          ],
          "parent_user_id": "U03DJTBMHFF",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Pdch9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "And the tech talk: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/tech-talks/10100",
                      "text": "Explore and manipulate data in Swift with TabularData - Tech Talks - Videos - Apple Developer"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "d10b04bc-2524-446d-ba71-653e5fa02443",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "\u003c!here\u003e *What do you think?* Great session if you ask me and such a powerful framework. Alejandro is now here along with the team to answer any and all questions you may have.",
      "ts": "1654641189.514639",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 8,
          "users": [
            "U03K3TPTFPA",
            "U03J4CASP0R",
            "U03J7UASVEU",
            "U03HK3N00TG",
            "U03HRMAAYM9",
            "U03KD24FB40",
            "U03HK4YQZ0W",
            "U03HVCSF7B8"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "vT=3a",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " "
                },
                {
                  "type": "text",
                  "text": "What do you think? ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "Great session if you ask me and such a powerful framework. Alejandro is now here along with the team to answer any and all questions you may have."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HUQAAWEB\u003e asked\n\u0026gt; Hi, so trying to do on device learning. If I do\n\u0026gt; \n\u0026gt; \n\u0026gt; \n\u0026gt; Import CoreML\n\u0026gt; …\n\u0026gt; Let model = try MLLinearRegressor(…)\n\u0026gt; \n\u0026gt; XCode alerts something like  “can’t find MLLinear regressor in scope”\n\u0026gt; \n\u0026gt; How can I fix this, thanks",
      "ts": "1654642470.504599",
      "thread_ts": "1654642470.504599",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 16,
      "latest_reply": "1654704904.652889",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "T9U2m",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HUQAAWEB"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi, so trying to do on device learning. If I do\\n\\n\\n\\nImport CoreML\\n\\u2026\\nLet model = try MLLinearRegressor(\\u2026)\\n\\nXCode alerts something like \\u00a0\\u201ccan\\u2019t find MLLinear regressor in scope\\u201d\\n\\nHow can I fix this, thanks\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "99baad49-d7a3-4955-b7b8-e1690aa171c8",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "`MLLinearRegressor` is a symbol from CreateML, do `Import CreateML` instead up there",
          "ts": "1654642508.876049",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "oRXGx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MLLinearRegressor",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is a symbol from CreateML, do "
                    },
                    {
                      "type": "text",
                      "text": "Import CreateML",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " instead up there"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "B77F6814-DCBE-4E8C-B40F-1E503E0A7EB5",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "I'm pretty sure I tried that too… but okay thanks",
          "ts": "1654642552.570109",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mgyLK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I'm pretty "
                    },
                    {
                      "type": "text",
                      "text": "sure"
                    },
                    {
                      "type": "text",
                      "text": " I "
                    },
                    {
                      "type": "text",
                      "text": "tried"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "too…"
                    },
                    {
                      "type": "text",
                      "text": " but okay "
                    },
                    {
                      "type": "text",
                      "text": "thanks"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "05cbcefe-e4a7-40ff-919a-a3e5a736d307",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "sure, let me if this works, quickest way is to try that in an Xcode Playground",
          "ts": "1654642609.741159",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4s=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "sure, let me if this works, quickest way is to try that in an Xcode Playground"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1275B625-FD49-4370-9714-C690BDE85ADE",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Away from computer. Will try soon. Hopefully. Thinking of questions",
          "ts": "1654642688.439339",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "BD5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Away"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "from"
                    },
                    {
                      "type": "text",
                      "text": " computer"
                    },
                    {
                      "type": "text",
                      "text": ". Will"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "try"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "soon. Hopefully. "
                    },
                    {
                      "type": "text",
                      "text": "Thinking "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "questions"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "877f684e-1511-47a8-bcf0-cff8e8b53b79",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Import CreateML\n“No such module createML”\nTried making new project, same thing\nTried in playground, auto completed this time, then again “No such module createML”\nMy Computer is up to date, with Xcode Version 13.4.1 (13F100)",
          "ts": "1654704267.761649",
          "thread_ts": "1654642470.504599",
          "files": [
            {
              "id": "F03JM7MH8BG",
              "created": 1654704263,
              "timestamp": 1654704263,
              "name": "someScreenShot.png",
              "title": "someScreenShot.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03HUQAAWEB",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 30381,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03JM7MH8BG-someScreenShot.png",
              "url_private_download": "C03H4A911EH/F03JM7MH8BG-someScreenShot.png",
              "original_h": 109,
              "original_w": 500,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JM7MH8BG-a5f74203bf/somescreenshot_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JM7MH8BG-a5f74203bf/somescreenshot_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JM7MH8BG-a5f74203bf/somescreenshot_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JM7MH8BG-a5f74203bf/somescreenshot_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 78,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JM7MH8BG-a5f74203bf/somescreenshot_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 105,
              "thumb_720": "",
              "thumb_720_w": 0,
              "thumb_720_h": 0,
              "thumb_960": "",
              "thumb_960_w": 0,
              "thumb_960_h": 0,
              "thumb_1024": "",
              "thumb_1024_w": 0,
              "thumb_1024_h": 0,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03HUQAAWEB/F03JM7MH8BG/somescreenshot.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03JM7MH8BG-4b0787a98e",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2PlZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Import CreateML\n“No such module createML”\nTried making new project, same thing\nTried in playground, auto completed this time, then again “No such module createML”\nMy Computer is up to date, with Xcode Version 13.4.1 (13F100)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "88616385-a839-40c3-9b67-e9a4fdef1182",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "what is the Xcode and macOS combo here?",
          "ts": "1654704292.441789",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Bkz",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "what is the Xcode and macOS combo here?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ca5115f5-5190-46fb-85fc-a13f604c1c6d",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "macOS Montery 12.4 and Xcode version 13.4.1 (13F100)",
          "ts": "1654704373.092709",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hTA",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "macOS Montery 12.4 and Xcode version 13.4.1 (13F100)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d8af2188-ecfc-49fd-abc4-1e8ede73c420",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Create ML doesn't support the simulator, make sure you are targeting a device or your mac.",
          "ts": "1654704383.089929",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6+0Dq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Create ML doesn't support the simulator, make sure you are targeting a device or your mac."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "eb2757c5-8d49-48f9-a868-c06deb98c76f",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "good point above, I thought it’s a playground for macOS, but is it an app, or a playground for iOS?",
          "ts": "1654704409.493319",
          "thread_ts": "1654642470.504599",
          "edited": {
            "user": "U03HRMABBDZ",
            "ts": "1654704442.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kXlIh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "good point above, I thought it’s a playground for macOS, but is it an app, or a playground for iOS?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8ba2e622-d0f2-4d66-bbc1-5b8b62f08390",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Yep it built and ran.  Just no code completion?",
          "ts": "1654704518.137739",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YQyP5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yep it built and ran.  Just no code completion?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "003ce188-3d7c-4ed9-a818-bdcfcb0212d1",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "I tried both an iOS project and a playgound. I get a warning in both cases",
          "ts": "1654704561.151469",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "IlR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I tried both an iOS project and a playgound. I get a warning in both cases"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ac192d08-dfd9-4135-871f-04eefc90e29f",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "for iOS project, what is the target?",
          "ts": "1654704576.240319",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ba6F",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "for iOS project, what is the target?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a85cbaee-5c33-4e75-97c6-8b525d56f8f4",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Wait its working",
          "ts": "1654704607.816939",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "grinning",
              "count": 1,
              "users": [
                "U03HRMABBDZ"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rRVy",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Wait its working"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5380cb0f-1a36-42ed-8fc7-cb09566e2043",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Okay it works apparently…including code completion. Thansk",
          "ts": "1654704664.561319",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Tgnt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Okay it works apparently…including code completion. Thansk"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b35790e1-0e09-432f-82d6-bb4b2fe2bef9",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "just to clarify, CreateML is NOT available on iOS simulator, so you build an app for iOS, please target a physical iOS device if you want to build \u0026amp; run. If you just want to build without worrying a physical iOS device, choose Any iOS device to build.",
          "ts": "1654704870.377299",
          "thread_ts": "1654642470.504599",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4zJI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "just to clarify, CreateML is NOT available on iOS simulator, so you build an app for iOS, please target a physical iOS device if you want to build \u0026 run. If you just want to build without worrying a physical iOS device, choose Any iOS device to build."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "caebefcb-fa58-47bb-b971-cfdc7e2c21c4",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "if you want to try with a Playground, please use macOS playground, since iOS playground uses iOS simulator as well.",
          "ts": "1654704904.652889",
          "thread_ts": "1654642470.504599",
          "edited": {
            "user": "U03HRMABBDZ",
            "ts": "1654705175.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "pmEe",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "if you want to try with a Playground, please use macOS playground, since iOS playground uses iOS simulator as well."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "6632e38b-74a4-4df5-8bfa-5cfacd733584",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "*Thank you, all, for joining.* We're going to close down the Q\u0026amp;A workflow for today. Don't worry though. We're back here tomorrow for some lively discussion:\n• _Q\u0026amp;A: Core ML_ — 9 AM\n• _Meet the Presenter: Compose advanced models with Create ML Components_ — 11 AM\n",
      "ts": "1654642965.568529",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "brain",
          "count": 2,
          "users": [
            "U03JZNY81L0",
            "U03JU4MPQEQ"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "IrI",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Thank you, all, for joining.",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " We're going to close down the Q\u0026A workflow for today. Don't worry though. We're back here tomorrow for some lively discussion:\n"
                }
              ]
            },
            {
              "Type": "rich_text_list",
              "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Q\u0026A: Core ML\",\"style\":{\"italic\":true}},{\"type\":\"text\",\"text\":\" \\u2014 9 AM\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Meet the Presenter: Compose advanced models with Create ML Components\",\"style\":{\"italic\":true}},{\"type\":\"text\",\"text\":\" \\u2014 11 AM\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
            },
            {
              "type": "rich_text_section",
              "elements": []
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "31cdf3f4-08d5-4be1-b5c4-db869b23edd4",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "Also, don't forget about *1:1 Labs*! The whole machine learning team is taking appointments throughout the week. It's not two late to submit your requests for Thursday or Friday. We'd love to chat more with you about your specific app needs.",
      "ts": "1654643045.419949",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "VB9h",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Also, don't forget about "
                },
                {
                  "type": "text",
                  "text": "1:1 Labs",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "! The whole machine learning team is taking appointments throughout the week. It's not two late to submit your requests for Thursday or Friday. We'd love to chat more with you about your specific app needs."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e added a workflow to this channel: *Machine Learning - Ask a Question*.",
      "ts": "1654703672.952529",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e added a workflow to this channel: *Machine Learning - Idea Submission*.",
      "ts": "1654703679.643739",
      "subtype": "bot_message",
      "bot_id": "B03HBPSE4R3",
      "username": "Machine Learning - Idea Submission",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "client_msg_id": "e8330070-b46c-4482-bf9a-3cb3d5345863",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Good morning! This lounge is about to go live!* Join us here, in 5 minutes for _*Q\u0026amp;A: Core ML*_. We have a great group of engineers from the Machine Learning team ready to answer all your questions about Core ML, including the new features introduced in the \u003chttps://developer.apple.com/videos/play/wwdc2022/10027/|Optimize your Core ML usage\u003e session that went live today. Feel free to bring questions you may have about how to bring machine learning to your app. It's going to be a fun conversation. Looking forward to your questions!",
      "ts": "1654703736.284489",
      "attachments": [
        {
          "fallback": "Apple Developer: Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022/10027/",
          "text": "Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6520/6520_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
          "blocks": null
        }
      ],
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "partying_face",
          "count": 5,
          "users": [
            "U03JRP87THN",
            "U03J4DR9GDS",
            "U03HRMAAYM9",
            "U03K67T11G8",
            "U03K2392NFJ"
          ]
        },
        {
          "name": "heart",
          "count": 4,
          "users": [
            "U03DJTBMHFF",
            "U03JRP87THN",
            "U03J4DR9GDS",
            "U03HRMAAYM9"
          ]
        },
        {
          "name": "whale",
          "count": 1,
          "users": [
            "U03K5C10EAU"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "fqqm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Good morning! This lounge is about to go live!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " Join us here, in 5 minutes for "
                },
                {
                  "type": "text",
                  "text": "Q\u0026A: Core ML",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ". We have a great group of engineers from the Machine Learning team ready to answer all your questions about Core ML, including the new features introduced in the "
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
                  "text": "Optimize your Core ML usage"
                },
                {
                  "type": "text",
                  "text": " session that went live today. Feel free to bring questions you may have about how to bring machine learning to your app. It's going to be a fun conversation. Looking forward to your questions!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "7954bc19-3670-4955-ad98-9d0b7c6fabb7",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Let's get started!* Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows.",
      "ts": "1654704016.723109",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "vsk",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Let's get started! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "3995b2fd-4c86-496d-85fe-d86978530bb0",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Open Discussion:* To get things going, who has already watched the \"Optimize your Core ML usage\" session? What are you most excited about? Reply in the thread… :thread:",
      "ts": "1654704077.270039",
      "thread_ts": "1654704077.270039",
      "reply_count": 1,
      "latest_reply": "1654717824.545569",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "YEn",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Open Discussion:",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " To get things going, who has already watched the \"Optimize your Core ML usage\" session? What are you most excited about? Reply in the thread… "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "73be596c-80a3-4511-b1b2-1bcfbc1dcf5e",
          "type": "message",
          "user": "U03K67T11G8",
          "text": ":man-raising-hand:\n\nperformance measurement looks cool! Who hasn’t already fallen into the trap of loading a model for every single prediction :slightly_smiling_face:",
          "ts": "1654717824.545569",
          "thread_ts": "1654704077.270039",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "smile",
              "count": 1,
              "users": [
                "U03J7UASVEU"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QCcC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "emoji",
                      "name": "man-raising-hand",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": "\n\nperformance measurement looks cool! Who hasn’t already fallen into the trap of loading a model for every single prediction "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JLPLTZ4M\u003e asked\n\u0026gt; - Given an image, I'm wondering about the best approach to identify \u0026amp; classify an object but also determine its color - could this be done by using separate components?\n\u0026gt;  - and how would you recommend handling multi-colored objects",
      "ts": "1654704290.154289",
      "thread_ts": "1654704290.154289",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654705021.447279",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "gK+F7",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JLPLTZ4M"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"- Given an image, I'm wondering about the best approach to identify \u0026 classify an object but also determine its color - could this be done by using separate components?\\n - and how would you recommend handling multi-colored objects\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "adbcd980-5275-405b-aa24-f415014b64da",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "This seems like a good fit for a standard image classifier. The main step to building an effective model would be to get enough sample data in each color and of each object to get a reasonably well-trained model. You could go about this in 2 ways:\n• 2 completely separate models, one for type of object and one for color, and combine the outputs together\n• A single model, in which each [color/object] pair is its own class. This could help out with interaction effects (if there are no purple bowling balls you'll never predict one), but it also expands the range of classes which might make the accuracy harder.\nFor multi-color, One option would just be to consider \"multi-color\" as its own color. As long as your training data reflects that your model should train accurately to that standard.",
          "ts": "1654704472.141629",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "IiR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This seems like a good fit for a standard image classifier. The main step to building an effective model would be to get enough sample data in each color and of each object to get a reasonably well-trained model. You could go about this in 2 ways:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"2 completely separate models, one for type of object and one for color, and combine the outputs together\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"A single model, in which each [color\\/object] pair is its own class. This could help out with interaction effects (if there are no purple bowling balls you'll never predict one), but it also expands the range of classes which might make the accuracy harder.\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "For multi-color, One option would just be to consider \"multi-color\" as its own color. As long as your training data reflects that your model should train accurately to that standard."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "62e0e7f1-4f49-4974-9dab-7bc8b3450c4f",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Or if you need to identify the color and you can crop to the object or even a relevant sub-section like middle square then you can use CIAreaAverage to get 1x1 image of the average color",
          "ts": "1654704682.020769",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "k55",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Or if you need to identify the color and you can crop to the object or even a relevant sub-section like middle square then you can use CIAreaAverage to get 1x1 image of the average color"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5e51adc9-8d9b-42de-ac91-fa9518992652",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "Thanks - I was thinking of the first approach.  I’m interested in multi-color but also patterning e.g. plaid, floral etc….  I had read somewhere about using pixel counts by color to determine color rather than ML",
          "ts": "1654704686.873299",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "paZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks - I was thinking of the first approach.  I’m interested in multi-color but also patterning e.g. plaid, floral etc….  I had read somewhere about using pixel counts by color to determine color rather than ML"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0336cc16-3c32-4933-8023-0f38b36ca7d9",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "yes, that pixel-based approach sounds similar",
          "ts": "1654704742.079069",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "JxO",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "yes, that pixel-based approach sounds similar"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c478e5ca-ce3b-4183-bf57-fb3fa15ba772",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Yes it turns more into an image processing problem at that point. Core Image provides a variety of filters that are very good for image analysis in the \"traditional\" space",
          "ts": "1654704779.507019",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hl6N",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes it turns more into an image processing problem at that point. Core Image provides a variety of filters that are very good for image analysis in the \"traditional\" space"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "67b013f0-c9b4-4a64-ad84-6b52edeec497",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "You could combine it with VNGenerateObjectnessBasedSaliencyImageRequest to get a mask and bounding box of the object",
          "ts": "1654704903.354419",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iUMvU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You could combine it with VNGenerateObjectnessBasedSaliencyImageRequest to get a mask and bounding box of the object"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a9374c3e-50a9-4344-86d1-25d19812a588",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "thanks \u003c@U03JFF1S5U0\u003e this is v. helpful",
          "ts": "1654705021.447279",
          "thread_ts": "1654704290.154289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/Gkc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "thanks "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JFF1S5U0"
                    },
                    {
                      "type": "text",
                      "text": " this is v. helpful"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J1RX038W\u003e asked\n\u0026gt; Would Create ML/Components/Core ML be capable of dealing with music data? Specifically I want to train a model that can predict the tempo (BPM) of a song.",
      "ts": "1654704427.726389",
      "thread_ts": "1654704427.726389",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654704825.560069",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Pw=",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J1RX038W"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Would Create ML\\/Components\\/Core ML be capable of dealing with music data? Specifically I want to train a model that can predict the tempo (BPM) of a song.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "48e7b634-c260-43cf-b033-b5780fe9dd16",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Core ML models can support audio data through MultiArray inputs of audio samples.  Create ML does support audio/sound classification but not tempo estimation.",
          "ts": "1654704549.664299",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ftdEb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Core ML models can support audio data through MultiArray inputs of audio samples.  Create ML does support audio/sound classification but not tempo estimation."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f9cd5a3f-d466-4468-95a8-e81dcb26a9e3",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Doing a quick search online it seems there is some work on models for tempo estimation, many of which could be converted to Core ML",
          "ts": "1654704593.201889",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XKYX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Doing a quick search online it seems there is some work on models for tempo estimation, many of which could be converted to Core ML"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ef521705-f15a-4517-9f8d-3abad2ae2b85",
          "type": "message",
          "user": "U03J1RX038W",
          "text": "Thanks. I have training data where songs have been corrected to have a perfectly consistent BPM throughout and I have a platform that transforms raw audio data into less granular transient magnitudes across frequency bands. I've tried building tabular regressors with the data in a few different ways but haven't gotten useful results so far.",
          "ts": "1654704624.627649",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6aMG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks. I have training data where songs have been corrected to have a perfectly consistent BPM throughout and I have a platform that transforms raw audio data into less granular transient magnitudes across frequency bands. I've tried building tabular regressors with the data in a few different ways but haven't gotten useful results so far."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "acd4e28b-6fa7-484e-b333-3a897423904c",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "With a tabular regressor you may want to explore some additional feature engineering steps.",
          "ts": "1654704703.922469",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ALOS2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "With a tabular regressor you may want to explore some additional feature engineering steps."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d0288d33-7647-4279-80dc-6b8f96759472",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Using more traditional DSP style algorithms",
          "ts": "1654704724.804699",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4A+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Using more traditional DSP style algorithms"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ecaa511d-0aa1-4cc2-8fb1-8c10ab63d5b1",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Perhaps windowed autocorrelation",
          "ts": "1654704739.908429",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Td3mi",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Perhaps windowed autocorrelation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "22d2da5c-cc65-437f-a169-8321e39dafe6",
          "type": "message",
          "user": "U03J1RX038W",
          "text": "That's exactly what the existing app (Tempi, on the app store) uses now. It works pretty well but I suspect an ML approach could be much better.",
          "ts": "1654704780.663379",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lhh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That's exactly what the existing app (Tempi, on the app store) uses now. It works pretty well but I suspect an ML approach could be much better."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "33819f92-4a83-478e-9634-1af3557a65b3",
          "type": "message",
          "user": "U03J1RX038W",
          "text": "I never thought of combining them though. Interesting…",
          "ts": "1654704825.560069",
          "thread_ts": "1654704427.726389",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PtxG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I never thought of combining them though. Interesting…"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HUQAAWEB\u003e asked\n\u0026gt; How to ball park run time for linear regressor predictions func? What I really want to know, how often I can call this?",
      "ts": "1654704773.779899",
      "thread_ts": "1654704773.779899",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654705947.820099",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Mrsn",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HUQAAWEB"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"How to ball park run time for linear regressor predictions func? What I really want to know, how often I can call this?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "35d558a9-34e9-4e86-bf8e-631937ffc225",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Hi Adrien: I love this question, and we have a *great* session for you which dropped today.\n\n\u003chttps://developer.apple.com/wwdc22/10027\u003e\n\nIn _Optimize your Core ML usage_ you can see how you can open a model in xcode, and with a few clicks get an estimate of how long it will take to do inference (as well as other facts) on your device.",
          "ts": "1654704952.338069",
          "thread_ts": "1654704773.779899",
          "attachments": [
            {
              "fallback": "Apple Developer: Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/wwdc22/10027",
              "text": "Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6520/6520_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/wwdc22/10027",
              "original_url": "https://developer.apple.com/wwdc22/10027",
              "blocks": null
            }
          ],
          "files": [
            {
              "id": "F03K6HKC4KB",
              "created": 1654704948,
              "timestamp": 1654704948,
              "name": "image.png",
              "title": "image.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03HB4T0CA3",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 469629,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03K6HKC4KB-image.png",
              "url_private_download": "C03H4A911EH/F03K6HKC4KB-image.png",
              "original_h": 670,
              "original_w": 1132,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 213,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 284,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_720.png",
              "thumb_720_w": 720,
              "thumb_720_h": 426,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_960.png",
              "thumb_960_w": 960,
              "thumb_960_h": 568,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6HKC4KB-891ba6bb50/image_1024.png",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 606,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03HB4T0CA3/F03K6HKC4KB/image.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03K6HKC4KB-ec258d7b1e",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "COI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Adrien: I love this question, and we have a "
                    },
                    {
                      "type": "text",
                      "text": "great",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " session for you which dropped today.\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/wwdc22/10027",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\nIn "
                    },
                    {
                      "type": "text",
                      "text": "Optimize your Core ML usage",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " you can see how you can open a model in xcode, and with a few clicks get an estimate of how long it will take to do inference (as well as other facts) on your device."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "620ebc09-99a5-4a9a-a862-290578a864c6",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "I’ll check it out and come back with any questions ,comments, concerns. Thanks",
          "ts": "1654705947.820099",
          "thread_ts": "1654704773.779899",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "uW0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I’ll check it out and come back with any questions ,comments, concerns. Thanks"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JHAD6E3T\u003e asked\n\u0026gt; Why do older devices use more memory when loading MLModel?\n\u0026gt; A model used about 600MB of memory on the iPhone 12 Pro, but on the iPhone 11 Pro the app crashed over 1.2GB of memory.\n\u0026gt; ```\n\u0026gt; let model = try await SomeModel.load()  // load a model\n\u0026gt; ```",
      "ts": "1654704797.282509",
      "thread_ts": "1654704797.282509",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654706179.363959",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "m=Rs",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JHAD6E3T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Why do older devices use more memory when loading MLModel?\\nA model used about 600MB of memory on the iPhone 12 Pro, but on the iPhone 11 Pro the app crashed over 1.2GB of memory.\\n```\\nlet model = try await SomeModel.load()  \\/\\/ load a model\\n```\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "680be56b-0386-40d4-a033-e100e02c9603",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "We need to look at the model in question. Please submit a problem report through Feedback Assistant.",
          "ts": "1654704909.352249",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "VFV3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We need to look at the model in question. Please submit a problem report through Feedback Assistant."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "74644183-0468-45ff-a6f1-6521e55f78a8",
          "type": "message",
          "user": "U03JHAD6E3T",
          "text": "OK. Is it not a general tendency? It’s a GAN model.",
          "ts": "1654705179.274459",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Q7b",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "OK. Is it not a general tendency? It’s a GAN model."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9435a550-f170-46b2-b28c-336a3acc88c3",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "We are not aware of such memory usage issue in general between the hardwares.",
          "ts": "1654705395.280869",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "k94",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We are not aware of such memory usage issue in general between the hardwares."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "adf4d0f9-a4ee-48f3-a778-45bd89ed7f9a",
          "type": "message",
          "user": "U03JHAD6E3T",
          "text": "Thanks. I’ll feedback it!",
          "ts": "1654705542.169619",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "pqjO",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks. I’ll feedback it!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "727c7d96-f585-475f-a7e4-4faf9755ee81",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "Thank you, we appreciate it.",
          "ts": "1654705603.908799",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QRH6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you, we appreciate it."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "be342726-f075-4782-9c84-82ac3792e92b",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "You can also request a 1:1 lab so that we can investigate together.",
          "ts": "1654705839.641999",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nCI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can also request a 1:1 lab so that we can investigate together."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9533d338-18cf-41db-a02f-6b6393f077c4",
          "type": "message",
          "user": "U03JHAD6E3T",
          "text": "Oh, lab! I’ll refer the lab to a more knowledgeable colleague! Thank you.",
          "ts": "1654706179.363959",
          "thread_ts": "1654704797.282509",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PoCs",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Oh, lab! I’ll refer the lab to a more knowledgeable colleague! Thank you."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HUQAAWEB\u003e asked\n\u0026gt; What if my data is exponential, would that need a quadratic regressor? In one of the videos the data was parabolic and you normalized it. What’s going on here",
      "ts": "1654704951.551669",
      "thread_ts": "1654704951.551669",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654707531.899459",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "G8gv",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HUQAAWEB"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What if my data is exponential, would that need a quadratic regressor? In one of the videos the data was parabolic and you normalized it. What\\u2019s going on here\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "5bdf5525-a274-4e4a-b92b-1900d851af22",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "So this is a technique from classical statistics, but basically a common approach is to take a raw set of data and normalize it before applying a fit function. You could run data with an exponential distribution through a log transform, for example. One of the risks if you don't is that the leverage from a few points can be very high and skew your model. There's tradeoffs to this approach and many different techniques that can help you find a good model, but data normalization is often a helpful technique.",
          "ts": "1654705206.894399",
          "thread_ts": "1654704951.551669",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "S5Md",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So this is a technique from classical statistics, but basically a common approach is to take a raw set of data and normalize it before applying a fit function. You could run data with an exponential distribution through a log transform, for example. One of the risks if you don't is that the leverage from a few points can be very high and skew your model. There's tradeoffs to this approach and many different techniques that can help you find a good model, but data normalization is often a helpful technique."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7cdfbf32-e1e4-48b7-aaa2-67fc6d2ae44d",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "hmm, so generally for better models aim for data in a linear form with minimal outliers",
          "ts": "1654705657.990699",
          "thread_ts": "1654704951.551669",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5Me",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "hmm, so generally for better models aim for data in a linear form with minimal outliers"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "120468d3-69b5-4473-a109-9ae2cf747ede",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "I think in general it can be helpful in data preparation as one tool that's available to use. But there's an entire field on this and I don't want to oversimplify.",
          "ts": "1654706687.372429",
          "thread_ts": "1654704951.551669",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZDG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I think in general it can be helpful in data preparation as one tool that's available to use. But there's an entire field on this and I don't want to oversimplify."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1bce7b82-5c87-4ca8-b786-794434090e9f",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Ok \u003c@U03HB4V9Q5V\u003e I appreciate your responses. Thanks",
          "ts": "1654707531.899459",
          "thread_ts": "1654704951.551669",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lOmnt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ok "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4V9Q5V"
                    },
                    {
                      "type": "text",
                      "text": " I appreciate your responses. Thanks"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03KG1FM5R6\u003e asked\n\u0026gt; I've noticed you have a new way to store model weights in sparse form (which is a great addition) and am wondering it there's some fundamental blocker to using sparse operations at inference time too?",
      "ts": "1654705134.319739",
      "thread_ts": "1654705134.319739",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654706327.467469",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Z2WC",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03KG1FM5R6"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I've noticed you have a new way to store model weights in sparse form (which is a great addition) and am wondering it there's some fundamental blocker to using sparse operations at inference time too?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "c1bffdc0-68bb-4cf7-a85b-2cc2e70e141d",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "The sparsity is leveraged during execution on certain compute units such as the neural enigne",
          "ts": "1654705189.304349",
          "thread_ts": "1654705134.319739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2mH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The sparsity is leveraged during execution on certain compute units such as the neural enigne"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "34c01224-1834-42be-b8b6-82d16a53f85a",
          "type": "message",
          "user": "U03KG1FM5R6",
          "text": "interesting, would that be the case for any model which contains sparse matrices? i.e. not just for ones which have been compressed using `sparsify_weights`",
          "ts": "1654705640.356879",
          "thread_ts": "1654705134.319739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Qy=p",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "interesting, would that be the case for any model which contains sparse matrices? i.e. not just for ones which have been compressed using "
                    },
                    {
                      "type": "text",
                      "text": "sparsify_weights",
                      "style": {
                        "code": true
                      }
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "88ffe9b7-d600-47be-a259-d99eea889784",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "That is not generally the case. It is best to use the sparse encoding explicitly.",
          "ts": "1654706327.467469",
          "thread_ts": "1654705134.319739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qPe",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That is not generally the case. It is best to use the sparse encoding explicitly."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J6AKH19V\u003e asked\n\u0026gt; We're facing some challenges on converting our AI models to CoreML, some operations 'state-of-the-art' aren't fully supported and we're considering running it in a different approach. Would it be feasable to have a model in C++ and leverage the GPU power of the devices, if yes... how?",
      "ts": "1654705234.243079",
      "thread_ts": "1654705234.243079",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654719993.733549",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "1xsjc",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J6AKH19V"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"We're facing some challenges on converting our AI models to CoreML, some operations 'state-of-the-art' aren't fully supported and we're considering running it in a different approach. Would it be feasable to have a model in C++ and leverage the GPU power of the devices, if yes... how?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "60319919-187d-47e6-a89a-65847d3ada27",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "A composite operator may help you convert these operations: \u003chttps://coremltools.readme.io/docs/composite-operators\u003e",
          "ts": "1654705333.811679",
          "thread_ts": "1654705234.243079",
          "attachments": [
            {
              "fallback": "coremltools: Composite Operators",
              "id": 1,
              "title": "Composite Operators",
              "title_link": "https://coremltools.readme.io/docs/composite-operators",
              "text": "As machine learning continually evolves, new operations are regularly added to source frameworks such as TensorFlow and PyTorch. While converting a model to Core ML, you may encounter an unsupported operation. In most cases, you can handle unsupported operations by using composite operators, which y...",
              "service_name": "coremltools",
              "from_url": "https://coremltools.readme.io/docs/composite-operators",
              "original_url": "https://coremltools.readme.io/docs/composite-operators",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "WQpp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A composite operator may help you convert these operations: "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/composite-operators",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "30c70f77-a6f5-40b4-acaa-bfb927833ac7",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "In some cases you can also supply a \u003chttps://coremltools.readme.io/docs/custom-operators|custom operator \u003ebut to leverage the full Core ML stack its best to see if you can represent the functionality as a composite op first",
          "ts": "1654705387.422319",
          "thread_ts": "1654705234.243079",
          "attachments": [
            {
              "fallback": "coremltools: Custom Operators",
              "id": 1,
              "title": "Custom Operators",
              "title_link": "https://coremltools.readme.io/docs/custom-operators",
              "text": "While converting a model to Core ML, you may encounter an unsupported operation that can't be represented by a composite operator. In such cases you can create a custom layer in your model for the custom operator, and implement the Swift classes that define the operator's computational behavior. For...",
              "service_name": "coremltools",
              "from_url": "https://coremltools.readme.io/docs/custom-operators",
              "original_url": "https://coremltools.readme.io/docs/custom-operators",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HkP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In some cases you can also supply a "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/custom-operators",
                      "text": "custom operator "
                    },
                    {
                      "type": "text",
                      "text": "but to leverage the full Core ML stack its best to see if you can represent the functionality as a composite op first"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "28453e5e-acd5-4840-b555-a444b50e5b17",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "\u003c@U03HJ5M01M0\u003e  is there any workaround for `torch.stft` and `torch.istft`  ?",
          "ts": "1654717252.460029",
          "thread_ts": "1654705234.243079",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4bZl5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HJ5M01M0"
                    },
                    {
                      "type": "text",
                      "text": "  is there any workaround for "
                    },
                    {
                      "type": "text",
                      "text": "torch.stft",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "text",
                      "text": "torch.istft",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "  ?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6187b026-7185-40bc-a12d-ec490cee1d78",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "Are the FFT operations in the beginning / pre processing stage of the model? If so, then you can accelerate the rest of the model by converting it to CoreML, and implementing the FFT operation using BNNS or Metal.\nWould be interested to know more of your use case, so if you can request for a lab session on this topic (mentioning my name), would be happy discuss more.\nIn any case, submitting a feedback request with your use case would be great.",
          "ts": "1654719993.733549",
          "thread_ts": "1654705234.243079",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1rc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Are the FFT operations in the beginning / pre processing stage of the model? If so, then you can accelerate the rest of the model by converting it to CoreML, and implementing the FFT operation using BNNS or Metal.\nWould be interested to know more of your use case, so if you can request for a lab session on this topic (mentioning my name), would be happy discuss more.\nIn any case, submitting a feedback request with your use case would be great."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JM1MMUG6\u003e asked\n\u0026gt; Are there any updates planned for coremltools, notably supporting complex numbers. I'm looking to convert a TensorFlow model, but even after implementing missing operations like Fast Fourier Transform, I'm blocked by the fact there is no complex number support.",
      "ts": "1654705234.908069",
      "thread_ts": "1654705234.908069",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654705576.178839",
      "reactions": [
        {
          "name": "eyes",
          "count": 1,
          "users": [
            "U03JM1MMUG6"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "=SkXR",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JM1MMUG6"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Are there any updates planned for coremltools, notably supporting complex numbers. I'm looking to convert a TensorFlow model, but even after implementing missing operations like Fast Fourier Transform, I'm blocked by the fact there is no complex number support.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b11fbafc-1d32-4c1d-b167-5fe2912cf986",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "Yes, you are right. There isn’t a complex number data type in the \u003chttps://coremltools.readme.io/docs/model-intermediate-language|CoreML MIL spec\u003e\nThe best way to use them is by treating complex numbers as 2D vectors of real and imaginary numbers",
          "ts": "1654705576.178839",
          "thread_ts": "1654705234.908069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rvc2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, you are right. There isn’t a complex number data type in the "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/model-intermediate-language",
                      "text": "CoreML MIL spec"
                    },
                    {
                      "type": "text",
                      "text": "\nThe best way to use them is by treating complex numbers as 2D vectors of real and imaginary numbers"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ3L98TF\u003e asked\n\u0026gt; Hi, can a ML model extract certain values off a json such as “VideoType” and URLs then return those values to make a network request? \n\u0026gt; I’m looking a making a video recommendation system with ML but not sure the best way to do it. ",
      "ts": "1654705299.296619",
      "thread_ts": "1654705299.296619",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654705611.833859",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "3eju5",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ3L98TF"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi, can a ML model extract certain values off a json such as \\u201cVideoType\\u201d and URLs then return those values to make a network request? \\nI\\u2019m looking a making a video recommendation system with ML but not sure the best way to do it. \"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "27f5a772-46fb-437f-8c1e-e4669a5c5bc4",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Have you tried the MLRecommender in Create ML? That is a great place to start for building recommender systems. Since your data is in json format, you can also try the TabularData framework which can help with the json loading!",
          "ts": "1654705483.337009",
          "thread_ts": "1654705299.296619",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mALe",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Have you tried the MLRecommender in Create ML? That is a great place to start for building recommender systems. Since your data is in json format, you can also try the TabularData framework which can help with the json loading!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5530BA43-E0F0-491A-973A-FA618CB56D17",
          "type": "message",
          "user": "U03HZ3L98TF",
          "text": "Didn't know MLRecommender existed! Thank you for that tip I'll have a look into it and use TabularData as well. Thanks again ",
          "ts": "1654705611.833859",
          "thread_ts": "1654705299.296619",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "grinning",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03JFGMTU8G"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "F1P",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Didn't "
                    },
                    {
                      "type": "text",
                      "text": "know"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "MLRecommender"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "existed!"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Thank"
                    },
                    {
                      "type": "text",
                      "text": " you "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "tip"
                    },
                    {
                      "type": "text",
                      "text": " I'll "
                    },
                    {
                      "type": "text",
                      "text": "have"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "look"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "into"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "it"
                    },
                    {
                      "type": "text",
                      "text": " and use "
                    },
                    {
                      "type": "text",
                      "text": "TabularData"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "as"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "well."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "again"
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "c54c25cb-42e3-4398-9dad-96292d2e3960",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Great questions everyone! Keep 'em coming!",
      "ts": "1654705616.840329",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "6zji",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Great questions everyone! Keep 'em coming!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HUQAAWEB\u003e asked\n\u0026gt; Can num features vary. Say I wanna guess catAge given numWhiskers, furDensity, and numLegs. But sometimes I may only have numWhiskers and numLegs and not furDensity. Would that require its own, separately trained, MLModel?\n\u0026gt; \n\u0026gt; Similarly, (same question different example)\n\u0026gt; \n\u0026gt; \n\u0026gt; Tic tac toe…\n\u0026gt; \n\u0026gt; trainingData.csv\n\u0026gt; position0,p1,p2…p8, didWin\n\u0026gt; 0,1,1…1,1\n\u0026gt; 0,0,1…1,1\n\u0026gt; 1,0,0…1,0\n\u0026gt; …\n\u0026gt; \n\u0026gt; Then at next move prediction time\n\u0026gt; Can I use this same model when the game is 2,3, or 4 moves in?\n\u0026gt; \n\u0026gt; Effectively \n\u0026gt; \n\u0026gt; nil,0,1, nil,nil,nil,nil,nil,nil,nil,nil\n\u0026gt; nil,0,1, nil,nil,nil,1,nil,nil,nil,nil\n\u0026gt; …\n\u0026gt; 1,0,0, nil,nil,nil,0,1,1,1,nil",
      "ts": "1654705745.141479",
      "thread_ts": "1654705745.141479",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654706837.765409",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "/e6",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HUQAAWEB"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Can num features vary. Say I wanna guess catAge given numWhiskers, furDensity, and numLegs. But sometimes I may only have numWhiskers and numLegs and not furDensity. Would that require its own, separately trained, MLModel?\\n\\nSimilarly, (same question different example)\\n\\n\\nTic tac toe\\u2026\\n\\ntrainingData.csv\\nposition0,p1,p2\\u2026p8, didWin\\n0,1,1\\u20261,1\\n0,0,1\\u20261,1\\n1,0,0\\u20261,0\\n\\u2026\\n\\nThen at next move prediction time\\nCan I use this same model when the game is 2,3, or 4 moves in?\\n\\nEffectively \\n\\nnil,0,1, nil,nil,nil,nil,nil,nil,nil,nil\\nnil,0,1, nil,nil,nil,1,nil,nil,nil,nil\\n\\u2026\\n1,0,0, nil,nil,nil,0,1,1,1,nil\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e26fc72c-baf4-43f1-b40e-8f3225c30615",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "This is such a cool example! Thanks for the question. Have you tried using the tabular classifiers in Create ML? When you have missing data in your feature columns you can try replacing them (imputing). The TabularData framework makes this part really easy.",
          "ts": "1654706068.582389",
          "thread_ts": "1654705745.141479",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "L7B6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This is such a cool example! Thanks for the question. Have you tried using the tabular classifiers in Create ML? When you have missing data in your feature columns you can try replacing them (imputing). The TabularData framework makes this part really easy."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0b59e069-d67d-44a2-bcd8-adf49c781184",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Im not sure if I tried it, maybe once. Mostly regressors. What do you mean I can try replacing them. Replacing them with what\n\nI did hear tabularData has a fill function or something like that\n\nI guess I need to read more about something\n\nI mainly wanted to know if it was possible, and it looks like you’re saying it is. cool",
          "ts": "1654706584.533069",
          "thread_ts": "1654705745.141479",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nlISu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Im not sure if I tried it, maybe once. Mostly regressors. What do you mean I can try replacing them. Replacing them with what\n\nI did hear tabularData has a fill function or something like that\n\nI guess I need to read more about something\n\nI mainly wanted to know if it was possible, and it looks like you’re saying it is. cool"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d11ecf9f-916e-49b1-a54f-f6701443b584",
          "type": "message",
          "user": "U03HUQAAWEB",
          "text": "Follow up…With linear regression models, if Im interested in predicting more than one target, do I need a model for each target I’m interested in?",
          "ts": "1654706837.765409",
          "thread_ts": "1654705745.141479",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "cuci",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Follow up…With linear regression models, if Im interested in predicting more than one target, do I need a model for each target I’m interested in?"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JN80PFRQ\u003e asked\n\u0026gt; I want to start learning ML and CoreML, and I have thought of a problem space that may be interesting that could use further exploration. I know NLP depends on extraordinarily large data sets, but I'm wondering about the utility of training a model on a constructed language with a much smaller data set. The one I have in mind has a very small official dictionary (slightly more than 100 official words), and rather simple grammar rules. Are there resources you would recommend for exploring this specific application of ML, or any pitfalls I might want to keep in mind?",
      "ts": "1654705981.222899",
      "thread_ts": "1654705981.222899",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654706487.072799",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "WLq",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JN80PFRQ"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I want to start learning ML and CoreML, and I have thought of a problem space that may be interesting that could use further exploration. I know NLP depends on extraordinarily large data sets, but I'm wondering about the utility of training a model on a constructed language with a much smaller data set. The one I have in mind has a very small official dictionary (slightly more than 100 official words), and rather simple grammar rules. Are there resources you would recommend for exploring this specific application of ML, or any pitfalls I might want to keep in mind?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "9c27c2d9-7b6d-4e8f-9d40-86076e6eb4ae",
          "type": "message",
          "user": "U03HRM0UK8B",
          "text": "This depends somewhat on what sort of tasks and models you are interested in. For example, for a classification task, the maxent classifier available through CreateML is not language-dependent. It should be able to take on classification tasks in an artificial language of this sort. Gazetteers are language-independent, so they would still be usable. Our built-in embeddings are language-dependent, so they would not be of use here. If you want to train your own embedding or language model using open-source tools, that probably would still require significant amounts of data, but perhaps not as much as with natural languages. Language modeling techniques have recently been applied with some success to programming languages. If your rules are similar to the syntax rules of programming languages, you might consider using the sorts of parsing tools that are used for them, but that is really a different area than NLP.",
          "ts": "1654706004.197909",
          "thread_ts": "1654705981.222899",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "SKYI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This depends somewhat on what sort of tasks and models you are interested in. For example, for a classification task, the maxent classifier available through CreateML is not language-dependent. It should be able to take on classification tasks in an artificial language of this sort. Gazetteers are language-independent, so they would still be usable. Our built-in embeddings are language-dependent, so they would not be of use here. If you want to train your own embedding or language model using open-source tools, that probably would still require significant amounts of data, but perhaps not as much as with natural languages. Language modeling techniques have recently been applied with some success to programming languages. If your rules are similar to the syntax rules of programming languages, you might consider using the sorts of parsing tools that are used for them, but that is really a different area than NLP."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "55f2a5fa-f64d-4d4f-aec4-641791199121",
          "type": "message",
          "user": "U03JN80PFRQ",
          "text": "I am exploring non-ML tokenizing and parsing as well, because the grammar rules are rather straightforward and the word set is small. The language has recently been given an ISO identifier, but there is little support for it in general. I’ll take a closer look at traditional parsing tools, thank you!",
          "ts": "1654706487.072799",
          "thread_ts": "1654705981.222899",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KnMQ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I am exploring non-ML tokenizing and parsing as well, because the grammar rules are rather straightforward and the word set is small. The language has recently been given an ISO identifier, but there is little support for it in general. I’ll take a closer look at traditional parsing tools, thank you!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J7ETMDHU\u003e asked\n\u0026gt; We converted a Tensorflow image segmentation model to CoreML. We notice that we get different results when running this CoreML model on macOS (with Python3 and coremltools) versus on iOS. Predictions are way less accurate on iOS and we cannot explain why (even when setting computeUnits parameter to .cpuOnly).",
      "ts": "1654706125.964699",
      "thread_ts": "1654706125.964699",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654762619.081679",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Ea1I",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J7ETMDHU"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"We converted a Tensorflow image segmentation model to CoreML. We notice that we get different results when running this CoreML model on macOS (with Python3 and coremltools) versus on iOS. Predictions are way less accurate on iOS and we cannot explain why (even when setting computeUnits parameter to .cpuOnly).\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "8bcde854-0b92-451f-b6af-9824a6827415",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "Have you tried setting the `compute_units=ct.precision.FLOAT32` as described \u003chttps://coremltools.readme.io/docs/typed-execution|here\u003e\nThat  said, for the CPU compute unit I would expect the predictions to match between iOS and macOS. There could be differences with the “all” compute unit depending on the actual hardware the model runs on mac and ios which can be different. If you could file a feed back request with the code to reproduce these differences that you are observing that would be great!",
          "ts": "1654706315.294349",
          "thread_ts": "1654706125.964699",
          "attachments": [
            {
              "fallback": "coremltools: Typed Execution",
              "id": 1,
              "title": "Typed Execution",
              "title_link": "https://coremltools.readme.io/docs/typed-execution",
              "text": "A model’s compute precision impacts its performance and numerical accuracy, which may impact the user experience of apps using the model. Core ML models saved as ML Programs or neural networks execute with either float 32 or float 16 precision. This page describes how the precision is determined by ...",
              "service_name": "coremltools",
              "from_url": "https://coremltools.readme.io/docs/typed-execution",
              "original_url": "https://coremltools.readme.io/docs/typed-execution",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03K5C10EAU"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "UC9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Have you tried setting the "
                    },
                    {
                      "type": "text",
                      "text": "compute_units=ct.precision.FLOAT32",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " as described "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/typed-execution",
                      "text": "here"
                    },
                    {
                      "type": "text",
                      "text": "\nThat  said, for the CPU compute unit I would expect the predictions to match between iOS and macOS. There could be differences with the “all” compute unit depending on the actual hardware the model runs on mac and ios which can be different. If you could file a feed back request with the code to reproduce these differences that you are observing that would be great!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e2c73a6f-f06b-48b9-a512-ac76495320a6",
          "type": "message",
          "user": "U03J7ETMDHU",
          "text": "Thank you for your reply. We actually did use the `compute_units=ct.precision.FLOAT32` setting when converting the Tensorflow model to CoreML. It did not change anything in our results. We also tried using the new MLPrograms but the results were unchanged.",
          "ts": "1654762619.081679",
          "thread_ts": "1654706125.964699",
          "edited": {
            "user": "U03J7ETMDHU",
            "ts": "1654762645.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6ixJi",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you for your reply. We actually did use the "
                    },
                    {
                      "type": "text",
                      "text": "compute_units=ct.precision.FLOAT32",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " setting when converting the Tensorflow model to CoreML. It did not change anything in our results. We also tried using the new MLPrograms but the results were unchanged."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JLPLTZ4M\u003e asked\n\u0026gt; - What would be the best way to figure out which objects go together - say you have 10 groups of three and a pool of 100 ungrouped objects \u0026amp; you want to group them similarly?",
      "ts": "1654706259.455149",
      "thread_ts": "1654706259.455149",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 5,
      "latest_reply": "1654707320.966649",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "tkNid",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JLPLTZ4M"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"- What would be the best way to figure out which objects go together - say you have 10 groups of three and a pool of 100 ungrouped objects \u0026 you want to group them similarly?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "d8230e60-1bce-4aa9-81ef-29bf7a370fef",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "Thanks for asking Derick! It would be helpful to clarify what the \"objects\" are in this context. If they are objects within image data, you could leverage the CreateML feature extractor to turn it into structured tabular data.\n\nFrom there, it's a classical unsupervised clustering problem, for which there are several approaches. Something like k-means is a quick and effective approach that might work in your case. \u003chttps://apple.github.io/turicreate/docs/userguide/clustering/kmeans.html\u003e",
          "ts": "1654706442.521229",
          "thread_ts": "1654706259.455149",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "f06=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for asking Derick! It would be helpful to clarify what the \"objects\" are in this context. If they are objects within image data, you could leverage the CreateML feature extractor to turn it into structured tabular data.\n\nFrom there, it's a classical unsupervised clustering problem, for which there are several approaches. Something like k-means is a quick and effective approach that might work in your case. "
                    },
                    {
                      "type": "link",
                      "url": "https://apple.github.io/turicreate/docs/userguide/clustering/kmeans.html",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "65a9e0de-3817-4718-86fd-54f9a1d6b4e7",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "Yes the objects would be images",
          "ts": "1654706611.178059",
          "thread_ts": "1654706259.455149",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PdLw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes the objects would be images"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "620737d9-000c-4f86-b6c8-201b37a17861",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "There is also a `CIKMeans` filter in CoreImage. That may be exactly what you need, Derick.",
          "ts": "1654706973.786279",
          "thread_ts": "1654706259.455149",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "p4LEt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There is also a "
                    },
                    {
                      "type": "text",
                      "text": "CIKMeans",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " filter in CoreImage. That may be exactly what you need, Derick."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "93dc5a19-974d-4bb3-a3fd-0383c0b728e8",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "Thanks - I need to learn my way around CoreImage a bit more….this has been really helpful!",
          "ts": "1654707173.981299",
          "thread_ts": "1654706259.455149",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5rh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks - I need to learn my way around CoreImage a bit more….this has been really helpful!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "abdf0b61-c7d9-4a02-87be-9c2f1bc580fd",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "There are some Core Image lab appointments available on Friday if you want to dive into this 1:1 with engineering",
          "ts": "1654707320.966649",
          "thread_ts": "1654706259.455149",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "K2t",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are some Core Image lab appointments available on Friday if you want to dive into this 1:1 with engineering"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "6585e6a8-bc0b-4ba2-87ad-a28eb83a08e3",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "We've still got some more time for questions, but has anyone tried out the new Core ML Performance report and Instruments in Xcode?  Reply to the thread with your thoughts or add some emojis to this post! :thread:",
      "ts": "1654706723.937729",
      "thread_ts": "1654706723.937729",
      "edited": {
        "user": "U03J7UASVEU",
        "ts": "1654706767.000000"
      },
      "reply_count": 7,
      "latest_reply": "1654710082.378109",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "fire",
          "count": 2,
          "users": [
            "U03DJTBMHFF",
            "U03HY66772A"
          ]
        },
        {
          "name": "brain",
          "count": 1,
          "users": [
            "U03J7UASVEU"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "pculR",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "We've still got some more time for questions, but has anyone tried out the new Core ML Performance report and Instruments in Xcode?  Reply to the thread with your thoughts or add some emojis to this post! "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "ed6e7f69-dd1f-4fe2-8498-09eadf96c4c6",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "I tried to get a report and it just said it couldn’t connect to the device, so don’t have much feedback yet. Excited fro the feature!",
          "ts": "1654707019.286209",
          "thread_ts": "1654706723.937729",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "MtZS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I tried to get a report and it just said it couldn’t connect to the device, so don’t have much feedback yet. Excited fro the feature!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2fb78840-cea3-4712-a45e-43f2a7da4285",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "This is a known issue. A workaround that may help is: After enabling developer mode, reboot the device, then unlock the device, *then* unplug and replug the cable.",
          "ts": "1654707233.776819",
          "thread_ts": "1654706723.937729",
          "edited": {
            "user": "U03JFGMUPCY",
            "ts": "1654707348.000000"
          },
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "61j8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This is a known issue. A workaround that may help is: After enabling developer mode, reboot the device, then unlock the device, "
                    },
                    {
                      "type": "text",
                      "text": "then",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " unplug and replug the cable."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5daa9dfd-f7f8-426a-8d4c-1cb4d91e7f9c",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Sorry, I should have mentioned this is on my mac (MBP M1 Pro). Currently filing a different radar, but I’ll drop a number in here soon.",
          "ts": "1654707381.628959",
          "thread_ts": "1654706723.937729",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6xq+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sorry, I should have mentioned this is on my mac (MBP M1 Pro). Currently filing a different radar, but I’ll drop a number in here soon."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9e7c4aeb-9e56-4a23-8c25-d4f1e9728282",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "FB10107014",
          "ts": "1654707641.081129",
          "thread_ts": "1654706723.937729",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HRMWNP4J"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "78m=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "FB10107014"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "20085464-f204-46b4-b761-9a586f2c5c8a",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "Thank you, we'll look into this. Can you please also add your macOS version?",
          "ts": "1654707797.580669",
          "thread_ts": "1654706723.937729",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ScGIa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you, we'll look into this. Can you please also add your macOS version?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c6066263-979b-4b77-90fb-a6f7b202ae47",
          "type": "message",
          "user": "U03JK7MD2EP",
          "text": "Yes I tried it. I would like to know if it is possible to have a layer-wise profiling on the neural engine ?",
          "ts": "1654708218.410279",
          "thread_ts": "1654706723.937729",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lWUJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes I tried it. I would like to know if it is possible to have a layer-wise profiling on the neural engine ?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "28eb6d0f-6db4-4fcc-ab10-08ca3b4c899f",
          "type": "message",
          "user": "U03JECKH8EQ",
          "text": "Thanks Andrew, we'll take a look",
          "ts": "1654710082.378109",
          "thread_ts": "1654706723.937729",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "z87S",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks Andrew, we'll take a look"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "ec99e222-147e-4804-9755-e76338bcd157",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "We're going to keep this going for a bit longer, any ideas for sample code projects you'd like to see in the future? :thread:",
      "ts": "1654707312.654799",
      "thread_ts": "1654707312.654799",
      "reply_count": 9,
      "latest_reply": "1654709767.238919",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "uD8c",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "We're going to keep this going for a bit longer, any ideas for sample code projects you'd like to see in the future? "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "22297ED9-353B-47E5-A987-133DC363320C",
          "type": "message",
          "user": "U03HZ3L98TF",
          "text": "CoreML/CreateML to be tied in with other frameworks like ARKit, PhotoKit etc ",
          "ts": "1654707437.596789",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "FKP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "CoreML/CreateML"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "be"
                    },
                    {
                      "type": "text",
                      "text": " tied "
                    },
                    {
                      "type": "text",
                      "text": "in"
                    },
                    {
                      "type": "text",
                      "text": " with "
                    },
                    {
                      "type": "text",
                      "text": "other"
                    },
                    {
                      "type": "text",
                      "text": " frameworks "
                    },
                    {
                      "type": "text",
                      "text": "like"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "ARKit,"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "PhotoKit"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "etc"
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "de777188-092c-41e6-8eba-81cd9859b370",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "\u0026gt; \u003c@U03K7TYHFHN\u003e posted yesterday “an image recognition app with live video capture using SwiftUI as a starting point” - for new developers trying to start with Swift UI, converting from UIKit is not so easy\n",
          "ts": "1654707549.350409",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 3,
              "users": [
                "U03HB4T0CA3",
                "U03DJTBMHFF",
                "U03K7TYHFHN"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Yo9gs",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"user\",\"user_id\":\"U03K7TYHFHN\"},{\"type\":\"text\",\"text\":\" posted yesterday \\u201can image recognition app with live video capture using SwiftUI as a starting point\\u201d - for new developers trying to start with Swift UI, converting from UIKit is not so easy\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": []
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "65304fa1-ba6a-4425-b427-17d7e631b6d7",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "Last year's talk \"Extract document data using Vision\" mentioned training a model to recognize page layouts, but didn't go into details. I'd like to see a worked out example of that. My context is word balloons and panels in a comic strip.",
          "ts": "1654707756.050329",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5h36",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Last year's talk \"Extract document data using Vision\" mentioned training a model to recognize page layouts, but didn't go into details. I'd like to see a worked out example of that. My context is word balloons and panels in a comic strip."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e3a6881d-0c44-4d30-bd5a-8bdd83165ceb",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "Not sure of the specific sample code project \u003c@U03JLPLTZ4M\u003e, but here are the links to this year's with Swift as a keyword: \u003chttps://developer.apple.com/sample-code/wwdc/2022/?q=swift\u003e",
          "ts": "1654707786.465009",
          "thread_ts": "1654707312.654799",
          "attachments": [
            {
              "fallback": "Sample Code - WWDC22 - Apple Developer",
              "id": 1,
              "title": "Sample Code - WWDC22 - Apple Developer",
              "title_link": "https://developer.apple.com/sample-code/wwdc/2022/?q=swift",
              "text": "View sample code to see how the latest Apple technologies are implemented.",
              "service_name": "developer.apple.com",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/sample-code/wwdc/2022/?q=swift",
              "original_url": "https://developer.apple.com/sample-code/wwdc/2022/?q=swift",
              "blocks": null
            }
          ],
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "RRw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Not sure of the specific sample code project "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JLPLTZ4M"
                    },
                    {
                      "type": "text",
                      "text": ", but here are the links to this year's with Swift as a keyword: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/sample-code/wwdc/2022/?q=swift",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c2bcdadd-f00d-40a9-a0d7-4254156aad35",
          "type": "message",
          "user": "U03JLPLTZ4M",
          "text": "Here’s the orginal post: \u003chttps://wwdc22.slack.com/archives/C03H4A911EH/p1654635850229399\u003e",
          "ts": "1654707846.743699",
          "thread_ts": "1654707312.654799",
          "attachments": [
            {
              "fallback": "[June 7th, 2022 2:04 PM] Machine Learning - Ask a Question: \u003c@U03K7TYHFHN\u003e asked\n\u0026gt; There was a great WWDC video back in 2018 titled Vision with Core ML. The example app used live video capture to feed an image through the model using scene stability. There is sample code out there for UIKit, but always wanted to try and re-make using SwiftUI as a starting point. Any tips or pointers on making an image recognition app with live video capture using SwiftUI as a starting point ?",
              "id": 1,
              "author_subname": "Machine Learning - Ask a Question",
              "author_link": "https://wwdc22.slack.com/services/B03H04756BH",
              "author_icon": "https://avatars.slack-edge.com/2022-05-29/3590386215573_f35a7fddb4ea9d20cd18_48.png",
              "text": "\u003c@U03K7TYHFHN\u003e asked\n\u0026gt; There was a great WWDC video back in 2018 titled Vision with Core ML. The example app used live video capture to feed an image through the model using scene stability. There is sample code out there for UIKit, but always wanted to try and re-make using SwiftUI as a starting point. Any tips or pointers on making an image recognition app with live video capture using SwiftUI as a starting point ?",
              "from_url": "https://wwdc22.slack.com/archives/C03H4A911EH/p1654635850229399",
              "original_url": "https://wwdc22.slack.com/archives/C03H4A911EH/p1654635850229399",
              "mrkdwn_in": [
                "text"
              ],
              "blocks": null,
              "footer": "Thread in #machine-learning-lounge",
              "ts": 1654635850.229399
            }
          ],
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vCu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Here’s the orginal post: "
                    },
                    {
                      "type": "link",
                      "url": "https://wwdc22.slack.com/archives/C03H4A911EH/p1654635850229399",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "784404aa-5297-4514-86f8-548865142dd9",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "\u003c@U03JLPLTZ4M\u003e Check out this demo app (just dropped!) that goes with  from yesterday's session:\n\n\u003chttps://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed\u003e",
          "ts": "1654707963.811049",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "z+/0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JLPLTZ4M"
                    },
                    {
                      "type": "text",
                      "text": " Check out this demo app (just dropped!) that goes with  from yesterday's session:\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9ed8ffd8-5b1f-456e-b459-60461f0ff32f",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Some high-end GAN and BERT examples would be great:)",
          "ts": "1654709705.269359",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6A=4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Some high-end GAN and BERT examples would be great:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2ea6bf16-fec8-4413-b546-fc691885532b",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Also, I would love to get a NeRF realtime example:)",
          "ts": "1654709722.407849",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YAia",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also, I would love to get a NeRF realtime example:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "00aa4bb6-96f1-4362-8c4d-8c4f1468e965",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "A sample project demonstrating how to take already captured photos, and how to best annotate objects for Object Detection models, would be super cool!",
          "ts": "1654709767.238919",
          "thread_ts": "1654707312.654799",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "q9l4Z",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A sample project demonstrating how to take already captured photos, and how to best annotate objects for Object Detection models, would be super cool!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JK7MD2EP\u003e asked\n\u0026gt; Do you know if it is possible to have a layer-wise execution time profiling with XCode14 for the operations that run on the Neural Engine or GPU?",
      "ts": "1654708380.240039",
      "thread_ts": "1654708380.240039",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 6,
      "latest_reply": "1654809040.165599",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "glE",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JK7MD2EP"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Do you know if it is possible to have a layer-wise execution time profiling with XCode14 for the operations that run on the Neural Engine or GPU?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e3925015-e9ef-4a49-8c17-e1c8f44ec816",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "The new performance reports give a layer-wise break down of compute unit support, but not execution time. Please consider filing some feedback at \u003chttp://feedbackassistant.apple.com\u003e",
          "ts": "1654708468.142619",
          "thread_ts": "1654708380.240039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "BHMad",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The new performance reports give a layer-wise break down of compute unit support, but not execution time. Please consider filing some feedback at "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "add5a667-505e-4a89-81f8-362b70dc1301",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u003c@U03JK7MD2EP\u003e we also recommend setting up a 1:1 lab to discuss this further. We would love to understand your use case and how we can support that. You can mention our names in the description.",
          "ts": "1654708719.322149",
          "thread_ts": "1654708380.240039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=v3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JK7MD2EP"
                    },
                    {
                      "type": "text",
                      "text": " we also recommend setting up a 1:1 lab to discuss this further. We would love to understand your use case and how we can support that. You can mention our names in the description."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "277570b6-8d35-4abd-9ab4-30e61dd36f5b",
          "type": "message",
          "user": "U03JK7MD2EP",
          "text": "\u003c@U03HB4VBDGX\u003e Unfortunately all is booked on Thursday 9 and it will not be possible for me on Friday. Let me know if there is another possibility to scheduled a 1:1",
          "ts": "1654760530.459599",
          "thread_ts": "1654708380.240039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "m2Tf8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " Unfortunately all is booked on Thursday 9 and it will not be possible for me on Friday. Let me know if there is another possibility to scheduled a 1:1"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5d849646-3e98-4bed-99cc-53085ddaf3f5",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Oh no. \u003c@U03DJTBMHFF\u003e \u003c@U03J7UASVEU\u003e any thoughts on how we can proceed in this case?",
          "ts": "1654789370.115229",
          "thread_ts": "1654708380.240039",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "slightly_smiling_face",
              "count": 1,
              "users": [
                "U03JK7MD2EP"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+v1dS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Oh no. "
                    },
                    {
                      "type": "user",
                      "user_id": "U03DJTBMHFF"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J7UASVEU"
                    },
                    {
                      "type": "text",
                      "text": " any thoughts on how we can proceed in this case?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "043f21dc-f1a8-45ad-ac69-4ea3bfa5ccde",
          "type": "message",
          "user": "U03JK7MD2EP",
          "text": "Thanks for taking care \u003c@U03HB4VBDGX\u003e",
          "ts": "1654792336.243679",
          "thread_ts": "1654708380.240039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1Sxr",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for taking care "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "206bec84-ac63-4869-b451-ec97b3514de8",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u003c@U03JK7MD2EP\u003e I heard back from WWDC organizers. Unfortunately, we can't guarantee a 1:1 outside of this week. If you could share your Feedback Assistant number (that starts with FB), we'll follow-up soon. Follow up might just be in the FeedbackAssistant itself if it's minor or warrant a call which we can assess once we see the details.\n\nIf you can make an exception to register for tomorrow's 1:1 lab, it would be great if not, no worries! We will sync via FeedbackAssistant and figure out next steps.",
          "ts": "1654809040.165599",
          "thread_ts": "1654708380.240039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "W5K",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JK7MD2EP"
                    },
                    {
                      "type": "text",
                      "text": " I heard back from WWDC organizers. Unfortunately, we can't guarantee a 1:1 outside of this week. If you could share your Feedback Assistant number (that starts with FB), we'll follow-up soon. Follow up might just be in the FeedbackAssistant itself if it's minor or warrant a call which we can assess once we see the details.\n\nIf you can make an exception to register for tomorrow's 1:1 lab, it would be great if not, no worries! We will sync via FeedbackAssistant and figure out next steps."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JK7MD2EP\u003e asked\n\u0026gt; Do you have some method to share with us to benefit from sparse weight features (very nice features) without sacrificing the applicative performances?",
      "ts": "1654708388.185929",
      "thread_ts": "1654708388.185929",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654708419.299369",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "BkRdk",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JK7MD2EP"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Do you have some method to share with us to benefit from sparse weight features (very nice features) without sacrificing the applicative performances?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "7aacfb5a-3d34-443f-a20f-ecaba4eb7d6e",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "The sparsity weight feature will be useful for models that have been trained with “weight pruning” techniques",
          "ts": "1654708419.299369",
          "thread_ts": "1654708388.185929",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=wccl",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The sparsity weight feature will be useful for models that have been trained with “weight pruning” techniques"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; CoreML Performance Report is great, but I can't find per-layer performance stats to find bottlenecks in our model.",
      "ts": "1654709554.401819",
      "thread_ts": "1654709554.401819",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654896026.800539",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "bYY7",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"CoreML Performance Report is great, but I can't find per-layer performance stats to find bottlenecks in our model.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "0b5f95cd-eaa7-4f7b-9283-0abdb6f350d4",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "The new performance reports offer per-layer compute unit support, but not per-layer timings. One further step you can take to find bottlenecks in the model is to press the \"Open in Instruments\" button where you can see further details in the Core ML Instrument. This won't offer per layer timing details, but it can help find bottlenecks related to data operations and compute unit changes.",
          "ts": "1654709586.210049",
          "thread_ts": "1654709554.401819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "reDR4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The new performance reports offer per-layer compute unit support, but not per-layer timings. One further step you can take to find bottlenecks in the model is to press the \"Open in Instruments\" button where you can see further details in the Core ML Instrument. This won't offer per layer timing details, but it can help find bottlenecks related to data operations and compute unit changes."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5a5ca8ed-07d2-48bb-8212-babd40c927c1",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Thanks, I've seed similar question posted, and I've tried to book a lab session for tomorrow:)",
          "ts": "1654709638.535759",
          "thread_ts": "1654709554.401819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "89VYn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, I've seed similar question posted, and I've tried to book a lab session for tomorrow:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2ac9f55e-23a6-4bfe-9129-b635ae6b8895",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Filed a suggestion for this idea: FB10165588",
          "ts": "1654895877.211879",
          "thread_ts": "1654709554.401819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "AfI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Filed a suggestion for this idea: FB10165588"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "15a62d11-7fb8-41f3-b8a1-32f365b6167f",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Additional suggestions following our lab session:\nFB10165684 — list of layers supported by ANE\nFB10165713 — more articles on ANE optimizations\nFB10165760 — specific flag to let developers know if model running only on ANE",
          "ts": "1654896026.800539",
          "thread_ts": "1654709554.401819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XQPlL",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Additional suggestions following our lab session:\nFB10165684 — list of layers supported by ANE\nFB10165713 — more articles on ANE optimizations\nFB10165760 — specific flag to let developers know if model running only on ANE"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; Is there a way to run performance report on older verions of iOS? I suspect new compiler runs model differently that the Xcode 13 one.",
      "ts": "1654709647.730619",
      "thread_ts": "1654709647.730619",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654709779.296789",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Z0f",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a way to run performance report on older verions of iOS? I suspect new compiler runs model differently that the Xcode 13 one.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "7055ccfe-5a10-4068-90d0-aae412bf9b41",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Performance reports require iOS 16",
          "ts": "1654709676.178829",
          "thread_ts": "1654709647.730619",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mLp=i",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Performance reports require iOS 16"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ba346eeb-ba49-40c5-8b65-1d3f18e8334a",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Older iOS version unfortunately cannot provide the same information.",
          "ts": "1654709779.296789",
          "thread_ts": "1654709647.730619",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "smiling_face_with_tear",
              "count": 1,
              "users": [
                "U03HY66772A"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "O58qw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Older iOS version unfortunately cannot provide the same information."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "d7b8a3da-3800-41f5-9fd3-705b3cf6b824",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "\u003c!here\u003e Thank you all for joining today’s lounge for *Q\u0026amp;A: Core ML*. We're going to close down the Q\u0026amp;A workflow for now. We’ll be back here at 11am PST for _\u003chttps://developer.apple.com/videos/play/wwdc2022-10020|Meet the Presenter: Compose advanced models with Create ML Components\u003e._ \n\nAlso, \u003chttps://developer.apple.com/wwdc22/labs-and-lounges/dashboard/AURCAXYN45/dashboard|don't forget to sign up for 1:1 Labs\u003e! The whole machine learning team is taking appointments throughout the week. It's not too late to submit your requests for Thursday or Friday. We'd love to chat more with you about your specific app needs.",
      "ts": "1654710014.613219",
      "attachments": [
        {
          "fallback": "Apple Developer: Compose advanced models with Create ML Components - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Compose advanced models with Create ML Components - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022-10020",
          "text": "Take your custom machine learning models to the next level with Create ML Components. We'll show you how to work with temporal data like...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6513/6513_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022-10020",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022-10020",
          "blocks": null
        },
        {
          "fallback": "Sign In - Apple",
          "id": 2,
          "title": "Sign In - Apple",
          "title_link": "https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/AURCAXYN45/dashboard",
          "text": "Sign in with your Apple ID",
          "service_name": "idmsa.apple.com",
          "service_icon": "https://appleid.cdn-apple.com/daw/IDMSWebAuth/static/20Apr2022/images/favicon.ico",
          "from_url": "https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/AURCAXYN45/dashboard",
          "original_url": "https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/AURCAXYN45/dashboard",
          "blocks": null
        }
      ],
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 3,
          "users": [
            "U03JRP87THN",
            "U03HY66772A",
            "U03J88L7YJG"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "zR4",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " Thank you all for joining today’s lounge for "
                },
                {
                  "type": "text",
                  "text": "Q\u0026A: Core ML",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": ". We're going to close down the Q\u0026A workflow for now. We’ll be back here at 11am PST for "
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/videos/play/wwdc2022-10020",
                  "text": "Meet the Presenter: Compose advanced models with Create ML Components",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ". ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "\n\nAlso, "
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/AURCAXYN45/dashboard",
                  "text": "don't forget to sign up for 1:1 Labs"
                },
                {
                  "type": "text",
                  "text": "! The whole machine learning team is taking appointments throughout the week. It's not too late to submit your requests for Thursday or Friday. We'd love to chat more with you about your specific app needs."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRQ81NEL\u003e asked\n\u0026gt; What is a good approach in case of Image classification problem. I am trying to classify two similar shapes lets say Circle and Oval, in some case the confidence for the Ovel is very high for the circle input. ",
      "ts": "1654710569.459129",
      "thread_ts": "1654710569.459129",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654732779.670999",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "ylj",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRQ81NEL"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What is a good approach in case of Image classification problem. I am trying to classify two similar shapes lets say Circle and Oval, in some case the confidence for the Ovel is very high for the circle input. \"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "4343e88c-cb35-4959-9fdd-707ccd16a012",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Have you looked at VNDetectContourDetection? Using this traditional computer Vision approach might give you better results.",
          "ts": "1654710630.943599",
          "thread_ts": "1654710569.459129",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KNFK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Have you looked at VNDetectContourDetection? Using this traditional computer Vision approach might give you better results."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "36789BED-CBB5-4793-B948-A2199749F08D",
          "type": "message",
          "user": "U03JRQ81NEL",
          "text": "Thanks, will look into it",
          "ts": "1654732779.670999",
          "thread_ts": "1654710569.459129",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qU65",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": ","
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "will"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "look"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "into"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "it"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JML9Q51T\u003e asked\n\u0026gt; Vision question: does a VNRectangleObservation contain information about the shape's full outline?\n\u0026gt; \n\u0026gt; For example: a document scanner that needs to fully extract a page from its background.  VNDetectRectanglesRequest will provide the position of each corner of the page, which allows us to clip the shape assuming the page is flat.  But if the paper is curled, we can end up with bits of background in our cropped image.\n\u0026gt; \n\u0026gt; Is there a way to trace an accurate outline for imperfect rectangles?",
      "ts": "1654710681.414309",
      "thread_ts": "1654710681.414309",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654715906.465459",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "bT=v",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JML9Q51T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Vision question: does a VNRectangleObservation contain information about the shape's full outline?\\n\\nFor example: a document scanner that needs to fully extract a page from its background.  VNDetectRectanglesRequest will provide the position of each corner of the page, which allows us to clip the shape assuming the page is flat.  But if the paper is curled, we can end up with bits of background in our cropped image.\\n\\nIs there a way to trace an accurate outline for imperfect rectangles?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "44b0dde7-891e-4b13-8180-d2c3b26c8600",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "For imperfect documents, you might want to look at VNDetectDocumentSegmentationRequest and they combine it with a contour detection on the globalSegmentationMask that you get as a result",
          "ts": "1654711012.407559",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Rt+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "For imperfect documents, you might want to look at VNDetectDocumentSegmentationRequest and they combine it with a contour detection on the globalSegmentationMask that you get as a result"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3C8AE349-A7F6-4B9C-A0BF-70346A226D0D",
          "type": "message",
          "user": "U03JML9Q51T",
          "text": "Can you explain how VNDetectDocumentSegmentationRequest differs from VNDetectRectanglesRequest? The result types look similar - does VNDetectDocumentSegmentationRequest somehow rely on text being on the page?\n\nMy real app scans papers that don't necessarily contain any text - so not a true “document”.",
          "ts": "1654713606.674789",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/BJVH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Can"
                    },
                    {
                      "type": "text",
                      "text": " you "
                    },
                    {
                      "type": "text",
                      "text": "explain"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "how"
                    },
                    {
                      "type": "text",
                      "text": " VNDetectDocumentSegmentationRequest "
                    },
                    {
                      "type": "text",
                      "text": "differs"
                    },
                    {
                      "type": "text",
                      "text": " from "
                    },
                    {
                      "type": "text",
                      "text": "VNDetectRectanglesRequest?"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "The"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "result"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "types"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "look"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "similar"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "-"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "does"
                    },
                    {
                      "type": "text",
                      "text": " VNDetectDocumentSegmentationRequest "
                    },
                    {
                      "type": "text",
                      "text": "somehow"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "rely"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "being"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " page"
                    },
                    {
                      "type": "text",
                      "text": "?"
                    },
                    {
                      "type": "text",
                      "text": "\n\n"
                    },
                    {
                      "type": "text",
                      "text": "My"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "real"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "app"
                    },
                    {
                      "type": "text",
                      "text": " scans papers "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " don't necessarily "
                    },
                    {
                      "type": "text",
                      "text": "contain"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "any"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "-"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "so"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "not"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "true"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "“document”."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "06e2208c-1ff5-47bd-b572-7ed58e18d432",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "DocumentSegmentation is ML based and trained on all kinds of documents, labels, papers, etc. Rectangle detection is a traditional algorithm that works of edges that intersect to form a quad.",
          "ts": "1654715189.418169",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mKWv=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "DocumentSegmentation is ML based and trained on all kinds of documents, labels, papers, etc. Rectangle detection is a traditional algorithm that works of edges that intersect to form a quad."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0710FF0A-38C4-46B4-93A9-04D068D37E0F",
          "type": "message",
          "user": "U03JML9Q51T",
          "text": "I'm having a hard time finding any mention of globalSegmentationMask online, aside from the property itself in the docs. Can you provide some clarity on how this might be used for contour detection? Thanks for the help.",
          "ts": "1654715340.283119",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rwJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I'm having "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "hard"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "time"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "finding"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "any"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "mention"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "globalSegmentationMask"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "online,"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "aside"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "from"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "property"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "itself"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "in"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "docs."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Can"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "you"
                    },
                    {
                      "type": "text",
                      "text": " provide "
                    },
                    {
                      "type": "text",
                      "text": "some"
                    },
                    {
                      "type": "text",
                      "text": " clarity "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "how"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "this"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "might"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "be"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "used"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "contour"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "detection?"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "help."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "A9E9AD6C-0DC3-474D-B9C0-B99910D10CB0",
          "type": "message",
          "user": "U03JML9Q51T",
          "text": "It's not clear to me which region of the image this pixel buffer would contain.",
          "ts": "1654715398.166559",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HWR/8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It's not "
                    },
                    {
                      "type": "text",
                      "text": "clear"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "me"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "which"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "region"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "image"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "this"
                    },
                    {
                      "type": "text",
                      "text": " pixel "
                    },
                    {
                      "type": "text",
                      "text": "buffer"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "would"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "contain."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b742b453-a838-4eea-bd21-eff85b26c4cf",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "\u003chttps://developer.apple.com/documentation/vision/vndetectedobjectobservation/3798796-globalsegmentationmask?language=objc\u003e",
          "ts": "1654715838.410609",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5qX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/vision/vndetectedobjectobservation/3798796-globalsegmentationmask?language=objc",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ed20d738-7ef9-4a81-bce4-75ea50c81d6d",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "It is a low res pixel buffer that represents the shape of the detected document. Where each pixel represents a confidence of being or not being part of the document",
          "ts": "1654715906.465459",
          "thread_ts": "1654710681.414309",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "swB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It is a low res pixel buffer that represents the shape of the detected document. Where each pixel represents a confidence of being or not being part of the document"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "3ee6024c-e703-492d-80d7-94d15729636d",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*This lounge is about to go live!* Join us here, in 5 minutes for _*Meet the Presenters: Compose advanced models with Create ML Components*_. We have a great group of engineers, including the presenter David from the Machine Learning team, ready to answer all your questions about Create ML Components. Feel free to bring questions you may have about how to bring machine learning to your app. It's going to be a fun conversation. Looking forward to your questions!\n\n\u003chttps://developer.apple.com/videos/play/wwdc2022/10020/\u003e",
      "ts": "1654710935.066399",
      "attachments": [
        {
          "fallback": "Apple Developer: Compose advanced models with Create ML Components - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Compose advanced models with Create ML Components - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022/10020/",
          "text": "Take your custom machine learning models to the next level with Create ML Components. We'll show you how to work with temporal data like...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6513/6513_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022/10020/",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022/10020/",
          "blocks": null
        }
      ],
      "edited": {
        "user": "U03J7UASVEU",
        "ts": "1654710957.000000"
      },
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "TH4nI",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "This lounge is about to go live!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " Join us here, in 5 minutes for "
                },
                {
                  "type": "text",
                  "text": "Meet the Presenters: Compose advanced models with Create ML Components",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ". We have a great group of engineers, including the presenter David from the Machine Learning team, ready to answer all your questions about Create ML Components. Feel free to bring questions you may have about how to bring machine learning to your app. It's going to be a fun conversation. Looking forward to your questions!\n\n"
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/videos/play/wwdc2022/10020/",
                  "text": ""
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e added a workflow to this channel: *Machine Learning - Ask a Question*.",
      "ts": "1654711020.142459",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e added a workflow to this channel: *Machine Learning - Idea Submission*.",
      "ts": "1654711028.832179",
      "subtype": "bot_message",
      "bot_id": "B03HBPSE4R3",
      "username": "Machine Learning - Idea Submission",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "client_msg_id": "3d2333df-7220-47b9-9929-e8d6d5b94f31",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Let's get started!* Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows.",
      "ts": "1654711211.365779",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Nrd",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Let's get started! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "62ff07d1-62a5-430c-9237-cbf5c7d677aa",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Open Discussion:* To get things going, who has already watched the \"Compose advanced models with Create ML Components\" session? What are you most excited about? Reply in the thread… :thread:",
      "ts": "1654711237.487549",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "MiBMy",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Open Discussion:",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " To get things going, who has already watched the \"Compose advanced models with Create ML Components\" session? What are you most excited about? Reply in the thread… "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "d820ca54-4608-43a1-856f-ce0e2782c903",
      "type": "message",
      "user": "U03JFGMTU8G",
      "text": "Hi :wave: I'm David, looking forward to your ideas and questions!",
      "ts": "1654711437.603929",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "NwRJ0",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Hi "
                },
                {
                  "type": "emoji",
                  "name": "wave",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " I'm David, looking forward to your ideas and questions!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; There are a lot of CoreMLCompiler versions throughout the Xcode history. Some break inference (e.g. some of the Xcode 13 coremlcompilers broke iOS 14 runtime). Is there a way to diagnose these errors without compiling under every compiler and running on all iOS devices?) And is there a known stable version of a compiler?",
      "ts": "1654711454.747469",
      "thread_ts": "1654711454.747469",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 15,
      "latest_reply": "1654898323.811469",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "D2J",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"There are a lot of CoreMLCompiler versions throughout the Xcode history. Some break inference (e.g. some of the Xcode 13 coremlcompilers broke iOS 14 runtime). Is there a way to diagnose these errors without compiling under every compiler and running on all iOS devices?) And is there a known stable version of a compiler?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "98a9a115-da44-4f89-9299-b13efa2c376c",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u0026gt; Some break inference (e.g. some of the Xcode 13 coremlcompilers broke iOS 14 runtime). \nMaintaining backward compatibility is really important for us. We would like to understand this better. Could you file a bug report on \u003chttp://feedbackassistant.apple.com|feedbackassistant.apple.com\u003e? It it works for you, could you please set up a 1:1 lab with Apple engineers so we can understand the issue better?",
          "ts": "1654711801.167879",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "x5q",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Some break inference (e.g. some of the Xcode 13 coremlcompilers broke iOS 14 runtime). \"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Maintaining backward compatibility is really important for us. We would like to understand this better. Could you file a bug report on "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": "feedbackassistant.apple.com"
                    },
                    {
                      "type": "text",
                      "text": "? It it works for you, could you please set up a 1:1 lab with Apple engineers so we can understand the issue better?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ff16be41-db4e-4f65-97ee-707081384dcb",
          "type": "message",
          "user": "U03HY66772A",
          "text": "I've booked a session for CoreML Performance, do I need to book another one?",
          "ts": "1654711907.148579",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+OQn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I've booked a session for CoreML Performance, do I need to book another one?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "156db3ea-cf5a-4f8d-9fa9-0cb53a9dbb4d",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u003c@U03J7UASVEU\u003e we might need another session to discuss this. Could a developer request more than 1 sessions?",
          "ts": "1654711982.279449",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U03HY66772A"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "OBcUg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J7UASVEU"
                    },
                    {
                      "type": "text",
                      "text": " we might need another session to discuss this. Could a developer request more than 1 sessions?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f57e87e2-3bd9-4c4b-abf2-e26b5eb97e2d",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "\u003c@U03HB4VBDGX\u003e I would recommend requesting multiple labs.",
          "ts": "1654712109.152989",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U03HY66772A"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/2Bc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " I would recommend requesting multiple labs."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "163231ba-29f8-4a02-acd3-2d3e9c53fc48",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Should I reference you, \u003c@U03HB4VBDGX\u003e?",
          "ts": "1654712222.141749",
          "thread_ts": "1654711454.747469",
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654712226.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mfuf9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Should I reference you, "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": "?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "53f47ac2-683a-4034-9bf5-44a47df18eef",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Thanks! Looking forward to chatting with you, Viacheslav.",
          "ts": "1654712222.726129",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "z6Q1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks! Looking forward to chatting with you, Viacheslav."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9b5a637a-f94b-4279-8d18-f76a0725e601",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Yes, you could add my name in the description / notes.",
          "ts": "1654712234.870239",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zOfU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, you could add my name in the description / notes."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c6c742e3-d51e-4f5b-a678-f06435383c72",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "\u003c@U03HY66772A\u003e We'll also make a note when scheduling :slightly_smiling_face:",
          "ts": "1654712254.016709",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "sFFFL",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HY66772A"
                    },
                    {
                      "type": "text",
                      "text": " We'll also make a note when scheduling "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0fbccc3e-24e1-45f9-a966-288ed1a53f31",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Ah, looks like I can't request another session:(",
          "ts": "1654712286.881759",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "gXGe",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah, looks like I can't request another session:("
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "69af4eeb-394b-4ba3-840c-be3269b5d4b8",
          "type": "message",
          "user": "U03HY66772A",
          "text": "I can cancel previous one (and request another?)",
          "ts": "1654712313.109669",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5qaN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I can cancel previous one (and request another?)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0ff317cb-d22f-465d-8647-d83736f4f65c",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "\u003c@U03HY66772A\u003e our labs are still open for Thursday/Friday, are you able to request for those days? I would not cancel at this point.",
          "ts": "1654712349.265199",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YbI2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HY66772A"
                    },
                    {
                      "type": "text",
                      "text": " our labs are still open for Thursday/Friday, are you able to request for those days? I would not cancel at this point."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "810ed49f-1b9a-49cd-a547-a530c620084e",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Done, thanks!",
          "ts": "1654712577.893459",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U03J7UASVEU"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0aO3u",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Done, thanks!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2ab38083-aaf9-4921-a7f7-321a38e3a405",
          "type": "message",
          "user": "U03HY66772A",
          "text": "\u003c@U03HB4VBDGX\u003e Filed a feedback: FB10165528",
          "ts": "1654895285.664989",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "FK0r",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " Filed a feedback: FB10165528"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "66fdaf59-ab0b-42b8-94d8-9136aab76d28",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Thanks for sharing this! Let's continue discussion on this.",
          "ts": "1654895348.818489",
          "thread_ts": "1654711454.747469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "UDnOv",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for sharing this! Let's continue discussion on this."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "82b06bb0-318f-42df-8b98-f421d3f0a359",
          "type": "message",
          "user": "U03HY66772A",
          "text": "\u003c@U03HB4VBDGX\u003e I’ve requested a contact, btw",
          "ts": "1654898323.811469",
          "thread_ts": "1654711454.747469",
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654898541.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vXx2h",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " I’ve requested a contact, btw"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "bfa012ff-c06e-4061-a2d8-8d0e9aa289f3",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "If you are watching along with us, we are about to build a human action repetition counter with Create ML Components. Feel free to check out \u003chttps://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed|our sample code project as well\u003e! :thread:",
      "ts": "1654711777.220389",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "hjN",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "If you are watching along with us, we are about to build a human action repetition counter with Create ML Components. Feel free to check out "
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed",
                  "text": "our sample code project as well"
                },
                {
                  "type": "text",
                  "text": "! "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J20E7UBV\u003e asked\n\u0026gt; Is `VideoReader` a `CoreML` component, or is this a custom component, as referenced in #wwdc2022-10020, built on top of the new async `AVImageGenerator`?",
      "ts": "1654711779.300909",
      "thread_ts": "1654711779.300909",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654716132.158719",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "RN6",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20E7UBV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is `VideoReader` a `CoreML` component, or is this a custom component, as referenced in #wwdc2022-10020, built on top of the new async `AVImageGenerator`?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b1b4c7ea-c09e-4aba-a596-af97f489698f",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "It is a ~custom~ built-in Create ML component .",
          "ts": "1654711806.794229",
          "thread_ts": "1654711779.300909",
          "edited": {
            "user": "U03J4CASP0R",
            "ts": "1654712006.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HOI51",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It is a "
                    },
                    {
                      "type": "text",
                      "text": "custom",
                      "style": {
                        "strike": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " built-in Create ML component ."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a5f838c2-9475-46a8-b1bd-7836522e7b1e",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "Thanks, \u003c@U03J4CASP0R\u003e!",
          "ts": "1654711911.031649",
          "thread_ts": "1654711779.300909",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XpL",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J4CASP0R"
                    },
                    {
                      "type": "text",
                      "text": "!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c89bf5f3-876e-4b96-bd7c-60377c337349",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "\u003chttps://developer.apple.com/documentation/createmlcomponents/videoreader\u003e",
          "ts": "1654712002.384419",
          "thread_ts": "1654711779.300909",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "V+O2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/videoreader",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "00ee336c-fcdd-4034-b7b4-1d72e1e9722b",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "Ah, thanks, \u003c@U03HRMWNP4J\u003e!  I was searching in the general `CreateML` docs and not the `CreateML Components`.",
          "ts": "1654712256.312049",
          "thread_ts": "1654711779.300909",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bzF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah, thanks, "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HRMWNP4J"
                    },
                    {
                      "type": "text",
                      "text": "!  I was searching in the general "
                    },
                    {
                      "type": "text",
                      "text": "CreateML",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " docs and not the "
                    },
                    {
                      "type": "text",
                      "text": "CreateML Components",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "75a43917-a0e8-4d36-8f27-17cabe8938c7",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Hi Brandon, can you point me at where you found async `AVImageGenerator` ?  Was it in a session this year? I'd like to check whether it would be another good way to generate frames for CreateML.",
          "ts": "1654713136.742679",
          "thread_ts": "1654711779.300909",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Qg0T",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Brandon, can you point me at where you found async "
                    },
                    {
                      "type": "text",
                      "text": "AVImageGenerator",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " ?  Was it in a session this year? I'd like to check whether it would be another good way to generate frames for CreateML."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "bc415ee2-5b67-4daa-896d-a89cc5dacb97",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "\u003c@U03HB4T0CA3\u003e I noticed the latest changes to `AVAssetImageGenerator` (sorry, I mistakenly referred to it as `AVImageGenerator` above) in the documentation here; \u003chttps://developer.apple.com/documentation/avfoundation/avassetimagegenerator?changes=latest_minor\u003e\n\nIt seems like there are two new methods;\n`func image(at time: CMTime) async throws -\u0026gt; (image: CGImage, actualTime: CMTime)`\n\n`func images(for times: [CMTime]) -\u0026gt; AVAssetImageGenerator.Images`\n\nThat can take a `CMTime` or array of `CMTime`s and asynchronously return a `CGImage` or an `AsyncSequence` that includes the generated image(s).",
          "ts": "1654715096.951099",
          "thread_ts": "1654711779.300909",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tCW",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4T0CA3"
                    },
                    {
                      "type": "text",
                      "text": " I noticed the latest changes to "
                    },
                    {
                      "type": "text",
                      "text": "AVAssetImageGenerator",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " (sorry, I mistakenly referred to it as "
                    },
                    {
                      "type": "text",
                      "text": "AVImageGenerator",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " above) in the documentation here; "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/avfoundation/avassetimagegenerator?changes=latest_minor",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\nIt seems like there are two new methods;\n"
                    },
                    {
                      "type": "text",
                      "text": "func image(at time: CMTime) async throws -\u003e (image: CGImage, actualTime: CMTime)",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n\n"
                    },
                    {
                      "type": "text",
                      "text": "func images(for times: [CMTime]) -\u003e AVAssetImageGenerator.Images",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n\nThat can take a "
                    },
                    {
                      "type": "text",
                      "text": "CMTime",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " or array of "
                    },
                    {
                      "type": "text",
                      "text": "CMTime",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "s and asynchronously return a "
                    },
                    {
                      "type": "text",
                      "text": "CGImage",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " or an "
                    },
                    {
                      "type": "text",
                      "text": "AsyncSequence",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " that includes the generated image(s)."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "C27316F5-8368-4814-9674-8D0CFA6182FF",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Thanks for the recommendation Brandon, I will enjoy watching that.",
          "ts": "1654716132.158719",
          "thread_ts": "1654711779.300909",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "MzI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "recommendation"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Brandon,"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "will"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "enjoy"
                    },
                    {
                      "type": "text",
                      "text": " watching "
                    },
                    {
                      "type": "text",
                      "text": "that."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03K19A2324\u003e asked\n\u0026gt; Hi @David! Congratulations on a fine presentation. May I ask if there is some functionality to enable to recognise the direction (arrow) of time in a video?",
      "ts": "1654711821.416229",
      "thread_ts": "1654711821.416229",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 6,
      "latest_reply": "1654713611.799329",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "WEX7",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03K19A2324"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi @David! Congratulations on a fine presentation. May I ask if there is some functionality to enable to recognise the direction (arrow) of time in a video?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "099032c4-7618-4853-b6d5-c081accbb2aa",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Thank you! I'm so glad that you enjoyed the session. Can you clarify your problem? You may be able to detect the direction of play in a video without machine learning.",
          "ts": "1654711900.597769",
          "thread_ts": "1654711821.416229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Dy2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you! I'm so glad that you enjoyed the session. Can you clarify your problem? You may be able to detect the direction of play in a video without machine learning."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "04d5d744-343a-4551-abea-8f528b446dd0",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Yes \u003c@U03JFGMTU8G\u003e! It's actually a pretty recent research question but it has high value and I was wondering if you have implemented some kind of relevant functionality. It's not so much about detection the direction of play itself but recognising time itself in a video sequence. For example, suppose you have a video of traffic (or even the gym training data in your nice presentation :slightly_smiling_face: ) and that the flow of time is scrambled sometimes going forward and sometimes in reverse. Can we recognise the flow of time at each time point?",
          "ts": "1654712295.662459",
          "thread_ts": "1654711821.416229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "87XfI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JFGMTU8G"
                    },
                    {
                      "type": "text",
                      "text": "! It's actually a pretty recent research question but it has high value and I was wondering if you have implemented some kind of relevant functionality. It's not so much about detection the direction of play itself but recognising time itself in a video sequence. For example, suppose you have a video of traffic (or even the gym training data in your nice presentation "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " ) and that the flow of time is scrambled sometimes going forward and sometimes in reverse. Can we recognise the flow of time at each time point?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "40ff4724-77a4-4c91-9eda-a7c4fa9520ce",
          "type": "message",
          "user": "U03K19A2324",
          "text": "In an actual application, the problem would be simpler: Find the direction of time is some video data (is it as the video plays or the reverse?)",
          "ts": "1654712383.633139",
          "thread_ts": "1654711821.416229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mia/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In an actual application, the problem would be simpler: Find the direction of time is some video data (is it as the video plays or the reverse?)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1d97d9e5-fdb1-4ef8-8905-3f3485cabfc8",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Of course the more symmetric the video motion is the harder will be to tell the difference:\nFor example, in a video data representing flowing water the direction of time is clearly defined (is there some functionality in Create ML to find it?). But in a video that, let's say zooms in in an Apple Maps windows, playing it in reverse would be equivalent to zooming out, so this anti-symmetry does not allows us to have a clearly defined direction of time.",
          "ts": "1654712663.551349",
          "thread_ts": "1654711821.416229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "uwvR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Of course the more symmetric the video motion is the harder will be to tell the difference:\nFor example, in a video data representing flowing water the direction of time is clearly defined (is there some functionality in Create ML to find it?). But in a video that, let's say zooms in in an Apple Maps windows, playing it in reverse would be equivalent to zooming out, so this anti-symmetry does not allows us to have a clearly defined direction of time."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9192c22e-685f-437a-ab5e-ab9fefd1a0b3",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Interesting problem! You may find understanding the apparent motion of pixels in the image a useful input. You can compute optical flow using Vision \u003chttps://developer.apple.com/documentation/vision/vngenerateopticalflowrequest|VNGenerateOpticalFlowRequest \u003e",
          "ts": "1654712897.567759",
          "thread_ts": "1654711821.416229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kj4a9",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Interesting problem! You may find understanding the apparent motion of pixels in the image a useful input. You can compute optical flow using Vision "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest",
                      "text": "VNGenerateOpticalFlowRequest "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "524a3845-8f39-40a1-a808-b95245d145e3",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Hmm thanks \u003c@U03HRMWNP4J\u003e! That is a very useful input indeed! However, one would still have to interpret these motions as representing something going forward in actual time or backwards (like decide if water is flowing forwards, out of a faucet, or backwards, into the faucet). My guess would be entropy estimation/learning would be critical in making such a decision.\n\nAs I wrote, it is still a pretty recent research question (for example, this is an ERC grand given last year to further study it in detail\n\u003chttps://cordis.europa.eu/project/id/950086\u003e\n) but it has important practical implications that it would be very nice to see any such functionality implemented in Create ML, to be able to working with temporal (agnostic) data.",
          "ts": "1654713611.799329",
          "thread_ts": "1654711821.416229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "B8Nh4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hmm thanks "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HRMWNP4J"
                    },
                    {
                      "type": "text",
                      "text": "! That is a very useful input indeed! However, one would still have to interpret these motions as representing something going forward in actual time or backwards (like decide if water is flowing forwards, out of a faucet, or backwards, into the faucet). My guess would be entropy estimation/learning would be critical in making such a decision.\n\nAs I wrote, it is still a pretty recent research question (for example, this is an ERC grand given last year to further study it in detail\n"
                    },
                    {
                      "type": "link",
                      "url": "https://cordis.europa.eu/project/id/950086",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n) but it has important practical implications that it would be very nice to see any such functionality implemented in Create ML, to be able to working with temporal (agnostic) data."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "772bf730-04b5-498e-8484-6829dc7226ca",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "If you are watching along, we're learning about Sound Classification :sound: :thread:",
      "ts": "1654711910.944189",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "cwKs",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "If you are watching along, we're learning about Sound Classification "
                },
                {
                  "type": "emoji",
                  "name": "sound",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPTG8BS\u003e asked\n\u0026gt; Hi! I was surprised that the session video did not discuss (unless I missed it, which is definitely possible) how to write custom components. Let's say I want to write my own PoseSelector component. For example to pick the person closest to the center of the frame, and to keep the selection consistent across frames in a video.  Can I? And if yes, how?",
      "ts": "1654711964.814379",
      "thread_ts": "1654711964.814379",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 9,
      "latest_reply": "1654713211.890669",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "2fdV",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPTG8BS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi! I was surprised that the session video did not discuss (unless I missed it, which is definitely possible) how to write custom components. Let's say I want to write my own PoseSelector component. For example to pick the person closest to the center of the frame, and to keep the selection consistent across frames in a video.  Can I? And if yes, how?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "f3392c64-b3ca-4f71-b15d-128518b7f5d1",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Hello! Thanks for your question. You can definitely build your own custom components. In the session _Get to know Create ML Components_, \u003chttps://developer.apple.com/videos/play/wwdc2022/10019/\u003e, Alejandro provides an example of how to do it by building a saliency transformer. All you need to do is to conform to the `Transformer` protocol!",
          "ts": "1654712133.480769",
          "thread_ts": "1654711964.814379",
          "attachments": [
            {
              "fallback": "Apple Developer: Get to know Create ML Components - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Get to know Create ML Components - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10019/",
              "text": "Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6512/6512_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KDy",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hello! Thanks for your question. You can definitely build your own custom components. In the session "
                    },
                    {
                      "type": "text",
                      "text": "Get to know Create ML Components",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ", "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": ", Alejandro provides an example of how to do it by building a saliency transformer. All you need to do is to conform to the "
                    },
                    {
                      "type": "text",
                      "text": "Transformer",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " protocol!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "80927590-9229-46a2-a127-e75b66aa6d62",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "Ok, great, thank you!",
          "ts": "1654712431.408909",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "b/4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ok, great, thank you!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "677c8da5-e831-45df-836a-3f8843b16b20",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "\u003chttps://developer.apple.com/documentation/createmlcomponents/transformer\u003e",
          "ts": "1654712547.049689",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kisQz",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/transformer",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a0388a0d-c587-45a4-b605-9bf973f592f8",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "here is the protocol for Transformer",
          "ts": "1654712554.134139",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "pray",
              "count": 1,
              "users": [
                "U03JRPTG8BS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "VOa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "here is the protocol for Transformer"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f64db2a3-3ea7-4daf-9322-2f8c70fb8675",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "The protocol has a lot of methods. I assume/hope I would not have to implement all of them for a custom transformer?",
          "ts": "1654712875.424779",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "C/xt6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The protocol has a lot of methods. I assume/hope I would not have to implement all of them for a custom transformer?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "21385150-5166-423f-aa0a-759c09055152",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Good question! The only required method for you to implement is `applied`",
          "ts": "1654713051.961589",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1qa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Good question! The only required method for you to implement is "
                    },
                    {
                      "type": "text",
                      "text": "applied",
                      "style": {
                        "code": true
                      }
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "16e8889b-8d74-4adc-889c-c2a19cc81f38",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "\u003chttps://developer.apple.com/documentation/createmlcomponents/transformer/applied(to:eventhandler:)-38h86\u003e",
          "ts": "1654713058.628149",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "UWk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/transformer/applied(to:eventhandler:)-38h86",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "11eb9d39-c87d-41ad-b883-d6a31b70b5ea",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "search for `Required` on the page",
          "ts": "1654713090.504139",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "pray",
              "count": 1,
              "users": [
                "U03JRPTG8BS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Tr6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "search for "
                    },
                    {
                      "type": "text",
                      "text": "Required",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " on the page"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "42c80791-bfd8-44bf-8955-5f68047fb366",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "Perfect!",
          "ts": "1654713211.890669",
          "thread_ts": "1654711964.814379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ePjw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Perfect!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J20E7UBV\u003e asked\n\u0026gt; Just wanted to say thank you for such thorough content and for building a solution that makes machine learning easy to implement in our apps.  This is incredibly powerful, and so many solutions can have such a steep learning curve.  CreateML and CoreML are truly amazing; great job to you all!",
      "ts": "1654712254.625179",
      "thread_ts": "1654712254.625179",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654712377.481849",
      "reactions": [
        {
          "name": "heart",
          "count": 8,
          "users": [
            "U03J4CASP0R",
            "U03JFGMTU8G",
            "U03HRMABBDZ",
            "U03HRMAAYM9",
            "U03J7UASVEU",
            "U03HB4T0CA3",
            "U03HB4XJENT",
            "U03JLPLTZ4M"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "p6LT",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20E7UBV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Just wanted to say thank you for such thorough content and for building a solution that makes machine learning easy to implement in our apps.  This is incredibly powerful, and so many solutions can have such a steep learning curve.  CreateML and CoreML are truly amazing; great job to you all!\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e7dc1e75-aef5-4a09-8496-7c529dd87adf",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Thank you for your positive feedback! :smile:",
          "ts": "1654712377.481849",
          "thread_ts": "1654712254.625179",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mwj6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you for your positive feedback! "
                    },
                    {
                      "type": "emoji",
                      "name": "smile",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "16eb4413-56d3-476d-a844-5399952c0ff4",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Alright, we've wrapped up the watch party on our end, keep those questions coming!",
      "ts": "1654712666.919509",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Fol",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Alright, we've wrapped up the watch party on our end, keep those questions coming!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPTG8BS\u003e asked\n\u0026gt; In addition to the built-in classifiers and regressors, is it possible to specify a neural network with a custom structure (by specifying the layers) and use that for training and inference?",
      "ts": "1654712775.901679",
      "thread_ts": "1654712775.901679",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 9,
      "latest_reply": "1654713842.614969",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "3G/m=",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPTG8BS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"In addition to the built-in classifiers and regressors, is it possible to specify a neural network with a custom structure (by specifying the layers) and use that for training and inference?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "f2e61891-67dc-40bd-9eaa-914e0b37c84b",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "You can use a FullyConnectedNetworkClassifier or regressor, which support specifying the number of hidden layers.",
          "ts": "1654712814.152109",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ddtu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can use a FullyConnectedNetworkClassifier or regressor, which support specifying the number of hidden layers."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "78ae6db4-78c0-4e8c-8889-3c4294998304",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "other network architectures would require using Metal or Accelerate",
          "ts": "1654712834.937089",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XJnGK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "other network architectures would require using Metal or Accelerate"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b7e3e7a0-b4ba-443c-8bae-d3e704fa7b34",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "also number of hidden units in each layer for fully connected network",
          "ts": "1654713179.352279",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "btA4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "also number of hidden units in each layer for fully connected network"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9191f7ac-c4e7-4233-bf22-93b320268280",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "That’s nice that you can configure arbitrary MLPs :+1:",
          "ts": "1654713333.471729",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2oP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That’s nice that you can configure arbitrary MLPs "
                    },
                    {
                      "type": "emoji",
                      "name": "+1",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "40990349-b7f2-49cd-8bdc-30b1ba80bbf6",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "Or is it actually an MLP? Are there any nonlinearities between the fully connected layers? Such as ReLUs?",
          "ts": "1654713459.981589",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "t8D",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Or is it actually an MLP? Are there any nonlinearities between the fully connected layers? Such as ReLUs?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "71b18926-1edc-4c57-9946-4e84fafb1d42",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Yes, there are ReLUs on every hidden layer.",
          "ts": "1654713518.877799",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nR1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, there are ReLUs on every hidden layer."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2e186989-cf6b-4365-8fcc-f14e04282a1c",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "Ok, thanks. And it seems the activation function isn’t configurable, but hardcoded to ReLU? Which is probably fine for most use cases.",
          "ts": "1654713605.901909",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "e4R",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ok, thanks. And it seems the activation function isn’t configurable, but hardcoded to ReLU? Which is probably fine for most use cases."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e4e68a2a-9fa4-45be-a09a-657ffc5b339b",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "no, it’s not configurable, only ReLu",
          "ts": "1654713680.945289",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JRPTG8BS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hVEq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "no, it’s not configurable, only ReLu"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8775b4ad-4268-4582-b5c5-3ffca9b9815f",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "If you have a use case please file a ticket in Feedback Assistant",
          "ts": "1654713842.614969",
          "thread_ts": "1654712775.901679",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "ok",
              "count": 1,
              "users": [
                "U03JRPTG8BS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "92f3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you have a use case please file a ticket in Feedback Assistant"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPTG8BS\u003e asked\n\u0026gt; Does training (using the fitted or update methods) take advantage of the Neural Engine, for example when training a fully connected classifier?",
      "ts": "1654713193.971519",
      "thread_ts": "1654713193.971519",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654713273.555849",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Bow",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPTG8BS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Does training (using the fitted or update methods) take advantage of the Neural Engine, for example when training a fully connected classifier?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "8f0a58a9-d9ba-48a9-8f91-3219beec686d",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "Not on neural engine, but training of fully connected network is  optimized on best possible compute unit",
          "ts": "1654713273.555849",
          "thread_ts": "1654713193.971519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "whr",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Not on neural engine, but training of fully connected network is  optimized on best possible compute unit"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J6AKH19V\u003e asked\n\u0026gt; Whats the meaning of the error below when calling model.predict()?\n\u0026gt; \n\u0026gt; RuntimeError: {\n\u0026gt;     NSLocalizedDescription = \"Error in declaring network.\";\n\u0026gt; }",
      "ts": "1654713266.902559",
      "thread_ts": "1654713266.902559",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654717050.812539",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "XBm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J6AKH19V"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Whats the meaning of the error below when calling model.predict()?\\n\\nRuntimeError: {\\n    NSLocalizedDescription = \\\"Error in declaring network.\\\";\\n}\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "d83163e4-5831-4739-b760-0a5974daf6e9",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "This indicates that there is an error setting up the model. Can you please file a feedback report with code to reproduce the issue? Also can you verify if this issue reproduces with different compute unit options used when loading the model?",
          "ts": "1654713394.313979",
          "thread_ts": "1654713266.902559",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J6AKH19V"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4BiTI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This indicates that there is an error setting up the model. Can you please file a feedback report with code to reproduce the issue? Also can you verify if this issue reproduces with different compute unit options used when loading the model?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e94cc6b4-5c2f-41d0-a2a9-4ad108833aef",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "\u003c@U03JFGMUPCY\u003e is there a workaround for `torch.stft` and `torch.istft` (which are unsupported)?",
          "ts": "1654717050.812539",
          "thread_ts": "1654713266.902559",
          "edited": {
            "user": "U03J6AKH19V",
            "ts": "1654717072.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hf6g",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JFGMUPCY"
                    },
                    {
                      "type": "text",
                      "text": " is there a workaround for "
                    },
                    {
                      "type": "text",
                      "text": "torch.stft",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "text",
                      "text": "torch.istft",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " (which are unsupported)?"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J4CL2WMA\u003e asked\n\u0026gt; Great updates. How do I go about training a dance classifier with with video files? Are there any components for audio, video to get started with?",
      "ts": "1654713333.803089",
      "thread_ts": "1654713333.803089",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654713513.809149",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "+gM+6",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J4CL2WMA"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Great updates. How do I go about training a dance classifier with with video files? Are there any components for audio, video to get started with?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e55ee9c5-879c-4a54-a632-0392fc8b0e44",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Thank you! You can definitely build a dance classifier using the action classifier in Create ML \u003chttps://developer.apple.com/videos/play/wwdc2020/10043/\u003e",
          "ts": "1654713513.809149",
          "thread_ts": "1654713333.803089",
          "attachments": [
            {
              "fallback": "Apple Developer: Build an Action Classifier with Create ML - WWDC20 - Videos - Apple Developer",
              "id": 1,
              "title": "Build an Action Classifier with Create ML - WWDC20 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2020/10043/",
              "text": "Discover how to build Action Classification models in Create ML. With a custom action classifier, your app can recognize and understand...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/49/3438/3438_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2020/10043/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2020/10043/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "VwIm",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you! You can definitely build a dance classifier using the action classifier in Create ML "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2020/10043/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JA6H3Z38\u003e asked\n\u0026gt; With the initializers for MLImageClassifier.ModelParameters being deprecated, what is the easiest way of increasing the iterations being performed?",
      "ts": "1654713365.593609",
      "thread_ts": "1654713365.593609",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 5,
      "latest_reply": "1654715139.669269",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "4g+Ur",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JA6H3Z38"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"With the initializers for MLImageClassifier.ModelParameters being deprecated, what is the easiest way of increasing the iterations being performed?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "eccffd38-e0e0-4f84-bcd0-b4858f68f26c",
          "type": "message",
          "user": "U03HB4XKB8X",
          "text": "`MLImageClassifier.ModelParameters` still has the\n```init(\nvalidation: ValidationData,\nmaxIterations: Int,\naugmentation: ImageAugmentationOptions, algorithm: ModelAlgorithmType\n)```\n initializer which you can use to set the maxIterations along with other parameters that you want.",
          "ts": "1654713645.594029",
          "thread_ts": "1654713365.593609",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Q6Qtw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MLImageClassifier.ModelParameters",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " still has the\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"init(\\nvalidation: ValidationData,\\nmaxIterations: Int,\\naugmentation: ImageAugmentationOptions, algorithm: ModelAlgorithmType\\n)\"}],\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": " initializer which you can use to set the maxIterations along with other parameters that you want."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "45eb7fc7-9a91-43fd-b412-54aa43a0cec9",
          "type": "message",
          "user": "U03JA6H3Z38",
          "text": "When I try to only assign the maxIterations, XCode complains that the function is deprecated, is that the expected behavior?",
          "ts": "1654714200.362909",
          "thread_ts": "1654713365.593609",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "p0yq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "When I try to only assign the maxIterations, XCode complains that the function is deprecated, is that the expected behavior?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7ddb40a2-75e0-49bc-aba1-7fc6a4ab8ac6",
          "type": "message",
          "user": "U03HB4XKB8X",
          "text": "There are a few initializers that were deprecated so if you are trying to use one of those old ones to set the maxIterations, you will get this warning.\n\nHowever, are you trying to use this exact initializer and still getting a warning?",
          "ts": "1654714397.143909",
          "thread_ts": "1654713365.593609",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "d0W45",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are a few initializers that were deprecated so if you are trying to use one of those old ones to set the maxIterations, you will get this warning.\n\nHowever, are you trying to use this exact initializer and still getting a warning?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d9dd3ddf-cac6-43ab-9285-5391dc7d22bd",
          "type": "message",
          "user": "U03JA6H3Z38",
          "text": "This is how I am accessing it MLImageClassifier.ModelParameters(maxIterations: 50)",
          "ts": "1654714675.072869",
          "thread_ts": "1654713365.593609",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XoB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This is how I am accessing it MLImageClassifier.ModelParameters(maxIterations: 50)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9b1ce92b-fe97-4c0c-a63f-141b85c591f0",
          "type": "message",
          "user": "U03HB4XKB8X",
          "text": "Yeah because the initializer that I am suggesting has no default value for `augmentation` . So, when you call this initializer you need to pass the `augmentation` parameter as well. If you don't want to set any `augmentation`, you can just pass a value of 0 for this parameter.\n\nIf you do not specify `augmentation` in the init at all, one of the old initializers will be used and hence you will get the deprecated warning.",
          "ts": "1654715139.669269",
          "thread_ts": "1654713365.593609",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JA6H3Z38"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KVu7W",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yeah because the initializer that I am suggesting has no default value for "
                    },
                    {
                      "type": "text",
                      "text": "augmentation",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " . So, when you call this initializer you need to pass the "
                    },
                    {
                      "type": "text",
                      "text": "augmentation",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " parameter as well. If you don't want to set any "
                    },
                    {
                      "type": "text",
                      "text": "augmentation",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ", you can just pass a value of 0 for this parameter.\n\nIf you do not specify "
                    },
                    {
                      "type": "text",
                      "text": "augmentation",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " in the init at all, one of the old initializers will be used and hence you will get the deprecated warning."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; Is there a way to use one IOSurface for moth ANE and GPU work? Or access ANE iosurface directly, and map it to MTLTexture by hand?",
      "ts": "1654713602.839369",
      "thread_ts": "1654713602.839369",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 17,
      "latest_reply": "1654730864.778929",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "LIT9",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a way to use one IOSurface for moth ANE and GPU work? Or access ANE iosurface directly, and map it to MTLTexture by hand?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "2b5595e2-99ed-47be-aff2-b4e059dd1f44",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "IOSurface-backed `CVPixelBuffer` with `OneComponent16Half` pixel format type can be shared with Neural Engine without copy. Likewise, `MLMultiArray` which is backed by the pixel buffer can also be shared without copy. (See `-[MLMultiArray initWithPixelBuffer:shape:]`).\n\nFor input features, using these objects in MLFeatureValue is enough to take advantage of the efficient data processing. When the output feature type matches the type above, CoreML automatically uses these objects in the output feature values as well.\n\nFor output features, you can even request the neural engine to write into your own buffer directly. See `-[MLPredictionOptions outputBackings]`.\n\nYou can create `MTLTexture` view into the pixel buffer with `CVMetalTextureCache`.",
          "ts": "1654713800.047229",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "fire",
              "count": 2,
              "users": [
                "U03HY66772A",
                "U03JF5PC27R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mvD8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "IOSurface-backed "
                    },
                    {
                      "type": "text",
                      "text": "CVPixelBuffer",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " with "
                    },
                    {
                      "type": "text",
                      "text": "OneComponent16Half",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " pixel format type can be shared with Neural Engine without copy. Likewise, "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " which is backed by the pixel buffer can also be shared without copy. (See "
                    },
                    {
                      "type": "text",
                      "text": "-[MLMultiArray initWithPixelBuffer:shape:]",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ").\n\nFor input features, using these objects in MLFeatureValue is enough to take advantage of the efficient data processing. When the output feature type matches the type above, CoreML automatically uses these objects in the output feature values as well.\n\nFor output features, you can even request the neural engine to write into your own buffer directly. See "
                    },
                    {
                      "type": "text",
                      "text": "-[MLPredictionOptions outputBackings]",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ".\n\nYou can create "
                    },
                    {
                      "type": "text",
                      "text": "MTLTexture",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " view into the pixel buffer with "
                    },
                    {
                      "type": "text",
                      "text": "CVMetalTextureCache",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "DF4B1652-E20B-4228-873A-3D5AB3289B08",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Amazing, thanks a lot for your help!",
          "ts": "1654715566.092229",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qqJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Amazing, thanks a lot for your help!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9892DE35-613D-4B33-9148-B4050717AA9C",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "\u003c@U03J52T5J22\u003e \n\nis there any chance you will add support for at least 4 channels no-copy buffers?\n\nthese features are really cool and long-awaited but almost all cv models produce or take as input rgb or n channels data",
          "ts": "1654726451.870269",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "LgAQX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "\n\n"
                    },
                    {
                      "type": "text",
                      "text": "is"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "there"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "any"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "chance"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "you"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "will"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "add"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "support"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "at"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "least"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "4"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "channels"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "no-copy"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "buffers?"
                    },
                    {
                      "type": "text",
                      "text": "\n\n"
                    },
                    {
                      "type": "text",
                      "text": "these"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "features"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "are"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "really"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "cool"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "long-awaited"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "but"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "almost"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "all"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "cv"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "models"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "produce"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "take"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "as"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "input"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "rgb"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "n"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "channels"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "data"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e1116744-617b-4186-b7e6-ba55ac77ff30",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "\u0026gt;  is there any chance you will add support for at least 4 channels no-copy buffers?\nOne way is to stack each channel on height axis and use that big pixel buffer as a backing of MLMultiArray. So:\n```let backingPixelBuffer = ... // size = (width, height * 4), format = kCVPixelFormatType_OneComponent16Half\nlet multiArray = MLMultiArray(pixelBuffer: backingPixelBuffer, shape: [4, height, width])```\nThis won’t work if your image representation is so called “packed” format, where each channel is interleaved in the frame buffer. We appreciate your feedback assistant report with a use case if that’s what you are looking for.",
          "ts": "1654727475.327869",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "boom",
              "count": 1,
              "users": [
                "U03JF5PC27R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vrK",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\" is there any chance you will add support for at least 4 channels no-copy buffers?\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "One way is to stack each channel on height axis and use that big pixel buffer as a backing of MLMultiArray. So:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"let backingPixelBuffer = ... \\/\\/ size = (width, height * 4), format = kCVPixelFormatType_OneComponent16Half\\nlet multiArray = MLMultiArray(pixelBuffer: backingPixelBuffer, shape: [4, height, width])\"}],\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This won’t work if your image representation is so called “packed” format, where each channel is interleaved in the frame buffer. We appreciate your feedback assistant report with a use case if that’s what you are looking for."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f1ca5c11-b3d8-4294-8f00-89d91fcd3e2e",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "\u003c@U03J52T5J22\u003e ugh, that's not gonna work for my rgb-rgb models\n\nare `float16` `MLMultiArray` no-copy also? will there be a copy event if I specify user-allocated `MLMultuArray` with float16 data into `outputBackings`?",
          "ts": "1654728608.912879",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "unP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " ugh, that's not gonna work for my rgb-rgb models\n\nare "
                    },
                    {
                      "type": "text",
                      "text": "float16",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " no-copy also? will there be a copy event if I specify user-allocated "
                    },
                    {
                      "type": "text",
                      "text": "MLMultuArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " with float16 data into "
                    },
                    {
                      "type": "text",
                      "text": "outputBackings",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5d902757-0d59-4910-85e5-537bddcf6a88",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "my goal is to share a memory between `MPSImage` and `MLModel` output so there is a seamless ANE-to-GPU transition",
          "ts": "1654728764.738439",
          "thread_ts": "1654713602.839369",
          "edited": {
            "user": "U03JF5PC27R",
            "ts": "1654728777.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "t7K",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "my goal is to share a memory between "
                    },
                    {
                      "type": "text",
                      "text": "MPSImage",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "text",
                      "text": "MLModel",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " output so there is a seamless ANE-to-GPU transition"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "13ad0053-dd90-4ecb-b166-8f2c638a69aa",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "The neural engine is unable to write into `MLMultiArray` directly unless it was initialized with an IOSurface-backed pixel buffer. (`-initWithPixelBuffer:shape:`)",
          "ts": "1654729068.943479",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lng7i",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The neural engine is unable to write into "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " directly unless it was initialized with an IOSurface-backed pixel buffer. ("
                    },
                    {
                      "type": "text",
                      "text": "-initWithPixelBuffer:shape:",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ")"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "006911fb-5180-4612-b55c-b3982079a56d",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "is direct writing to IOSurface-backed buffers compatible with flexible shape input/outputs?",
          "ts": "1654729140.068919",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kio",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "is direct writing to IOSurface-backed buffers compatible with flexible shape input/outputs?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6E4FD0B4-F9E1-474C-A1CE-F346E8007B12",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Hello, Andrew:) I’ve guessed you'll be interested in this thread:)",
          "ts": "1654729458.676039",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "y2/3U",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hello, Andrew:) I’ve guessed you'll be interested in this thread:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e0936e5b-baf9-45cb-88eb-e39ce27d4f6e",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "\u003c@U03J52T5J22\u003e friendly tagging you so the question doesn't get lost in thread notifications :innocent:",
          "ts": "1654729465.610129",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0nq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " friendly tagging you so the question doesn't get lost in thread notifications "
                    },
                    {
                      "type": "emoji",
                      "name": "innocent",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ed4e1548-02e5-4081-acaf-56e332d88472",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "\u003c@U03HY66772A\u003e these insights are crazy valuable, really appreciate the opportunity",
          "ts": "1654729551.855409",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U03HY66772A"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "u6/Km",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HY66772A"
                    },
                    {
                      "type": "text",
                      "text": " these insights are crazy valuable, really appreciate the opportunity"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "74373980-97e5-4099-a179-7d6d8fb14731",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "\u0026gt; is direct writing to IOSurface-backed buffers compatible with flexible shape input/outputs?\nAs the application needs to prepare the output buffer in `-outputBackings` property, it must know the shape of the output before invoking the inference. Some flexible shape models (e.g. enumerated input shapes) meet the criteria, but so called “dynamic shape”, where the output shape is dynamically determined by the network, won’t work.",
          "ts": "1654729610.972949",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4UhIp",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"is direct writing to IOSurface-backed buffers compatible with flexible shape input\\/outputs?\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "\nAs the application needs to prepare the output buffer in "
                    },
                    {
                      "type": "text",
                      "text": "-outputBackings",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " property, it must know the shape of the output before invoking the inference. Some flexible shape models (e.g. enumerated input shapes) meet the criteria, but so called “dynamic shape”, where the output shape is dynamically determined by the network, won’t work."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "10e4caf1-dbca-4d2a-a4db-9afe3ee2474c",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "yup, enumerated shapes is something I'm targeting for",
          "ts": "1654729659.007559",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "wwLbk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "yup, enumerated shapes is something I'm targeting for"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "727bb013-e86c-4fe2-85ff-13d804148dfb",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "so If I have both input and output with an enumaratedly flexible shape I can pass one of pre-allocated buffer pairs and still no copy will occur?",
          "ts": "1654729766.897149",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EP0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "so If I have both input and output with an enumaratedly flexible shape I can pass one of pre-allocated buffer pairs and still no copy will occur?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7fe73a08-665f-479a-b404-aeb28962654b",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "Sorry, I do not have a definitive answer. I would suggest to test it with the latest Instruments, which \u003chttps://developer.apple.com/videos/play/wwdc2022/10027/|shows the buffer copy event in the analysis\u003e and send feedback assistant request if the outcome is not desired.",
          "ts": "1654730371.099589",
          "thread_ts": "1654713602.839369",
          "attachments": [
            {
              "fallback": "Apple Developer: Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "text": "Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6520/6520_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "q=x",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sorry, I do not have a definitive answer. I would suggest to test it with the latest Instruments, which "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
                      "text": "shows the buffer copy event in the analysis"
                    },
                    {
                      "type": "text",
                      "text": " and send feedback assistant request if the outcome is not desired."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4b1f3e62-c0bf-4332-b669-11407bada81a",
          "type": "message",
          "user": "U03JF5PC27R",
          "text": "\u003c@U03J52T5J22\u003e thanks for quick and excellent answers! it was crazy useful and insightful",
          "ts": "1654730773.491659",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "L56",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " thanks for quick and excellent answers! it was crazy useful and insightful"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5b238872-3b6b-457d-ba69-7362aa6cb85b",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "You are welcome, Andrew and Viacheslav! Enjoy the rest of WWDC 2022!!",
          "ts": "1654730864.778929",
          "thread_ts": "1654713602.839369",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 2,
              "users": [
                "U03HY66772A",
                "U03JF5PC27R"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "sGf1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You are welcome, Andrew and Viacheslav! Enjoy the rest of WWDC 2022!!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVCXR1T8\u003e asked\n\u0026gt; Hi! As far as I understand, extracting text from images is not possible for Arabic language, would it be possible to use CreateML to achieve the same effect that is built in to extract Arabic text from images and documents? thanks",
      "ts": "1654713812.344759",
      "thread_ts": "1654713812.344759",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654714552.669109",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "y2CHm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVCXR1T8"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi! As far as I understand, extracting text from images is not possible for Arabic language, would it be possible to use CreateML to achieve the same effect that is built in to extract Arabic text from images and documents? thanks\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "361a2452-7c67-4616-a1ef-08111a2b0e34",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Hi \u003c@U03HVCXR1T8\u003e.  You are right about Arabic support; While Apple announced more language support for Live Text this year, Arabic was not one of them.  The complete list is here:\n\n\u003chttps://www.apple.com/ios/feature-availability/#live-text-live-text\u003e",
          "ts": "1654714172.496339",
          "thread_ts": "1654713812.344759",
          "attachments": [
            {
              "fallback": "Apple: iOS and iPadOS - Feature Availability",
              "id": 1,
              "title": "iOS and iPadOS - Feature Availability",
              "title_link": "https://www.apple.com/ios/feature-availability/#live-text-live-text",
              "text": "iOS and iPadOS has so many amazing features. Find out which ones are available on iPhone, iPad, and iPod touch in your country or region.",
              "image_url": "https://www.apple.com/ac/structured-data/images/open_graph_logo.png?202205251312",
              "service_name": "Apple",
              "service_icon": "https://www.apple.com/favicon.ico",
              "from_url": "https://www.apple.com/ios/feature-availability/#live-text-live-text",
              "original_url": "https://www.apple.com/ios/feature-availability/#live-text-live-text",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hLXMk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HVCXR1T8"
                    },
                    {
                      "type": "text",
                      "text": ".  You are right about Arabic support; While Apple announced more language support for Live Text this year, Arabic was not one of them.  The complete list is here:\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://www.apple.com/ios/feature-availability/#live-text-live-text",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ff66fdd5-55df-4d46-9156-1ad97e2cd3e8",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "It's not possible to extend Live Text to add additional user languages at this time.\n\nTo build your own system would require solving multiple ML problems, including locating text in the image, decomposing it into graphemes (characters), and to be robust it should probably include some sort of spelling/grammar layer to reduce transcription errors.\n\nBuilding a complete solution like this is beyond what CreateML is designed for today.",
          "ts": "1654714552.669109",
          "thread_ts": "1654713812.344759",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qH4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It's not possible to extend Live Text to add additional user languages at this time.\n\nTo build your own system would require solving multiple ML problems, including locating text in the image, decomposing it into graphemes (characters), and to be robust it should probably include some sort of spelling/grammar layer to reduce transcription errors.\n\nBuilding a complete solution like this is beyond what CreateML is designed for today."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "e689459a-ce74-4c08-a390-6ee8f82760f6",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "There's still a few more minutes to get your questions answered! Don't be shy, we're happy to help.",
      "ts": "1654714275.638999",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "/+Uo+",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "There's still a few more minutes to get your questions answered! Don't be shy, we're happy to help."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J20E7UBV\u003e asked\n\u0026gt; Is there any guidance on using CreateML for creating segmentation models (great for use in ARKit to segment unique types of objects as detected in the camera feed)?  Or would this be a case for building a custom Turi/other model and converting to CoreML?",
      "ts": "1654714324.858189",
      "thread_ts": "1654714324.858189",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654714533.624369",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "A0LB",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20E7UBV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there any guidance on using CreateML for creating segmentation models (great for use in ARKit to segment unique types of objects as detected in the camera feed)?  Or would this be a case for building a custom Turi\\/other model and converting to CoreML?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "92ee36a7-a161-445c-a61c-05cade61fc41",
          "type": "message",
          "user": "U03JFGMTU8G",
          "text": "Thanks for you question! The first thing I would try is the DeepLabV3 model!  \u003chttps://developer.apple.com/machine-learning/models\u003e You can also convert your custom model using coremltools. \u003chttps://coremltools.readme.io/docs\u003e",
          "ts": "1654714533.624369",
          "thread_ts": "1654714324.858189",
          "attachments": [
            {
              "fallback": "Models - Machine Learning - Apple Developer",
              "id": 1,
              "title": "Models - Machine Learning - Apple Developer",
              "title_link": "https://developer.apple.com/machine-learning/models",
              "text": "Build intelligence into your apps using machine learning models from the research community designed for Core ML.",
              "service_name": "developer.apple.com",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/machine-learning/models",
              "original_url": "https://developer.apple.com/machine-learning/models",
              "blocks": null
            },
            {
              "fallback": "coremltools: Introduction",
              "id": 2,
              "title": "Introduction",
              "title_link": "https://coremltools.readme.io/docs",
              "text": "Use coremltools to convert models from third-party libraries to Core ML.",
              "service_name": "coremltools",
              "from_url": "https://coremltools.readme.io/docs",
              "original_url": "https://coremltools.readme.io/docs",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "gratitude-thank-you",
              "count": 1,
              "users": [
                "U03J20E7UBV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "E31wd",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for you question! The first thing I would try is the DeepLabV3 model!  "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/machine-learning/models",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " You can also convert your custom model using coremltools. "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPTG8BS\u003e asked\n\u0026gt; In the \"Get to know Create ML Components\" session around the 14m mark the presenter mentioned that the augmentation applied is only used during training, not validation. Is that really true, given that it was just applied using flatMap() to the combined dataset in the code shown? It is not what I would expect based on reading the code.",
      "ts": "1654714345.754229",
      "thread_ts": "1654714345.754229",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654714869.208689",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "22Au",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPTG8BS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"In the \\\"Get to know Create ML Components\\\" session around the 14m mark the presenter mentioned that the augmentation applied is only used during training, not validation. Is that really true, given that it was just applied using flatMap() to the combined dataset in the code shown? It is not what I would expect based on reading the code.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "dc409597-d24c-444f-a7f2-36165aa27228",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "It applies for training and validation data, but only at training time, not when doing predictions.",
          "ts": "1654714379.285029",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Xvg9t",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It applies for training and validation data, but only at training time, not when doing predictions."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "102e25a6-d9ef-4b11-828a-693e4ffffb63",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "Ah yes, I misunderstood “predictions” as “validation”. Thank you.\nApplying augmentations during validation seems not ideal though? Don’t you typically want to test/validate on unadulterated input data?",
          "ts": "1654714519.015729",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03J4CASP0R",
                "U03K67T11G8"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "aFf7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah yes, I misunderstood “predictions” as “validation”. Thank you.\nApplying augmentations during validation seems not ideal though? Don’t you typically want to test/validate on unadulterated input data?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "86793dc4-f76b-47df-a677-444e5caceaff",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "In other words, the augmentations are not part of the task definition. They are applied directly to the data.",
          "ts": "1654714526.752429",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=bVDF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In other words, the augmentations are not part of the task definition. They are applied directly to the data."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c22231a5-24ae-4e16-aefd-f1801813ad07",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "That's a good point!",
          "ts": "1654714564.367279",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "FWYY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That's a good point!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f5dfd530-cd1d-473d-8d38-0c3da75e6c2b",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "In terms of the overall design, I think it might be better if the augmentation transform could be passed to the fitting or update method, so that you can have random augmentations that are different for each epoch, rather than baking a fixed set of augmentations into the training data.",
          "ts": "1654714691.806909",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Sx9wE",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In terms of the overall design, I think it might be better if the augmentation transform could be passed to the fitting or update method, so that you can have random augmentations that are different for each epoch, rather than baking a fixed set of augmentations into the training data."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "83665bc2-42d6-4080-810a-ea038da39abd",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "I mean that the outputs of the augmentations would be different for each epoch.",
          "ts": "1654714762.286229",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4VhkZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I mean that the outputs of the augmentations would be different for each epoch."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2a25b484-fcff-419f-b3ba-ff8ca7aad830",
          "type": "message",
          "user": "U03J4CASP0R",
          "text": "Yes. Please share your feedback through Feedback Assistant.",
          "ts": "1654714869.208689",
          "thread_ts": "1654714345.754229",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JRPTG8BS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "jwhaT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes. Please share your feedback through Feedback Assistant."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVCXR1T8\u003e asked\n\u0026gt; Is there a way create a federated learning/training solution. I'm planning to create on-device training and since the data is private I'd want to update the model while respecting user privacy and have all users benefit from a new model I can redistribute in updates. Is there a way to achieve this? Especially while respecting user data privacy.",
      "ts": "1654714684.057579",
      "thread_ts": "1654714684.057579",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654714730.046949",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "t1s",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVCXR1T8"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a way create a federated learning\\/training solution. I'm planning to create on-device training and since the data is private I'd want to update the model while respecting user privacy and have all users benefit from a new model I can redistribute in updates. Is there a way to achieve this? Especially while respecting user data privacy.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "3fdd4efc-0d66-4a11-8811-838eac583afc",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Hi \u003c@U03HVCXR1T8\u003e thanks for the question.\n\nApple doesn't offer an out-of-box solution for federated learning at the moment. Differential privacy team at Apple published this article to discuss how to think about this problem: \u003chttps://machinelearning.apple.com/research/learning-with-privacy-at-scale\u003e\n\nI would recommend scheduling a 1:1 lab with us so we can discuss your use case and provide specific feedback.",
          "ts": "1654714730.046949",
          "thread_ts": "1654714684.057579",
          "attachments": [
            {
              "fallback": "Apple Machine Learning Research: Learning with Privacy at Scale",
              "id": 1,
              "title": "Learning with Privacy at Scale",
              "title_link": "https://machinelearning.apple.com/research/learning-with-privacy-at-scale",
              "text": "Understanding how people use their devices often helps in improving the user experience. However, accessing the data that provides such…",
              "image_url": "https://mlr.cdn-apple.com/media/open_Graph_d53fcef327.jpg",
              "service_name": "Apple Machine Learning Research",
              "service_icon": "https://machinelearning.apple.com/favicon.ico",
              "from_url": "https://machinelearning.apple.com/research/learning-with-privacy-at-scale",
              "original_url": "https://machinelearning.apple.com/research/learning-with-privacy-at-scale",
              "blocks": null
            }
          ],
          "edited": {
            "user": "U03HB4VBDGX",
            "ts": "1654714741.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "wT2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HVCXR1T8"
                    },
                    {
                      "type": "text",
                      "text": " thanks for the question.\n\nApple doesn't offer an out-of-box solution for federated learning at the moment. Differential privacy team at Apple published this article to discuss how to think about this problem: "
                    },
                    {
                      "type": "link",
                      "url": "https://machinelearning.apple.com/research/learning-with-privacy-at-scale",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\nI would recommend scheduling a 1:1 lab with us so we can discuss your use case and provide specific feedback."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "ba926d41-cb00-4ead-9c05-91c13a842f26",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Thank you all for joining today’s lounge for _*Meet the Presenters: Compose Advanced Models with Create ML Components*._ We're going to close down the Q\u0026amp;A workflow for now, but there are still opportunities to sign up for our labs and lounges for the rest of the week!\n\nWe'll be back here tomorrow from 2-3 for Q\u0026amp;A: Vision. Looking forward to meeting some of you in the labs later today and throughout the week! Don't forget to sign up for activities on Thursday and Friday with the links below.\n\n\u003chttps://developer.apple.com/wwdc22/labs/|Lab Signup\u003e\n\u003chttps://developer.apple.com/wwdc22/digital-lounges/|Slack Lounges\u003e",
      "ts": "1654715014.468069",
      "attachments": [
        {
          "fallback": "WWDC22 Labs",
          "id": 1,
          "title": "WWDC22 Labs",
          "title_link": "https://developer.apple.com/wwdc22/labs/",
          "text": "Throughout the week of the conference, you can request an appointment with an Apple engineer for personal technical help.",
          "image_url": "https://developer.apple.com/news/images/og/wwdc22/wwdc22-og-v3.jpg",
          "service_name": "developer.apple.com",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/wwdc22/labs/",
          "original_url": "https://developer.apple.com/wwdc22/labs/",
          "blocks": null
        },
        {
          "fallback": "WWDC22 Digital Lounges",
          "id": 2,
          "title": "WWDC22 Digital Lounges",
          "title_link": "https://developer.apple.com/wwdc22/digital-lounges/",
          "text": "Join Apple engineers and designers throughout the week as they host text-based Q\u0026amp;As and special activities related to developer tools, SwiftUI, accessibility, and machine learning.",
          "image_url": "https://developer.apple.com/news/images/og/wwdc22/wwdc22-og-v3.jpg",
          "service_name": "developer.apple.com",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/wwdc22/digital-lounges/",
          "original_url": "https://developer.apple.com/wwdc22/digital-lounges/",
          "blocks": null
        }
      ],
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "eyes",
          "count": 1,
          "users": [
            "U03J6AKH19V"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "F5CJ",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Thank you all for joining today’s lounge for "
                },
                {
                  "type": "text",
                  "text": "Meet the Presenters: Compose Advanced Models with Create ML Components",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ".",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": " We're going to close down the Q\u0026A workflow for now, but there are still opportunities to sign up for our labs and lounges for the rest of the week!\n\nWe'll be back here tomorrow from 2-3 for Q\u0026A: Vision. Looking forward to meeting some of you in the labs later today and throughout the week! Don't forget to sign up for activities on Thursday and Friday with the links below.\n\n"
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/wwdc22/labs/",
                  "text": "Lab Signup"
                },
                {
                  "type": "text",
                  "text": "\n"
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/wwdc22/digital-lounges/",
                  "text": "Slack Lounges"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e removed a workflow from this channel: *Machine Learning - Idea Submission*.",
      "ts": "1654715025.280069",
      "subtype": "bot_message",
      "bot_id": "B03HBPSE4R3",
      "username": "Machine Learning - Idea Submission",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e removed a workflow from this channel: *Machine Learning - Ask a Question*.",
      "ts": "1654715034.034139",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "type": "message",
      "text": "\u003c@U03J20E7UBV\u003e asked\n\u0026gt; When building an Object Detection model, do you have any specific tools or recommendations on how to best annotate objects?  There are a lot of tools out there, but they often feel cumbersome to use in comparison to the ease of CreateML.",
      "ts": "1654715124.317189",
      "thread_ts": "1654715124.317189",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654760120.200629",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "cNS",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20E7UBV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"When building an Object Detection model, do you have any specific tools or recommendations on how to best annotate objects?  There are a lot of tools out there, but they often feel cumbersome to use in comparison to the ease of CreateML.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "7d8a1d46-9827-465e-b5ec-0fa34bcb9475",
          "type": "message",
          "user": "U03HB4V9Q5V",
          "text": "Check out \u003chttps://developer.apple.com/documentation/createml/building-an-object-detector-data-source\u003e for documentation on how to structure annotated data for object detection. Apple doesn't have a tool we can provide to create the annotations, and I agree it can be a bit of a cumbersome process. Hope that helps!",
          "ts": "1654715216.277719",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "gratitude-thank-you",
              "count": 1,
              "users": [
                "U03J20E7UBV"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6ey",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Check out "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createml/building-an-object-detector-data-source",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " for documentation on how to structure annotated data for object detection. Apple doesn't have a tool we can provide to create the annotations, and I agree it can be a bit of a cumbersome process. Hope that helps!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9f7a108d-aa1e-4608-bfc7-22f95fb3bf50",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "That is very helpful, thank you!",
          "ts": "1654715312.160129",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fdUg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That is very helpful, thank you!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e871ac0a-2bd2-4902-b1cd-ace44f2344ee",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "\u003c@U03J20E7UBV\u003e Take a look at \u003chttps://roboflow.com|Roboflow\u003e",
          "ts": "1654715483.736989",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "z6t",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J20E7UBV"
                    },
                    {
                      "type": "text",
                      "text": " Take a look at "
                    },
                    {
                      "type": "link",
                      "url": "https://roboflow.com",
                      "text": "Roboflow"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f04f9d77-ee45-4b32-b9a9-3db0dde373f5",
          "type": "message",
          "user": "U03JRPTG8BS",
          "text": "Though if their public plan doesn’t work for you, then it is quite expensive.",
          "ts": "1654715629.828379",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "c3uo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Though if their public plan doesn’t work for you, then it is quite expensive."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1bba78b4-9666-4e8b-9ad7-744a11b48155",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "Thanks, \u003c@U03JRPTG8BS\u003e!  I will take a look!  Definitely not familiar with Roboflow!",
          "ts": "1654716053.991929",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4mBcp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JRPTG8BS"
                    },
                    {
                      "type": "text",
                      "text": "!  I will take a look!  Definitely not familiar with Roboflow!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f0d852da-41c9-4bd2-87b3-e1b1d82fb6c2",
          "type": "message",
          "user": "U03K67T11G8",
          "text": "\u003c@U03J20E7UBV\u003e \u003chttps://labelstud.io/|Label Studio\u003e is another one to check out, it’s open source.",
          "ts": "1654717461.690839",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Sg+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J20E7UBV"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "link",
                      "url": "https://labelstud.io/",
                      "text": "Label Studio"
                    },
                    {
                      "type": "text",
                      "text": " is another one to check out, it’s open source."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "16cab442-7084-420e-973f-6b9e4569e700",
          "type": "message",
          "user": "U03J20E7UBV",
          "text": "Thanks, \u003c@U03K67T11G8\u003e!  Also not familiar with that one, either!",
          "ts": "1654718622.804709",
          "thread_ts": "1654715124.317189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "oZG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, "
                    },
                    {
                      "type": "user",
                      "user_id": "U03K67T11G8"
                    },
                    {
                      "type": "text",
                      "text": "!  Also not familiar with that one, either!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "79a6af0e-6a81-42b0-bbc3-67657f4fe286",
          "type": "message",
          "user": "U03JZAYNMMJ",
          "text": "\u003c@U03J20E7UBV\u003e We're using CVAT at my research institution. There is a hosted, free version available at \u003chttp://cvat.org|cvat.org\u003e, but I'd recommend going with the self-hosted option since it allows you to integrate ML models to assist with the labeling effort: \u003chttps://github.com/openvinotoolkit/cvat\u003e",
          "ts": "1654760120.200629",
          "thread_ts": "1654715124.317189",
          "attachments": [
            {
              "fallback": "GitHub: GitHub - openvinotoolkit/cvat: Powerful and efficient Computer Vision Annotation Tool (CVAT)",
              "id": 1,
              "title": "GitHub - openvinotoolkit/cvat: Powerful and efficient Computer Vision Annotation Tool (CVAT)",
              "title_link": "https://github.com/openvinotoolkit/cvat",
              "text": "Powerful and efficient Computer Vision Annotation Tool (CVAT) - GitHub - openvinotoolkit/cvat: Powerful and efficient Computer Vision Annotation Tool (CVAT)",
              "image_url": "https://repository-images.githubusercontent.com/139156354/cd9b3435-498b-4412-87c7-00e5e0a5d64c",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/openvinotoolkit/cvat",
              "original_url": "https://github.com/openvinotoolkit/cvat",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lvYrD",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J20E7UBV"
                    },
                    {
                      "type": "text",
                      "text": " We're using CVAT at my research institution. There is a hosted, free version available at "
                    },
                    {
                      "type": "link",
                      "url": "http://cvat.org",
                      "text": "cvat.org"
                    },
                    {
                      "type": "text",
                      "text": ", but I'd recommend going with the self-hosted option since it allows you to integrate ML models to assist with the labeling effort: "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/openvinotoolkit/cvat",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e added a workflow to this channel: *Machine Learning - Ask a Question*.",
      "ts": "1654745909.333259",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "type": "message",
      "text": "\u003c@U03J7UASVEU\u003e added a workflow to this channel: *Machine Learning - Idea Submission*.",
      "ts": "1654745919.323599",
      "subtype": "bot_message",
      "bot_id": "B03HBPSE4R3",
      "username": "Machine Learning - Idea Submission",
      "replace_original": false,
      "delete_original": false,
      "blocks": null
    },
    {
      "client_msg_id": "e565435d-5287-4df6-9ef7-d413aba70903",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "\u003c!here\u003e :eyes: We’re also going to try something different for our friends outside of the US...we’re going to leave the :workflowbolt: enabled so you can go ahead and submit your questions for any of our lounge topics tomorrow (or any lingering ML and Computer Vision questions we can answer).  We’re all going to sign off for the night but we’ll answer your questions throughout the day tomorrow so you can check back when you wake up! Chat soon...",
      "ts": "1654745941.659719",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "heart",
          "count": 16,
          "users": [
            "U03JEKGLQ65",
            "U03JRQAFUKA",
            "U03JBMMB10A",
            "U03JH3TKE3V",
            "U03HVCK66P8",
            "U03HMDL7GP9",
            "U03HVCXR1T8",
            "U03JMUP9D9B",
            "U03JTCWP22H",
            "U03HYBRAJNB",
            "U03J6AKH19V",
            "U03JYQ3CLCC",
            "U03J2125SNP",
            "U03JZNY81L0",
            "U03J21CNQ1G",
            "U03JRP87THN"
          ]
        },
        {
          "name": "+1",
          "count": 2,
          "users": [
            "U03JPP4E9K8",
            "U03K67T11G8"
          ]
        },
        {
          "name": "squirrel",
          "count": 1,
          "users": [
            "U03HZ5JQGS1"
          ]
        },
        {
          "name": "working-in-office",
          "count": 2,
          "users": [
            "U03HZ5JQGS1",
            "U03J20A2MB4"
          ]
        },
        {
          "name": "flag-ch",
          "count": 1,
          "users": [
            "U03JEAD3SQ7"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "jQ4i",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " "
                },
                {
                  "type": "emoji",
                  "name": "eyes",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " We’re also going to try something different for our friends outside of the US...we’re going to leave the "
                },
                {
                  "type": "emoji",
                  "name": "workflowbolt",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " enabled so you can go ahead and submit your questions for any of our lounge topics tomorrow (or any lingering ML and Computer Vision questions we can answer).  We’re all going to sign off for the night but we’ll answer your questions throughout the day tomorrow so you can check back when you wake up! Chat soon..."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J6AKH19V\u003e asked\n\u0026gt; Do you have any code snippets showing how to load a stereo audio file into MLMultiArray object?",
      "ts": "1654789193.412069",
      "thread_ts": "1654789193.412069",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 10,
      "latest_reply": "1654835160.484259",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Grpum",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J6AKH19V"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Do you have any code snippets showing how to load a stereo audio file into MLMultiArray object?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "51d7000b-8f7e-48c2-bd14-d75a84865a30",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "It depends on the desired buffer layout of PCM data (interleaved or separate, int16 or float32, etc). A good starting point can be:\n• \u003chttps://developer.apple.com/documentation/soundanalysis/snclassifysoundrequest\u003e\n• \u003chttps://apple.github.io/turicreate/docs/userguide/sound_classifier/export-coreml.html\u003e",
          "ts": "1654789329.214949",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HI0X",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It depends on the desired buffer layout of PCM data (interleaved or separate, int16 or float32, etc). A good starting point can be:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/documentation\\/soundanalysis\\/snclassifysoundrequest\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"link\",\"url\":\"https:\\/\\/apple.github.io\\/turicreate\\/docs\\/userguide\\/sound_classifier\\/export-coreml.html\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "701665f2-f58d-41d4-a609-0be35a808107",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "\u003c@U03J52T5J22\u003e What would be the best way to have a MLMultiArray of shape [2, 44100] being 44100 equivalent to one second of audio? The main objective is to use the MLMultiArray on a CoreML prediction.",
          "ts": "1654790427.443339",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "9jQo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " What would be the best way to have a MLMultiArray of shape [2, 44100] being 44100 equivalent to one second of audio? The main objective is to use the MLMultiArray on a CoreML prediction."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cf43e6e1-89d6-4df2-8365-25c76f489ab1",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "Where does the audio data come from? If you already have a buffer with the desired layout, you can use: `-[MLMultiArray initWithDataPointer:shape:dataType:strides:deallocator:error]` or `MLShapedArray(bytesNoCopy:shape:strides:deallocator:)`.\n\nIf you need to copy with some data type conversion (e.g. `short` to `float32`) `MLShapedArray(unsafeUninitializedShape:initializingWith:)` would work.",
          "ts": "1654791052.329769",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "w1gEn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Where does the audio data come from? If you already have a buffer with the desired layout, you can use: "
                    },
                    {
                      "type": "text",
                      "text": "-[MLMultiArray initWithDataPointer:shape:dataType:strides:deallocator:error]",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " or "
                    },
                    {
                      "type": "text",
                      "text": "MLShapedArray(bytesNoCopy:shape:strides:deallocator:)",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ".\n\nIf you need to copy with some data type conversion (e.g. "
                    },
                    {
                      "type": "text",
                      "text": "short",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " to "
                    },
                    {
                      "type": "text",
                      "text": "float32",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ") "
                    },
                    {
                      "type": "text",
                      "text": "MLShapedArray(unsafeUninitializedShape:initializingWith:)",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " would work."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "70a6731b-9115-45fa-bbad-165746fb7062",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "\u003c@U03J52T5J22\u003e it comes from an AAC audio file as we mentioned in the question.",
          "ts": "1654792120.389839",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "g76d/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " it comes from an AAC audio file as we mentioned in the question."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "65f5697e-d36a-4ccd-90d1-f3c0d80b441d",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "I see. AAC audio is often decoded to a sequence of audio chunks which is not aligned to one second boundary. So, we would need to do some (tedious) buffer munching. I will look for and/or write up some sample code. Stay tuned!",
          "ts": "1654792828.738619",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U03J6AKH19V"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XTut",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I see. AAC audio is often decoded to a sequence of audio chunks which is not aligned to one second boundary. So, we would need to do some (tedious) buffer munching. I will look for and/or write up some sample code. Stay tuned!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6a844fec-cc1e-4a2c-82f3-d698db902cfd",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "yay!",
          "ts": "1654794246.587589",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "8VqHM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "yay!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "caccf08c-f7b4-4ce5-8c33-e8679d7b4661",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "looking forward to it!",
          "ts": "1654794252.513679",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "dIT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "looking forward to it!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "be1c82b5-ac78-4ccb-8c29-3908995715da",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "Hi Eddie! Thank you for waiting. The following code loads an AAC file into `MLShapedArray` for every one second and write each back to a new AAC file.\n\n`MLShapedArray` is a Swift-y cousin of `MLMultiArray` and, if you are using Swift, it is preferred over `MLMultiArray`. CoreML accepts either type. `MLMultiArray(_ shapedArray:)` and `MLShapedArray(_ multiArray:)` can convert between them.\n\nHope it helps.\n```import AVFoundation\nimport CoreML\n\nlet audioFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 44100, channels: 2, interleaved: false)!\n\nlet sourceAudioFile = try! AVAudioFile(forReading: URL(filePath: \"/Users/apple/sample.aac\"))\nlet sourceAudioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: AVAudioFrameCount(audioFormat.sampleRate))!\n\nlet aacSettings = [AVFormatIDKey : kAudioFormatMPEG4AAC,\n                 AVSampleRateKey : 44100,\n            AVNumberOfChannelsKey: 2\n]\nlet outputAudioFile = try! AVAudioFile(forWriting: URL(filePath: \"/Users/apple/output.aac\"), settings: aacSettings)\n\n// Loop to read and decode source audio file.\nwhile sourceAudioFile.framePosition \u0026lt; sourceAudioFile.length {\n    try! sourceAudioFile.read(into: sourceAudioBuffer)\n    let frameLength = Int(sourceAudioBuffer.frameLength)\n\n    // Make MLShapedArray from the audio buffer.\n    let leftChannels = MLShapedArray\u0026lt;Float32\u0026gt;(bytesNoCopy: sourceAudioBuffer.floatChannelData![0], shape: [1, frameLength], strides: [frameLength, 1], deallocator: .none)\n    let rightChannels = MLShapedArray\u0026lt;Float32\u0026gt;(bytesNoCopy: sourceAudioBuffer.floatChannelData![1], shape: [1, frameLength], strides: [frameLength, 1], deallocator: .none)\n    let audioShapedArray = MLShapedArray(concatenating: [leftChannels, rightChannels], alongAxis: 0)\n\n    // Write the MLShapedArray back to a audio buffer.\n    let outputAudioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: sourceAudioBuffer.frameLength)!\n    audioShapedArray[0].withUnsafeShapedBufferPointer { ptr, _, _ in\n        outputAudioBuffer.floatChannelData![0].initialize(from: ptr.baseAddress!, count: frameLength)\n    }\n    audioShapedArray[1].withUnsafeShapedBufferPointer { ptr, _, _ in\n        outputAudioBuffer.floatChannelData![1].initialize(from: ptr.baseAddress!, count: frameLength)\n    }\n    outputAudioBuffer.frameLength = sourceAudioBuffer.frameLength\n\n    // And encode and write to an AAC file.\n    try! outputAudioFile.write(from: outputAudioBuffer)\n}```\n",
          "ts": "1654808212.344279",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart_eyes",
              "count": 1,
              "users": [
                "U03J6AKH19V"
              ]
            },
            {
              "name": "sob",
              "count": 1,
              "users": [
                "U03J6AKH19V"
              ]
            },
            {
              "name": "rocket",
              "count": 1,
              "users": [
                "U03J6AKH19V"
              ]
            },
            {
              "name": "zap",
              "count": 1,
              "users": [
                "U03J6AKH19V"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "UJE",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Eddie! Thank you for waiting. The following code loads an AAC file into "
                    },
                    {
                      "type": "text",
                      "text": "MLShapedArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " for every one second and write each back to a new AAC file.\n\n"
                    },
                    {
                      "type": "text",
                      "text": "MLShapedArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is a Swift-y cousin of "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " and, if you are using Swift, it is preferred over "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ". CoreML accepts either type. "
                    },
                    {
                      "type": "text",
                      "text": "MLMultiArray(_ shapedArray:)",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "text",
                      "text": "MLShapedArray(_ multiArray:)",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " can convert between them.\n\nHope it helps.\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"import AVFoundation\\nimport CoreML\\n\\nlet audioFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 44100, channels: 2, interleaved: false)!\\n\\nlet sourceAudioFile = try! AVAudioFile(forReading: URL(filePath: \\\"\\/Users\\/apple\\/sample.aac\\\"))\\nlet sourceAudioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: AVAudioFrameCount(audioFormat.sampleRate))!\\n\\nlet aacSettings = [AVFormatIDKey : kAudioFormatMPEG4AAC,\\n                 AVSampleRateKey : 44100,\\n            AVNumberOfChannelsKey: 2\\n]\\nlet outputAudioFile = try! AVAudioFile(forWriting: URL(filePath: \\\"\\/Users\\/apple\\/output.aac\\\"), settings: aacSettings)\\n\\n\\/\\/ Loop to read and decode source audio file.\\nwhile sourceAudioFile.framePosition \u003c sourceAudioFile.length {\\n    try! sourceAudioFile.read(into: sourceAudioBuffer)\\n    let frameLength = Int(sourceAudioBuffer.frameLength)\\n\\n    \\/\\/ Make MLShapedArray from the audio buffer.\\n    let leftChannels = MLShapedArray\u003cFloat32\u003e(bytesNoCopy: sourceAudioBuffer.floatChannelData![0], shape: [1, frameLength], strides: [frameLength, 1], deallocator: .none)\\n    let rightChannels = MLShapedArray\u003cFloat32\u003e(bytesNoCopy: sourceAudioBuffer.floatChannelData![1], shape: [1, frameLength], strides: [frameLength, 1], deallocator: .none)\\n    let audioShapedArray = MLShapedArray(concatenating: [leftChannels, rightChannels], alongAxis: 0)\\n\\n    \\/\\/ Write the MLShapedArray back to a audio buffer.\\n    let outputAudioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: sourceAudioBuffer.frameLength)!\\n    audioShapedArray[0].withUnsafeShapedBufferPointer { ptr, _, _ in\\n        outputAudioBuffer.floatChannelData![0].initialize(from: ptr.baseAddress!, count: frameLength)\\n    }\\n    audioShapedArray[1].withUnsafeShapedBufferPointer { ptr, _, _ in\\n        outputAudioBuffer.floatChannelData![1].initialize(from: ptr.baseAddress!, count: frameLength)\\n    }\\n    outputAudioBuffer.frameLength = sourceAudioBuffer.frameLength\\n\\n    \\/\\/ And encode and write to an AAC file.\\n    try! outputAudioFile.write(from: outputAudioBuffer)\\n}\"}],\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": []
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5667ae93-0422-42e4-931f-b791fd83e776",
          "type": "message",
          "user": "U03J6AKH19V",
          "text": "OMG! Probably our best experience in WWDC, you're a \u003c@U03J52T5J22\u003e rockstar!",
          "ts": "1654813813.172819",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 4,
              "users": [
                "U03J52T5J22",
                "U03HB4VBDGX",
                "U03J4CL9UJD",
                "U03JECKH8EQ"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "cWU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "OMG! Probably our best experience in WWDC, you're a "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " rockstar!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d58502e8-dd86-46b7-9fc0-496b1787fa54",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "This is awesome \u003c@U03J52T5J22\u003e :slightly_smiling_face:",
          "ts": "1654835160.484259",
          "thread_ts": "1654789193.412069",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Hr++P",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This is awesome "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JW43N2TV\u003e asked\n\u0026gt; Can we take compiled and encrypted models from the bundle, and host them in our own cloud? Do you see any issues with it?",
      "ts": "1654790775.387379",
      "thread_ts": "1654790775.387379",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 32,
      "latest_reply": "1654879436.315209",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "JmVUA",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JW43N2TV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Can we take compiled and encrypted models from the bundle, and host them in our own cloud? Do you see any issues with it?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b1ebc327-4b40-4af1-9d26-61b4dd5a95bc",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Hi \u003c@U03JW43N2TV\u003e does your cloud instance understand CoreML format? I am assuming you are referring to compiled and encrypted CoreML models?",
          "ts": "1654790834.502049",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "98CFA",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JW43N2TV"
                    },
                    {
                      "type": "text",
                      "text": " does your cloud instance understand CoreML format? I am assuming you are referring to compiled and encrypted CoreML models?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2c901b4f-f9b2-4e60-b460-2e786425e7ad",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Yes, compiled and encrypted models. I am zipping them, download and then unzip on the device.",
          "ts": "1654790873.198799",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ol8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, compiled and encrypted models. I am zipping them, download and then unzip on the device."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "236f7836-143e-42fd-ae1e-09f3ecc97787",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Hosting in S3. It is working locally but when ipa is generated, decryption isn't working. Using the same Team ID both locally and generating ipa. Is it supposed to work?",
          "ts": "1654790968.031649",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "RXh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hosting in S3. It is working locally but when ipa is generated, decryption isn't working. Using the same Team ID both locally and generating ipa. Is it supposed to work?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "80b7dd2f-b859-4d83-905b-bb883abc19f1",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "I see what you are saying. Could you share the error logs? It could be sensitive so, please consider using \u003chttp://feedbackassistant.apple.com|feedbackassistant.apple.com\u003e for that.",
          "ts": "1654791054.075939",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1ktHw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I see what you are saying. Could you share the error logs? It could be sensitive so, please consider using "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": "feedbackassistant.apple.com"
                    },
                    {
                      "type": "text",
                      "text": " for that."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a6282a2c-9528-4219-a30e-47287971b5d6",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Sure, will do. Follow up question, does encryption key and/or encryption process involve any app specific info like bundle identifier? Or, as long as the same Team is used, is it supposed to work?",
          "ts": "1654791192.404199",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=X3B4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sure, will do. Follow up question, does encryption key and/or encryption process involve any app specific info like bundle identifier? Or, as long as the same Team is used, is it supposed to work?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "82e7b867-2c1b-4390-868a-3e9115b71b0a",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "I am following \u003chttps://developer.apple.com/documentation/coreml/generating_a_model_encryption_key?language=objc|Generating a Model Encryption Key\u003e and \u003chttps://developer.apple.com/documentation/coreml/encrypting_a_model_in_your_app?language=objc|Encrypting a Model in Your App\u003e.",
          "ts": "1654791239.386019",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "a7/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I am following "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/coreml/generating_a_model_encryption_key?language=objc",
                      "text": "Generating a Model Encryption Key"
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/coreml/encrypting_a_model_in_your_app?language=objc",
                      "text": "Encrypting a Model in Your App"
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9DB283CB-4BF3-4648-A5DF-76B53154BF3C",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "Note: I would take a look at how much space savings 'zipping' is actually making in this case: you may just be wasting computation: encryption in particular produces files that won't compress well.  ",
          "ts": "1654792105.704949",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+bPZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Note: I would take a look at how much space savings 'zipping' is actually making in this case: you may just be wasting computation: encryption in particular produces files that won't compress well.  "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "60740b0d-07ef-497e-9852-f6338be83f9b",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "ya, agree. Compiled and encrypted model is a directory. So, zipping is done to download it as a single file rather than to save space.",
          "ts": "1654792283.768069",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 2,
              "users": [
                "U03HB4VBDGX",
                "U03HB4T0CA3"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+M7m",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "ya, agree. Compiled and encrypted model is a directory. So, zipping is done to download it as a single file rather than to save space."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6a6a044b-655a-41af-8183-c1e9d6410aad",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u0026gt; Follow up question, does encryption key and/or encryption process involve any app specific info like bundle identifier? Or, as long as the same Team is used, is it supposed to work?\nDecryption process does involve a bundle-id check but, it should be transparent to your app. If your app (signed with correct team cert) loads the encrypted model, decryption should work as expected.",
          "ts": "1654792512.105669",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZzG85",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Follow up question, does encryption key and\\/or encryption process involve any app specific info like bundle identifier? Or, as long as the same Team is used, is it supposed to work?\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Decryption process does involve a bundle-id check but, it should be transparent to your app. If your app (signed with correct team cert) loads the encrypted model, decryption should work as expected."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "74aa0b64-bdf5-4e76-80ab-32feb36a9cb3",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "\u003c@U03HB4VBDGX\u003e If we have different bundle identifiers for different environments, is there a way to generate encryption key that works for all bundle identifiers? For eg: if debug/dev environ has identifier that ends with `.development` and other envs ends with a different string.",
          "ts": "1654793726.735759",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "8io",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " If we have different bundle identifiers for different environments, is there a way to generate encryption key that works for all bundle identifiers? For eg: if debug/dev environ has identifier that ends with "
                    },
                    {
                      "type": "text",
                      "text": ".development",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " and other envs ends with a different string."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "783a3088-61a6-4c49-a4c0-91cb19f6d5f6",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Bundle ID is only used to make sure app that fetched the key for the first time is using the fetched key before decryption.",
          "ts": "1654793814.515059",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xhE",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Bundle ID is only used to make sure app that fetched the key for the first time is using the fetched key before decryption."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e62b6ae8-fcea-497c-93a7-7c96d15c8e9b",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "If you are not mixing those, you should be fine",
          "ts": "1654793839.095439",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "JCSu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you are not mixing those, you should be fine"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e80ab151-36eb-499a-bf51-3eef2a34223d",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "\u003c@U03HB4VBDGX\u003e In that case, since the bundle ids are different for different environments, looks like I need to generate separate keys and host multiple encrypted models. Does wild card entries in bundle identifier field work while generating key and compiling with encryption? for eg: `com.something.something.*` . (Only the last `*` is variable in my case.)",
          "ts": "1654795235.119169",
          "thread_ts": "1654790775.387379",
          "edited": {
            "user": "U03JW43N2TV",
            "ts": "1654795329.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "33Wv",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " In that case, since the bundle ids are different for different environments, looks like I need to generate separate keys and host multiple encrypted models. Does wild card entries in bundle identifier field work while generating key and compiling with encryption? for eg: "
                    },
                    {
                      "type": "text",
                      "text": "com.something.something.*",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " . (Only the last "
                    },
                    {
                      "type": "text",
                      "text": "*",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is variable in my case.)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7bb7c64d-f07b-4216-b2ac-ccb08ce5d7d9",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u0026gt; Does wild card entries in bundle identifier field work while generating key and compiling with encryption? for eg: `com.something.something.*` . (Only the last `*` is variable in my case.)\nIt is not checked at the time of generating the key (in Xcode). It is checked at the time of fetching the key (at model load time before decryption).",
          "ts": "1654798963.472729",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2dON",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Does wild card entries in bundle identifier field work while generating key and compiling with encryption? for eg: \"},{\"type\":\"text\",\"text\":\"com.something.something.*\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\" . (Only the last \"},{\"type\":\"text\",\"text\":\"*\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\" is variable in my case.)\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It is not checked at the time of generating the key (in Xcode). It is checked at the time of fetching the key (at model load time before decryption)."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3274af0e-4956-448d-be7b-633edc70419a",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "\u003c@U03HB4VBDGX\u003e Is it checked at the time of fetching against the bundle id from the encryption key? If so, is it a char to char comparison? Or, having something like `com.something.something.*` when I generate the key, and `com.something.something.development`  and `com.something.something.Enterprise` with the actual app builds work?",
          "ts": "1654802103.438559",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "OLU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " Is it checked at the time of fetching against the bundle id from the encryption key? If so, is it a char to char comparison? Or, having something like "
                    },
                    {
                      "type": "text",
                      "text": "com.something.something.*",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " when I generate the key, and "
                    },
                    {
                      "type": "text",
                      "text": "com.something.something.development",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "  and "
                    },
                    {
                      "type": "text",
                      "text": "com.something.something.Enterprise",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " with the actual app builds work?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a2a51c57-93ab-4b2f-a12b-49e6df597a88",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u0026gt; Is it checked at the time of fetching against the bundle id from the encryption key?\nBundle id is looked up from your app on the device (so, in this case  `com.something.something.Enterprise`) and persisted for cross checking on future model loads. `com.something.something.development` is not stored anywhere. So, the example you provided *should* work. If you have the feedback assistant ID, I can take a closer look at the error and get back to with what might be going on.",
          "ts": "1654802883.701139",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ogkc2",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it checked at the time of fetching against the bundle id from the encryption key?\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Bundle id is looked up from your app on the device (so, in this case  "
                    },
                    {
                      "type": "text",
                      "text": "com.something.something.Enterprise",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ") and persisted for cross checking on future model loads. "
                    },
                    {
                      "type": "text",
                      "text": "com.something.something.development",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is not stored anywhere. So, the example you provided "
                    },
                    {
                      "type": "text",
                      "text": "should",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " work. If you have the feedback assistant ID, I can take a closer look at the error and get back to with what might be going on."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "de7b02ba-6b6f-4d02-bf00-9cee1bd25673",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Yes, bundle id is looked from the app on the device and is compared against what? I am assuming it is compared against the ID from the encryption key?",
          "ts": "1654803218.367809",
          "thread_ts": "1654790775.387379",
          "edited": {
            "user": "U03JW43N2TV",
            "ts": "1654803227.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PY/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, bundle id is looked from the app on the device and is compared against what? I am assuming it is compared against the ID from the encryption key?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7a56b82a-8350-4baf-b2bf-1cdbf3ddd7b7",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "I tried submitting in the feedback assistant with sysdiagnose logs but it is failing. It may be coz the logs are above 400MB. In the console, I see `MLModelAsset: load failed with error Error Domain=com.apple.CoreML Code=8 UserInfo={NSLocalizedDescription=\u0026lt;private\u0026gt;}` from the build. I am trying to generate another build with an added `NSLog` so that I see more details in the console. I will follow up once it is ready. In the meanwhile, before I lose you, I wanted to make sure if I can use wild chars in the bundle id at the time of key generation so that it matches with dev and prod builds' bundle ids.",
          "ts": "1654803388.184409",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=r6S",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I tried submitting in the feedback assistant with sysdiagnose logs but it is failing. It may be coz the logs are above 400MB. In the console, I see "
                    },
                    {
                      "type": "text",
                      "text": "MLModelAsset: load failed with error Error Domain=com.apple.CoreML Code=8 UserInfo={NSLocalizedDescription=\u003cprivate\u003e}",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " from the build. I am trying to generate another build with an added "
                    },
                    {
                      "type": "text",
                      "text": "NSLog",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " so that I see more details in the console. I will follow up once it is ready. In the meanwhile, before I lose you, I wanted to make sure if I can use wild chars in the bundle id at the time of key generation so that it matches with dev and prod builds' bundle ids."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6e980aba-ea09-47b2-9491-dc125b9e183d",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "\u003c@U03HB4VBDGX\u003e",
          "ts": "1654803677.159369",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fnT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "617beeb1-b96d-4673-a3e4-d4f44658e651",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Bundle id is looked up from your app when the key is fetched and persisted. On subsequent model loads, bundle id from your app is looked up to validate against the first bundle id that was persisted at the time of key fetch.\n\u0026gt; I am assuming it is compared against the ID from the encryption key?\nNo, bundle id is not associated with your encryption key.\n\n\u0026gt; I wanted to make sure if I can use wild chars in the bundle id at the time of key generation so that it matches with dev and prod builds' bundle ids.\nThis should not matter - CoreML only uses bundle ID from your app and if app store allows, it can have wild chars.",
          "ts": "1654804066.773609",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1S3Jf",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Bundle id is looked up from your app when the key is fetched and persisted. On subsequent model loads, bundle id from your app is looked up to validate against the first bundle id that was persisted at the time of key fetch.\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I am assuming it is compared against the ID from the encryption key?\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "No, bundle id is not associated with your encryption key.\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I wanted to make sure if I can use wild chars in the bundle id at the time of key generation so that it matches with dev and prod builds' bundle ids.\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "This should not matter - CoreML only uses bundle ID from your app and if app store allows, it can have wild chars."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b863b2ed-d5cc-42e6-9f3a-54e2dfcffaae",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u0026gt; I will follow up once it is ready.\nSure",
          "ts": "1654804117.248529",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1Qit5",
              "elements": [
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I will follow up once it is ready.\"}]}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sure"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "58560193-9d52-430e-b4f3-7c021b721d8d",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Ah got it. Bundle id only matter for subsequent loads then. My issue is with first load itself.",
          "ts": "1654806330.285009",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "RoD",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah got it. Bundle id only matter for subsequent loads then. My issue is with first load itself."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8720c987-98f7-4011-9866-5d666c7ebe99",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Exactly. If I see the entire error message, I might know what is differently with your case.",
          "ts": "1654807395.999529",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Z+vZm",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Exactly. If I see the entire error message, I might know what is differently with your case."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3c20c0e3-4b1f-4c75-be4e-572919f4bd5f",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "I see, `Fetching decryption key from server failed: noEntryFound(\"No records found\"). Make sure the encryption key was generated with correct team ID.` . I believe our remote builds use the same Team ID as the one I used locally to generate the key. I am waiting for a confirmation on this. While I wait for it, I wanted to see if there could be any other reason for failure because I see different errors before and after this error.",
          "ts": "1654814129.212519",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+MEP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I see, "
                    },
                    {
                      "type": "text",
                      "text": "Fetching decryption key from server failed: noEntryFound(\"No records found\"). Make sure the encryption key was generated with correct team ID.",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " . I believe our remote builds use the same Team ID as the one I used locally to generate the key. I am waiting for a confirmation on this. While I wait for it, I wanted to see if there could be any other reason for failure because I see different errors before and after this error."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "860ca5ae-bb5b-4937-8b32-398a370574a8",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Eg:\n```Client \"com.apple.CoreMLModelSecurityService\" tried to access environment \"Production\", even though the entitlements specified \"Sandbox\"```",
          "ts": "1654814147.772989",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "erpd2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Eg:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"Client \\\"com.apple.CoreMLModelSecurityService\\\" tried to access environment \\\"Production\\\", even though the entitlements specified \\\"Sandbox\\\"\"}],\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b2ef9f02-e1b4-4789-b2a3-cd4fe9e3caec",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "Looks like the Team IDs are different. Thanks a lot for you patience going through my issue.",
          "ts": "1654815659.273509",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "J7QvL",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Looks like the Team IDs are different. Thanks a lot for you patience going through my issue."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5d7ddc49-3e8b-4cc3-9afa-afeac9c52f31",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "No worries \u003c@U03JW43N2TV\u003e! I am glad we got to the bottom of this!",
          "ts": "1654816538.891839",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/om",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "No worries "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JW43N2TV"
                    },
                    {
                      "type": "text",
                      "text": "! I am glad we got to the bottom of this!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4380d613-5ae7-4bff-b19b-461e131f6d26",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Which app is this, if you don't mind sharing",
          "ts": "1654816558.058119",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "X=INj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Which app is this, if you don't mind sharing"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "231bf1f4-1ccd-4d3c-a395-7cf14491201d",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "I personally don't mind but I may not be allowed to share unfortunately. I wanted to do a DM but looks like it is disabled. Sorry. :neutral_face:",
          "ts": "1654817440.973469",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kWUwj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I personally don't mind but I may not be allowed to share unfortunately. I wanted to do a DM but looks like it is disabled. Sorry. "
                    },
                    {
                      "type": "emoji",
                      "name": "neutral_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "abca6082-55bb-4e2e-a39e-cecce57648a4",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "No worries. Thanks!",
          "ts": "1654822966.494709",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Qaxtb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "No worries. Thanks!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ee02bd7e-127b-46cd-beea-f186f19756d7",
          "type": "message",
          "user": "U03JW43N2TV",
          "text": "\u003c@U03HB4VBDGX\u003e We use different team IDs for distribution and enterprise program. Do you have any suggestions on how to make the encryption work for our use case?",
          "ts": "1654877275.704659",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rAa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03HB4VBDGX"
                    },
                    {
                      "type": "text",
                      "text": " We use different team IDs for distribution and enterprise program. Do you have any suggestions on how to make the encryption work for our use case?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f9c172b6-98b7-42e9-a024-537c78c33d98",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "The only way I can think of is to encrypt the model with two different encryption keys (with different team IDs) and include both encrypted models in the app / make it available for download. App would pick the right model based on the variant.",
          "ts": "1654879436.315209",
          "thread_ts": "1654790775.387379",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=iSH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The only way I can think of is to encrypt the model with two different encryption keys (with different team IDs) and include both encrypted models in the app / make it available for download. App would pick the right model based on the variant."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03K67T11G8\u003e asked\n\u0026gt; What image-sharpening torch model did you use in the \"Optimize your Core ML usage\" talk?",
      "ts": "1654791443.512319",
      "thread_ts": "1654791443.512319",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654794700.539199",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "tC7",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03K67T11G8"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What image-sharpening torch model did you use in the \\\"Optimize your Core ML usage\\\" talk?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "551d5afc-493e-4829-a385-0d1b5f54d119",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "Hi \u003c@U03K67T11G8\u003e, we got a similar question here \u003chttps://developer.apple.com/forums/thread/707607\u003e. Please let me know if the answer there helps.",
          "ts": "1654791559.660009",
          "thread_ts": "1654791443.512319",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZSk4n",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03K67T11G8"
                    },
                    {
                      "type": "text",
                      "text": ", we got a similar question here "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/forums/thread/707607",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": ". Please let me know if the answer there helps."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0D6208A1-1320-4E70-A660-AD8848FEFF98",
          "type": "message",
          "user": "U03K67T11G8",
          "text": "Thanks \u003c@U03JFGMUPCY\u003e, that answers my question!",
          "ts": "1654794700.539199",
          "thread_ts": "1654791443.512319",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Cxl",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JFGMUPCY"
                    },
                    {
                      "type": "text",
                      "text": ","
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "answers"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "my"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "question!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J4DR9GDS\u003e asked\n\u0026gt; I'd like to classify larger article-sized bodies of text. One of the ways I'm working on doing this is by doing text classification. Given 5 categories of diary entry (eg family, health, spiritual, work, recreational), would it be preferred to use a single model that labels text with one of the 5? Or should I follow the SentimentClassifier example and use 5 separate models that each classify a string in 3 ways (notFamily, neutralFamily, isFamily)? If the latter, is this a use case for components?",
      "ts": "1654791449.424019",
      "thread_ts": "1654791449.424019",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654793265.620809",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "a2v4",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J4DR9GDS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I'd like to classify larger article-sized bodies of text. One of the ways I'm working on doing this is by doing text classification. Given 5 categories of diary entry (eg family, health, spiritual, work, recreational), would it be preferred to use a single model that labels text with one of the 5? Or should I follow the SentimentClassifier example and use 5 separate models that each classify a string in 3 ways (notFamily, neutralFamily, isFamily)? If the latter, is this a use case for components?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "7728529c-5c0d-49b0-a06b-d34144ba91e2",
          "type": "message",
          "user": "U03HRM0UK8B",
          "text": "Usually what I would tend to try first would be a single classifier model that labels text with one of your classes. That should be efficient and robust, and scale reasonably well with the number of classes. If you train multiple models, you would need to run each of them on each article you want to classify. However, the questions being asked are subtly different in the two different cases. With a single model, you are asking which category or categories best match a given document. When you use separate models, you are asking whether a given document relates to a given category or not. The question you want to ask informs everything from your annotation to the type of model you use to the way you present your results. Ultimately you will have to decide what analysis is best suited to your particular application.",
          "ts": "1654791765.670269",
          "thread_ts": "1654791449.424019",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "i8wqg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Usually what I would tend to try first would be a single classifier model that labels text with one of your classes. That should be efficient and robust, and scale reasonably well with the number of classes. If you train multiple models, you would need to run each of them on each article you want to classify. However, the questions being asked are subtly different in the two different cases. With a single model, you are asking which category or categories best match a given document. When you use separate models, you are asking whether a given document relates to a given category or not. The question you want to ask informs everything from your annotation to the type of model you use to the way you present your results. Ultimately you will have to decide what analysis is best suited to your particular application."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "675fc38c-98bf-4c54-867e-621f38881a8a",
          "type": "message",
          "user": "U03J4DR9GDS",
          "text": "Thanks for your great response. This definitely gives me a good direction",
          "ts": "1654793265.620809",
          "thread_ts": "1654791449.424019",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zY6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for your great response. This definitely gives me a good direction"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JF5PC27R\u003e asked\n\u0026gt; are there any limitations on number of iosurface-backed in a model?",
      "ts": "1654793960.716409",
      "thread_ts": "1654793960.716409",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654794024.586759",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "af0Y",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JF5PC27R"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"are there any limitations on number of iosurface-backed in a model?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "26d4eea8-4351-4280-a8bf-50fd79d6896a",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "Hi Andrew, no, CoreML doesn’t put the limitations.",
          "ts": "1654794024.586759",
          "thread_ts": "1654793960.716409",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rG9E",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Andrew, no, CoreML doesn’t put the limitations."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HVCK66P8\u003e asked\n\u0026gt; Can the live text selected automatically (in an designated area/ simply select all) without user highlight?\n\u0026gt; \u003chttps://developer.apple.com/videos/play/wwdc2022/10026/|https://developer.apple.com/videos/play/wwdc2022/10026/\u003e",
      "ts": "1654794549.071739",
      "thread_ts": "1654794549.071739",
      "attachments": [
        {
          "fallback": "Apple Developer: Add Live Text interaction to your app - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Add Live Text interaction to your app - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022/10026/",
          "text": "Learn how you can bring Live Text support for still photos or paused video frames to your app. We'll share how you can easily enable text...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6519/6519_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022/10026/",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022/10026/",
          "blocks": null
        }
      ],
      "edited": {
        "user": "B03H04756BH",
        "ts": "1654794549.000000"
      },
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654804852.220139",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "gcD",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HVCK66P8"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Can the live text selected automatically (in an designated area\\/ simply select all) without user highlight?\\n\"},{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/videos\\/play\\/wwdc2022\\/10026\\/\",\"text\":\"https:\\/\\/developer.apple.com\\/videos\\/play\\/wwdc2022\\/10026\\/\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "9163193d-268a-46f3-8566-27f64b99f590",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "Hi. Sorry, select all isn't an option currently.",
          "ts": "1654794750.928849",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xibh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi. Sorry, select all isn't an option currently."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "934e2bfc-4122-44fb-a912-e0a86e81b06e",
          "type": "message",
          "user": "U03HVCK66P8",
          "text": "Oic. So the user must select the text manually first, right?",
          "ts": "1654794955.605009",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EeMNg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Oic. So the user must select the text manually first, right?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e8d67338-539b-4faf-86d3-ab4c4c9df30d",
          "type": "message",
          "user": "U03J4B065TK",
          "text": "Hi, yes, the only thing you can do is reset the selection if one exists.",
          "ts": "1654794995.470049",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kBnS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi, yes, the only thing you can do is reset the selection if one exists."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "773edddd-ff6d-499b-ba5e-e6385b1a9e06",
          "type": "message",
          "user": "U03HVCK66P8",
          "text": "ok",
          "ts": "1654795047.221489",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YkkPa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "ok"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cf7fecd6-e3a6-4be8-ab31-83549799a4a3",
          "type": "message",
          "user": "U03HVCK66P8",
          "text": "Thanks, \u003c@U03HB4KUYH5\u003e \u003c@U03J4B065TK\u003e",
          "ts": "1654795082.970029",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB4KUYH5",
                "U03J4B065TK"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lKcv",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4KUYH5"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J4B065TK"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0352e702-f131-454e-8c2a-019085438925",
          "type": "message",
          "user": "U03JE2RJ2DA",
          "text": "Can we use Core ML to recognize objects such as dog in a live camera feed and then use Live Text to cut the image out of it's background?",
          "ts": "1654799844.896399",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "thinking_face",
              "count": 1,
              "users": [
                "U03HVCK66P8"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "C1Vb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Can we use Core ML to recognize objects such as dog in a live camera feed and then use Live Text to cut the image out of it's background?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1bd7331e-1ee5-42c6-b3a8-74d06c40a957",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "The subject lifting feature does not have an API in iOS 16. We do have a Shortcuts workflow if that helps.",
          "ts": "1654804852.220139",
          "thread_ts": "1654794549.071739",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HVCK66P8"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bwF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The subject lifting feature does not have an API in iOS 16. We do have a Shortcuts workflow if that helps."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J4DR9GDS\u003e asked\n\u0026gt; Is it appropriate to try to use word embeddings to match long-form text up to single worded categories? For example, figuring out the distance between `\"exercise\"` and `\"Today I decided to ride my bike to the store. I needed to get a workout in.\"` I'd like to match sentences and paragraphs up to to tags.",
      "ts": "1654795981.516849",
      "thread_ts": "1654795981.516849",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654796267.024679",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "3m7x",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J4DR9GDS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it appropriate to try to use word embeddings to match long-form text up to single worded categories? For example, figuring out the distance between `\\\"exercise\\\"` and `\\\"Today I decided to ride my bike to the store. I needed to get a workout in.\\\"` I'd like to match sentences and paragraphs up to to tags.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b48559bf-3825-4436-a769-eddd4732a9a8",
          "type": "message",
          "user": "U03HRM0UK8B",
          "text": "The most robust approach to this sort of categorization would be to pick a set of categories in advance, collect training data, and train a classifier to classify sentences according to these categories. If you need to handle words outside the originally chosen set of categories, you could then use word embeddings to find an existing category similar to the entered word. If you aren't able to train a model, things get a bit trickier. You can use tools such as part-of-speech tagging to identify relevant words in a sentence, e.g. nouns in the example you give, and determine how similar those are to the word you are trying to match. You would then need to figure out some way to take scores for individual words and form a score for an entire sentence. Overall I think you would get better results by training a classifier, although it would require more work in advance for training.",
          "ts": "1654796267.024679",
          "thread_ts": "1654795981.516849",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tGw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The most robust approach to this sort of categorization would be to pick a set of categories in advance, collect training data, and train a classifier to classify sentences according to these categories. If you need to handle words outside the originally chosen set of categories, you could then use word embeddings to find an existing category similar to the entered word. If you aren't able to train a model, things get a bit trickier. You can use tools such as part-of-speech tagging to identify relevant words in a sentence, e.g. nouns in the example you give, and determine how similar those are to the word you are trying to match. You would then need to figure out some way to take scores for individual words and form a score for an entire sentence. Overall I think you would get better results by training a classifier, although it would require more work in advance for training."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JF5PC27R\u003e asked\n\u0026gt; is it possible to have flexible shape (enumaratedly) inputs (and therefore outputs) to be compatible with outputBackings and IOSurface-backed MultiArray?",
      "ts": "1654796533.864529",
      "thread_ts": "1654796533.864529",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654796677.857019",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Iaj9",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JF5PC27R"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"is it possible to have flexible shape (enumaratedly) inputs (and therefore outputs) to be compatible with outputBackings and IOSurface-backed MultiArray?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "7208a467-cded-4b2d-ad1d-af027039aca2",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "Yes this will work as long as the output backing buffer is the correct size corresponding to the size of the input. One note is that being able to avoid data copies during inference for models with flexible shapes will vary depending on circumstances. You can use the Core ML Instrument and look in the Data lane to see if data copies are occurring.",
          "ts": "1654796677.857019",
          "thread_ts": "1654796533.864529",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "gJ661",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes this will work as long as the output backing buffer is the correct size corresponding to the size of the input. One note is that being able to avoid data copies during inference for models with flexible shapes will vary depending on circumstances. You can use the Core ML Instrument and look in the Data lane to see if data copies are occurring."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J4DR9GDS\u003e asked\n\u0026gt; Does CoreML have everything necessary to perform keyword extraction? How would you go about extracting keywords from articles of text?",
      "ts": "1654797079.224419",
      "thread_ts": "1654797079.224419",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654797867.200769",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "/QdH",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J4DR9GDS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Does CoreML have everything necessary to perform keyword extraction? How would you go about extracting keywords from articles of text?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "ea4a5d81-ec66-4509-a639-2ee63a96e9ab",
          "type": "message",
          "user": "U03HRM0UK8B",
          "text": "Natural Language has a number of tools that can be useful in keyword extraction: tokenization, part-of-speech tagging, named entity recognition, gazetteers that could be used to identify stop words, and so on. We don't provide an implementation of a specific keyword or keyphrase extraction algorithm, but there are algorithms that are sometimes used that take into account features such as frequency, co-occurrence statistics, TF-IDF, etc. that can be calculated from text that has been tokenized and processed using some of these tools. Doing this fully unsupervised is a difficult task, though. You might be able to do better if you have some advance knowledge of the vocabulary that is relevant to the sort of text you will be working with.",
          "ts": "1654797867.200769",
          "thread_ts": "1654797079.224419",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Rnkm",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Natural Language has a number of tools that can be useful in keyword extraction: tokenization, part-of-speech tagging, named entity recognition, gazetteers that could be used to identify stop words, and so on. We don't provide an implementation of a specific keyword or keyphrase extraction algorithm, but there are algorithms that are sometimes used that take into account features such as frequency, co-occurrence statistics, TF-IDF, etc. that can be calculated from text that has been tokenized and processed using some of these tools. Doing this fully unsupervised is a difficult task, though. You might be able to do better if you have some advance knowledge of the vocabulary that is relevant to the sort of text you will be working with."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPP4S3A\u003e asked\n\u0026gt; can optical flow be used in situations where more than one object is moving at the same time?",
      "ts": "1654803715.096299",
      "thread_ts": "1654803715.096299",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654810318.838519",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "dObj",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPP4S3A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"can optical flow be used in situations where more than one object is moving at the same time?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "fd6ec641-98d2-4de1-aa96-08b7ad6e93f6",
          "type": "message",
          "user": "U03J4AXTF1P",
          "text": "Optical flow output is per pixel",
          "ts": "1654805461.791959",
          "thread_ts": "1654803715.096299",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "eFyYj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Optical flow output is per pixel"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ae18096a-810b-458e-ab74-4ff58cd19f14",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "Yes, motion information will be returned for all parts of the image, and therefore for all moving objects in the scene.",
          "ts": "1654810217.448099",
          "thread_ts": "1654803715.096299",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qEff",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, motion information will be returned for all parts of the image, and therefore for all moving objects in the scene."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9dfa8e9a-99c7-4b8b-9b1a-33a83dc0b429",
          "type": "message",
          "user": "U03JRPP4S3A",
          "text": "Thank You",
          "ts": "1654810318.838519",
          "thread_ts": "1654803715.096299",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ebS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank You"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JZBYT73N\u003e asked\n\u0026gt; Is there a document from that talks about how ML development works with Apple products and what is needed to get started?",
      "ts": "1654806251.721259",
      "thread_ts": "1654806251.721259",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654806268.128289",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "g6Tjs",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JZBYT73N"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a document from that talks about how ML development works with Apple products and what is needed to get started?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "3303bd8f-3c08-4bcc-8755-0ab44c0f9893",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "A good starting point is to check out an overview of Apple’s ML focused development tools here: \u003chttps://developer.apple.com/machine-learning/\u003e\n\nThere are also some past WWDC videos which show you an example journey from idea to implementation such as this talk: \u003chttps://developer.apple.com/videos/play/wwdc2019/228/|Creating Great Apps Using Core ML and ARKit.\u003e\n\nI highly recommend checking out this session on Friday: \u003chttps://developer.apple.com/videos/play/wwdc2022/10017/|Explore the machine learning development experience\u003e",
          "ts": "1654806268.128289",
          "thread_ts": "1654806251.721259",
          "attachments": [
            {
              "fallback": "Apple Developer: Machine Learning - Apple Developer",
              "id": 1,
              "title": "Machine Learning - Apple Developer",
              "title_link": "https://developer.apple.com/machine-learning/",
              "text": "Create intelligent features and enable new experiences for your apps by leveraging powerful on-device machine learning.",
              "image_url": "https://developer.apple.com/news/images/og/coreml-og.png",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/machine-learning/",
              "original_url": "https://developer.apple.com/machine-learning/",
              "blocks": null
            },
            {
              "fallback": "Apple Developer: Creating Great Apps Using Core ML and ARKit - WWDC19 - Videos - Apple Developer",
              "id": 2,
              "title": "Creating Great Apps Using Core ML and ARKit - WWDC19 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2019/228/",
              "text": "Take a journey through the creation of an educational game that brings together Core ML, ARKit, and other app frameworks. Discover...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/48/2832/2832_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2019/228/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2019/228/",
              "blocks": null
            },
            {
              "fallback": "Apple Developer: Explore the machine learning development experience - WWDC22 - Videos - Apple Developer",
              "id": 3,
              "title": "Explore the machine learning development experience - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10017/",
              "text": "Learn how to bring great machine learning (ML) based experiences to your app. We'll take you through model discovery, conversion, and...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6510/6510_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10017/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10017/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2IQU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A good starting point is to check out an overview of Apple’s ML focused development tools here: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/machine-learning/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\nThere are also some past WWDC videos which show you an example journey from idea to implementation such as this talk: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2019/228/",
                      "text": "Creating Great Apps Using Core ML and ARKit."
                    },
                    {
                      "type": "text",
                      "text": "\n\nI highly recommend checking out this session on Friday: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10017/",
                      "text": "Explore the machine learning development experience"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JF5PC27R\u003e asked\n\u0026gt; how to handle situations where older ANE versions might not support certain layers and it will result in cpuAndNeuralEngine config being extremely slower on some devices?",
      "ts": "1654807715.557469",
      "thread_ts": "1654807715.557469",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654807781.426739",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "H2+AU",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JF5PC27R"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"how to handle situations where older ANE versions might not support certain layers and it will result in cpuAndNeuralEngine config being extremely slower on some devices?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "21bf3bef-9e7e-4529-8474-f81f2e035b83",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "`MLComputeUnits.all` is the default option and we recommend using that in most of the cases. CoreML tries to optimize for latency while utilizing all the available compute units. `MLComputeUnits.cpuAndNeuralEngine` is helpful when your app is using GPU for pre or post processing and would like CoreML to not dispatch the model on GPU. Other than that `MLComputeUnits.cpuAndNeuralEngine` behaves very similar to `MLComputeUnits.all`. If you have a model that is running much slower on certain devices, we recommend filing some feedback at \u003chttp://feedbackassistant.apple.com\u003e with the model and specific device(s) included.",
          "ts": "1654807781.426739",
          "thread_ts": "1654807715.557469",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6OmT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MLComputeUnits.all",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is the default option and we recommend using that in most of the cases. CoreML tries to optimize for latency while utilizing all the available compute units. "
                    },
                    {
                      "type": "text",
                      "text": "MLComputeUnits.cpuAndNeuralEngine",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is helpful when your app is using GPU for pre or post processing and would like CoreML to not dispatch the model on GPU. Other than that "
                    },
                    {
                      "type": "text",
                      "text": "MLComputeUnits.cpuAndNeuralEngine",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " behaves very similar to "
                    },
                    {
                      "type": "text",
                      "text": "MLComputeUnits.all",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ". If you have a model that is running much slower on certain devices, we recommend filing some feedback at "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " with the model and specific device(s) included."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JA6H3Z38\u003e asked\n\u0026gt; What is the difference between MLTrainingSessionParameters and MLObjectDetector.ModelParameters?",
      "ts": "1654808099.511129",
      "thread_ts": "1654808099.511129",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654808206.107389",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Ffs=",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JA6H3Z38"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What is the difference between MLTrainingSessionParameters and MLObjectDetector.ModelParameters?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "196f278d-f6e5-42aa-8855-bff8a19fa4e8",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "`MLTrainingSessionParameters` for async training API, e.g., `.train()`, to specify training related parameters, such as checkpointing saving location, whereas `MLObjectDetector.ModelParameters` is for both sync and asyn ctraining to specify model-specific parameters",
          "ts": "1654808206.107389",
          "thread_ts": "1654808099.511129",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "l46",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MLTrainingSessionParameters",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " for async training API, e.g., "
                    },
                    {
                      "type": "text",
                      "text": ".train()",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ", to specify training related parameters, such as checkpointing saving location, whereas "
                    },
                    {
                      "type": "text",
                      "text": "MLObjectDetector.ModelParameters",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is for both sync and asyn ctraining to specify model-specific parameters"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "f49f5d30-e904-4f92-8700-705c95909c66",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "\u003c!here\u003e\n*Next Activity… \"Q\u0026amp;A: Vision\"*\nWe'll be taking a pause on answering all the great, general machine learning questions that have been flowing in today to focus on one topic for an hour—Vision and VisionKit frameworks. Joining us is a great team of engineers that cover these two API surfaces. They are your resident experts on everything that is possible in this domain including things like rectangle and document detection, counter detection, text recognition, face and body pose detection, optical flow, object tracking, machine readable codes, and Live Text. With so much to talk about, I just know this is going to be lively. Start submitting your questions by clicking the  :heavy_plus_sign: button and using the workflow. :workflowbolt:",
      "ts": "1654808345.356199",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "H7Hab",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": "\n"
                },
                {
                  "type": "text",
                  "text": "Next Activity… \"Q\u0026A: Vision\"",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\nWe'll be taking a pause on answering all the great, general machine learning questions that have been flowing in today to focus on one topic for an hour—Vision and VisionKit frameworks. Joining us is a great team of engineers that cover these two API surfaces. They are your resident experts on everything that is possible in this domain including things like rectangle and document detection, counter detection, text recognition, face and body pose detection, optical flow, object tracking, machine readable codes, and Live Text. With so much to talk about, I just know this is going to be lively. Start submitting your questions by clicking the  "
                },
                {
                  "type": "emoji",
                  "name": "heavy_plus_sign",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " button and using the workflow. "
                },
                {
                  "type": "emoji",
                  "name": "workflowbolt",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JLUH1V8F\u003e asked\n\u0026gt; We can use Shipment Tracking Number, URL as a source for live text. Can we define our own source for the live text ? Let’s say I wanna add detection of new couriers other than FedEx or UPS. ",
      "ts": "1654808559.673649",
      "thread_ts": "1654808559.673649",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654814078.260669",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "GYE=J",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JLUH1V8F"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"We can use Shipment Tracking Number, URL as a source for live text. Can we define our own source for the live text ? Let\\u2019s say I wanna add detection of new couriers other than FedEx or UPS. \"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "c19199bb-8070-4b42-af64-e69d5768e5b8",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "For the DataScannerViewController? Sorry, right now just the 1 option for shipment tracking numbers and it's whatever carrier we're able to detect",
          "ts": "1654808594.555489",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/2b",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "For the DataScannerViewController? Sorry, right now just the 1 option for shipment tracking numbers and it's whatever carrier we're able to detect"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "B333C0B7-1AF8-4B14-9860-D1527BF6F74D",
          "type": "message",
          "user": "U03JLUH1V8F",
          "text": "Thanks for the answer. It could be awesome if there was a protocol that we can conform to so that we could extend the detected source.",
          "ts": "1654808717.227589",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "9Uhv",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " answer"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "It"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "could"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "be"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "awesome"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "if"
                    },
                    {
                      "type": "text",
                      "text": " there "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "protocol"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "we"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "can"
                    },
                    {
                      "type": "text",
                      "text": " conf"
                    },
                    {
                      "type": "text",
                      "text": "orm"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "so"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "we"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "could"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "extend"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "detected"
                    },
                    {
                      "type": "text",
                      "text": " source"
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "38a4386d-891d-47f7-8749-d60d59ec06b1",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "How would you imagine that protocol working?",
          "ts": "1654811055.993169",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "X3N",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "How would you imagine that protocol working?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "23A08AA9-00DE-4ED0-86C3-B34C5E3325B7",
          "type": "message",
          "user": "U03JLUH1V8F",
          "text": "I assume that you're checking some kind of Regex for the tracking numbers. I also believe that maybe they have other validations as well like international tracking numbers have some check code. I wish there could be a protocol or a delegate method so that we could define our new methods or the extend the existing ones. I know I just can just scan for text but it would be better to extend the existing ones. ",
          "ts": "1654811315.397209",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YKXq7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "assume"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " you're "
                    },
                    {
                      "type": "text",
                      "text": "checking"
                    },
                    {
                      "type": "text",
                      "text": " some "
                    },
                    {
                      "type": "text",
                      "text": "kind"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Regex"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " the tracking numbers"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "also"
                    },
                    {
                      "type": "text",
                      "text": " believe "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "maybe"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "they"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "have"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "other"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "validations"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "as"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "well"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "like"
                    },
                    {
                      "type": "text",
                      "text": " international tracking numbers "
                    },
                    {
                      "type": "text",
                      "text": "have"
                    },
                    {
                      "type": "text",
                      "text": " some "
                    },
                    {
                      "type": "text",
                      "text": "check"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "code."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "wish"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "there"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "could"
                    },
                    {
                      "type": "text",
                      "text": " be "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " protocol "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "delegate"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "method"
                    },
                    {
                      "type": "text",
                      "text": " so "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "we"
                    },
                    {
                      "type": "text",
                      "text": " could "
                    },
                    {
                      "type": "text",
                      "text": "define"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "our"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "new"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "methods"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "extend"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "existing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "ones."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " know I "
                    },
                    {
                      "type": "text",
                      "text": "just"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "can"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "just"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "scan"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "but"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "it"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "would"
                    },
                    {
                      "type": "text",
                      "text": " be "
                    },
                    {
                      "type": "text",
                      "text": "better"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "extend"
                    },
                    {
                      "type": "text",
                      "text": " the "
                    },
                    {
                      "type": "text",
                      "text": "existing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "ones."
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "19afc147-261b-4782-801d-9494674313e6",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "Gotcha, I think I understand. Maybe something that gives you some text and you can tell us the range you’re interested in.",
          "ts": "1654811481.844589",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qzm",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Gotcha, I think I understand. Maybe something that gives you some text and you can tell us the range you’re interested in."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "EB2FC163-7D2E-4B3D-ADCE-5F1A77F6E5FB",
          "type": "message",
          "user": "U03JLUH1V8F",
          "text": "Exactly. Currently live text can find the FedEx tracking number because it's 12 digits. It knows that 12 digits is a tracking number and it draws a box around it. So maybe my courier Acme Corp has a tracking number of ACME-\u003ctel:12526737|12526737\u003e. It can't detect this as a tracking number because live text supports limited number of couriers. Only way to get this with camera is, just using a default source and selecting it manually. I just wish we could extend the default ones or give visionkit a pattern to look for.",
          "ts": "1654812063.356429",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 2,
              "users": [
                "U03HB4KUYH5",
                "U03K02X4LP6"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bNLf",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Exactly."
                    },
                    {
                      "type": "text",
                      "text": " Currently "
                    },
                    {
                      "type": "text",
                      "text": "live"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "can"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "find"
                    },
                    {
                      "type": "text",
                      "text": " the FedEx tracking "
                    },
                    {
                      "type": "text",
                      "text": "number"
                    },
                    {
                      "type": "text",
                      "text": " because it's "
                    },
                    {
                      "type": "text",
                      "text": "12"
                    },
                    {
                      "type": "text",
                      "text": " digits"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "It"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "knows"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "that"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "12"
                    },
                    {
                      "type": "text",
                      "text": " digits "
                    },
                    {
                      "type": "text",
                      "text": "is"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " tracking "
                    },
                    {
                      "type": "text",
                      "text": "number"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "it"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "draws"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " box "
                    },
                    {
                      "type": "text",
                      "text": "around"
                    },
                    {
                      "type": "text",
                      "text": " it"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "So"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "maybe"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "my"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "courier"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Acme"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Corp has"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " tracking "
                    },
                    {
                      "type": "text",
                      "text": "number"
                    },
                    {
                      "type": "text",
                      "text": " of "
                    },
                    {
                      "type": "text",
                      "text": "ACME-"
                    },
                    {
                      "type": "link",
                      "url": "tel:12526737",
                      "text": "12526737"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "It"
                    },
                    {
                      "type": "text",
                      "text": " can't detect "
                    },
                    {
                      "type": "text",
                      "text": "this"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "as"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " tracking "
                    },
                    {
                      "type": "text",
                      "text": "number"
                    },
                    {
                      "type": "text",
                      "text": " because "
                    },
                    {
                      "type": "text",
                      "text": "live"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "text"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "supports"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "limited"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "number"
                    },
                    {
                      "type": "text",
                      "text": " of cou"
                    },
                    {
                      "type": "text",
                      "text": "riers."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Only"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "way"
                    },
                    {
                      "type": "text",
                      "text": " to "
                    },
                    {
                      "type": "text",
                      "text": "get"
                    },
                    {
                      "type": "text",
                      "text": " this with "
                    },
                    {
                      "type": "text",
                      "text": "camera"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "is, just"
                    },
                    {
                      "type": "text",
                      "text": " using "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "default"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "source"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " selecting "
                    },
                    {
                      "type": "text",
                      "text": "it"
                    },
                    {
                      "type": "text",
                      "text": " manually"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "just"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "wish"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "we"
                    },
                    {
                      "type": "text",
                      "text": " could "
                    },
                    {
                      "type": "text",
                      "text": "extend"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " default "
                    },
                    {
                      "type": "text",
                      "text": "ones"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " give "
                    },
                    {
                      "type": "text",
                      "text": "visionkit"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "pattern"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "look"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "95289f3a-cf17-45dc-b916-7c1aabb0ce6a",
          "type": "message",
          "user": "U03J1TN6WBD",
          "text": "^ This may be useful for detecting car license plates aswell imo (different patterns depending on countries though)",
          "ts": "1654814078.260669",
          "thread_ts": "1654808559.673649",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 1,
              "users": [
                "U03HB4KUYH5"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "sQ=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "^ This may be useful for detecting car license plates aswell imo (different patterns depending on countries though)"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ2VBE21\u003e asked\n\u0026gt; With VNDocumentCameraViewController, is it possible to limit it to just one scan, so that the user doesn't have to press \"Save\" at the end?",
      "ts": "1654808811.704669",
      "thread_ts": "1654808811.704669",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654808971.186359",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Z2TmH",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ2VBE21"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"With VNDocumentCameraViewController, is it possible to limit it to just one scan, so that the user doesn't have to press \\\"Save\\\" at the end?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "3b0743e5-f1fa-4d54-af60-ce89a9095fa2",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "No, sorry. Would appreciate a Feedback for an enhancement request, though",
          "ts": "1654808829.726709",
          "thread_ts": "1654808811.704669",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "a+I",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "No, sorry. Would appreciate a Feedback for an enhancement request, though"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2074be84-5c94-4a5f-90c2-ee960afa4f7e",
          "type": "message",
          "user": "U03HZ2VBE21",
          "text": "Done: FB10140507",
          "ts": "1654808971.186359",
          "thread_ts": "1654808811.704669",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7rSt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Done: FB10140507"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPP4S3A\u003e asked\n\u0026gt; I am looking to detect or classify a jersey number from a sporting event such as hockey in a video,  I have tried VNRecongnizeTextRequest but do not get good results is there a better way to do such a task? Would I be better off creating my own model for this?",
      "ts": "1654808902.584709",
      "thread_ts": "1654808902.584709",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 7,
      "latest_reply": "1654809587.574279",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "IsX4",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPP4S3A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I am looking to detect or classify a jersey number from a sporting event such as hockey in a video,  I have tried VNRecongnizeTextRequest but do not get good results is there a better way to do such a task? Would I be better off creating my own model for this?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "dd168da7-dcb1-496e-9b1c-eec00d3cc7d4",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Are those numbers obscured or deformed?",
          "ts": "1654808997.933909",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "8A1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Are those numbers obscured or deformed?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f0ff45cf-de9d-4af2-8b6b-07a05006a1d7",
          "type": "message",
          "user": "U03JRPP4S3A",
          "text": "Potentially yes to both",
          "ts": "1654809031.807219",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EUJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Potentially yes to both"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7b8acfb1-b294-4e61-94c6-f1757560bf87",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "You could train a custom classifier but that requires loads of images to get good results from that",
          "ts": "1654809073.586279",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ur+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You could train a custom classifier but that requires loads of images to get good results from that"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7a0c4f86-90b9-492d-8975-bc95af532a1c",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "The results should have improved a bit using Revision3 of the VNRecognizeTextRequest. Have you tried it?",
          "ts": "1654809129.700619",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fTlG1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The results should have improved a bit using Revision3 of the VNRecognizeTextRequest. Have you tried it?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "82263ff7-a131-4293-8bad-d6192cf8990e",
          "type": "message",
          "user": "U03JRPP4S3A",
          "text": "I have not yet, I certainly intend on giving it a try, I just wondered if there was a better approach",
          "ts": "1654809214.286449",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ESCRr",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I have not yet, I certainly intend on giving it a try, I just wondered if there was a better approach"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "edce6479-c8dd-4fda-88eb-a7ef182977fc",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "When text gets deformed on fabric or obscured it gets very difficult to read.",
          "ts": "1654809442.612159",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "d1j4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "When text gets deformed on fabric or obscured it gets very difficult to read."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4c11d5e6-8e9e-4fbf-9b0d-f46b544b4d1b",
          "type": "message",
          "user": "U03JRPP4S3A",
          "text": "Is there any way that VNRecognizeTextRequest can be restrained to only numbers?",
          "ts": "1654809587.574279",
          "thread_ts": "1654808902.584709",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZfBut",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Is there any way that VNRecognizeTextRequest can be restrained to only numbers?"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JPBUSHLZ\u003e asked\n\u0026gt; Clarifying question on what's new in vision. I think v3 brings improved face recognition \u0026amp; barcode recognition \u0026amp; previews for those. Optical flow is entirely new, and the UI for text recognition through video is entirely new. Do I have this right? Anything else new in Vision?",
      "ts": "1654808903.236529",
      "thread_ts": "1654808903.236529",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654809021.429949",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "ehrIK",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JPBUSHLZ"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Clarifying question on what's new in vision. I think v3 brings improved face recognition \u0026 barcode recognition \u0026 previews for those. Optical flow is entirely new, and the UI for text recognition through video is entirely new. Do I have this right? Anything else new in Vision?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "f90cc251-3fa5-4b9b-b277-be5fa4a24463",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "Optical flow is not entirely new, there was already a prior revision 1 for optical flow. You are correct about barcode, but face recognition is not offered by Vision. You are also correct that the UI for text recognition is new. Other things new in Vision are a new text recognition revision, and the new functionality in Xcode for Quick Look Preview support. We also deprecated older face detection and face landmarks revisions.",
          "ts": "1654809021.429949",
          "thread_ts": "1654808903.236529",
          "edited": {
            "user": "U03J98R7N5A",
            "ts": "1654809107.000000"
          },
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "gratitude-thank-you",
              "count": 1,
              "users": [
                "U03JPBUSHLZ"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ym2C",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Optical flow is not entirely new, there was already a prior revision 1 for optical flow. You are correct about barcode, but face recognition is not offered by Vision. You are also correct that the UI for text recognition is new. Other things new in Vision are a new text recognition revision, and the new functionality in Xcode for Quick Look Preview support. We also deprecated older face detection and face landmarks revisions."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ3RCQLV\u003e asked\n\u0026gt; Up until iOS15, the rectangle tracking VNTrackRectangleRequest returned precise corners of tracked rectangle. Since iOS15, it seems to only return the bounding box. This is present even in the original detection/tracking demos.\n\u0026gt; \n\u0026gt; What is the suggested way to get tracked rectangles (and also support the original vision framework to support iOS11+)?",
      "ts": "1654808931.056519",
      "thread_ts": "1654808931.056519",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654859675.245399",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "GhFl",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ3RCQLV"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Up until iOS15, the rectangle tracking VNTrackRectangleRequest returned precise corners of tracked rectangle. Since iOS15, it seems to only return the bounding box. This is present even in the original detection\\/tracking demos.\\n\\nWhat is the suggested way to get tracked rectangles (and also support the original vision framework to support iOS11+)?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "4cefcdff-c9a8-4c90-b9bf-c3d158ddfa93",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Have you tried the iOS16 beta?",
          "ts": "1654809168.555769",
          "thread_ts": "1654808931.056519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nTQ0M",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Have you tried the iOS16 beta?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5be16261-b60d-4136-b5cd-125931e0e179",
          "type": "message",
          "user": "U03HZ3RCQLV",
          "text": "Not yet, but definitely will try as I get the chance :pray:\n\nIt seemed like an intentional change in iOS15, as I remember the documentation was previously describing the real corners of tracked rectangle being available, while now it describes the corners as being bounding box only (as for any other `VNDetectedObjectObservation`).",
          "ts": "1654859574.935489",
          "thread_ts": "1654808931.056519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "CsYW",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Not yet, but definitely will try as I get the chance "
                    },
                    {
                      "type": "emoji",
                      "name": "pray",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": "\n\nIt seemed like an intentional change in iOS15, as I remember the documentation was previously describing the real corners of tracked rectangle being available, while now it describes the corners as being bounding box only (as for any other "
                    },
                    {
                      "type": "text",
                      "text": "VNDetectedObjectObservation",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ")."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b554f4be-ff70-45c7-b8a4-430622a55d65",
          "type": "message",
          "user": "U03HZ3RCQLV",
          "text": "What would be the suggested workaround for iOS15 - is there any other vision API that could be used for rectangle corners tracking? (ideally similar to what the native scanner in Notes nad Files offers)",
          "ts": "1654859675.245399",
          "thread_ts": "1654808931.056519",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "TGs42",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "What would be the suggested workaround for iOS15 - is there any other vision API that could be used for rectangle corners tracking? (ideally similar to what the native scanner in Notes nad Files offers)"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JPBUSHLZ\u003e asked\n\u0026gt; What's the best way to create a \"silhouette\" video as opposed to a silhouette photo? Would Optical flow be best for this or sampling every frame for a silhoette or... Thank you.",
      "ts": "1654809156.551689",
      "thread_ts": "1654809156.551689",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654809234.495589",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "e2p5",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JPBUSHLZ"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What's the best way to create a \\\"silhouette\\\" video as opposed to a silhouette photo? Would Optical flow be best for this or sampling every frame for a silhoette or... Thank you.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "63949cb8-5127-4eb1-bdf9-9510cd58b08b",
          "type": "message",
          "user": "U03HK3N00TG",
          "text": "It depends! The two key considerations are:\n\n• how expensive is it to generate a silhouette a priori for every frame? If that’s cheap enough, it might be simpler and better to do that;\n• on the other hand, optical flow can help in frame-to-frame stability.\nIt’s really going to depend on surrounding context and performance requirements (both latency and accuracy).",
          "ts": "1654809234.495589",
          "thread_ts": "1654809156.551689",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "gratitude-thank-you",
              "count": 1,
              "users": [
                "U03JPBUSHLZ"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0leI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It depends! The two key considerations are:\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"how expensive is it to generate a silhouette a priori for every frame? If that\\u2019s cheap enough, it might be simpler and better to do that;\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"on the other hand, optical flow can help in frame-to-frame stability.\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "\nIt’s really going to depend on surrounding context and performance requirements (both latency and accuracy)."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J2AD912N\u003e asked\n\u0026gt; What is your recommendation for using DataScannerViewController to detect money/currency values? DataScannerViewController.TextContentType does not appear to support money, currencies, or generic numbers (see FB10139138). The iOS Camera app supports money/currency detection in iOS 16. What is the best practice for me to implement a similar feature in my app? Should I recognize all text and then parse each recognized text item myself to determine if the string value contains number or currency amount?",
      "ts": "1654809180.187819",
      "thread_ts": "1654809180.187819",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654810066.569859",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "ZwA",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J2AD912N"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What is your recommendation for using DataScannerViewController to detect money\\/currency values? DataScannerViewController.TextContentType does not appear to support money, currencies, or generic numbers (see FB10139138). The iOS Camera app supports money\\/currency detection in iOS 16. What is the best practice for me to implement a similar feature in my app? Should I recognize all text and then parse each recognized text item myself to determine if the string value contains number or currency amount?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "96e13f7f-5f84-4fdc-87ec-42be4a33005b",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "Ah, good enhancement request. Currently we're not supporting currency.",
          "ts": "1654809212.554709",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PlMya",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah, good enhancement request. Currently we're not supporting currency."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "60bccbed-2fb3-4671-96dc-a9d14cc0b94a",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "you might be able to detect the presence of currency with UIDataDetectors, but you won't be able to highlight them.",
          "ts": "1654809294.000119",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4NYBc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "you might be able to detect the presence of currency with UIDataDetectors, but you won't be able to highlight them."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0c1c3e72-1d78-4d6b-a596-48c0b7f8565b",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "another option is to use `capturePhoto()` to take as till then use the Live Text APIs",
          "ts": "1654809317.981129",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "P2ci",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "another option is to use "
                    },
                    {
                      "type": "text",
                      "text": "capturePhoto()",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " to take as till then use the Live Text APIs"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "98959e91-1aab-469a-805f-98aec91ab25e",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "That'll highlight all the data detector elements, not just money.",
          "ts": "1654809353.306889",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "FSuJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That'll highlight all the data detector elements, not just money."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "90655586-6329-45d8-b509-cd6c6367b16b",
          "type": "message",
          "user": "U03DJTBMHFF",
          "text": "Maybe some business logic on top of Vision text recognition to dial in on numbers only, specific (relative) text size or even position of the text in the rectangle of the currency?",
          "ts": "1654809448.484629",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "p9q/x",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Maybe some business logic on top of Vision text recognition to dial in on numbers only, specific (relative) text size or even position of the text in the rectangle of the currency?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1f1c81af-f369-4a6f-8baa-c98f8b2ac885",
          "type": "message",
          "user": "U03J2AD912N",
          "text": "Ah, I see. So I assume that I could grab all text, but wouldn’t necessarily be able to easily find the exact position of the numbers/currency in the view to provide an accurate highlight rectangle of just that portion.",
          "ts": "1654809588.477149",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Vmky",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah, I see. So I assume that I could grab all text, but wouldn’t necessarily be able to easily find the exact position of the numbers/currency in the view to provide an accurate highlight rectangle of just that portion."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cfc1bfa3-2b3a-4ac1-9a42-afd019c45ef4",
          "type": "message",
          "user": "U03J2AD912N",
          "text": "Although I will give Eric’s suggestion a shot to see if that works.",
          "ts": "1654809614.218459",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "cku1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Although I will give Eric’s suggestion a shot to see if that works."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5d817845-cedb-4327-87d9-76055ead82db",
          "type": "message",
          "user": "U03J2AD912N",
          "text": "I also just filed FB10140834\nDetermine bounds of substring of text from DataScannerViewController RecognizedItem",
          "ts": "1654810066.569859",
          "thread_ts": "1654809180.187819",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 1,
              "users": [
                "U03HB4KUYH5"
              ]
            },
            {
              "name": "gratitude-thank-you",
              "count": 1,
              "users": [
                "U03HB4KUYH5"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6v6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I also just filed FB10140834\nDetermine bounds of substring of text from DataScannerViewController RecognizedItem"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRPP4S3A\u003e asked\n\u0026gt; What is the difference between optical flow and a VNTrajectoryRequest? Would tracking a trajectory of an object benefit from a work flow that used both?",
      "ts": "1654809212.399269",
      "thread_ts": "1654809212.399269",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654809444.663009",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "RReO",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRPP4S3A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What is the difference between optical flow and a VNTrajectoryRequest? Would tracking a trajectory of an object benefit from a work flow that used both?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "f0eb78de-6b71-4aa8-9dec-7e10cf303bb0",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "The trajectory request is especially developed to track objects on a trajectory meaning not any kind of zig zag path. Optical flow will detect any motion without the constraint of a trajectory",
          "ts": "1654809319.049859",
          "thread_ts": "1654809212.399269",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5g6F",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The trajectory request is especially developed to track objects on a trajectory meaning not any kind of zig zag path. Optical flow will detect any motion without the constraint of a trajectory"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "06c5c47c-d127-40eb-80df-be4bef59cdc5",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "So if you want to track a ball been thrown, you will get better results with the trajectory request.",
          "ts": "1654809364.150849",
          "thread_ts": "1654809212.399269",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zgc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So if you want to track a ball been thrown, you will get better results with the trajectory request."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "adcf0c8f-b3fa-4d61-b731-f13176cb6456",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "If you want to see if something moved in for instance security camera footage then you use optical flow.",
          "ts": "1654809397.727909",
          "thread_ts": "1654809212.399269",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "23K",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you want to see if something moved in for instance security camera footage then you use optical flow."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b209adf3-10c4-4e21-b806-f0370fb5d46a",
          "type": "message",
          "user": "U03JRPP4S3A",
          "text": "understood thank you \u003c@U03JFF1S5U0\u003e",
          "ts": "1654809444.663009",
          "thread_ts": "1654809212.399269",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0JxN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "understood thank you "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JFF1S5U0"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JQRD5KQS\u003e asked\n\u0026gt; v3 extends VNRecognizeTextRequest with automaticallyDetectsLanguage - If I turn this on, how do I discover what language it decided to use?",
      "ts": "1654809268.528409",
      "thread_ts": "1654809268.528409",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654811426.514839",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "rE0K",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JQRD5KQS"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"v3 extends VNRecognizeTextRequest with automaticallyDetectsLanguage - If I turn this on, how do I discover what language it decided to use?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "116a34c9-662a-40da-bc80-c11d04057fa0",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "Vision will not tell you which languages have been detected. The intent of this is to allow the client to give a \"hint\" to the algorithm. If you already know the language up-front, it's best to specify that language explicitly, which allows the framework to target that language for better accuracy. If you do not, it's better to set automaticallyDetectsLanguage to true, which essentially is communicating to the framework \"I don't know which language\" and the framework will do its best to decode any language.",
          "ts": "1654809407.800359",
          "thread_ts": "1654809268.528409",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2W+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Vision will not tell you which languages have been detected. The intent of this is to allow the client to give a \"hint\" to the algorithm. If you already know the language up-front, it's best to specify that language explicitly, which allows the framework to target that language for better accuracy. If you do not, it's better to set automaticallyDetectsLanguage to true, which essentially is communicating to the framework \"I don't know which language\" and the framework will do its best to decode any language."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "75b1a983-d283-443d-a460-9c9469845f0b",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "You can use \u003chttps://developer.apple.com/documentation/naturallanguage/nllanguagerecognizer|NLLanguageRecognizer\u003e to detect the dominate language after the text has been extracted by vision.",
          "ts": "1654809422.557569",
          "thread_ts": "1654809268.528409",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JQRD5KQS"
              ]
            },
            {
              "name": "100",
              "count": 1,
              "users": [
                "U03J98R7N5A"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HsCK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can use "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/naturallanguage/nllanguagerecognizer",
                      "text": "NLLanguageRecognizer"
                    },
                    {
                      "type": "text",
                      "text": " to detect the dominate language after the text has been extracted by vision."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7b8242ce-775f-4397-ac2e-8a412be491d4",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Usually a sentence is sufficient to identify language. You can pass in as much as you like, but the algorithm limits the amount of text it will consider. Less than maybe 5-10 words is challenging.",
          "ts": "1654809466.731909",
          "thread_ts": "1654809268.528409",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2r7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Usually a sentence is sufficient to identify language. You can pass in as much as you like, but the algorithm limits the amount of text it will consider. Less than maybe 5-10 words is challenging."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e13dad88-2105-4c70-81e9-4451d1a2d712",
          "type": "message",
          "user": "U03HRM0UK8B",
          "text": "If you have some prior information as to what the language might be, you can also pass hints and/or constraints to NLLanguageRecognizer.",
          "ts": "1654811426.514839",
          "thread_ts": "1654809268.528409",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "adyiQ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you have some prior information as to what the language might be, you can also pass hints and/or constraints to NLLanguageRecognizer."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ2VBE21\u003e asked\n\u0026gt; How different are VNRecognizedTextObservations (returned by VNRecognizeTextRequest) to the RecognizedItem array returned by DataScannerViewController? Do they have the same information in them? Also, is the DataScannerViewController using the same VNRecognizeTextRequest (with revision3) in the background to process the results?",
      "ts": "1654809594.940289",
      "thread_ts": "1654809594.940289",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654810007.721259",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "cVbKe",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ2VBE21"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"How different are VNRecognizedTextObservations (returned by VNRecognizeTextRequest) to the RecognizedItem array returned by DataScannerViewController? Do they have the same information in them? Also, is the DataScannerViewController using the same VNRecognizeTextRequest (with revision3) in the background to process the results?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "49d7844c-4e87-423c-9e43-9b14b90b083f",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "RecognizedItem contains a lot of the same information. transcript, corners (RecognizedItem's are in view coordinates, however)... and RecognizedItem exposes the related Vision observation. RecognizedItem however learns over time so the longer we see a text group, the more accurate the transcript will be. The Vision observation that is exposed, it is really just based on the last frame processed.",
          "ts": "1654809865.205439",
          "thread_ts": "1654809594.940289",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03JQRD5KQS",
                "U03HZ2VBE21"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "W2ra",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "RecognizedItem contains a lot of the same information. transcript, corners (RecognizedItem's are in view coordinates, however)... and RecognizedItem exposes the related Vision observation. RecognizedItem however learns over time so the longer we see a text group, the more accurate the transcript will be. The Vision observation that is exposed, it is really just based on the last frame processed."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c81afc17-7a0b-4492-bc1e-7ce72492505a",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "I cannot state which revision it uses, if any (sorry to be vague). But DataScanner supports the same languages as VNRecognizeTextRequest.",
          "ts": "1654809952.167769",
          "thread_ts": "1654809594.940289",
          "edited": {
            "user": "U03HB4KUYH5",
            "ts": "1654810051.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hL1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I cannot state which revision it uses, if any (sorry to be vague). But DataScanner supports the same languages as VNRecognizeTextRequest."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "11062c21-5624-45e6-b807-817bb69fd2d7",
          "type": "message",
          "user": "U03HZ2VBE21",
          "text": "Thank you!",
          "ts": "1654810007.721259",
          "thread_ts": "1654809594.940289",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HB4KUYH5"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xDhx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ81AHCN\u003e asked\n\u0026gt; Can VNRecongnizeTextRequest be used to perform text recognition on images with handwritten content or should it only be used for typed text (or very close to typed)?",
      "ts": "1654810299.311039",
      "thread_ts": "1654810299.311039",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 22,
      "latest_reply": "1654813231.459739",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "G2UO",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ81AHCN"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Can VNRecongnizeTextRequest be used to perform text recognition on images with handwritten content or should it only be used for typed text (or very close to typed)?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "c87e365f-e72b-4958-a5f8-d71ee088bb00",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Yes it can to some degree. It won't read my bad handwriting for sure but others will work. Handwriting is of course so varied that it really depends on the person who writes it.",
          "ts": "1654810404.558939",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Doa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes it can to some degree. It won't read my bad handwriting for sure but others will work. Handwriting is of course so varied that it really depends on the person who writes it."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "20d14b0f-786d-4129-9bac-20b0f7a0b4ba",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Closely typed text is a bit vague. Are you saying the characters are touching?",
          "ts": "1654810450.086979",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7Ee",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Closely typed text is a bit vague. Are you saying the characters are touching?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fd5d5982-d94c-4811-8778-7c86d7aec8f8",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "Sorry, my bad. I intended to say something which is written in a way that is very similar to typed text",
          "ts": "1654810493.605759",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vCan",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sorry, my bad. I intended to say something which is written in a way that is very similar to typed text"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c9dc6695-1ee5-4975-b642-73ce3eab02f5",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "*Try it*: Open the image in Preview on the Mac, select all the text and paste it into Text Editor",
          "ts": "1654810515.552189",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "JNQh7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Try it",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ": Open the image in Preview on the Mac, select all the text and paste it into Text Editor"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b369a39b-ba59-4ef9-9578-8186f5eb761d",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "\u003c@U03JQRD5KQS\u003e Thanks for the suggestion! Indeed it works quite well, but isn’t Preview using Live Text for the recognition?",
          "ts": "1654810907.746839",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "d23",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JQRD5KQS"
                    },
                    {
                      "type": "text",
                      "text": " Thanks for the suggestion! Indeed it works quite well, but isn’t Preview using Live Text for the recognition?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f1c534c6-8824-4fe7-91a4-8d5b34e9e708",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "I'm under the impression that Live Text is mostly a marketing name: that it is powered by the Vision framework.",
          "ts": "1654810961.321779",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "C=MFN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I'm under the impression that Live Text is mostly a marketing name: that it is powered by the Vision framework."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "00a9a492-619e-4188-b408-f11b771d9fd3",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "Better: download Simple Comic from the Apple Mac App store: it is free, and open source. create a .cbz file by compressing your image in Finder, then give rename that .zip to a .cbz. Open it in Simple Comic. If it works, then you'll know Vision Framework is good enough.",
          "ts": "1654811217.649149",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kyPQg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Better: download Simple Comic from the Apple Mac App store: it is free, and open source. create a .cbz file by compressing your image in Finder, then give rename that .zip to a .cbz. Open it in Simple Comic. If it works, then you'll know Vision Framework is good enough."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6160e86f-0a23-4c70-9c42-e2f60b812f04",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "Yeah I had the same impression, that Live Text is a higher level of abstraction over the underlying Vision API",
          "ts": "1654811297.372079",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "NFH4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yeah I had the same impression, that Live Text is a higher level of abstraction over the underlying Vision API"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "12b2e1e0-fb7c-4e78-aa5b-3ef215dffe71",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "Thanks for the suggestion of Simple Comic, I’ll definitely try to use it to check how Vision behaves!",
          "ts": "1654811345.311209",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "DsD8+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for the suggestion of Simple Comic, I’ll definitely try to use it to check how Vision behaves!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7bd9e70e-0eaa-427a-9f83-9a1a57d8c8c2",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "\u003chttps://github.com/MaddTheSane/Simple-Comic/blob/arc/Classes/Session/OCRVision/OCRVision.m\u003e",
          "ts": "1654811389.419059",
          "thread_ts": "1654810299.311039",
          "attachments": [
            {
              "fallback": "GitHub: Simple-Comic/OCRVision.m at arc · MaddTheSane/Simple-Comic",
              "id": 1,
              "title": "Simple-Comic/OCRVision.m at arc · MaddTheSane/Simple-Comic",
              "title_link": "https://github.com/MaddTheSane/Simple-Comic/blob/arc/Classes/Session/OCRVision/OCRVision.m",
              "text": "macOS comic viewer. Contribute to MaddTheSane/Simple-Comic development by creating an account on GitHub.",
              "image_url": "https://opengraph.githubassets.com/7b11a7e805806a13f98d8910286551a4431a01217ba3c63a5a7df63656016009/MaddTheSane/Simple-Comic",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/MaddTheSane/Simple-Comic/blob/arc/Classes/Session/OCRVision/OCRVision.m",
              "original_url": "https://github.com/MaddTheSane/Simple-Comic/blob/arc/Classes/Session/OCRVision/OCRVision.m",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PYpJy",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://github.com/MaddTheSane/Simple-Comic/blob/arc/Classes/Session/OCRVision/OCRVision.m",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "397b3b3c-fec3-4428-9335-a57d51702e94",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Live text is using Vision for its recognition work",
          "ts": "1654811505.770209",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "y8zy",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Live text is using Vision for its recognition work"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3751171b-7d3e-4321-818c-303c151042ee",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "Thanks a lot \u003c@U03JFF1S5U0\u003e and \u003c@U03JQRD5KQS\u003e! Just one last question: I saw on another thread that there is a new revision of VNRecognizeTextRequest (v3), but I can’t find it in the documentation: how can I enable it and can I “force” Vision to use a “minimum revision” (for example, 3 and later)?",
          "ts": "1654811883.035809",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HlQH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks a lot "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JFF1S5U0"
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JQRD5KQS"
                    },
                    {
                      "type": "text",
                      "text": "! Just one last question: I saw on another thread that there is a new revision of VNRecognizeTextRequest (v3), but I can’t find it in the documentation: how can I enable it and can I “force” Vision to use a “minimum revision” (for example, 3 and later)?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "05affcf7-fbcb-473f-b247-385bd1e2ee0f",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "See the header files inside the Xcode 14 beta. - Don't expect it to work before you have the new operating systems",
          "ts": "1654811985.554579",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zK3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "See the header files inside the Xcode 14 beta. - Don't expect it to work before you have the new operating systems"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0865e981-8006-4cc9-a681-1d31ee58126d",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "@*\u003chttps://app.slack.com/team/U03HZ81AHCN|Alessandro\u003e* - the source file I posted shows how you set the `revision` the request should use. You can also use responsdsToSelector() but I wouldn't trust it",
          "ts": "1654812105.212439",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zuP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "@"
                    },
                    {
                      "type": "link",
                      "url": "https://app.slack.com/team/U03HZ81AHCN",
                      "text": "Alessandro",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " - the source file I posted shows how you set the "
                    },
                    {
                      "type": "text",
                      "text": "revision",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " the request should use. You can also use responsdsToSelector() but I wouldn't trust it"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a21ecf07-55a1-4bac-a762-f3c5158ff892",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "I handle code from Beta SDKs like this:\n```#if defined(MAC_OS_VERSION_13_0) \u0026amp;\u0026amp; MAC_OS_X_VERSION_MAX_ALLOWED \u0026gt;= MAC_OS_VERSION_13_0\n#warn \"if you can see this, it's time to remove the #if.\"\n\t\t\tif (@available(macOS 13.0, *))\n\t\t\t{\n\t\t\t\trevision = VNRecognizeTextRequestRevision3;\n\t\t\t} else\n#endif```",
          "ts": "1654812312.699849",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "b7ho",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I handle code from Beta SDKs like this:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"#if defined(MAC_OS_VERSION_13_0) \u0026\u0026 MAC_OS_X_VERSION_MAX_ALLOWED \u003e= MAC_OS_VERSION_13_0\\n#warn \\\"if you can see this, it's time to remove the #if.\\\"\\n\\t\\t\\tif (@available(macOS 13.0, *))\\n\\t\\t\\t{\\n\\t\\t\\t\\trevision = VNRecognizeTextRequestRevision3;\\n\\t\\t\\t} else\\n#endif\"}],\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dbc82a24-76f4-41f0-8433-7bcf4bfb9c9d",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "…\n\n```#if defined(MAC_OS_VERSION_13_0) \u0026amp;\u0026amp; MAC_OS_X_VERSION_MAX_ALLOWED \u0026gt;= MAC_OS_VERSION_13_0\n#warn \"if you can see this, it's time to remove the #if.\"\n\t\t\tif (@available(macOS 13.0, *))\n\t\t\t{\n\t\t\t\ttextRequest.automaticallyDetectsLanguage = YES;\n\t\t\t}\n#endif```",
          "ts": "1654812377.575959",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "K243",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "…\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"#if defined(MAC_OS_VERSION_13_0) \u0026\u0026 MAC_OS_X_VERSION_MAX_ALLOWED \u003e= MAC_OS_VERSION_13_0\\n#warn \\\"if you can see this, it's time to remove the #if.\\\"\\n\\t\\t\\tif (@available(macOS 13.0, *))\\n\\t\\t\\t{\\n\\t\\t\\t\\ttextRequest.automaticallyDetectsLanguage = YES;\\n\\t\\t\\t}\\n#endif\"}],\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8885f1c6-f9a0-44ea-97b4-f3e738103e66",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "I don’t like `respondsToSelector()`either… but is `VNRecognizeTextRequestRevision3` already public? I couldn’t find it in the documentation",
          "ts": "1654812444.029689",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "MeM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I don’t like "
                    },
                    {
                      "type": "text",
                      "text": "respondsToSelector()",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "either… but is "
                    },
                    {
                      "type": "text",
                      "text": "VNRecognizeTextRequestRevision3",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " already public? I couldn’t find it in the documentation"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "741b30cc-7133-4653-8527-678f15a4549e",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "\u003chttp://Xcode-beta.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Vision.framework/Versions/A/Headers/VNRecognizeTextRequest.h|Xcode-beta.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Vision.framework/Versions/A/Headers/VNRecognizeTextRequest.h\u003e",
          "ts": "1654812516.507869",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vzLM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Xcode-beta.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Vision.framework/Versions/A/Headers/VNRecognizeTextRequest.h"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e8f8baca-5fce-41c8-aaf8-34f387a2727c",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "Also mentioned in \u003chttps://developer.apple.com/videos/play/wwdc2022/10024/\u003e",
          "ts": "1654812684.029949",
          "thread_ts": "1654810299.311039",
          "attachments": [
            {
              "fallback": "Apple Developer: What's new in Vision - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "What's new in Vision - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10024/",
              "text": "Learn about the latest updates to Vision APIs that help your apps recognize text, detect faces and face landmarks, and implement optical...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6517/6517_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10024/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10024/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lRs",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also mentioned in "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10024/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a4dad3e2-a644-449c-98eb-2590c18087aa",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "So then it probably hasn’t been added to the online documentation yet but is already available for use",
          "ts": "1654813186.326849",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7DB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So then it probably hasn’t been added to the online documentation yet but is already available for use"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "bab67ad1-d7fc-43d0-8eec-530dd0542d98",
          "type": "message",
          "user": "U03HZ81AHCN",
          "text": "Thanks a lot \u003c@U03JQRD5KQS\u003e!",
          "ts": "1654813197.312369",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "98YaK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks a lot "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JQRD5KQS"
                    },
                    {
                      "type": "text",
                      "text": "!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ed1f4472-0cfc-4daa-9d0e-00958a017778",
          "type": "message",
          "user": "U03JQRD5KQS",
          "text": "Happy to help",
          "ts": "1654813231.459739",
          "thread_ts": "1654810299.311039",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "a3GSG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Happy to help"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J20RJQ2X\u003e asked\n\u0026gt; Last year you introduced the VNGeneratePersonSegmentationRequest. I know you can't comment on future plans, but it would be amazing if the new pet / object segmentation of iOS 16 was available to developers",
      "ts": "1654810464.957099",
      "thread_ts": "1654810464.957099",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654810497.874529",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 1,
          "users": [
            "U03J7JKA23F"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "KbwlP",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20RJQ2X"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Last year you introduced the VNGeneratePersonSegmentationRequest. I know you can't comment on future plans, but it would be amazing if the new pet \\/ object segmentation of iOS 16 was available to developers\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "598b03dd-538d-4241-af62-34ec910ef282",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "It is always good to file feedback and explain what you are looking for.",
          "ts": "1654810497.874529",
          "thread_ts": "1654810464.957099",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 2,
              "users": [
                "U03J20RJQ2X",
                "U03J7JKA23F"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5JCp6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It is always good to file feedback and explain what you are looking for."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JRR42L48\u003e asked\n\u0026gt; Live Text seems to be added to UIImageView via `.addInteraction(\u0026lt;ImageAnalysisInteraction\u0026gt;)`. Is there a way to add this interaction to SwiftUI's `Image`?",
      "ts": "1654810723.841029",
      "thread_ts": "1654810723.841029",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 5,
      "latest_reply": "1654811812.983689",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Vs9",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JRR42L48"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Live Text seems to be added to UIImageView via `.addInteraction(\u003cImageAnalysisInteraction\u003e)`. Is there a way to add this interaction to SwiftUI's `Image`?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "33e749ea-c097-4a57-a372-45e23af637c2",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "I don’t believe so. You can probably wrap a UIImageView in a ViewRepresentable - but of course it wouldn’t be a SwiftUI image anymore. I think we might have sample code that does something similar in the State of the Union donut app",
          "ts": "1654810776.113959",
          "thread_ts": "1654810723.841029",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "HzlBn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I don’t believe so. You can probably wrap a UIImageView in a ViewRepresentable - but of course it wouldn’t be a SwiftUI image anymore. I think we might have sample code that does something similar in the State of the Union donut app"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f145fb70-b93c-4431-b727-0201b2029796",
          "type": "message",
          "user": "U03HB4KUYH5",
          "text": "But yeah, we definitely need SwiftUI support for the new VisionKit APIs :slightly_smiling_face: feedbacks may help",
          "ts": "1654811012.567879",
          "thread_ts": "1654810723.841029",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 2,
              "users": [
                "U03HB4T0CA3",
                "U03J4DR9GDS"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hI9G",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "But yeah, we definitely need SwiftUI support for the new VisionKit APIs "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " feedbacks may help"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b787073d-20e0-41e2-98f0-af44fe0c7dde",
          "type": "message",
          "user": "U03JRR42L48",
          "text": "ahh. Thanks! Yes, I can file a feedback",
          "ts": "1654811044.039639",
          "thread_ts": "1654810723.841029",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "o4c8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "ahh. Thanks! Yes, I can file a feedback"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6e9c2642-2d7f-480a-99b3-cf2f737bdea7",
          "type": "message",
          "user": "U03HB4T0CA3",
          "text": "at 1:06:32 in the SOTU Video, it shows how to add the Interaction to a UIView.\n\n\u003chttps://developer.apple.com/wwdc22/102?time=3992\u003e",
          "ts": "1654811487.278829",
          "thread_ts": "1654810723.841029",
          "attachments": [
            {
              "fallback": "Apple Developer: Platforms State of the Union - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Platforms State of the Union - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/wwdc22/102?time=3992",
              "text": "Take a deeper dive into the latest tools, technologies, and advances across Apple platforms to help you create even better apps.",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6855/6855_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/wwdc22/102?time=3992",
              "original_url": "https://developer.apple.com/wwdc22/102?time=3992",
              "blocks": null
            }
          ],
          "files": [
            {
              "id": "F03JMF8D19V",
              "created": 1654811387,
              "timestamp": 1654811387,
              "name": "Screenshot 2022-06-09 at 2.48.56 PM.png",
              "title": "Screenshot 2022-06-09 at 2.48.56 PM.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03HB4T0CA3",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 177308,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03JMF8D19V-Screenshot 2022-06-09 at 2.48.56 PM.png",
              "url_private_download": "C03H4A911EH/F03JMF8D19V-Screenshot 2022-06-09 at 2.48.56 PM.png",
              "original_h": 360,
              "original_w": 618,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JMF8D19V-9801b15d0f/screenshot_2022-06-09_at_2.48.56_pm_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JMF8D19V-9801b15d0f/screenshot_2022-06-09_at_2.48.56_pm_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JMF8D19V-9801b15d0f/screenshot_2022-06-09_at_2.48.56_pm_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JMF8D19V-9801b15d0f/screenshot_2022-06-09_at_2.48.56_pm_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 210,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03JMF8D19V-9801b15d0f/screenshot_2022-06-09_at_2.48.56_pm_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 280,
              "thumb_720": "",
              "thumb_720_w": 0,
              "thumb_720_h": 0,
              "thumb_960": "",
              "thumb_960_w": 0,
              "thumb_960_h": 0,
              "thumb_1024": "",
              "thumb_1024_w": 0,
              "thumb_1024_h": 0,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03HB4T0CA3/F03JMF8D19V/screenshot_2022-06-09_at_2.48.56_pm.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03JMF8D19V-b01827ff32",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "927DM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "at 1:06:32 in the SOTU Video, it shows how to add the Interaction to a UIView.\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/wwdc22/102?time=3992",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4404814d-565f-404c-9292-fabb0e8912f1",
          "type": "message",
          "user": "U03JRR42L48",
          "text": "nice, thanks \u003c@U03HB4T0CA3\u003e! Bundling it all in a UIView and then wrapping that seems cleaner :+1:",
          "ts": "1654811812.983689",
          "thread_ts": "1654810723.841029",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "sgdmo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "nice, thanks "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HB4T0CA3"
                    },
                    {
                      "type": "text",
                      "text": "! Bundling it all in a UIView and then wrapping that seems cleaner "
                    },
                    {
                      "type": "emoji",
                      "name": "+1",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03K0BHRMNC\u003e asked\n\u0026gt; Im part of a team that is building an app that is wishing to identify and recognise faces in a collection of photos. At the moment Ive had success with Photos/Vision framework to find faces in photos and isolate them, but we're currently then sending those faces to AWS Amazon Rekognition service to help compare the face to a set of others and associate them to an existing face, or create a new face model.\n\u0026gt; \n\u0026gt; If I wanted to move this type of modelling onto the device itself (rather going through a network request to a 3rd party service), could you possibly guide me where to start? Im assuming I could do the same thing locally on device using Apple frameworks?",
      "ts": "1654810906.522209",
      "thread_ts": "1654810906.522209",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654811482.175729",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 1,
          "users": [
            "U03J4DR9GDS"
          ]
        },
        {
          "name": "eyes",
          "count": 1,
          "users": [
            "U03J4DR9GDS"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "+idV",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03K0BHRMNC"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Im part of a team that is building an app that is wishing to identify and recognise faces in a collection of photos. At the moment Ive had success with Photos\\/Vision framework to find faces in photos and isolate them, but we're currently then sending those faces to AWS Amazon Rekognition service to help compare the face to a set of others and associate them to an existing face, or create a new face model.\\n\\nIf I wanted to move this type of modelling onto the device itself (rather going through a network request to a 3rd party service), could you possibly guide me where to start? Im assuming I could do the same thing locally on device using Apple frameworks?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "aa16a034-0673-4c65-a5e5-0d6a06c461a4",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "We do not offer on-device face recognition solutions. Generally speaking you would to either find (or train, if you have the data and the know-how) a face recognition model, which could then be run on-device through CoreML once converted into that format. Often such models return some descriptor, which can be compared to other similar descriptors to provide a distance. How best to measure that distance is often tied in to how the face recognition model was trained.",
          "ts": "1654811148.099409",
          "thread_ts": "1654810906.522209",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "pl9k",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We do not offer on-device face recognition solutions. Generally speaking you would to either find (or train, if you have the data and the know-how) a face recognition model, which could then be run on-device through CoreML once converted into that format. Often such models return some descriptor, which can be compared to other similar descriptors to provide a distance. How best to measure that distance is often tied in to how the face recognition model was trained."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a09a0d6e-7b09-4bc8-b33a-2e2047f0be1f",
          "type": "message",
          "user": "U03K0BHRMNC",
          "text": "ah ok. Thanks for the detail. :slightly_smiling_face: Seems like it would be a bit of work to bring it on device (as no one on our team currently has the \"know how\" to train models), but its good to know that that is the suggested direction to go in. :slightly_smiling_face: Or continue using a service like AWS's which have really made it quite simple (at the expense of network request).\nBut thank you for the response!",
          "ts": "1654811378.273769",
          "thread_ts": "1654810906.522209",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PnM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "ah ok. Thanks for the detail. "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " Seems like it would be a bit of work to bring it on device (as no one on our team currently has the \"know how\" to train models), but its good to know that that is the suggested direction to go in. "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " Or continue using a service like AWS's which have really made it quite simple (at the expense of network request).\nBut thank you for the response!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "80a96db2-6605-40d1-a84b-02ff6898a22e",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "You may file a Feedback Assistant request if you'd like Apple to offer face recognition in the future.",
          "ts": "1654811432.365379",
          "thread_ts": "1654810906.522209",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "o1b",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You may file a Feedback Assistant request if you'd like Apple to offer face recognition in the future."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7a101e25-b853-4a39-aa2d-85b82ed1aa86",
          "type": "message",
          "user": "U03K0BHRMNC",
          "text": "Haha that would be a dream feature indeed. So I shall contribute a feedback request! :blush: thanks!",
          "ts": "1654811482.175729",
          "thread_ts": "1654810906.522209",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "GDu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Haha that would be a dream feature indeed. So I shall contribute a feedback request! "
                    },
                    {
                      "type": "emoji",
                      "name": "blush",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " thanks!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ5ALQ5T\u003e asked\n\u0026gt; We currently use a CoreML model with a C+ framework to handle initialization parameters in our processing queue (how long to hold an object, time an object should be in frame etc) and then run the ML model on the image captured with those parameters. Is Vision a better alternative than running our own initializers like that? Can we specify with Vision the retention time of images for processing images asyncronously? What is best practice there? Thank you!",
      "ts": "1654811601.664869",
      "thread_ts": "1654811601.664869",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654875138.330819",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "J9BP",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ5ALQ5T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"We currently use a CoreML model with a C+ framework to handle initialization parameters in our processing queue (how long to hold an object, time an object should be in frame etc) and then run the ML model on the image captured with those parameters. Is Vision a better alternative than running our own initializers like that? Can we specify with Vision the retention time of images for processing images asyncronously? What is best practice there? Thank you!\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "091f823c-46b7-4d0e-8e81-89484b72b45d",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "Not sure about C+ in terms of its retention. But as long as you hold a VNImageRequestHandler, the image will be held.",
          "ts": "1654811675.796189",
          "thread_ts": "1654811601.664869",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QMj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Not sure about C+ in terms of its retention. But as long as you hold a VNImageRequestHandler, the image will be held."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "715398ed-7a70-4af0-ad46-1947ae4aad5f",
          "type": "message",
          "user": "U03HZ5ALQ5T",
          "text": "Awesome! Thank you Frank",
          "ts": "1654875138.330819",
          "thread_ts": "1654811601.664869",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tlBeI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Awesome! Thank you Frank"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J20RJQ2X\u003e asked\n\u0026gt; My app iterates over the user's entire photo library using VNDetectHumanRectanglesRequest and VNRecognizeAnimalsRequest, in order to find all the photos containing humans and pets. For performance reasons, I'm only loading a small version of the photo. I've noticed that this (obviously) affects the results. Is there a recommended image size when using these requests? I'd also appreciate any other ideas on how to optimize the performance for such a task.",
      "ts": "1654811939.505059",
      "thread_ts": "1654811939.505059",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654812077.179899",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "OwOs1",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J20RJQ2X"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"My app iterates over the user's entire photo library using VNDetectHumanRectanglesRequest and VNRecognizeAnimalsRequest, in order to find all the photos containing humans and pets. For performance reasons, I'm only loading a small version of the photo. I've noticed that this (obviously) affects the results. Is there a recommended image size when using these requests? I'd also appreciate any other ideas on how to optimize the performance for such a task.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "09d2d04b-b76a-48e4-b12a-dc1c4d7105dd",
          "type": "message",
          "user": "U03JFF1S5U0",
          "text": "There is no hard and fast size that works for everything. The reason is that is limited by the ratio of the dog or human in respect to the image to be detected. So it depends on your use case if you for instance want to find a small dog in the background in a large panorama.",
          "ts": "1654812077.179899",
          "thread_ts": "1654811939.505059",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J20RJQ2X"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2y99v",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There is no hard and fast size that works for everything. The reason is that is limited by the ratio of the dog or human in respect to the image to be detected. So it depends on your use case if you for instance want to find a small dog in the background in a large panorama."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "b0f13f5e-d6cc-4668-a9ce-d1760f6c3345",
      "type": "message",
      "user": "U03DJTBMHFF",
      "text": "\u003c!here\u003e *That's a wrap… sort of!* Thank you for all the great questions over the past hour. The Vision and VisionKit engineering teams will begin peeling off to tend to other things, but they'll be checking back periodically throughout the rest of the day, night, and week. So… we're going to keep the workflows active. Don't hesitate to drop new questions anytime on this topic or anything machine learning related.",
      "ts": "1654812171.466179",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 5,
          "users": [
            "U03JQRD5KQS",
            "U03JQAC4S6M",
            "U03HMBQ0KJB",
            "U03JEAD3SQ7",
            "U03J2125E0J"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "vI8lO",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " "
                },
                {
                  "type": "text",
                  "text": "That's a wrap… sort of!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " Thank you for all the great questions over the past hour. The Vision and VisionKit engineering teams will begin peeling off to tend to other things, but they'll be checking back periodically throughout the rest of the day, night, and week. So… we're going to keep the workflows active. Don't hesitate to drop new questions anytime on this topic or anything machine learning related."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JE2RJ2DA\u003e asked\n\u0026gt; Is it possible to have player (end-user) enabled Machine Learning? For example in my game Follow the White Rabbit it would be helpful to adjust the model. For example supporting different hand sizes, skin tones, as well as support hands that had more/less than the standard number of fingers.",
      "ts": "1654832453.113229",
      "thread_ts": "1654832453.113229",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654832763.278669",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "o+YHt",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JE2RJ2DA"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to have player (end-user) enabled Machine Learning? For example in my game Follow the White Rabbit it would be helpful to adjust the model. For example supporting different hand sizes, skin tones, as well as support hands that had more\\/less than the standard number of fingers.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e06f72b0-7540-4664-ae58-d6c49aae5344",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Hi \u003c@U03JE2RJ2DA\u003e. Yes, you can adapt a model on-device using any one of our ML frameworks, including Core ML, Create ML Components, MPSGraph, and BNNS. The approach you take depends on the data and problem you are working with. To detect hand poses, I recommend checking out the sample code project \u003chttps://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision|Detecting Hand Poses with Vision\u003e. If you foresee training on a small dataset, then it might be worth looking into using the KNN algorithm available in CoreML, check out the sample code project \u003chttps://developer.apple.com/documentation/coreml/model_personalization/personalizing_a_model_with_on-device_updates|Personalizing a Model with On-Device Updates\u003e to learn more. Finally, it is worth browsing through the documentation for the newly release API, \u003chttps://developer.apple.com/documentation/createmlcomponents/|Create ML Components\u003e.",
          "ts": "1654832559.536909",
          "thread_ts": "1654832453.113229",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JE2RJ2DA"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Fm6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JE2RJ2DA"
                    },
                    {
                      "type": "text",
                      "text": ". Yes, you can adapt a model on-device using any one of our ML frameworks, including Core ML, Create ML Components, MPSGraph, and BNNS. The approach you take depends on the data and problem you are working with. To detect hand poses, I recommend checking out the sample code project "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision",
                      "text": "Detecting Hand Poses with Vision"
                    },
                    {
                      "type": "text",
                      "text": ". If you foresee training on a small dataset, then it might be worth looking into using the KNN algorithm available in CoreML, check out the sample code project "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/coreml/model_personalization/personalizing_a_model_with_on-device_updates",
                      "text": "Personalizing a Model with On-Device Updates"
                    },
                    {
                      "type": "text",
                      "text": " to learn more. Finally, it is worth browsing through the documentation for the newly release API, "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/",
                      "text": "Create ML Components"
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1B187612-1198-40A3-961F-2064ACE7DEF1",
          "type": "message",
          "user": "U03J9GM2ESE",
          "text": "sample code for human action repetition counting? ",
          "ts": "1654832763.278669",
          "thread_ts": "1654832453.113229",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U03JE2RJ2DA"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1LWyj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "sample"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "code"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "human"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "action"
                    },
                    {
                      "type": "text",
                      "text": " repetition "
                    },
                    {
                      "type": "text",
                      "text": "counting?"
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J9GM2ESE\u003e asked\n\u0026gt; sample code for human action repetition counting available?",
      "ts": "1654833175.235119",
      "thread_ts": "1654833175.235119",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 6,
      "latest_reply": "1654894623.966849",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "3KOAm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J9GM2ESE"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"sample code for human action repetition counting available?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "AAAB8C86-C52D-4F6A-AE63-F5264DE9DA8B",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "You can find the sample code here: \u003chttps://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed|https://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed\u003e",
          "ts": "1654833204.727919",
          "thread_ts": "1654833175.235119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Is9lW",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "can"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "find"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " sam"
                    },
                    {
                      "type": "text",
                      "text": "ple"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "code"
                    },
                    {
                      "type": "text",
                      "text": " here"
                    },
                    {
                      "type": "text",
                      "text": ": "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed",
                      "text": "https://developer.apple.com/documentation/createmlcomponents/counting_human_body_action_repetitions_in_a_live_video_feed"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "77E3D383-61E4-4DC1-A943-D26F60E679DD",
          "type": "message",
          "user": "U03J9GM2ESE",
          "text": "I tried the sample code on 6th gen. iPad Mini doing 20 jumper jacks at VARYING paces. I found delays and missing counts. The first few jumper jacks were always not being counted. I’m guessing the hard coded stride 5 and length 90 used for sliding window transformer may be the culprit. To me there isn’t a correct set of numbers to use because I have no control on how fast or slow my users will do his or her jumper jacks. Please advise. ",
          "ts": "1654870940.104649",
          "thread_ts": "1654833175.235119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "B3Dw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "tried"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "sample"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "code"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "6th"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "gen."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "iPad"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Mini"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "doing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "20"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "jumper"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "jacks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "at"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "VARYING"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "paces."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "found"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "delays"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "missing"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "counts."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "The"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "first"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "few"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "jumper"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "jacks"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "were"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "always"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "not"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "being"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "counted."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "I’m"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "guessing the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "hard"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "coded"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "stride"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "5"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "length"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "90"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "used"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "for"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "sliding"
                    },
                    {
                      "type": "text",
                      "text": " window "
                    },
                    {
                      "type": "text",
                      "text": "transformer"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "may"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "be"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "culprit."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "To"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "me"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "there"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "isn’t"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "a"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "correct"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "set"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "of"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "numbers"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "to"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "use"
                    },
                    {
                      "type": "text",
                      "text": " because "
                    },
                    {
                      "type": "text",
                      "text": "I"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "have"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "no"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "control"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "on"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "how"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "fast"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "slow"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "my"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "users"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "will"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "do"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "his"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "or"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "her"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "jumper"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "jacks."
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "Please"
                    },
                    {
                      "type": "text",
                      "text": " advise"
                    },
                    {
                      "type": "text",
                      "text": "."
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0a685ff9-ac15-44b4-b821-e85d8fd02b66",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "Hello \u003c@U03J9GM2ESE\u003e , a few things you may further check:\n• frame rate, the sample app should print out the frame rate as debug information in the Xcode console. Please check if it is roughly 30fps. If not, try to improve the environment lighting, charge the device, etc. and see if it improves.\n• body pose, please check if a single person’s full body pose is in the middle of the screen, and while the person moves, check if the poses are accurate (e.g., no missing joints or joints jumping everywhere, no visible delays of pose tracking, etc.) \n• ignored joints in `JointsSelector` . The initial setting here has 5 joints ignored (for demonstration of this transformer purpose). You may remove them if you are interested in the full body and all joints. \n• `stride`  determines how often (in terms of frame count) the counter is refreshed, and it can be set to other numbers. The `length` 90 however should not be changed. This is fixed for the model. \n• `Downsampler` transformer has a factor of 1, which works best for actions close to ~1s per repetition. It can tolerate the varying speed to some extend. If your targeted action is typically much slower, you may set the factor to 2, or other numbers, this may increase the counter delay too. Unfortunately, you have to manually set it in the sample app for now.  \n• You are also free to change some of the other logics in the sample app, such as how `uiCount`  is rounded and reset, etc. ",
          "ts": "1654877147.201929",
          "thread_ts": "1654833175.235119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fqBjW",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hello "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J9GM2ESE"
                    },
                    {
                      "type": "text",
                      "text": " , a few things you may further check:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"frame rate, the sample app should print out the frame rate as debug information in the Xcode console. Please check if it is roughly 30fps. If not, try to improve the environment lighting, charge the device, etc. and see if it improves.\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"body pose, please check if a single person\\u2019s full body pose is in the middle of the screen, and while the person moves, check if the poses are accurate (e.g., no missing joints or joints jumping everywhere, no visible delays of pose tracking, etc.) \"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"ignored joints in \"},{\"type\":\"text\",\"text\":\"JointsSelector\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\" . The initial setting here has 5 joints ignored (for demonstration of this transformer purpose). You may remove them if you are interested in the full body and all joints. \"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"stride\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\"  determines how often (in terms of frame count) the counter is refreshed, and it can be set to other numbers. The \"},{\"type\":\"text\",\"text\":\"length\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\" 90 however should not be changed. This is fixed for the model. \"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Downsampler\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\" transformer has a factor of 1, which works best for actions close to ~1s per repetition. It can tolerate the varying speed to some extend. If your targeted action is typically much slower, you may set the factor to 2, or other numbers, this may increase the counter delay too. Unfortunately, you have to manually set it in the sample app for now.  \"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"You are also free to change some of the other logics in the sample app, such as how \"},{\"type\":\"text\",\"text\":\"uiCount\",\"style\":{\"code\":true}},{\"type\":\"text\",\"text\":\"  is rounded and reset, etc. \"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "75D68159-A3E6-42BC-A9BB-61D17B0B1555",
          "type": "message",
          "user": "U03J9GM2ESE",
          "text": "Correct me if I’m wrong. \n1. Virtual HIIT fitness coach is not a good app idea for today. Action classifier can’t classify actions fast and accurate enough on mobile devices today?\n2. The model was trained using 30 fps and a prediction windows size of 90 frames under the assumption that each human body action lasts about 3 seconds?\n",
          "ts": "1654885882.700759",
          "thread_ts": "1654833175.235119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XvRS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Correct me if I’m wrong. "
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Virtual\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"HIIT\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"fitness\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"coach\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"is\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"not\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"a\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"good\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"app\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"idea\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"for\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"today. Action\"},{\"type\":\"text\",\"text\":\" classifier \"},{\"type\":\"text\",\"text\":\"can\\u2019t\"},{\"type\":\"text\",\"text\":\" classify \"},{\"type\":\"text\",\"text\":\"actions\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"fast\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"and\"},{\"type\":\"text\",\"text\":\" accurate\"},{\"type\":\"text\",\"text\":\" enough\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"on\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"mobile\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"devices\"},{\"type\":\"text\",\"text\":\" \"},{\"type\":\"text\",\"text\":\"today?\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"The model was trained using 30 fps and a prediction windows size of 90 frames under the assumption that each human body action lasts about 3 seconds?\"}]}],\"style\":\"ordered\",\"indent\":0,\"offset\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "\n"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "69f5632e-990f-4c49-9963-131868c28c6c",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "Hi \u003c@U03J9GM2ESE\u003e , see my comments below:\n1. Action classifier is a model template that needs to be trained. So it is good with fitness actions that it was trained with, such as jumping jacks, squats, and some HIIT actions. Depending on your specific needs, we may further talk about how fast the actions could be and how accurate etc. topics. Some resources are also here:\n• How to train an action classifier with your custom action videos: \u003chttps://developer.apple.com/documentation/createml/creating-an-action-classifier-model\u003e\n• WWDC20 session for action classifier: \u003chttps://developer.apple.com/videos/play/wwdc2020/10043/\u003e\n• iOS sample app for action classifier (can classify jumping jacks, etc.): \u003chttps://developer.apple.com/documentation/createml/detecting_human_actions_in_a_live_video_feed\u003e",
          "ts": "1654894249.157269",
          "thread_ts": "1654833175.235119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "xgo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J9GM2ESE"
                    },
                    {
                      "type": "text",
                      "text": " , see my comments below:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Action classifier is a model template that needs to be trained. So it is good with fitness actions that it was trained with, such as jumping jacks, squats, and some HIIT actions. Depending on your specific needs, we may further talk about how fast the actions could be and how accurate etc. topics. Some resources are also here:\"}]}],\"style\":\"ordered\",\"indent\":0,\"border\":0}"
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"How to train an action classifier with your custom action videos: \"},{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/documentation\\/createml\\/creating-an-action-classifier-model\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"WWDC20 session for action classifier: \"},{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/videos\\/play\\/wwdc2020\\/10043\\/\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"iOS sample app for action classifier (can classify jumping jacks, etc.): \"},{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/documentation\\/createml\\/detecting_human_actions_in_a_live_video_feed\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "14b89fc0-758f-492c-bb7b-355b972ab6a6",
          "type": "message",
          "user": "U03HRNBHZEX",
          "text": "2. For your 2nd bullet, this model is a separate model for counting actions (not action classifier back to WWDC20). It is class-agnostic, exposed via our API and does not need to be trained. It was trained with 30fps videos, and window size is 90 frames. But this is a completely different model, the window size 90 isn’t the same concept with action classifier’s window size. Within these 90 frames, multiple completed actions are OK (e.g., best with 2~4 action repetitions captured within the window). If you have videos or camera feed other than 30fps, you could choose to downsample, using `Downsampler`  transformer. If your targeted actions are 30fps, but quite slower, such as push-ups, you could choose to do downsampling too.",
          "ts": "1654894623.966849",
          "thread_ts": "1654833175.235119",
          "edited": {
            "user": "U03HRNBHZEX",
            "ts": "1654894703.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "p0Pk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "2. For your 2nd bullet, this model is a separate model for counting actions (not action classifier back to WWDC20). It is class-agnostic, exposed via our API and does not need to be trained. It was trained with 30fps videos, and window size is 90 frames. But this is a completely different model, the window size 90 isn’t the same concept with action classifier’s window size. Within these 90 frames, multiple completed actions are OK (e.g., best with 2~4 action repetitions captured within the window). If you have videos or camera feed other than 30fps, you could choose to downsample, using "
                    },
                    {
                      "type": "text",
                      "text": "Downsampler",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "  transformer. If your targeted actions are 30fps, but quite slower, such as push-ups, you could choose to do downsampling too."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "ecca4e6d-c2ef-4ebe-bd69-fc4f3e3d3881",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "\u003c!here\u003e A quick reminder, we kept the Workflows option open, so use the  :workflowbolt: to submit your questions from any time zone. We’re all going to sign off for the night but we’ll answer your questions throughout the day tomorrow so you can check back in a bit! Chat soon...",
      "ts": "1654844330.086929",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "ax8Ag",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " A quick reminder, we kept the Workflows option open, so use the  "
                },
                {
                  "type": "emoji",
                  "name": "workflowbolt",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " to submit your questions from any time zone. We’re all going to sign off for the night but we’ll answer your questions throughout the day tomorrow so you can check back in a bit! Chat soon..."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JM1PJE9G\u003e asked\n\u0026gt; Is it possible to train the model generated by MLRecommender on device when new data is available?",
      "ts": "1654868989.800019",
      "thread_ts": "1654868989.800019",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654869204.449209",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "qtj1d",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JM1PJE9G"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to train the model generated by MLRecommender on device when new data is available?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "128a5570-2d83-48bf-9aee-b0004bfeed4c",
          "type": "message",
          "user": "U03HRMABBDZ",
          "text": "MLRecommender does not support on-device training/updating. However, I suggest you check our WWDC21 session below to build personalized recommendation-like experience into your app:\n\u003chttps://developer.apple.com/videos/play/wwdc2021/10037/\u003e",
          "ts": "1654869088.575489",
          "thread_ts": "1654868989.800019",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JM1PJE9G"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "oqKt",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MLRecommender does not support on-device training/updating. However, I suggest you check our WWDC21 session below to build personalized recommendation-like experience into your app:\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2021/10037/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d391200b-13c4-4711-aab4-934a89576ca4",
          "type": "message",
          "user": "U03JM1PJE9G",
          "text": "thanks",
          "ts": "1654869204.449209",
          "thread_ts": "1654868989.800019",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nNSI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "thanks"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "6521b700-740b-4251-823a-b407db7a7dbf",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Welcome to our final day at WWDC22!* Join us here, in a few minutes for _*Meet the Presenters: Accelerate machine learning with Metal*_. We have a great group of engineers ready to answer all your questions about Metal and machine learning. Feel free to bring questions you may have. It's going to be a fun conversation. Looking forward to your questions!\n\n\u003chttps://developer.apple.com/videos/play/wwdc2022/10063/\u003e",
      "ts": "1654876638.220079",
      "attachments": [
        {
          "fallback": "Apple Developer: Accelerate machine learning with Metal - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Accelerate machine learning with Metal - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022/10063/",
          "text": "Discover how you can use Metal to accelerate your PyTorch model training on macOS. We'll take you through updates to TensorFlow training...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6556/6556_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022/10063/",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022/10063/",
          "blocks": null
        }
      ],
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "JLZmu",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Welcome to our final day at WWDC22!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " Join us here, in a few minutes for "
                },
                {
                  "type": "text",
                  "text": "Meet the Presenters: Accelerate machine learning with Metal",
                  "style": {
                    "bold": true,
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ". We have a great group of engineers ready to answer all your questions about Metal and machine learning. Feel free to bring questions you may have. It's going to be a fun conversation. Looking forward to your questions!\n\n"
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/videos/play/wwdc2022/10063/",
                  "text": ""
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "29d73188-65a8-46ec-b711-aedcefdda001",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Let's get started!* Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows.\n\n *Open Discussion:* To get things going, who has already watched the \"Accelerate machine learning with Metal\" session? I've been a bit busy this week, so I'm going to hit play and watch it now! What are you most excited about? Reply in the thread… :thread:",
      "ts": "1654876812.533959",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "vMEBK",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Let's get started! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "Hit the \"plus\" button to use either the \"Ask a question\" or \"What's your idea?\" workflows.\n\n "
                },
                {
                  "type": "text",
                  "text": "Open Discussion:",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " To get things going, who has already watched the \"Accelerate machine learning with Metal\" session? I've been a bit busy this week, so I'm going to hit play and watch it now! What are you most excited about? Reply in the thread… "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "bfa83b4a-1eb1-441b-bcab-fe224fc57e03",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Currently learning about PyTorch acceleration...:thread:",
      "ts": "1654877091.798709",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "7nt",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Currently learning about PyTorch acceleration..."
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "c8e13a32-e49e-451f-9df7-a5f9990168d6",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Wow, TensorFlow performance on Mac Studio :heart_eyes:",
      "ts": "1654877945.592779",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Z3q",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Wow, TensorFlow performance on Mac Studio "
                },
                {
                  "type": "emoji",
                  "name": "heart_eyes",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "404c9093-7f9a-4b9b-bd93-686b8dd07234",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Moving on over to Matteo sharing about What's new in MPSGraph :thread:",
      "ts": "1654877986.491139",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "wbM",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Moving on over to Matteo sharing about What's new in MPSGraph "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03K19A2324\u003e asked\n\u0026gt; Great presentation! Thank you!\n\u0026gt; The MetalFX team has presented a very nice (classical) method for video upscaling. What is the potential of using MSP to achieve machine learning upscaling?",
      "ts": "1654878687.996169",
      "thread_ts": "1654878687.996169",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654886712.325019",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "dYB",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03K19A2324"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Great presentation! Thank you!\\nThe MetalFX team has presented a very nice (classical) method for video upscaling. What is the potential of using MSP to achieve machine learning upscaling?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "2f2add33-ca11-47fe-b6f8-bfd46fcf696c",
          "type": "message",
          "user": "U03JFGKK1C0",
          "text": "MPSGraph supports most common neural-network machine learning layers and operations so you should be able to create an upscaling network from the basic components, but MPSGraph doesn't have prebuilt graphs or networks so you would need to investigate and research the network architecture yourself, train it (using MPSGraph or other training frameworks) and deploy on MPSGraph.\nOne benefit of using MPSGraph is that you can pretty easily incorporate other Metal kernels (for example MPS image processing kernels or your own kernels) and encode them to the same Metal CommandQueue (or MPSCommandBuffer) to achieve low-latency, often zero-copy execution between the pre/post-processing kernels and the MPSGraph segment(s).",
          "ts": "1654879023.630159",
          "thread_ts": "1654878687.996169",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "IrX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MPSGraph supports most common neural-network machine learning layers and operations so you should be able to create an upscaling network from the basic components, but MPSGraph doesn't have prebuilt graphs or networks so you would need to investigate and research the network architecture yourself, train it (using MPSGraph or other training frameworks) and deploy on MPSGraph.\nOne benefit of using MPSGraph is that you can pretty easily incorporate other Metal kernels (for example MPS image processing kernels or your own kernels) and encode them to the same Metal CommandQueue (or MPSCommandBuffer) to achieve low-latency, often zero-copy execution between the pre/post-processing kernels and the MPSGraph segment(s)."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3f983d04-8038-4d13-95ed-b362bcab2c86",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Thank you Teemu for the extended answer. Pretty impressive job and wonderful integration with the rest of the framework! Congratulations guys! :)",
          "ts": "1654879236.678859",
          "thread_ts": "1654878687.996169",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Yd=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you Teemu for the extended answer. Pretty impressive job and wonderful integration with the rest of the framework! Congratulations guys! :)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2dfcf63d-5d2e-4dc7-85c5-a37f0403f460",
          "type": "message",
          "user": "U03K19A2324",
          "text": "\u003c@U03JFGKK1C0\u003e\n(sorry I didn't include the ref in my previous message) :slightly_smiling_face:",
          "ts": "1654886712.325019",
          "thread_ts": "1654878687.996169",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mkO5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JFGKK1C0"
                    },
                    {
                      "type": "text",
                      "text": "\n(sorry I didn't include the ref in my previous message) "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "971158c3-5967-4e96-a9af-9c0f32cb21b9",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "I just finished watching the session, thanks so much to Dhruva and Matteo! Keep your questions coming.",
      "ts": "1654878824.774169",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "of3CD",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "I just finished watching the session, thanks so much to Dhruva and Matteo! Keep your questions coming."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; I’ve tried to run a resnet50 on PyTorch MPS backend, while running Mac Pro with 6900XT, and achieved 23% utilization, while 3090 was running 10 times as fast on the same code. Do you have ideas on why is this happening, and how to further optimize things on Radeon GPU’s?",
      "ts": "1654879266.550629",
      "thread_ts": "1654879266.550629",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654894452.971019",
      "reactions": [
        {
          "name": "eyes",
          "count": 1,
          "users": [
            "U03HY66772A"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "fcOxB",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I\\u2019ve tried to run a resnet50 on PyTorch MPS backend, while running Mac Pro with 6900XT, and achieved 23% utilization, while 3090 was running 10 times as fast on the same code. Do you have ideas on why is this happening, and how to further optimize things on Radeon GPU\\u2019s?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "820e8ef3-6fac-4357-b8df-42f6f0051d28",
          "type": "message",
          "user": "U03HY66772A",
          "text": "P.S. I was running training, so maybe there was an issue with backprop, and GPU was stalling?",
          "ts": "1654880241.260799",
          "thread_ts": "1654879266.550629",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zE2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "P.S. I was running training, so maybe there was an issue with backprop, and GPU was stalling?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "88dc6abb-97de-4f99-b2c3-e7916468c15f",
          "type": "message",
          "user": "U03HJ4J1BMG",
          "text": "Our current Proto release is focused on functionality and we have not tuned the performance yet. Do look out for performance improvements in the PyTorch nightly builds in the upcoming months.\n\nFor this particular case, we would like to know:\n1. What’s the current PyTorch nightly you are using? Do update to latest and see if it still is giving bad utilization.\n2. Can you share the network code?\n3. Are there any operations falling back to the cpu? That hurts performance.\n4. What’s the OS is it 12.3/12.4 or Ventura?\nDo file an issue on PyTorch on GitHub and to us through FeedbackAssistant.\n\u0026lt;\u003chttps://github.com/pytorch/pytorch/issues\u003e\u0026gt;",
          "ts": "1654880326.499189",
          "thread_ts": "1654879266.550629",
          "attachments": [
            {
              "fallback": "GitHub: Issues · pytorch/pytorch",
              "id": 1,
              "title": "Issues · pytorch/pytorch",
              "title_link": "https://github.com/pytorch/pytorch/issues",
              "text": "Tensors and Dynamic neural networks in Python with strong GPU acceleration - Issues · pytorch/pytorch",
              "image_url": "https://opengraph.githubassets.com/e0ec1e186afc07abf900a214bf92f635a1f6f2dd33d59906d0f632965d2468b5/pytorch/pytorch",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/pytorch/pytorch/issues",
              "original_url": "https://github.com/pytorch/pytorch/issues",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "9Ez",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Our current Proto release is focused on functionality and we have not tuned the performance yet. Do look out for performance improvements in the PyTorch nightly builds in the upcoming months.\n\nFor this particular case, we would like to know:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"What\\u2019s the current PyTorch nightly you are using? Do update to latest and see if it still is giving bad utilization.\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Can you share the network code?\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Are there any operations falling back to the cpu? That hurts performance.\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"What\\u2019s the OS is it 12.3\\/12.4 or Ventura?\"}]}],\"style\":\"ordered\",\"indent\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Do file an issue on PyTorch on GitHub and to us through FeedbackAssistant.\n\u003c"
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/pytorch/pytorch/issues",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\u003e"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e0973b63-889a-445e-9fec-32af44ebd6c4",
          "type": "message",
          "user": "U03HY66772A",
          "text": "1. 1.13.0.dev20220609\n2. \u003chttps://github.com/tcapelle/apple_m1_pro_python/tree/main/pytorch\u003e\n3. I’ve seen no warnings while running the code, but I may have to enable some flags (if you can suggest one)\n4. Ventura",
          "ts": "1654880506.044789",
          "thread_ts": "1654879266.550629",
          "attachments": [
            {
              "fallback": "GitHub: apple_m1_pro_python/pytorch at main · tcapelle/apple_m1_pro_python",
              "id": 1,
              "title": "apple_m1_pro_python/pytorch at main · tcapelle/apple_m1_pro_python",
              "title_link": "https://github.com/tcapelle/apple_m1_pro_python/tree/main/pytorch",
              "text": "A collection of ML scripts to test the M1 Pro MacBook Pro - apple_m1_pro_python/pytorch at main · tcapelle/apple_m1_pro_python",
              "image_url": "https://opengraph.githubassets.com/b6aa533c9ae59be80a88cf03b9a8ec63f679eb42b84827b267236b3492c3ba3c/tcapelle/apple_m1_pro_python",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/tcapelle/apple_m1_pro_python/tree/main/pytorch",
              "original_url": "https://github.com/tcapelle/apple_m1_pro_python/tree/main/pytorch",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "StyE",
              "elements": [
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"1.13.0.dev20220609\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"link\",\"url\":\"https:\\/\\/github.com\\/tcapelle\\/apple_m1_pro_python\\/tree\\/main\\/pytorch\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"I\\u2019ve seen no warnings while running the code, but I may have to enable some flags (if you can suggest one)\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Ventura\"}]}],\"style\":\"ordered\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c8496dd5-1e88-4587-a63e-b247c02865bc",
          "type": "message",
          "user": "U03HJ4J1BMG",
          "text": "Your perf concern is noted and the feedback is appreciated. Do file the issue on the Pytorch Github page and tag it with \"module:mps\" and also send it to Apple through FeedbackAssistant. Thank you for sharing this info!",
          "ts": "1654894452.971019",
          "thread_ts": "1654879266.550629",
          "edited": {
            "user": "U03HJ4J1BMG",
            "ts": "1654894475.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2wLVo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Your perf concern is noted and the feedback is appreciated. Do file the issue on the Pytorch Github page and tag it with \"module:mps\" and also send it to Apple through FeedbackAssistant. Thank you for sharing this info!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JTDSCS86\u003e asked\n\u0026gt; Is it possible to convert a PyTorch Text-\u0026gt;Image  model such a vqgan , to CoreML?",
      "ts": "1654879999.885989",
      "thread_ts": "1654879999.885989",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654883162.319049",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "J/S+E",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JTDSCS86"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to convert a PyTorch Text-\u003eImage  model such a vqgan , to CoreML?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "04d16ffb-eb7e-4fb2-80ee-60555f8851f0",
          "type": "message",
          "user": "U03HK4YQZ0W",
          "text": "You can try using `coremltools` - \u003chttps://coremltools.readme.io/docs/pytorch-conversion\u003e",
          "ts": "1654880049.240199",
          "thread_ts": "1654879999.885989",
          "attachments": [
            {
              "fallback": "coremltools: PyTorch Conversion",
              "id": 1,
              "title": "PyTorch Conversion",
              "title_link": "https://coremltools.readme.io/docs/pytorch-conversion",
              "text": "You can convert a model trained in PyTorch to the Core ML format directly, without requiring an explicit step to save the PyTorch model in ONNX format. Converting the model directly is recommended. (This feature was introduced in coremltools 4.0.) TorchScript is an intermediate representation of a P...",
              "service_name": "coremltools",
              "from_url": "https://coremltools.readme.io/docs/pytorch-conversion",
              "original_url": "https://coremltools.readme.io/docs/pytorch-conversion",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "t=If8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can try using "
                    },
                    {
                      "type": "text",
                      "text": "coremltools",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " - "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/pytorch-conversion",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7f39baef-0f61-493c-a1a3-6d2fd18479af",
          "type": "message",
          "user": "U03JTDSCS86",
          "text": "thanks \u003c@U03HK4YQZ0W\u003e, I used this for  Pix2Pix, but does coreml tools support text-\u0026gt;image conversion also?",
          "ts": "1654880400.788979",
          "thread_ts": "1654879999.885989",
          "edited": {
            "user": "U03JTDSCS86",
            "ts": "1654880460.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Mq2ho",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "thanks "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HK4YQZ0W"
                    },
                    {
                      "type": "text",
                      "text": ", I used this for  Pix2Pix, but does coreml tools support text-\u003eimage conversion also?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "59dfccc4-8d24-4abe-8cef-2616b39d2de2",
          "type": "message",
          "user": "U03HK4YQZ0W",
          "text": "I was trying to find something more specific for you, but couldn’t. Personally I haven’t tried VQGAN, but it seems like CLIP model can be converted. Here’s an issue that has been resolved regarding CLIP: \u003chttps://github.com/apple/coremltools/issues/1418\u003e",
          "ts": "1654883162.319049",
          "thread_ts": "1654879999.885989",
          "attachments": [
            {
              "fallback": "GitHub: Error converting CLIP pytorch model to CoreML \"tensor should have value of type ndarray\" · Issue #1418 · apple/coremltools",
              "id": 1,
              "title": "Error converting CLIP pytorch model to CoreML \"tensor should have value of type ndarray\" · Issue #1418 · apple/coremltools",
              "title_link": "https://github.com/apple/coremltools/issues/1418",
              "text": "CLIP_CoreML.ipynb.zip :ladybug:Describe the bug Model doesn\u0026amp;#39;t convert cleanly, gives this error: ValueError: tensor should have value of type ndarray, got \u0026lt;class \u0026amp;#39;numpy.float32\u0026amp;#39;\u0026gt; instead ...",
              "image_url": "https://opengraph.githubassets.com/3f59e4b591e42acf18e41cb7c4864cdf13118f3ff7ca352396e979b79adecf28/apple/coremltools/issues/1418",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/apple/coremltools/issues/1418",
              "original_url": "https://github.com/apple/coremltools/issues/1418",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "eK58e",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I was trying to find something more specific for you, but couldn’t. Personally I haven’t tried VQGAN, but it seems like CLIP model can be converted. Here’s an issue that has been resolved regarding CLIP: "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/apple/coremltools/issues/1418",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "2758bcea-0d44-41d2-82cb-6db06bb04a9d",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Just a few more minutes to get your Metal and machine learning questions in before we shift the conversation over to _Meet The Presenters: Explore the machine learning development experience_ with Geppy!",
      "ts": "1654880086.438179",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "yVQ",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Just a few more minutes to get your Metal and machine learning questions in before we shift the conversation over to "
                },
                {
                  "type": "text",
                  "text": "Meet The Presenters: Explore the machine learning development experience",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": " with Geppy!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03K19A2324\u003e asked\n\u0026gt; GPU acceleration and federated learning are two very appealing approaches for large scale training (or even training over the edge using multiple mobile devices). Is there some special provision in the MPSGraphs framework to enable/enhance such functionality?",
      "ts": "1654880320.619859",
      "thread_ts": "1654880320.619859",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 3,
      "latest_reply": "1654886736.401759",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "0FIz",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03K19A2324"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"GPU acceleration and federated learning are two very appealing approaches for large scale training (or even training over the edge using multiple mobile devices). Is there some special provision in the MPSGraphs framework to enable\\/enhance such functionality?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "0c1a0b0d-ae0e-4929-bd20-e33348bfa329",
          "type": "message",
          "user": "U03JFGKK1C0",
          "text": "MPSGraph should run just fine with iOS and iPadOS. There are no special pre-built functions that achieve techniques like PFL, but using for example the random-number generators provided by MPSGraph, one should be able to generate these operations from basic building-blocks. Then as long as you can aggregate the gradients or other weight-updates across the network (something outside the scope of MPSGraph) you should be able to do this. But again quite a bit of manual work is needed",
          "ts": "1654880389.428169",
          "thread_ts": "1654880320.619859",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "aj0w",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MPSGraph should run just fine with iOS and iPadOS. There are no special pre-built functions that achieve techniques like PFL, but using for example the random-number generators provided by MPSGraph, one should be able to generate these operations from basic building-blocks. Then as long as you can aggregate the gradients or other weight-updates across the network (something outside the scope of MPSGraph) you should be able to do this. But again quite a bit of manual work is needed"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ed586f3c-7660-4ad2-9a55-93eb385311cb",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Thank you again Teemu for the a quite detailed reply! Exactly, it requires some manual work indeed to make it work, and that's why I was wondering if there was some interfacing functionality enabling a more efficient merging between these two important techniques was developed. It's nice to know that the technology is readily available and fully operational for mobile devices as well! :slightly_smiling_face:",
          "ts": "1654880768.125899",
          "thread_ts": "1654880320.619859",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "83J",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you again Teemu for the a quite detailed reply! Exactly, it requires some manual work indeed to make it work, and that's why I was wondering if there was some interfacing functionality enabling a more efficient merging between these two important techniques was developed. It's nice to know that the technology is readily available and fully operational for mobile devices as well! "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3382b5f8-28a0-4cad-a7fc-c50bf05888e5",
          "type": "message",
          "user": "U03K19A2324",
          "text": "\u003c@U03JFGKK1C0\u003e\n(sorry I didn't include the ref in my previous message) :slightly_smiling_face:",
          "ts": "1654886736.401759",
          "thread_ts": "1654880320.619859",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "N+od",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JFGKK1C0"
                    },
                    {
                      "type": "text",
                      "text": "\n(sorry I didn't include the ref in my previous message) "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; In \"Accelerate machine learning with Metal\" Drhuva referenced a new sample code for NeRFs at 14:08. But I can't find it anywhere:( P.S. Yaaay, NeRFs!:)",
      "ts": "1654880342.865959",
      "thread_ts": "1654880342.865959",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654880582.972629",
      "reactions": [
        {
          "name": "+1",
          "count": 1,
          "users": [
            "U03JTDSCS86"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "W5N8E",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"In \\\"Accelerate machine learning with Metal\\\" Drhuva referenced a new sample code for NeRFs at 14:08. But I can't find it anywhere:( P.S. Yaaay, NeRFs!:)\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "01faf3d6-e42a-4171-adff-d413bb675002",
          "type": "message",
          "user": "U03HJ4J1BMG",
          "text": "Hi, here is the link:\n\u0026lt;\u003chttps://developer.apple.com/documentation/metal/metal_sample_code_library/customizing_a_tensorflow_operation?language=objc\u003e\u0026gt;",
          "ts": "1654880496.338409",
          "thread_ts": "1654880342.865959",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U03JZNY81L0"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4dW7k",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi, here is the link:\n\u003c"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/metal/metal_sample_code_library/customizing_a_tensorflow_operation?language=objc",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\u003e"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2031fd05-c9f8-4ffd-b1fd-4936c7e65f68",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Thanks a lot! I’ll sure will test it out and share the results:)",
          "ts": "1654880582.972629",
          "thread_ts": "1654880342.865959",
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654880602.000000"
          },
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U03HJ4J1BMG"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Dbx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks a lot! I’ll sure will test it out and share the results:)"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; While using MPS backend in PyTorch, I've found out that there is no way to select a GPU. This feature would be really beneficial while running Mac Pro with multiple GPU's.",
      "ts": "1654880417.231659",
      "thread_ts": "1654880417.231659",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654880487.233259",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Mlz",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"While using MPS backend in PyTorch, I've found out that there is no way to select a GPU. This feature would be really beneficial while running Mac Pro with multiple GPU's.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "cf77dbd7-e16e-4aab-8223-e33e233aca8c",
          "type": "message",
          "user": "U03J605V2BF",
          "text": "We currently don’t have multi-GPU support.",
          "ts": "1654880487.233259",
          "thread_ts": "1654880417.231659",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "y8m4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We currently don’t have multi-GPU support."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "9ff8546c-384b-42d6-a3dd-d936da19c642",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Thats a wrap for our _Meet the Presenters:_ _Accelerate machine learning with Metal._ Next up, we have \u003c@U03HK3KNMDL\u003e to for _Meet the Presenters:_ Explore the machine learning development experience. Say hi to everyone!",
      "ts": "1654880523.251949",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "nDX=",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Thats a wrap for our "
                },
                {
                  "type": "text",
                  "text": "Meet the Presenters:",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": " "
                },
                {
                  "type": "text",
                  "text": "Accelerate machine learning with Metal. ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "Next up, we have "
                },
                {
                  "type": "user",
                  "user_id": "U03HK3KNMDL"
                },
                {
                  "type": "text",
                  "text": " to for "
                },
                {
                  "type": "text",
                  "text": "Meet the Presenters: ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "Explore the machine learning development experience. Say hi to everyone!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "8c9feef4-c206-4051-a547-b212c6b5e84e",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*Open Discussion:* To get things going, who has already watched the \"Explore the machine learning developer\" session? What was your favorite part? Reply in the thread… :thread:\n\n\u003chttps://developer.apple.com/videos/play/wwdc2022/10017/\u003e",
      "ts": "1654880593.142299",
      "attachments": [
        {
          "fallback": "Apple Developer: Explore the machine learning development experience - WWDC22 - Videos - Apple Developer",
          "id": 1,
          "title": "Explore the machine learning development experience - WWDC22 - Videos - Apple Developer",
          "title_link": "https://developer.apple.com/videos/play/wwdc2022/10017/",
          "text": "Learn how to bring great machine learning (ML) based experiences to your app. We'll take you through model discovery, conversion, and...",
          "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6510/6510_wide_250x141_2x.jpg",
          "service_name": "Apple Developer",
          "service_icon": "https://developer.apple.com/favicon.ico",
          "from_url": "https://developer.apple.com/videos/play/wwdc2022/10017/",
          "original_url": "https://developer.apple.com/videos/play/wwdc2022/10017/",
          "blocks": null
        }
      ],
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "UWZ",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Open Discussion:",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": " To get things going, who has already watched the \"Explore the machine learning developer\" session? What was your favorite part? Reply in the thread… "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": "\n\n"
                },
                {
                  "type": "link",
                  "url": "https://developer.apple.com/videos/play/wwdc2022/10017/",
                  "text": ""
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "8ae8604e-0ef1-4eee-b828-008a95ea4927",
      "type": "message",
      "user": "U03HK3KNMDL",
      "text": "Ciao a tutti! I am ready for your questions.",
      "ts": "1654880752.973979",
      "thread_ts": "1654880752.973979",
      "reply_count": 4,
      "latest_reply": "1654881259.505289",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "heart",
          "count": 7,
          "users": [
            "U03K19A2324",
            "U03HRMAAYM9",
            "U03HJ4J1BMG",
            "U03JELXD2V7",
            "U03JE2RJ2DA",
            "U03HB4VBDGX",
            "U03JFGMTU8G"
          ]
        },
        {
          "name": "+1",
          "count": 3,
          "users": [
            "U03K19A2324",
            "U03K593JEG2",
            "U03HB4VBDGX"
          ]
        },
        {
          "name": "popcorn",
          "count": 4,
          "users": [
            "U03K19A2324",
            "U03JE2RJ2DA",
            "U03J98R7N5A",
            "U03JFGMTU8G"
          ]
        },
        {
          "name": "coffee",
          "count": 3,
          "users": [
            "U03DJTBMHFF",
            "U03JELXD2V7",
            "U03JFGMTU8G"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "riU7l",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Ciao a tutti! I am ready for your questions."
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "39551552-16e5-4c87-a54c-ff9f3339b04c",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Hello Geppy! :slightly_smiling_face:",
          "ts": "1654880811.195129",
          "thread_ts": "1654880752.973979",
          "parent_user_id": "U03HK3KNMDL",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kx=/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hello Geppy! "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "178993d9-6cf7-45d0-b0b1-bb5e0df35194",
          "type": "message",
          "user": "U03JTGZ04HX",
          "text": "Fantastic example! Model conversion has been something that seemed scary to me, but this session really helped me understand it's very straight forward. Are there situations where the conversion isn't something you would try?",
          "ts": "1654881116.454779",
          "thread_ts": "1654880752.973979",
          "parent_user_id": "U03HK3KNMDL",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ve6eu",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Fantastic example! Model conversion has been something that seemed scary to me, but this session really helped me understand it's very straight forward. Are there situations where the conversion isn't something you would try?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f75bca20-e30e-47bb-9041-6f93d41189f7",
          "type": "message",
          "user": "U03JTGZ04HX",
          "text": "The realtime colorization blew my mind! :exploding_head:",
          "ts": "1654881176.273149",
          "thread_ts": "1654880752.973979",
          "parent_user_id": "U03HK3KNMDL",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Fpts",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The realtime colorization blew my mind! "
                    },
                    {
                      "type": "emoji",
                      "name": "exploding_head",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8f5849e6-cb25-48f8-88aa-b6dc20bd8fc9",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "You may find some of these past sessions helpful regarding model conversion:\n• \u003chttps://developer.apple.com/videos/play/tech-talks/10154/\u003e\n• \u003chttps://developer.apple.com/videos/play/wwdc2020/10153/\u003e",
          "ts": "1654881259.505289",
          "thread_ts": "1654880752.973979",
          "attachments": [
            {
              "fallback": "Apple Developer: Convert PyTorch models to Core ML - Tech Talks - Videos - Apple Developer",
              "id": 1,
              "title": "Convert PyTorch models to Core ML - Tech Talks - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/tech-talks/10154/",
              "text": "Bring your PyTorch models to Core ML and discover how you can leverage on-device machine learning in your apps. The PyTorch machine...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/8/3437/3437_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/tech-talks/10154/",
              "original_url": "https://developer.apple.com/videos/play/tech-talks/10154/",
              "blocks": null
            },
            {
              "fallback": "Apple Developer: Get models on device using Core ML Converters - WWDC20 - Videos - Apple Developer",
              "id": 2,
              "title": "Get models on device using Core ML Converters - WWDC20 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2020/10153/",
              "text": "With Core ML you can bring incredible machine learning models to your app and run them entirely on-device. And when you use Core ML...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/49/3436/3436_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2020/10153/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2020/10153/",
              "blocks": null
            }
          ],
          "parent_user_id": "U03HK3KNMDL",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zQ8t",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You may find some of these past sessions helpful regarding model conversion:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/videos\\/play\\/tech-talks\\/10154\\/\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/videos\\/play\\/wwdc2020\\/10153\\/\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "143e6be7-c38a-4b1e-be10-bd373ccb25ff",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "How many of you have old family photos that you could try this with?",
      "ts": "1654880980.421149",
      "thread_ts": "1654880980.421149",
      "reply_count": 1,
      "latest_reply": "1654881158.680209",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "0x6",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "How many of you have old family photos that you could try this with?"
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "6e98134f-f760-4af7-b115-02559360c5bb",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Not of the Italian country side….. but probably of some awkward/fun family gatherings",
          "ts": "1654881158.680209",
          "thread_ts": "1654880980.421149",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EIB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Not of the Italian country side….. but probably of some awkward/fun family gatherings"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "d8ee5b5a-6ddd-4630-b649-8b4af12bc6d3",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "Don't forget to submit your questions for \u003c@U03HK3KNMDL\u003e through the + button!",
      "ts": "1654881250.464939",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "YBNo",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Don't forget to submit your questions for "
                },
                {
                  "type": "user",
                  "user_id": "U03HK3KNMDL"
                },
                {
                  "type": "text",
                  "text": " through the + button!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JELXD2V7\u003e asked\n\u0026gt; Great talk Geppy, and very well explained. I can see that the code of the slides is available, but what about the demo app? could we have it as other sample apps, just to see how are all the pieces glued together? Thanks!!!",
      "ts": "1654881475.729609",
      "thread_ts": "1654881475.729609",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654881583.652939",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "cMs3=",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JELXD2V7"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Great talk Geppy, and very well explained. I can see that the code of the slides is available, but what about the demo app? could we have it as other sample apps, just to see how are all the pieces glued together? Thanks!!!\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "d134b470-6ea4-4ebc-b301-a8c1c93ef556",
          "type": "message",
          "user": "U03HK3KNMDL",
          "text": "The demo app is not available at the moment, but I am glad that you like it.",
          "ts": "1654881583.652939",
          "thread_ts": "1654881475.729609",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "t72",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The demo app is not available at the moment, but I am glad that you like it."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JE2RJ2DA\u003e asked\n\u0026gt; Great session on image colorization @Geppy P (Apple) . Do you have any examples of user-customizable hand tracking? Think magic spells.",
      "ts": "1654881611.758379",
      "thread_ts": "1654881611.758379",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654881659.778419",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Ssrx4",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JE2RJ2DA"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Great session on image colorization @Geppy P (Apple) . Do you have any examples of user-customizable hand tracking? Think magic spells.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "fbc7e5a4-4d7b-4508-ab2b-b3228d5e205d",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Interestingly, Geppy is a super hero as well. You can watch him demonstrate his powers with hand pose and action classification at the end of this session from last year: \u003chttps://developer.apple.com/videos/play/wwdc2021/10039/\u003e",
          "ts": "1654881659.778419",
          "thread_ts": "1654881611.758379",
          "attachments": [
            {
              "fallback": "Apple Developer: Classify hand poses and actions with Create ML - WWDC21 - Videos - Apple Developer",
              "id": 1,
              "title": "Classify hand poses and actions with Create ML - WWDC21 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2021/10039/",
              "text": "With Create ML, your app's ability to understand the expressiveness of the human hand has never been easier. Discover how you can build...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/119/4929/4929_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2021/10039/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2021/10039/",
              "blocks": null
            }
          ],
          "edited": {
            "user": "U03HRMWNP4J",
            "ts": "1654881674.000000"
          },
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U03JE2RJ2DA"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "exNVN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Interestingly, Geppy is a super hero as well. You can watch him demonstrate his powers with hand pose and action classification at the end of this session from last year: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2021/10039/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JTDSCS86\u003e asked\n\u0026gt; Does coremltools support converison of PYTorch Text-\u0026gt; Image models like CLIP?VQGAN?",
      "ts": "1654881733.551209",
      "thread_ts": "1654881733.551209",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654883069.601089",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "+af1",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JTDSCS86"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Does coremltools support converison of PYTorch Text-\u003e Image models like CLIP?VQGAN?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "45969831-3b5e-4a86-897f-faaed78fdaab",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "Both these models (CLIP and VQGAN) are based on CNNs and transformer architectures, both of which should be supported.\nIn fact here (\u003chttps://github.com/apple/coremltools/issues/1418\u003e) is a resolved issue of a CLIP model conversion.\nNote that, depending on the details,  you may have to perform the pre-processing of the text input transformation to a tensor representation outside the pytorch model given to the coremltools convert API. The conversion operates on pytorch models with tensor in tensor out interface.",
          "ts": "1654881946.903129",
          "thread_ts": "1654881733.551209",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "brain",
              "count": 1,
              "users": [
                "U03JZNY81L0"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "dopcg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Both these models (CLIP and VQGAN) are based on CNNs and transformer architectures, both of which should be supported.\nIn fact here ("
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/apple/coremltools/issues/1418",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": ") is a resolved issue of a CLIP model conversion.\nNote that, depending on the details,  you may have to perform the pre-processing of the text input transformation to a tensor representation outside the pytorch model given to the coremltools convert API. The conversion operates on pytorch models with tensor in tensor out interface."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3e306db1-f44f-463f-a650-5513946cb022",
          "type": "message",
          "user": "U03JTDSCS86",
          "text": "\u003c@U03J4CNMT6D\u003e thanks for sharing this!\n\nI have no idea how to do this:\n“perform the pre-processing of the text input transformation to a tensor representation outside the pytorch model”\n\nAny reference you could share on how to do that?",
          "ts": "1654882376.286369",
          "thread_ts": "1654881733.551209",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "CXp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J4CNMT6D"
                    },
                    {
                      "type": "text",
                      "text": " thanks for sharing this!\n\nI have no idea how to do this:\n“perform the pre-processing of the text input transformation to a tensor representation outside the pytorch model”\n\nAny reference you could share on how to do that?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fbd9692d-3767-4e40-8b1f-0738e47e18e8",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "You might not need it, depends on the exact pytorch model.\nI’d say just give the converter a try, and please take a look at some of the examples on the \u003chttp://coremltools.readme.io|doc\u003e page and if you run into issues, post on the Github repo.",
          "ts": "1654882547.221979",
          "thread_ts": "1654881733.551209",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PHc2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You might not need it, depends on the exact pytorch model.\nI’d say just give the converter a try, and please take a look at some of the examples on the "
                    },
                    {
                      "type": "link",
                      "url": "http://coremltools.readme.io",
                      "text": "doc"
                    },
                    {
                      "type": "text",
                      "text": " page and if you run into issues, post on the Github repo."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ca77125a-31f7-40b7-b9bf-d95ce0ed08f6",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Pre-processing of the text usually involves looking up a vector for a given word. The python code for CLIP is here \u003chttps://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py\u003e (btw hi thanks for fixing my issue #1418, really appreciate it)",
          "ts": "1654883069.601089",
          "thread_ts": "1654881733.551209",
          "attachments": [
            {
              "fallback": "GitHub: CLIP/simple_tokenizer.py at main · openai/CLIP",
              "id": 1,
              "title": "CLIP/simple_tokenizer.py at main · openai/CLIP",
              "title_link": "https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py",
              "text": "Contrastive Language-Image Pretraining. Contribute to openai/CLIP development by creating an account on GitHub.",
              "image_url": "https://repository-images.githubusercontent.com/321960447/9b77a200-4fd1-11eb-9506-ffa2f50df680",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py",
              "original_url": "https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Iz5bQ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Pre-processing of the text usually involves looking up a vector for a given word. The python code for CLIP is here "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " (btw hi thanks for fixing my issue #1418, really appreciate it)"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JPBUSHLZ\u003e asked\n\u0026gt; Hello! You mention searching for models in various \"specialized\" websites and such... do you have favorite places you've gone to find models? Thank you.",
      "ts": "1654881783.836869",
      "thread_ts": "1654881783.836869",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 13,
      "latest_reply": "1654884216.403269",
      "reactions": [
        {
          "name": "heart",
          "count": 1,
          "users": [
            "U03JE2RJ2DA"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Zmz",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JPBUSHLZ"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hello! You mention searching for models in various \\\"specialized\\\" websites and such... do you have favorite places you've gone to find models? Thank you.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "214306e0-feba-47e5-8c06-7df9040371d9",
          "type": "message",
          "user": "U03HK3KNMDL",
          "text": "There are different places that I use. Github is one of them, but I also use \u003chttps://developer.apple.com/machine-learning/models/\u003e",
          "ts": "1654881908.238479",
          "thread_ts": "1654881783.836869",
          "attachments": [
            {
              "fallback": "Models - Machine Learning - Apple Developer",
              "id": 1,
              "title": "Models - Machine Learning - Apple Developer",
              "title_link": "https://developer.apple.com/machine-learning/models/",
              "text": "Build intelligence into your apps using machine learning models from the research community designed for Core ML.",
              "service_name": "developer.apple.com",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/machine-learning/models/",
              "original_url": "https://developer.apple.com/machine-learning/models/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03JE2RJ2DA"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XoYz",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are different places that I use. Github is one of them, but I also use "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/machine-learning/models/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f9f7a110-223e-41db-abf1-af577279169c",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "\u003chttps://paperswithcode.com\u003e is also a fun place to browse",
          "ts": "1654882008.294329",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "uzc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://paperswithcode.com",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " is also a fun place to browse"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7ffe2182-6959-48c4-96a2-35aea222dbd1",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "\u003chttps://huggingface.co/apple\u003e",
          "ts": "1654882018.461549",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U03JZNY81L0"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2qhBU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://huggingface.co/apple",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fbddf1d5-20ea-4df5-98bb-bff013dd9b43",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Others feel free to share places you like",
          "ts": "1654882055.890619",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 2,
              "users": [
                "U03J7UASVEU",
                "U03HRMAAYM9"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tLvJ7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Others feel free to share places you like"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b2084f0d-3fee-4212-afc5-6b8d05fe46a1",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Another nice place with trained models:\n\u003chttps://resources.wolframcloud.com/NeuralNetRepository/\u003e",
          "ts": "1654882495.738539",
          "thread_ts": "1654881783.836869",
          "attachments": [
            {
              "fallback": "Wolfram Neural Net Repository of Neural Network Models",
              "id": 1,
              "title": "Wolfram Neural Net Repository of Neural Network Models",
              "title_link": "https://resources.wolframcloud.com/NeuralNetRepository/",
              "text": "Expanding collection of trained and untrained neural network models, suitable for immediate evaluation, training, visualization, transfer learning.",
              "service_name": "resources.wolframcloud.com",
              "service_icon": "https://www.wolframcloud.com/obj/resourcesystem/webresources/NeuralNetRepository/5.0.0/favicon/apple-touch-icon.png",
              "from_url": "https://resources.wolframcloud.com/NeuralNetRepository/",
              "original_url": "https://resources.wolframcloud.com/NeuralNetRepository/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "CzVvE",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Another nice place with trained models:\n"
                    },
                    {
                      "type": "link",
                      "url": "https://resources.wolframcloud.com/NeuralNetRepository/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d8e8076b-c30c-4e02-82d3-d18a9498403b",
          "type": "message",
          "user": "U03HY66772A",
          "text": "One of my favourite places:\n\u003chttps://google.github.io/mediapipe/\u003e\nThey are a bit of a hussle to convert to CoreML, but they try a lot of cutting edge stuff early on.",
          "ts": "1654882546.926349",
          "thread_ts": "1654881783.836869",
          "attachments": [
            {
              "fallback": "mediapipe: Home",
              "id": 1,
              "title": "Home",
              "title_link": "https://google.github.io/mediapipe/",
              "text": "Cross-platform, customizable ML solutions for live and streaming media.",
              "service_name": "mediapipe",
              "from_url": "https://google.github.io/mediapipe/",
              "original_url": "https://google.github.io/mediapipe/",
              "blocks": null
            }
          ],
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654882693.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ulk6g",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "One of my favourite places:\n"
                    },
                    {
                      "type": "link",
                      "url": "https://google.github.io/mediapipe/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\nThey are a bit of a hussle to convert to CoreML, but they try a lot of cutting edge stuff early on."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "73acc1ff-bda5-4d76-bf1a-fc41c6e81dc8",
          "type": "message",
          "user": "U03K19A2324",
          "text": "There are some marketplaces too, like this one:\n\u003chttps://www.modzy.com/marketplace/\u003e",
          "ts": "1654882576.509569",
          "thread_ts": "1654881783.836869",
          "attachments": [
            {
              "fallback": "Modzy: AI Model Marketplace",
              "id": 1,
              "title": "AI Model Marketplace",
              "title_link": "https://www.modzy.com/marketplace/",
              "text": "Gain access to vetted, trusted AI models from leading AI companies in Modzy's AI model marketplace to accelerate your team's performance.",
              "image_url": "https://www.modzy.com/wp-content/uploads/2021/08/logo-card.png",
              "service_name": "Modzy",
              "from_url": "https://www.modzy.com/marketplace/",
              "original_url": "https://www.modzy.com/marketplace/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iuU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are some marketplaces too, like this one:\n"
                    },
                    {
                      "type": "link",
                      "url": "https://www.modzy.com/marketplace/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "84f1423f-51ed-485b-961e-0582662b68b7",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Two other cool places with pre-trained models:",
          "ts": "1654882999.238449",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2Dyf",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Two other cool places with pre-trained models:"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4f529e31-f35e-46bd-a45b-af7b4bc12250",
          "type": "message",
          "user": "U03K19A2324",
          "text": "\u003chttps://github.com/BVLC/caffe/wiki/Model-Zoo\u003e",
          "ts": "1654883008.885879",
          "thread_ts": "1654881783.836869",
          "attachments": [
            {
              "fallback": "GitHub: Model Zoo · BVLC/caffe Wiki",
              "id": 1,
              "title": "Model Zoo · BVLC/caffe Wiki",
              "title_link": "https://github.com/BVLC/caffe/wiki/Model-Zoo",
              "text": "Caffe: a fast open framework for deep learning. Contribute to BVLC/caffe development by creating an account on GitHub.",
              "image_url": "https://opengraph.githubassets.com/803decf1c7fec3c7bdc529c932417661eb1598b325895962e92a808481463e17/BVLC/caffe",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/BVLC/caffe/wiki/Model-Zoo",
              "original_url": "https://github.com/BVLC/caffe/wiki/Model-Zoo",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6dV",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://github.com/BVLC/caffe/wiki/Model-Zoo",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "89ab8edb-d3e2-4524-b33e-4d3de2f5bfa9",
          "type": "message",
          "user": "U03K19A2324",
          "text": "\u003chttps://github.com/aiworld/pretrained_net_directory\u003e",
          "ts": "1654883033.898979",
          "thread_ts": "1654881783.836869",
          "attachments": [
            {
              "fallback": "GitHub: GitHub - aiworld/pretrained_net_directory",
              "id": 1,
              "title": "GitHub - aiworld/pretrained_net_directory",
              "title_link": "https://github.com/aiworld/pretrained_net_directory",
              "text": "Contribute to aiworld/pretrained_net_directory development by creating an account on GitHub.",
              "image_url": "https://opengraph.githubassets.com/d03b748783eb08533d205f7beef35c7aa5d4726d297d53d87079cc221c571b2f/aiworld/pretrained_net_directory",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/aiworld/pretrained_net_directory",
              "original_url": "https://github.com/aiworld/pretrained_net_directory",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rynTo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://github.com/aiworld/pretrained_net_directory",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "52d99ad8-198b-4ea9-89c6-627b2f624ab4",
          "type": "message",
          "user": "U03K19A2324",
          "text": "(first is also mentioned in the second one)",
          "ts": "1654883116.682989",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "E21p",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "(first is also mentioned in the second one)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4870bd56-ee5e-421f-b3eb-f6c1f7971339",
          "type": "message",
          "user": "U03JPBUSHLZ",
          "text": "Thank you so much! I’ll have so much fun exploring these :slightly_smiling_face:",
          "ts": "1654884095.862859",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03K19A2324"
              ]
            },
            {
              "name": "grinning",
              "count": 1,
              "users": [
                "U03K19A2324"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kU9zk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you so much! I’ll have so much fun exploring these "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dced7890-3881-4d83-9ff2-10e7ae33c982",
          "type": "message",
          "user": "U03K19A2324",
          "text": "You're welcome \u003c@U03JPBUSHLZ\u003e! :slightly_smiling_face:",
          "ts": "1654884216.403269",
          "thread_ts": "1654881783.836869",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7LFV",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You're welcome "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JPBUSHLZ"
                    },
                    {
                      "type": "text",
                      "text": "! "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JTGZ04HX\u003e asked\n\u0026gt; You mentioned re-training a few candidate replacement models before model integration. What's your process for deciding how many to try?",
      "ts": "1654882333.204979",
      "thread_ts": "1654882333.204979",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 4,
      "latest_reply": "1654882753.218239",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "ulKp",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JTGZ04HX"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"You mentioned re-training a few candidate replacement models before model integration. What's your process for deciding how many to try?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e9695ecc-11dd-4b26-bf6a-bfe864c80ee9",
          "type": "message",
          "user": "U03HK3KNMDL",
          "text": "I tried architectures from other two scientific publications too. But then I decided to “re-work” a bit the architecture of the model I used in the session and decided to go with that.",
          "ts": "1654882671.484849",
          "thread_ts": "1654882333.204979",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0rZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I tried architectures from other two scientific publications too. But then I decided to “re-work” a bit the architecture of the model I used in the session and decided to go with that."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7f7097ab-a4c5-4c4a-ba03-5203b0620e40",
          "type": "message",
          "user": "U03HK3KNMDL",
          "text": "The process can be different from model to model.",
          "ts": "1654882697.276969",
          "thread_ts": "1654882333.204979",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kIPDX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The process can be different from model to model."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a1c5201c-4bb5-46d9-865e-1023220c1e43",
          "type": "message",
          "user": "U03JTGZ04HX",
          "text": "That makes sense, thank you!!",
          "ts": "1654882749.483119",
          "thread_ts": "1654882333.204979",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ORX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That makes sense, thank you!!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "9fb53630-f41d-4e21-9217-736413d8824c",
          "type": "message",
          "user": "U03JTGZ04HX",
          "text": "Amazing session!!!",
          "ts": "1654882753.218239",
          "thread_ts": "1654882333.204979",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ufdJ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Amazing session!!!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JTGZ04HX\u003e asked\n\u0026gt; Will the source code for this project be available? It would be helpful to be able to dig in and change some things up to really understand the flow.",
      "ts": "1654882840.661119",
      "thread_ts": "1654882840.661119",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654882873.345839",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "IcJQz",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JTGZ04HX"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Will the source code for this project be available? It would be helpful to be able to dig in and change some things up to really understand the flow.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "eb85602b-9406-4fb8-99af-86b06159570a",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "The code is not available at this time. Your interest and requested is appreciated and noted :slightly_smiling_face:",
          "ts": "1654882873.345839",
          "thread_ts": "1654882840.661119",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "IP8kF",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The code is not available at this time. Your interest and requested is appreciated and noted "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HMDSQ9JB\u003e asked\n\u0026gt; Is it possible to dispatch a CoreML evaluation as part of a display or compute shader pipeline. \n\u0026gt; \n\u0026gt; Or do I need to wait for the cpu to be informed the frame has been rendered before dispatching from the cpu.",
      "ts": "1654883086.179099",
      "thread_ts": "1654883086.179099",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 6,
      "latest_reply": "1654884494.862379",
      "reactions": [
        {
          "name": "eyes",
          "count": 2,
          "users": [
            "U03JZNY81L0",
            "U03JE2RJ2DA"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "dgs",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HMDSQ9JB"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to dispatch a CoreML evaluation as part of a display or compute shader pipeline. \\n\\nOr do I need to wait for the cpu to be informed the frame has been rendered before dispatching from the cpu.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a981a477-6721-4f62-9344-bce328934a21",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Hi \u003c@U03HMDSQ9JB\u003e could you clarify this part?\n\n\u0026gt;\u0026gt; Is it possible to dispatch a CoreML evaluation as part of a display or compute shader pipeline.\n",
          "ts": "1654883484.161099",
          "thread_ts": "1654883086.179099",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "CzOY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03HMDSQ9JB"
                    },
                    {
                      "type": "text",
                      "text": " could you clarify this part?\n\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_quote",
                  "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to dispatch a CoreML evaluation as part of a display or compute shader pipeline.\"}],\"border\":1}"
                },
                {
                  "type": "rich_text_section",
                  "elements": []
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "534226fd-f3f6-4407-85b7-948cec59e51c",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Are you looking to run CoreML evaluation from a compute shader pipeline?",
          "ts": "1654883513.694799",
          "thread_ts": "1654883086.179099",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XaRh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Are you looking to run CoreML evaluation from a compute shader pipeline?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "58af5426-74b5-4d09-89bb-3cf9334c78ea",
          "type": "message",
          "user": "U03HMDSQ9JB",
          "text": "I would like to be able to have a coreML model run an inference evaluation as soon as a compute or even display render pass finishes rather than need to wait for the cpu to be informed that it has finshed.  Best of all would be if it could run on the ANE so that the GPU is free to work on the next frame.",
          "ts": "1654883542.282919",
          "thread_ts": "1654883086.179099",
          "edited": {
            "user": "U03HMDSQ9JB",
            "ts": "1654883640.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "qj01z",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I would like to be able to have a coreML model run an inference evaluation as soon as a compute or even display render pass finishes rather than need to wait for the cpu to be informed that it has finshed.  Best of all would be if it could run on the ANE so that the GPU is free to work on the next frame."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "110e8e60-56ca-4ac7-94a8-5e7914aed8d7",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "Thanks for the clarification.\n\nIf the output of GPU is in a IOSurface (in Float16 format), you can feed that to CoreML and let ANE work on it directly without any copies but, CPU does get triggered today for synchronizing these two computations. Would you be able to file a feature request on \u003chttp://feedbackassistant.apple.com|feedbackassistant.apple.com\u003e with a little more details about your use case and may be some sample code on how you want to accomplish this? That would really help us push the API in the right direction.",
          "ts": "1654883982.755469",
          "thread_ts": "1654883086.179099",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03HMDSQ9JB"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Z8tU8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for the clarification.\n\nIf the output of GPU is in a IOSurface (in Float16 format), you can feed that to CoreML and let ANE work on it directly without any copies but, CPU does get triggered today for synchronizing these two computations. Would you be able to file a feature request on "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": "feedbackassistant.apple.com"
                    },
                    {
                      "type": "text",
                      "text": " with a little more details about your use case and may be some sample code on how you want to accomplish this? That would really help us push the API in the right direction."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7ad0403c-2bd1-410b-92d4-d1036bdc33bd",
          "type": "message",
          "user": "U03HB4VBDGX",
          "text": "\u003c@U03J52T5J22\u003e \u003c@U03JFGMUPCY\u003e do you want to add anything specific?",
          "ts": "1654884016.954839",
          "thread_ts": "1654883086.179099",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 1,
              "users": [
                "U03JFGMUPCY"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ri/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J52T5J22"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JFGMUPCY"
                    },
                    {
                      "type": "text",
                      "text": " do you want to add anything specific?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4fb9b91e-ce5e-4a71-9ada-4a3f675b724b",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "Nothing much. CoreML doesn’t support `MTLSharedEvent`, if that’s what’s implied here.",
          "ts": "1654884494.862379",
          "thread_ts": "1654883086.179099",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "rhR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Nothing much. CoreML doesn’t support "
                    },
                    {
                      "type": "text",
                      "text": "MTLSharedEvent",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ", if that’s what’s implied here."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JZNY81L0\u003e asked\n\u0026gt; Hi, I'm excited to see more information about optimizing recent models for CoreML including the `ane_transformers` repo. If I wanted to optimize eg CLIP for ANE, should I use code from that repo, or just try to take recommendations from the case study?",
      "ts": "1654883117.722219",
      "thread_ts": "1654883117.722219",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654890035.925269",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "X=O",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JZNY81L0"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi, I'm excited to see more information about optimizing recent models for CoreML including the `ane_transformers` repo. If I wanted to optimize eg CLIP for ANE, should I use code from that repo, or just try to take recommendations from the case study?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "05f73b50-5c8a-4692-ace1-c82c5bf3ef08",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "yes I think using the code from the repo at the end of the article is the best way to get started, and definitely follow along the recommendations of the article as well. Sounds like you are already on the right track!",
          "ts": "1654883185.413909",
          "thread_ts": "1654883117.722219",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nG9kw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "yes I think using the code from the repo at the end of the article is the best way to get started, and definitely follow along the recommendations of the article as well. Sounds like you are already on the right track!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8335fd64-21ea-4f09-8d88-8d788b3367d8",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "afact, ane_transformers just includes distilbert…  how the recommendations apply to that code is maybe a little opaque to me. Need to think about how I would apply this to a different model, haven’t had enough time to digest it this week.",
          "ts": "1654883427.134059",
          "thread_ts": "1654883117.722219",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ghDS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "afact, ane_transformers just includes distilbert…  how the recommendations apply to that code is maybe a little opaque to me. Need to think about how I would apply this to a different model, haven’t had enough time to digest it this week."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "32b0c094-1dd3-43ae-b912-e767eb9781d1",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "The default conversion should be quite efficient as well for the neural engine (NE). With the new performance tab in Xcode 14, you will see whether the model is already neural engine resident or not.\nThere are details in the post \u003chttps://machinelearning.apple.com/research/neural-engine-transformers\u003e on some of the changes specific to distill bert which may or may not be required for the transformer architecture in CLIP.\nIn any case if you find any inefficiencies after conversion, feel free to share with us via a feedback request. We are constantly adding new converter and NE compiler optimizations to automatically detect patterns and map them efficiently to NE, so such feedback is very valuable!",
          "ts": "1654883744.817939",
          "thread_ts": "1654883117.722219",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fze",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The default conversion should be quite efficient as well for the neural engine (NE). With the new performance tab in Xcode 14, you will see whether the model is already neural engine resident or not.\nThere are details in the post "
                    },
                    {
                      "type": "link",
                      "url": "https://machinelearning.apple.com/research/neural-engine-transformers",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " on some of the changes specific to distill bert which may or may not be required for the transformer architecture in CLIP.\nIn any case if you find any inefficiencies after conversion, feel free to share with us via a feedback request. We are constantly adding new converter and NE compiler optimizations to automatically detect patterns and map them efficiently to NE, so such feedback is very valuable!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d7ab2481-4a28-405f-b632-803fb4427846",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Thanks, Aseem. I got stuck on the Performance tab not working on my M1 mac (FB10107014), but I tested it out with an attached iPhone 12 now.",
          "ts": "1654883978.858339",
          "thread_ts": "1654883117.722219",
          "files": [
            {
              "id": "F03K6RZCH6E",
              "created": 1654883967,
              "timestamp": 1654883967,
              "name": "Screen Shot 2022-06-10 at 1.59.20 PM.png",
              "title": "Screen Shot 2022-06-10 at 1.59.20 PM.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03JZNY81L0",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 281747,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03K6RZCH6E-Screen Shot 2022-06-10 at 1.59.20 PM.png",
              "url_private_download": "C03H4A911EH/F03K6RZCH6E-Screen Shot 2022-06-10 at 1.59.20 PM.png",
              "original_h": 1622,
              "original_w": 2156,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 271,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 361,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_720.png",
              "thumb_720_w": 720,
              "thumb_720_h": 542,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_960.png",
              "thumb_960_w": 960,
              "thumb_960_h": 722,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6RZCH6E-6283c33334/screen_shot_2022-06-10_at_1.59.20_pm_1024.png",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 770,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03JZNY81L0/F03K6RZCH6E/screen_shot_2022-06-10_at_1.59.20_pm.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03K6RZCH6E-de3ba7e540",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U03J4CNMT6D"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "73k",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, Aseem. I got stuck on the Performance tab not working on my M1 mac (FB10107014), but I tested it out with an attached iPhone 12 now."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "23a372ca-0f3d-48ae-8d54-d7a63c0ac333",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Will file an issue about this when I try to open Instruments",
          "ts": "1654884081.307689",
          "thread_ts": "1654883117.722219",
          "files": [
            {
              "id": "F03K98N7SNQ",
              "created": 1654884065,
              "timestamp": 1654884065,
              "name": "Screen Shot 2022-06-10 at 2.01.00 PM.png",
              "title": "Screen Shot 2022-06-10 at 2.01.00 PM.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03JZNY81L0",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 281716,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03K98N7SNQ-Screen Shot 2022-06-10 at 2.01.00 PM.png",
              "url_private_download": "C03H4A911EH/F03K98N7SNQ-Screen Shot 2022-06-10 at 2.01.00 PM.png",
              "original_h": 788,
              "original_w": 1264,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 224,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 299,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_720.png",
              "thumb_720_w": 720,
              "thumb_720_h": 449,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_960.png",
              "thumb_960_w": 960,
              "thumb_960_h": 598,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K98N7SNQ-16b76a361c/screen_shot_2022-06-10_at_2.01.00_pm_1024.png",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 638,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03JZNY81L0/F03K98N7SNQ/screen_shot_2022-06-10_at_2.01.00_pm.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03K98N7SNQ-d74e7ba83b",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XzZgx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Will file an issue about this when I try to open Instruments"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1a7f0048-c4ef-4818-a054-6f9955784c12",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Ah, it’s just the wrong version of instruments, will try to find the file on disk. Really appreciate the inspectability coming this year!",
          "ts": "1654884136.145939",
          "thread_ts": "1654883117.722219",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "OKle",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah, it’s just the wrong version of instruments, will try to find the file on disk. Really appreciate the inspectability coming this year!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c8325e89-d70b-44d3-a152-ee58eaa23bb9",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "very cool!",
          "ts": "1654884309.299139",
          "thread_ts": "1654883117.722219",
          "files": [
            {
              "id": "F03K6QXR6FM",
              "created": 1654884303,
              "timestamp": 1654884303,
              "name": "Screen Shot 2022-06-10 at 2.05.01 PM.png",
              "title": "Screen Shot 2022-06-10 at 2.05.01 PM.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U03JZNY81L0",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 525708,
              "url": "",
              "url_download": "",
              "url_private": "C03H4A911EH/F03K6QXR6FM-Screen Shot 2022-06-10 at 2.05.01 PM.png",
              "url_private_download": "C03H4A911EH/F03K6QXR6FM-Screen Shot 2022-06-10 at 2.05.01 PM.png",
              "original_h": 948,
              "original_w": 1398,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 244,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 325,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_720.png",
              "thumb_720_w": 720,
              "thumb_720_h": 488,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_960.png",
              "thumb_960_w": 960,
              "thumb_960_h": 651,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F03K6QXR6FM-6603585c0f/screen_shot_2022-06-10_at_2.05.01_pm_1024.png",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 694,
              "permalink": "https://appleevents.enterprise.slack.com/files/U03JZNY81L0/F03K6QXR6FM/screen_shot_2022-06-10_at_2.05.01_pm.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F03K6QXR6FM-ea54dd8879",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ns1nq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "very cool!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "3092054c-bfbe-46a6-9a2c-0e2d6f820b92",
          "type": "message",
          "user": "U03J98R7N5A",
          "text": "There are more details to be found in the following article as well, hope this helps: \u003chttps://machinelearning.apple.com/research/on-device-scene-analysis\u003e",
          "ts": "1654890035.925269",
          "thread_ts": "1654883117.722219",
          "attachments": [
            {
              "fallback": "Apple Machine Learning Research: A Multi-Task Neural Architecture for On-Device Scene Analysis",
              "id": 1,
              "title": "A Multi-Task Neural Architecture for On-Device Scene Analysis",
              "title_link": "https://machinelearning.apple.com/research/on-device-scene-analysis",
              "text": "Scene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search…",
              "image_url": "https://mlr.cdn-apple.com/media/open_Graph_d53fcef327.jpg",
              "service_name": "Apple Machine Learning Research",
              "service_icon": "https://machinelearning.apple.com/favicon.ico",
              "from_url": "https://machinelearning.apple.com/research/on-device-scene-analysis",
              "original_url": "https://machinelearning.apple.com/research/on-device-scene-analysis",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EOSgM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are more details to be found in the following article as well, hope this helps: "
                    },
                    {
                      "type": "link",
                      "url": "https://machinelearning.apple.com/research/on-device-scene-analysis",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "2c4249be-4c69-4c4f-b6e7-365b048dbc42",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "I love all the interesting questions, glad to see this session added some color to this year's WWDC :wink: Keep them coming!",
      "ts": "1654883155.366849",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "face_with_rolling_eyes",
          "count": 1,
          "users": [
            "U03HRMWNP4J"
          ]
        },
        {
          "name": "grin",
          "count": 2,
          "users": [
            "U03HRMWNP4J",
            "U03J4DR9GDS"
          ]
        },
        {
          "name": "raised_hands",
          "count": 3,
          "users": [
            "U03JZ3XKFC4",
            "U03J7JKA23F",
            "U03J4DR9GDS"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "aZ8",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "I love all the interesting questions, glad to see this session added some color to this year's WWDC "
                },
                {
                  "type": "emoji",
                  "name": "wink",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " Keep them coming!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "69c3f690-d495-47d0-a969-5d31754a4a70",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "\u003c!here\u003e Only a few more minutes with \u003c@U03HK3KNMDL\u003e but we'll keep these lounges open for all your machine learning and computer vision questions until 5pm PT today! We love hearing from you all :slightly_smiling_face:",
      "ts": "1654883977.307219",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "sSO",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " Only a few more minutes with "
                },
                {
                  "type": "user",
                  "user_id": "U03HK3KNMDL"
                },
                {
                  "type": "text",
                  "text": " but we'll keep these lounges open for all your machine learning and computer vision questions until 5pm PT today! We love hearing from you all "
                },
                {
                  "type": "emoji",
                  "name": "slightly_smiling_face",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "e5a654df-88b2-4568-a721-923b605aade9",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "*ICYMI:* here's a great article referenced in a thread about deploying Transformers on the Apple Neural Engine: \u003chttps://machinelearning.apple.com/research/neural-engine-transformers\u003e",
      "ts": "1654884291.721379",
      "thread_ts": "1654884291.721379",
      "attachments": [
        {
          "fallback": "Apple Machine Learning Research: Deploying Transformers on the Apple Neural Engine",
          "id": 1,
          "title": "Deploying Transformers on the Apple Neural Engine",
          "title_link": "https://machinelearning.apple.com/research/neural-engine-transformers",
          "text": "An increasing number of the machine learning (ML) models we build at Apple each year are either partly or fully adopting the [Transformer…",
          "image_url": "https://mlr.cdn-apple.com/media/open_Graph_d53fcef327.jpg",
          "service_name": "Apple Machine Learning Research",
          "service_icon": "https://machinelearning.apple.com/favicon.ico",
          "from_url": "https://machinelearning.apple.com/research/neural-engine-transformers",
          "original_url": "https://machinelearning.apple.com/research/neural-engine-transformers",
          "blocks": null
        }
      ],
      "reply_count": 2,
      "latest_reply": "1654884476.032829",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "fire",
          "count": 2,
          "users": [
            "U03HY66772A",
            "U03JZNY81L0"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "OMQN",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "ICYMI: ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "here's a great article referenced in a thread about deploying Transformers on the Apple Neural Engine: "
                },
                {
                  "type": "link",
                  "url": "https://machinelearning.apple.com/research/neural-engine-transformers",
                  "text": ""
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "caa0c78b-1a23-49a4-b3f4-a18051098b84",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Really would love you to continue posting these:) Amazing in-depth review, we’ve even reviewed this article with our whole ML team:)",
          "ts": "1654884374.717439",
          "thread_ts": "1654884291.721379",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "gHKC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Really would love you to continue posting these:) Amazing in-depth review, we’ve even reviewed this article with our whole ML team:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a690dd2b-c026-46fd-aa54-56114395fb0d",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Really appreciate the article. Going to have to sit down with a notebook to digest how this code is computing the attention, not used to intentionally running mulitple operations instead of a single larger one.\n```        attn_weights = [\n            torch.einsum('bchq,bkhc-\u0026gt;bkhq', [qi, ki]) * self.q_normalize_fact\n            for qi, ki in zip(mh_q, mh_k)\n        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)```",
          "ts": "1654884476.032829",
          "thread_ts": "1654884291.721379",
          "edited": {
            "user": "U03JZNY81L0",
            "ts": "1654884506.000000"
          },
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "6TN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Really appreciate the article. Going to have to sit down with a notebook to digest how this code is computing the attention, not used to intentionally running mulitple operations instead of a single larger one.\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"        attn_weights = [\\n            torch.einsum('bchq,bkhc-\u003ebkhq', [qi, ki]) * self.q_normalize_fact\\n            for qi, ki in zip(mh_q, mh_k)\\n        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\"}],\"border\":0}"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HZ5ALQ5T\u003e asked\n\u0026gt; In the \"Optimize your CoreML usage\" session, the presenter Ben explains that he got a latency of 22ms using the new performance metrics and that gives him a running frame rate of 45 frames per second. How did he come to that conclusion and how can I look at my performance metrics to determine our frames per second as well?",
      "ts": "1654884381.712589",
      "thread_ts": "1654884381.712589",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654884443.678619",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "h5OI",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HZ5ALQ5T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"In the \\\"Optimize your CoreML usage\\\" session, the presenter Ben explains that he got a latency of 22ms using the new performance metrics and that gives him a running frame rate of 45 frames per second. How did he come to that conclusion and how can I look at my performance metrics to determine our frames per second as well?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "9e42afa8-cfd0-414f-a378-af1f62fa2ef1",
          "type": "message",
          "user": "U03HY66772A",
          "text": "In 1 second there is 1000ms, so if your model runs at 22ms, you can run it 1000/22 = 45 times.",
          "ts": "1654884442.408169",
          "thread_ts": "1654884381.712589",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "grWB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In 1 second there is 1000ms, so if your model runs at 22ms, you can run it 1000/22 = 45 times."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5a888220-fd93-4b91-abef-d52d0a2158d3",
          "type": "message",
          "user": "U03JFGMUPCY",
          "text": "The number is just an upper bound estimate based on 1s / 22ms = ~45 prediction per seconds. Such estimates often help us to understand the amount of headroom we can use for other operations while meeting the real time requirement (30 fps, etc).",
          "ts": "1654884443.678619",
          "thread_ts": "1654884381.712589",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "DrO",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The number is just an upper bound estimate based on 1s / 22ms = ~45 prediction per seconds. Such estimates often help us to understand the amount of headroom we can use for other operations while meeting the real time requirement (30 fps, etc)."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03J1UX2CQK\u003e asked\n\u0026gt; Is it possible to run a CoreML model in the cloud/on Linux? \n\u0026gt; \n\u0026gt; We are using a CoreML model to power privacy-preserving, on-device features. But we want to offer a web-based demo to potential users, since downloading an app can be higher friction than just using a website.",
      "ts": "1654884571.684419",
      "thread_ts": "1654884571.684419",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 2,
      "latest_reply": "1654885660.207319",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "b1aU",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03J1UX2CQK"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to run a CoreML model in the cloud\\/on Linux? \\n\\nWe are using a CoreML model to power privacy-preserving, on-device features. But we want to offer a web-based demo to potential users, since downloading an app can be higher friction than just using a website.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "23313bdf-6dc8-4012-a664-afab2c154e0d",
          "type": "message",
          "user": "U03J52T5J22",
          "text": "No, it is not supported. It is an interesting use case. A feedback assistant report will be much appreciated!",
          "ts": "1654884663.459729",
          "thread_ts": "1654884571.684419",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iu8j",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "No, it is not supported. It is an interesting use case. A feedback assistant report will be much appreciated!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d384c91a-4beb-47c8-9077-afe8784cf82d",
          "type": "message",
          "user": "U03J1UX2CQK",
          "text": "Thanks for your consideration! Filed a feedback: FB10162636",
          "ts": "1654885660.207319",
          "thread_ts": "1654884571.684419",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tP+HN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for your consideration! Filed a feedback: FB10162636"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HYBRAJNB\u003e asked\n\u0026gt; As far as I know, multi-label image classification is not possible with Create ML. Is it possible with Create ML Components to create a multi-label classifier?",
      "ts": "1654884732.235609",
      "thread_ts": "1654884732.235609",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654884972.777689",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "A31",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HYBRAJNB"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"As far as I know, multi-label image classification is not possible with Create ML. Is it possible with Create ML Components to create a multi-label classifier?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "c6861129-36f8-46f6-b886-5d3edd3b07f2",
          "type": "message",
          "user": "U03HRPK5JKU",
          "text": "Hi Matthew, one option is to implement your own custom estimator using a framework like MPSGraph. To simplify the task (and data required), you may want to explore training the classifier on the features produced by a feature extractor, such as \u0026lt;https://developer.apple.com/documentation/createmlcomponents/imagefeatureprint\n|ImageFeaturePrint\u0026gt;.",
          "ts": "1654884972.777689",
          "thread_ts": "1654884732.235609",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7l6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Matthew, one option is to implement your own custom estimator using a framework like MPSGraph. To simplify the task (and data required), you may want to explore training the classifier on the features produced by a feature extractor, such as "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createmlcomponents/imagefeatureprint\n",
                      "text": "ImageFeaturePrint"
                    },
                    {
                      "type": "text",
                      "text": "."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JZNY81L0\u003e asked\n\u0026gt; Is there custom operation support for PyTorch?",
      "ts": "1654886223.307189",
      "thread_ts": "1654886223.307189",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654890641.867479",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "PNEDm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JZNY81L0"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there custom operation support for PyTorch?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "9e6049ac-efc4-4deb-9744-da546fab2f43",
          "type": "message",
          "user": "U03J4CNMT6D",
          "text": "Check out the \u003chttps://coremltools.readme.io/docs/composite-operators|composite ops doc\u003e that can be used to build ops composed of existing CoreML (MIL) ops",
          "ts": "1654886299.993809",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "G8nFz",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Check out the "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/composite-operators",
                      "text": "composite ops doc"
                    },
                    {
                      "type": "text",
                      "text": " that can be used to build ops composed of existing CoreML (MIL) ops"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cf6ecb1a-1e5b-41bf-b30a-36a1736b328b",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Ah, I meant when using PyTorch on its own rather than when converting to CoreML. Eg with a metal compute kernel.",
          "ts": "1654886661.404069",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=xaqg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ah, I meant when using PyTorch on its own rather than when converting to CoreML. Eg with a metal compute kernel."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "96fb1465-3891-4806-a4c3-63f030ac32e5",
          "type": "message",
          "user": "U03JLPLT8HK",
          "text": "\u003c@U03JZNY81L0\u003e, I don't think so you can use it as it is rather than converting to CoreML.",
          "ts": "1654888836.608069",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "wSg8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JZNY81L0"
                    },
                    {
                      "type": "text",
                      "text": ", I don't think so you can use it as it is rather than converting to CoreML."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dd688522-be1a-4c17-bc53-4bd1916e6996",
          "type": "message",
          "user": "U03JLPLT8HK",
          "text": "IF anyone knows, I'm interested to learn about it. Specially for tensorflow.",
          "ts": "1654888874.137509",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Jg79",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "IF anyone knows, I'm interested to learn about it. Specially for tensorflow."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "072a0e80-9180-4b71-8d65-13ee639ddce6",
          "type": "message",
          "user": "U03JLPLT8HK",
          "text": "I can't use MobileNetV3 due to Swish operation doesn't support with CoreML",
          "ts": "1654888908.468689",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KEY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I can't use MobileNetV3 due to Swish operation doesn't support with CoreML"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f7fb57bb-2a27-4707-8eab-9eb7d288b49e",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "\u003c@U03JLPLT8HK\u003e You can definitely \u003chttps://coremltools.readme.io/docs/custom-operators|implement a custom operator\u003e in the CoreML context, though the \u003chttps://paperswithcode.com/method/hard-swish|hard-swish operation\u003e might be able to be expressed as a combination of other ops in CoreML as well",
          "ts": "1654889450.142209",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5fZc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JLPLT8HK"
                    },
                    {
                      "type": "text",
                      "text": " You can definitely "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/custom-operators",
                      "text": "implement a custom operator"
                    },
                    {
                      "type": "text",
                      "text": " in the CoreML context, though the "
                    },
                    {
                      "type": "link",
                      "url": "https://paperswithcode.com/method/hard-swish",
                      "text": "hard-swish operation"
                    },
                    {
                      "type": "text",
                      "text": " might be able to be expressed as a combination of other ops in CoreML as well"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5c4cd1f9-8c41-416b-b240-dece73cebf46",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "\u003c@U03JLPLT8HK\u003e from that link it looks like you could use \u003chttps://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.activation.relu6|RELU6\u003e + \u003chttps://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.elementwise_binary.mul|multiply\u003e, etc to replace the hard-swish efficiently",
          "ts": "1654889822.340369",
          "thread_ts": "1654886223.307189",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7Wsq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03JLPLT8HK"
                    },
                    {
                      "type": "text",
                      "text": " from that link it looks like you could use "
                    },
                    {
                      "type": "link",
                      "url": "https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.activation.relu6",
                      "text": "RELU6"
                    },
                    {
                      "type": "text",
                      "text": " + "
                    },
                    {
                      "type": "link",
                      "url": "https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.elementwise_binary.mul",
                      "text": "multiply"
                    },
                    {
                      "type": "text",
                      "text": ", etc to replace the hard-swish efficiently"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "55bab5d3-96cb-40c1-a683-71ad41aa45af",
          "type": "message",
          "user": "U03HRPPNAE6",
          "text": "Hi \u003c@U03JZNY81L0\u003e, to learn more about GPU acceleration for Pytorch and Tensorflow, please refer to \"Accelerate machine learning with Metal\":\n\u003chttps://developer.apple.com/videos/play/wwdc2022/10063/\u003e\n\nSpecifically, Pytorch is open sourced, so you can leverage this to implement custom operations in Metal. Custom ops are also supported for TensorFlow as outlined in the session.",
          "ts": "1654890641.867479",
          "thread_ts": "1654886223.307189",
          "attachments": [
            {
              "fallback": "Apple Developer: Accelerate machine learning with Metal - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Accelerate machine learning with Metal - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10063/",
              "text": "Discover how you can use Metal to accelerate your PyTorch model training on macOS. We'll take you through updates to TensorFlow training...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6556/6556_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10063/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10063/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "V1mV",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi "
                    },
                    {
                      "type": "user",
                      "user_id": "U03JZNY81L0"
                    },
                    {
                      "type": "text",
                      "text": ", to learn more about GPU acceleration for Pytorch and Tensorflow, please refer to \"Accelerate machine learning with Metal\":\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10063/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n\nSpecifically, Pytorch is open sourced, so you can leverage this to implement custom operations in Metal. Custom ops are also supported for TensorFlow as outlined in the session."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03JE2RJ2DA\u003e asked\n\u0026gt; Do you have an example of how the ML image style transfer was created from an earlier session?",
      "ts": "1654888008.167289",
      "thread_ts": "1654888008.167289",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 8,
      "latest_reply": "1654896667.806709",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "roN",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03JE2RJ2DA"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Do you have an example of how the ML image style transfer was created from an earlier session?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a9cd28f5-cc1a-4195-989e-50a3c34427c0",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Are you interested in how the app was built or the model itself?",
          "ts": "1654888027.817789",
          "thread_ts": "1654888008.167289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "bbt/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Are you interested in how the app was built or the model itself?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "073604ae-89ac-4adc-81c4-99482eba9209",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "You can learn about how Create ML can help you build style transfer models with an example integration in this session: \u003chttps://developer.apple.com/videos/play/wwdc2020/10642/|Build Image and Video Style Transfer models in Create ML\u003e",
          "ts": "1654888054.364289",
          "thread_ts": "1654888008.167289",
          "attachments": [
            {
              "fallback": "Apple Developer: Build Image and Video Style Transfer models in Create ML - WWDC20 - Videos - Apple Developer",
              "id": 1,
              "title": "Build Image and Video Style Transfer models in Create ML - WWDC20 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2020/10642/",
              "text": "Bring stylized effects to your photos and videos with Style Transfer in Create ML. Discover how you can train models in minutes that make...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/49/3439/3439_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2020/10642/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2020/10642/",
              "blocks": null
            }
          ],
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U03JE2RJ2DA"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "o57",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can learn about how Create ML can help you build style transfer models with an example integration in this session: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2020/10642/",
                      "text": "Build Image and Video Style Transfer models in Create ML"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a77765c5-77bc-4a06-85cd-54ce5f459e91",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "There are also a wide variety of style transfer models online that can be converted to Core ML format with coremltools",
          "ts": "1654888072.762139",
          "thread_ts": "1654888008.167289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "OWEd",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are also a wide variety of style transfer models online that can be converted to Core ML format with coremltools"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5c7dea7b-3e22-41d8-a3b2-555c4fdca149",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "The model takes in an image and outputs an image",
          "ts": "1654888095.313529",
          "thread_ts": "1654888008.167289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Mb3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The model takes in an image and outputs an image"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "6BC38997-E711-4BB4-AC9F-2153AECCFF56",
          "type": "message",
          "user": "U03JE2RJ2DA",
          "text": "Both the app and the model was built",
          "ts": "1654888253.175629",
          "thread_ts": "1654888008.167289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0ibRz",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Both "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "app"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "and"
                    },
                    {
                      "type": "text",
                      "text": " the "
                    },
                    {
                      "type": "text",
                      "text": "model"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "built"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8A68B46B-53EE-49B6-A7A0-135B7F3DF509",
          "type": "message",
          "user": "U03JE2RJ2DA",
          "text": "…and how the model was built",
          "ts": "1654888832.736189",
          "thread_ts": "1654888008.167289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "m5afC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "…and"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "how"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "the"
                    },
                    {
                      "type": "text",
                      "text": " model "
                    },
                    {
                      "type": "text",
                      "text": "was"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "built"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ff186056-0337-458d-a268-0a0d85d5cf60",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Do the resources above help?",
          "ts": "1654896611.023369",
          "thread_ts": "1654888008.167289",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7zI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Do the resources above help?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e5574276-cd6e-4e2d-b44a-960fe0f13568",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "I believe the app is  streaming data from an AVCaptureSession and running each frame through the Core ML model and outputting the result properly scaled back to the original image size",
          "ts": "1654896667.806709",
          "thread_ts": "1654888008.167289",
          "edited": {
            "user": "U03HRMWNP4J",
            "ts": "1654896677.000000"
          },
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "SJlZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I believe the app is  streaming data from an AVCaptureSession and running each frame through the Core ML model and outputting the result properly scaled back to the original image size"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "297cf90d-c5ac-4ec7-8f71-9b0b564e5f18",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "As we come to a close near 5pm PT today, we figured we would drop a few last prompts in here for the end of WWDC. Thank you all for joining us in this lounge. We have loved your questions, engagement and ideas, and are looking forward to what you do with ML and Computer Vision in the next year.",
      "ts": "1654898991.779189",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Pwt",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "As we come to a close near 5pm PT today, we figured we would drop a few last prompts in here for the end of WWDC. Thank you all for joining us in this lounge. We have loved your questions, engagement and ideas, and are looking forward to what you do with ML and Computer Vision in the next year."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "a217e576-42a3-42c9-82dc-11cf060d3248",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "• :thread: What new features or ML capabilities would you like to see next year?",
      "ts": "1654899010.325779",
      "thread_ts": "1654899010.325779",
      "reply_count": 7,
      "latest_reply": "1654902334.688199",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "fJB",
          "elements": [
            {
              "Type": "rich_text_list",
              "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"emoji\",\"name\":\"thread\"},{\"type\":\"text\",\"text\":\" What new features or ML capabilities would you like to see next year?\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b59be10b-bea2-44c4-919f-ea28eb78489d",
          "type": "message",
          "user": "U03HMCT187R",
          "text": "A drawing classifier added to Create ML would be :100:  then I can move away from Turi Create :smirk: (FB7357120)",
          "ts": "1654899136.018409",
          "thread_ts": "1654899010.325779",
          "edited": {
            "user": "U03HMCT187R",
            "ts": "1654899210.000000"
          },
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "n/20",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A drawing classifier added to Create ML would be "
                    },
                    {
                      "type": "emoji",
                      "name": "100",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": "  then I can move away from Turi Create "
                    },
                    {
                      "type": "emoji",
                      "name": "smirk",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " (FB7357120)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "96258fb5-d94c-45f1-8795-f9b03899e98e",
          "type": "message",
          "user": "U03JWCQ8ZPY",
          "text": "Integration between Swift Charts and Create ML for scientist move from python to Swift :slightly_smiling_face:",
          "ts": "1654899293.543579",
          "thread_ts": "1654899010.325779",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "gWh",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Integration between Swift Charts and Create ML for scientist move from python to Swift "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d7eb5120-2599-4af1-915f-72360745a241",
          "type": "message",
          "user": "U03J8GWEFU7",
          "text": "It would be cool to think about how ML services could start to show up as Action primitives in Shortcuts.\n\nOne example, if a piece of music is detected around you, automatically shazam it and add it to an Apple Music playlist.",
          "ts": "1654899398.337909",
          "thread_ts": "1654899010.325779",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QBTBo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It would be cool to think about how ML services could start to show up as Action primitives in Shortcuts.\n\nOne example, if a piece of music is detected around you, automatically shazam it and add it to an Apple Music playlist."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "92b8ba88-cec5-4d1c-b667-b3b3f86a183a",
          "type": "message",
          "user": "U03HY66772A",
          "text": "NeRF’s in CreateML:) When I was giving a talk about CreateML on a conference, there was a lot of positive feedback on how easy it was to setup and run. So, expanding on common tasks with pretrained models is a great idea, but adding some ml models that are in a spotlight is a nice way to go too:)",
          "ts": "1654900332.041159",
          "thread_ts": "1654899010.325779",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5bJp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "NeRF’s in CreateML:) When I was giving a talk about CreateML on a conference, there was a lot of positive feedback on how easy it was to setup and run. So, expanding on common tasks with pretrained models is a great idea, but adding some ml models that are in a spotlight is a nice way to go too:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fff1268b-b159-4602-a6bb-c79ea064afa5",
          "type": "message",
          "user": "U03HY66772A",
          "text": "It would be great to see ANE running not only inference, but training too. At least, on macs, so people could train their models order of magnitude faster on local machines.",
          "ts": "1654900518.652299",
          "thread_ts": "1654899010.325779",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nnoK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It would be great to see ANE running not only inference, but training too. At least, on macs, so people could train their models order of magnitude faster on local machines."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0543a22e-9422-4264-b31a-dfafc399308c",
          "type": "message",
          "user": "U03HY66772A",
          "text": "Also, some pretrained language models in CreateML can be a huge boon for CoreML. I’ve always dreamed about creating ML chatbots for different personal usecases",
          "ts": "1654900739.493009",
          "thread_ts": "1654899010.325779",
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654900748.000000"
          },
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "gCqaZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also, some pretrained language models in CreateML can be a huge boon for CoreML. I’ve always dreamed about creating ML chatbots for different personal usecases"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cad9cff2-9cf4-4ac2-ba55-a407187b1897",
          "type": "message",
          "user": "U03J20RJQ2X",
          "text": "The new pet / object segmentation of iOS 16 coming to Vision would be amazing!",
          "ts": "1654902334.688199",
          "thread_ts": "1654899010.325779",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QQMAB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The new pet / object segmentation of iOS 16 coming to Vision would be amazing!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "c766491c-7c9c-46c7-b9a2-09a92b316469",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "• :thread: Any sample code or documentation requests?\n",
      "ts": "1654899022.440439",
      "thread_ts": "1654899022.440439",
      "reply_count": 4,
      "latest_reply": "1654903387.282959",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "cZn",
          "elements": [
            {
              "Type": "rich_text_list",
              "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"emoji\",\"name\":\"thread\"},{\"type\":\"text\",\"text\":\" Any sample code or documentation requests?\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
            },
            {
              "type": "rich_text_section",
              "elements": []
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "54b506ac-1106-427b-b93c-0c8e23d1d704",
          "type": "message",
          "user": "U03J8JGRCDD",
          "text": "It's be really cool to see how to do \"super resolution\" upscaling with ML.",
          "ts": "1654899919.284709",
          "thread_ts": "1654899022.440439",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Tp/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It's be really cool to see how to do \"super resolution\" upscaling with ML."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fc1e41b6-f81d-4918-8f53-53ca5958521d",
          "type": "message",
          "user": "U03HY66772A",
          "text": "More articles on optimisations tailored for ANE",
          "ts": "1654900361.824679",
          "thread_ts": "1654899022.440439",
          "edited": {
            "user": "U03HY66772A",
            "ts": "1654900365.000000"
          },
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "REER+",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "More articles on optimisations tailored for ANE"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "658fa2a6-3e36-4c93-aa3a-9560461f8c36",
          "type": "message",
          "user": "U03JELQLESV",
          "text": "More code listings would be appreciated on the docs pages. `MLShapedArray` is a random example.",
          "ts": "1654903280.262069",
          "thread_ts": "1654899022.440439",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U03JZNY81L0"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Gqko4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "More code listings would be appreciated on the docs pages. "
                    },
                    {
                      "type": "text",
                      "text": "MLShapedArray",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is a random example."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "aa1d6fed-184e-4e30-8a8f-caf36b09695c",
          "type": "message",
          "user": "U03JELQLESV",
          "text": "Also maybe the Q\u0026amp;As here in the Digital Lounges should guide conceptual documentation!  E.g. \u003chttps://wwdc22.slack.com/archives/C03H4A911EH/p1654633602775309\u003e. Bummer that so much valuable info will just disappear sometime after the end of the conference.",
          "ts": "1654903387.282959",
          "thread_ts": "1654899022.440439",
          "attachments": [
            {
              "fallback": "[June 7th, 2022 1:26 PM] Machine Learning - Ask a Question: \u003c@U03JELQLESV\u003e asked\n\u0026gt; When making activity classification predictions on video frames, are there ways or considerations for improving speed aside from skipping frames (cadence)? Is prediction window size a factor?",
              "id": 1,
              "author_subname": "Machine Learning - Ask a Question",
              "author_link": "https://wwdc22.slack.com/services/B03H04756BH",
              "author_icon": "https://avatars.slack-edge.com/2022-05-29/3590386215573_f35a7fddb4ea9d20cd18_48.png",
              "text": "\u003c@U03JELQLESV\u003e asked\n\u0026gt; When making activity classification predictions on video frames, are there ways or considerations for improving speed aside from skipping frames (cadence)? Is prediction window size a factor?",
              "from_url": "https://wwdc22.slack.com/archives/C03H4A911EH/p1654633602775309",
              "original_url": "https://wwdc22.slack.com/archives/C03H4A911EH/p1654633602775309",
              "mrkdwn_in": [
                "text"
              ],
              "blocks": null,
              "footer": "Thread in #machine-learning-lounge",
              "ts": 1654633602.775309
            }
          ],
          "edited": {
            "user": "U03JELQLESV",
            "ts": "1654903469.000000"
          },
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "G7=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Also maybe the Q\u0026As here in the Digital Lounges should guide conceptual documentation!  E.g. "
                    },
                    {
                      "type": "link",
                      "url": "https://wwdc22.slack.com/archives/C03H4A911EH/p1654633602775309",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": ". Bummer that so much valuable info will just disappear sometime after the end of the conference."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "e7dd8501-fc49-4497-b4a8-e88a72118c29",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "• :thread: What was your favorite ML related feature this year ?\n",
      "ts": "1654899031.288699",
      "thread_ts": "1654899031.288699",
      "reply_count": 4,
      "latest_reply": "1654904219.320849",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Kklj7",
          "elements": [
            {
              "Type": "rich_text_list",
              "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"emoji\",\"name\":\"thread\"},{\"type\":\"text\",\"text\":\" What was your favorite ML related feature this year ?\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
            },
            {
              "type": "rich_text_section",
              "elements": []
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "2b8c74fb-51a1-4477-ba33-84578a03062f",
          "type": "message",
          "user": "U03HY66772A",
          "text": "CoreML Performance Profiler, for sure:)",
          "ts": "1654900395.148489",
          "thread_ts": "1654899031.288699",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Yq4ip",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "CoreML Performance Profiler, for sure:)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c33c999d-b2e0-4337-9e6f-cbe445012796",
          "type": "message",
          "user": "U03JZNY81L0",
          "text": "Performance info! The instrument is amazing!",
          "ts": "1654902279.699659",
          "thread_ts": "1654899031.288699",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Fab=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Performance info! The instrument is amazing!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "33c0b133-3951-4387-8689-6041c2dd9e62",
          "type": "message",
          "user": "U03HZ2VBE21",
          "text": "VisionKit APIs!",
          "ts": "1654902944.637289",
          "thread_ts": "1654899031.288699",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "yUL2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "VisionKit APIs!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f83bb3fc-090c-4174-806f-51e77ff77d52",
          "type": "message",
          "user": "U03K19A2324",
          "text": "\u003c@U03J7UASVEU\u003e Automatic language identification is great. If conjunctured with machine learning translation (see these guys here: \u003chttp://www.deepl.com|www.deepl.com\u003e - they have done an amazing job, I can offer feedback if you'd like from using it to communicate with actual Japanese which is one of the toughest languages to translate due to contextual dependency :wink: ) it would be a really powerful tool!",
          "ts": "1654904219.320849",
          "thread_ts": "1654899031.288699",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "f69st",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U03J7UASVEU"
                    },
                    {
                      "type": "text",
                      "text": " Automatic language identification is great. If conjunctured with machine learning translation (see these guys here: "
                    },
                    {
                      "type": "link",
                      "url": "http://www.deepl.com",
                      "text": "www.deepl.com"
                    },
                    {
                      "type": "text",
                      "text": " - they have done an amazing job, I can offer feedback if you'd like from using it to communicate with actual Japanese which is one of the toughest languages to translate due to contextual dependency "
                    },
                    {
                      "type": "emoji",
                      "name": "wink",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " ) it would be a really powerful tool!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "7bab5712-80b2-4b46-8e5d-d77bab39fd90",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "\u003c!here\u003e 2 more hours until questions close!",
      "ts": "1654899053.990159",
      "thread_ts": "1654899053.990159",
      "reply_count": 3,
      "latest_reply": "1654905234.190909",
      "team": "T031SG5MZ7U",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "+aRXP",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " 2 more hours until questions close!"
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "e245c1a3-9135-4985-bdf8-46023af02154",
          "type": "message",
          "user": "U03K19A2324",
          "text": "Thank you \u003c@U03J7UASVEU\u003e and team for an amazing job and a wonderful lounge! Keep it going strong folks! ML is amazing and you're making sure Apple stays in the forefront! :slightly_smiling_face:",
          "ts": "1654904356.944519",
          "thread_ts": "1654899053.990159",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Hs0Ik",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J7UASVEU"
                    },
                    {
                      "type": "text",
                      "text": " and team for an amazing job and a wonderful lounge! Keep it going strong folks! ML is amazing and you're making sure Apple stays in the forefront! "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "02B1408A-5D7D-4160-BB3A-29B11B992FC0",
          "type": "message",
          "user": "U03J7UASVEU",
          "text": "Thank you! This lounge has had some amazing engagement, and it's been a joy to support our amazing developer community :-)",
          "ts": "1654904467.199349",
          "thread_ts": "1654899053.990159",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "T7l",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you! This lounge has had some amazing engagement, and it's been a joy to support our amazing developer community :-)"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7ff587a7-7aec-4e42-8790-4c0506c1e572",
          "type": "message",
          "user": "U03K19A2324",
          "text": "You are welcome \u003c@U03J7UASVEU\u003e! It's actually the people that in the end make the technologies awesome. It was my first WWDC but it was really nice seeing the great interaction and the chemistry between the engineers and the developers, besides being limited by text-only communication. It only shows how cool the overall community is and how awesome it must be when it gets together. I hope you will keep a part of the conference online in the coming years and enrich it with even more interaction (maybe beyond the WWDC too)! Keep this inclusive and enthusiastic community spirit alive!!! :slightly_smiling_face:",
          "ts": "1654905234.190909",
          "thread_ts": "1654899053.990159",
          "parent_user_id": "U03J7UASVEU",
          "team": "T031SG5MZ7U",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "f3J",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You are welcome "
                    },
                    {
                      "type": "user",
                      "user_id": "U03J7UASVEU"
                    },
                    {
                      "type": "text",
                      "text": "! It's actually the people that in the end make the technologies awesome. It was my first WWDC but it was really nice seeing the great interaction and the chemistry between the engineers and the developers, besides being limited by text-only communication. It only shows how cool the overall community is and how awesome it must be when it gets together. I hope you will keep a part of the conference online in the coming years and enrich it with even more interaction (maybe beyond the WWDC too)! Keep this inclusive and enthusiastic community spirit alive!!! "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03HY66772A\u003e asked\n\u0026gt; Does CoreML benefit from two ANE's in M1 Ultra?",
      "ts": "1654899513.167489",
      "thread_ts": "1654899513.167489",
      "subtype": "bot_message",
      "bot_id": "B03H04756BH",
      "username": "Machine Learning - Ask a Question",
      "reply_count": 1,
      "latest_reply": "1654899540.504589",
      "reactions": [
        {
          "name": "+1",
          "count": 1,
          "users": [
            "U03JZNY81L0"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "3rRj",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03HY66772A"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Does CoreML benefit from two ANE's in M1 Ultra?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "4ccafbb3-9a8d-4370-8af2-a1db9f375f05",
          "type": "message",
          "user": "U03HRMWNP4J",
          "text": "Yes when you are using multiple models or batch inference",
          "ts": "1654899540.504589",
          "thread_ts": "1654899513.167489",
          "team": "T031SG5MZ7U",
          "reactions": [
            {
              "name": "pencil2",
              "count": 2,
              "users": [
                "U03JZNY81L0",
                "U03J9R7MVJ7"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "naT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes when you are using multiple models or batch inference"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "7623f8ae-22c8-493b-81cf-d51f4e85c3b6",
      "type": "message",
      "user": "U03J7UASVEU",
      "text": "And that's all folks. Thank you for making this WWDC truly special for all of us. See you next year!",
      "ts": "1654905973.989419",
      "team": "T031SG5MZ7U",
      "reactions": [
        {
          "name": "gratitude-thank-you",
          "count": 3,
          "users": [
            "U03JRP87THN",
            "U03HY66772A",
            "U03JE2RJ2DA"
          ]
        },
        {
          "name": "wwdc22",
          "count": 3,
          "users": [
            "U03JRP87THN",
            "U03HY66772A",
            "U03HVC8S8DU"
          ]
        },
        {
          "name": "clap",
          "count": 5,
          "users": [
            "U03JRP87THN",
            "U03J9GM2ESE",
            "U03JN7USRDY",
            "U03J4DR9GDS",
            "U03J5SM2VUJ"
          ]
        },
        {
          "name": "eyes",
          "count": 1,
          "users": [
            "U03HY66772A"
          ]
        },
        {
          "name": "+1",
          "count": 3,
          "users": [
            "U03JZ3XKFC4",
            "U03J5SM2VUJ",
            "U03JQ04FR0E"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "DxD",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "And that's all folks. Thank you for making this WWDC truly special for all of us. See you next year!"
                }
              ]
            }
          ]
        }
      ]
    }
  ],
  "channel_id": "C03H4A911EH"
}
