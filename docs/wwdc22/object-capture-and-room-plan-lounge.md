# object-capture-and-room-plan-lounge QAs
### Lounge Contributors
#### [pol-piella](https://github.com/pol-piella)
#### [emin@github](https://github.com/roblack) / [emin@twitter](https://twitter.com/emin_ui)
#### [shirblc](https://github.com/shirblc)
#### [tikimcfee](https://github.com/tikimcfee)



---
> ####  Hi!  I'm just wondering if you have suggested guidance for capturing objects that are on a rotating platform, to minimize post-production work.  Is it advised to try to frame the platform out of the image as best as possible, to post-process each image and crop out the platform, wrap the platform in a similar color to the background, etc.?


|U03J7GGBZDW|:
We suggest to use a uniform color platform which is different than the object and keep the object inside the frame, but as large as possible. Use object masking to make PhotogrammetrySession remove the background <https://developer.apple.com/documentation/realitykit/photogrammetrysession/configuration-swift.struct/isobjectmaskingenabled>

For difficult objects you can also provide manual masks. Please see  <https://developer.apple.com/documentation/realitykit/photogrammetrysample/objectmask>

|U03J20E7UBV|:
Thank you, <@U03J7GGBZDW>!  Since I have the camera on a tripod and object(s) on a platform, doing some degree of post-processing or masking might be a viable solution when passing the photos into the Object Capture API.

|U03HHNPJVBM|:
<@U03J20E7UBV> To get the best outcome from post-processing, please keep the exif in the images and do not crop or resize the images.

---
> ####  can I get point cloud during updating progress of request or can only get point cloud data after request complete ?


|U03J7GGBZDW|:
There is a separate request for point cloud and model. You can get the point cloud output before the model is done.

---
> ####  How can I crop the model after it is made? 


|U03HL4SJY5S|:
We provide a `Geometry` field on the model reconstruction `Request` that will allow you to specify the cropping region _during_ the model creation step.  We show this flow in last year's WWDC session (<https://developer.apple.com/videos/play/wwdc2021/10076/>).  To recap, a GUI app can be made with the API that will let you request a preview model, arrange the reconstruction volume, and then get the cropped output.

We recommend cropping during reconstruction since this allows our algorithms to focus the texture and mesh resolution on areas that are being reconstructed.

Also note that this year we added the ability to specify an `OrientedBoundedBox` in the request to allow a more general cropping volume.

---
> ####  can I choose how many or range or point clouds to generate?


|U03JLQVJE67|:
The number of points or the range of the points in a point cloud request is automatically determined by the algorithms based on the input images.

---
> ####  Is it possible to add custom textures to the mesh map? For example, a furniture piece I scanned is of material Oak color, can I change it to a glossy silver color?


|U03HHNPJVBM|:
You can edit the texture map generated by the PhotogrammetrySession and assign it to the obj model by Reality Converter or USDZ Tools <https://developer.apple.com/augmented-reality/tools/>. If you need more information on usdz, please join “The Q&amp;A: USD” from 5-7pm today.

---
> ####  Can you clarify the underlying differences for a `RoomCaptureView` and a `RoomCaptureSession`?  `RoomCaptureView` seems like it is a `UIView` subclass that handles all of the underlying logic for scanning and creation of the 3D model for output, whereas `RoomCaptureSession` appears to be a custom implementation that allows us to customize the process.  What would be the use case for a custom `RoomCaptureSession`?


|U03J7GGBZDW|:
`RoomCaptureView` is a plug and play view that you can use in your app
`RoomCaptureSession` let’s you subscribe to updates using a delegate and gives you finer grain control over the surfaces and objects that RoomPlan detects.
RoomCapture session can be used for filtering out surfaces or objects and rendering them as per your liking.

---
> ####  I'm just now trying to get up to speed. Is it possible to use a room plan as an anchor for future AR sessions? A particular use case would be tying an AR experience to the architecture and physical properties of a room.


|U03J7GGBZDW|:
It is an interesting idea but this question may be better suited for tomorrow RoomPlan Q&amp;A.

|U03HMCSQ9EK|:
Thank you!

---
> ####  Is there a point of creating new models using this year's APIs (highResolution?) with last year's photos? should it get me a better model?


|U03HL4SJY5S|:
We added algorithmic improvements to this year's Object Capture API, so you may see improvements to your model if you re-run this year's API on images you already captured last year.

---
> ####  Perusing the room capture demo, would we have access to the view just like an ARView, so we have finer control over objects and experiences in it?   Looking at the documentation doesn’t yield anything useful


|U03J7GGBZDW|:
This is not supported using a `RoomCaptureView`. But you can have fine control using the data only API and creating a custom `ARView` (or `SCNView` or `MTKView`)

---
> ####  How can I convert the .usdz file generated by object capture to a format that I can edit with blender, such as .fbx or .usd?


|U03HL4SJY5S|:
The Object Capture API can directly provide OBJ and USD (ascii) outputs if you provide a folder URL instead of a USDZ filename when requesting the `modelFile`.

|U03HL4SJY5S|:
The maps will also be placed into this folder along with the OBJ and USD.

---
> ####  Is ARKit 6's new `session.captureHighResolutionFrame` intended for use with an Object Capture experience?  It's not clear if `session.captureHighResolutionFrame` captures just the high resolution camera feed, or the camera feed with overlaid AR content.


|U03J7GBRQ0G|:
The captureHighResolutionFrame can be used by developers creating their own Object Capture app using ARKit to get higher quality images to be used for Object Capture. It captures the high-res camera feed, returning an ARFrame with a high-res image along with other information such as the camera tracking results.

|U03J7GBRQ0G|:
Using this API you just get the camera feed though and not the overlaid content in AR

|U03J20E7UBV|:
Thanks, <@U03J7GBRQ0G>!  Does the output file format (I'm not sure if it's a HEIC or PNG, etc.) also include depth/gravity data baked in?

|U03J7GBRQ0G|:
You simply get an ARFrame and can access the CVPixelBuffer through that to eventually save it to the format of your choice

|U03J7GBRQ0G|:
<https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframe>

|U03J20E7UBV|:
Got it!  Thanks so much for the helpful information!

---
> ####  Does the output USDZ file from a RoomPlan session maintain some sort of a model hierarchy where the object types (and not surface types) could be modified/removed at a later time, or is the USDZ flattened?


|U03J7GGBZDW|:
These are great questions for the Q&amp;A session for RoomPlan tomorrow. There are two choices for you: You can either tune in at 9-11am or 4-6pm Pacific Time. Alternatively, you can also request a 1:1 lab appointment on the developer app for this Thursday.

|U03J20E7UBV|:
Thank you!  Will focus all future questions on Object Capture!

---
> ####  If someone were interested in filtering out [.objects] from a captured RoomPlan, would it be advised to do this in the `func captureSession(_ session: RoomCaptureSession,                          didEndWith data: CapturedRoomData, error: Error?)` method, just prior to exporting to a USDZ model, or at some other time during the capture process?


|U03J7GGBZDW|:
Another excellent question. It would be great to have you join us at the RoomPlan Q&amp;A session tomorrow where we can cover this and lots of other questions on this topic.

It all depends if you’re using `RoomCaptureView` or not: if you are, there’s no way of filtering out objects from live scan rendering, so you’ll have to do it prior to exporting to a USDZ. If you have your own custom view (using Data Only API), We’d recommend doing it directly in the live view (so that end users understand that objects won’t be part of the final USDZ)

|U03J20E7UBV|:
That makes sense, thank you very much!  :slightly_smiling_face:

---
> ####  As someone who is new to both those tools, but is working in Swift/Metal realm, Is the API of both of them accessible to have the functionality added to other applications?


|U03HHNPM8M9|:
Object Capture and Room Plan are both public Swift API’s intended to be used by developers to make their own applications.

|U03HHNPM8M9|:
We have sample code to <https://developer.apple.com/documentation/realitykit/taking_pictures_for_3d_object_capture|take pictures for 3D object capture> and <https://developer.apple.com/documentation/realitykit/creating_a_photogrammetry_command-line_app|creating a photogrammetry command-line app on Mac> from last year. This year we add another sample code for <https://developer.apple.com/documentation/realitykit/using_object_capture_assets_in_realitykit|using object capture assets in RealityKit> .

|U03JH3TKE3V|:
Thanks you

---
> ####  Are there any USD prim naming guarantees for the output room? For example, could the naming schema suddenly change in iOS 16.2?


|U03HHNW0MRR|:
There’s no guarantee that the naming schema will remain the same over time, but we do our best to maintain consistency and compatibility over time.

---
> ####  Are there any hints about the kinds of result degredation we can expect if we use it outside of residential settings? e.g. if we can live without "objects", or high confidence levels...


|U03HB55MWCE|:
Current technology is designed to support interior of residential homes.

---
> ####  Does RoomBuilder.ConfigurationOptions.beautifyObjects affect captured room surfaces, or just objects?


|U03HHNW0MRR|:
Just objects

---
> ####  Is there a recommended workflow for performing photogrammetry to capture textures alongside roomplan?


|U03HWCN4S0Z|:
Great question! We don’t have a recommended workflow for photogrammetry with RoomPlan. It is a great idea for developers to explore!

---
> ####  Should we be able to provide our own ARSession to the RoomCaptureSession? I attempted to set the arSession property with a custom configuration running, but room reconstruction never begins. I noticed without the custom ARSession, the RoomCaptureSession creates a RoomPlan.ARRoomCaptureConfiguration, with custom scene understanding techniques (keyframe with color). If this is a bug, I'd be happy to file feedback.


|U03HAQGCXC7|:
@Jack: ARSession property in RoomCaptureSession is intended to give you a handle to give developers the ability to customize the rendering detected CapturedRoom in the camera preview. It's not meant to be overwritten.

---
> ####  Should we create a RoomBuilder per CapturedRoomData, or are their advantages to reusing the same room builder instance?


|U03HHNW0MRR|:
This is an option that can be left up to the developer to implement for what makes sense for the design of their app. There aren’t RoomBuilder specific advantages or disadvantages to reusing the same instance.

---
> ####  Can sceneDepth depth maps be captured at the same time as as a RoomPlan scan?


|U03HWCNKLRF|:
We enable sceneDepth in underlying arsession in roomCaptureSession. Therefore yes, you could also access it via `roomCaptureSession.arSession`

|U03K6CZC8LQ|:
Thank you <@U03HWCNKLRF>!

---
> ####  Is there a way to get the generated usdz model in actual world size? Currently the exported model comes out to be very very small in real world scale


|U03HHNW0MRR|:
This is listed as a known issue in Beta 1

---
> ####  I noticed that capture only identifies a single floor with the configuration in the code sample. Would it be possible to combine multiple captures to capture multiple floors in one model with the same origin reference?


|U03J9L3CG2X|:
Great question! The current technology is designed to support single room and single floor scan. Combining different scans would be a great direction for developer to explore.

|U03K6CZC8LQ|:
Thank you <@U03J9L3CG2X>!

---
> ####  For the PhotogrammetrySession API, is there any way to geo-reference the result? (so that we can place model on a map accurately based on photos capture location)


|U03HB55MWCE|:
This could be a good question for the Object Capture Digital Lounge on Friday 10am-12pm.

---
> ####  Can this be used for scanning multiple rooms in a single scan?


|U03HHNW0MRR|:
We are currently only supporting a single room scan

---
> ####  Is it possible to real time texture the RoomPlan objects before or after export?


|U03H368GAB1|:
Currently we do not provide texture of 3D objects.

---
> ####  Not sure if this was asked, but is it possible to add textures to the resulting CapturedRoom (Maybe by using the data API) to create a room that looks closer to the real thing?


|U03HWCN4S0Z|:
The `CapturedRoom` struct itself does not allow adding textures but your custom visualizer can parse the structure to get dimension and locations and add textures.

---
> ####  Is there any recommendations on manipulating RoomCaptureData for custom display? Like rendering as 2D plans, or even using it as an overlay on the real world after the fact? (Imagining replacing the chair objects with photorealistic chairs, or fantastical objects for games).


|U03HWCN4S0Z|:
Both `CapturedRoom` and `CapturedRoomData` are read-only. `CapturedRoomData` is intended for generating the final room model with `RoomBuilder.` During capture, you can parse the `CapturedRoom` structures and add your custom visualization (eg. photorealist chairs, fantastical objects as you mentioned) on the real world.

---
> ####  I'm just now trying to get up to speed. Is it possible to use a room plan as an anchor for future AR sessions? A particular use case would be tying an AR experience to the architecture and physical properties of a room.


|U03HWCNKLRF|:
RoomPlan is not an ARAnchor in current design. Thanks for suggestion. We will take into consideration.

|U03HMCSQ9EK|:
Any other suggestions, by chance? Could this be reasonably solved with an archived world map, distributed to client devices, either in the app binary or over the network?

---
> ####  Hi all, super stoked to watch the RoomPlan session yesterday. I am quite new to ARKit and the USD filetype so forgive me if this question is out of scope for this session. What would be the best way to create a 2D floor-plan (image) from the outputted USD file on-device? We've got ways we could do this on our servers, but the ability to process this on device would be amazing.


|U03HWCNKLRF|:
USD file contains 3D dimension and transformation information. Therefore, you could do projection to 2D in any arbitrary angle.
For 2D floorplan, simply projection from y coordinate should work.

|U03K2SE36N5|:
Alex, are the dimensions available directly from the USD file? Is there sample code for this?

|U03HWCNKLRF|:
yes, dimensions are inside usdz file. You could import the exported usdz. I think you could find more info in documentation.

---
> ####  Would it be possible to choose which components to display from RoomCaptureView and adjust location and sizing on screen? Like for example only show the mini-map render and adjust it's location and size?


|U03HHNW0MRR|:
The Model (or mini-map as described) is not currently adjustable in the `RoomCaptureView`, but can be disabled optionally by setting `roomCaptureView.isModelEnabled = false` and a custom model could be implemented if desired using a custom visualizer.
This also seems like a good opportunity to use the data-only version of the API and implement a completely custom visualizer for the application

|U03K6CZC8LQ|:
Thank you <@U03HHNW0MRR>!

---
> ####  Is this API improving based on usage (ML?)


|U03HWCN4S0Z|:
No

---
> ####  Are there any recommendations on converting the resulting usdz file to something like an obj file to be used in Blender? The current structure of a usd file and folders of usda files don't seem to import nicely


|U03HWCN4S0Z|:
The current API does not support OBJ, developers are encouraged to explore using existing tools for conversion.

---
> ####  Are there corrections being done to fix close to 90 angles to 90, and parallel, perpendicular geometries by the API?


|U03J9L3CG2X|:
When user finish scanning and generating final result, the post processing would try to generate output as close to reality as possible.

---
> ####  Are we able to capture ARMeshAnchors while running a RoomPlan capture session at the same time?


|U03HAQGCXC7|:
No, you cannot capture ARMeshAnchors while running RoomCaptureSession.

|U03K6CZC8LQ|:
Thank you <@U03HAQGCXC7>!

|U03HAQGCXC7|:
You're welcome <@U03K6CZC8LQ>.

|U03KLP0LK24|:
It would be a cool feature to have the capture be right to the right anchor 

---
> ####  Definitely would like something slightly more general, still with rooms and objects, but more "industrial" applicability. Even if the confidence is low and it's more surfaces than objects with categories it would be very useful to us. Thanks


|U03HB55MWCE|:
Thanks for the question, can you clarify more what is the "industrial" application? Do you mean support commercial store scanning?

|U03HZ4PT2ER|:
Not just residential rooms with sinks, fireplaces, etc but more like a factory floor or other industrial setup. These may be small rooms with windows and doors, but the objects inside aren't home furnishings. We will have to see how well RoomPlan v1 works when we test it

|U03HB55MWCE|:
Got it, thanks Steven for the clarification!

---
> ####  Is there any documentation on what the output USDZ structure is like?  I brought my Room Plan USDZ output into RealityKit and looked at the children, and it’s very well organized.  Just wondering if there’s any documentation to know how we might remove certain subcomponents (like walls, windows, etc).


|U03HWCN4S0Z|:
Thank you for trying out USDZ file so quickly! Currently there is no documentation for USDZ structure. Thanks for the feedback and we will take it into consideration

|U03J20E7UBV|:
Thank you, <@U03HWCN4S0Z>!

|U03J97X79CL|:
I’m missing ceilings and floors in the export

|U03J97X79CL|:
and the metersPerUnit header detail in all files

|U03HAQGCXC7|:
<@U03J7B27ADD>: We don't support ceilings and floors. Thanks for noticing the missing metersPerUnit header in usd files. We've logged it in our database.

---
> ####  Thank you so much for taking the time to answer our questions!! This is incredibly helpful


|U03HHPGS2TU|:
We’re more than happy to see developers, such as yourself, are interested in RoomPlan, and would do our best possible to support your adoption of it in your project.

---
> ####  How are dimensions mapped in the in the dimensions simd_float3 for CapturedRoom.Object and CapturedRoom.Surface? Do these values represent height, width, and depth? If so in which order?


|U03HAQGCXC7|:
<@U03K6CZC8LQ>: The dimension value order is width, height, and length.

---
> ####  How can we get a scanned object tag/label (chair/fireplace...)


|U03HAQGCXC7|:
<@U03JTPRQ66Q>: Scanned object's label is a enum typed property named category in CapturedRoom.Object

---
> ####  Is there any guidance on how "floors" are calculated/rendered in a Room Plan capture?  Does the `.floor` type exist, or in the rendered `CapturedRoom`, is the output adding a general plane as the floor?


|U03HWCN4S0Z|:
We currently don’t support floor type in the framework. Developers are encouraged to explore custom visualizations!

|U03J20E7UBV|:
Thank you, <@U03HWCN4S0Z>!

|U03HWCN4S0Z|:
Thanks for the question

---
> ####  Can custom object classifications be implemented for RoomPlan other than the default object recognition? What level of ML integration can it support?


|U03H368GAB1|:
the current classification model in the pipeline is not a module. It cannot be easily replaced by an external model.

---
> ####  is the accuracy of the RoomPlan the same level of accuracy as the LiDAR scan? aside from the known issue with the scale being 100 times off, how much difference is it from real scale when scaled back appropriately when it's exported from USDZ to FBX or OBJ.


|U03HWCN4S0Z|:
We are constantly working on improving the accuracy of the RoomPlan results. RoomPlan is intended for simplified parametric representation of the room. It does not have the same level of accuracy as LiDAR scans. We do have a lot to improve to support applications that have high accuracy requirement.

---
> ####  Why isn't the object capture API available on iOS?


|U03HHPA938S|:
Please ask during the Object Capture Digital Lounge on Friday June 10th from 10am-12pm PST, or ask in one of the Object Capture Labs. This Digital Lounge is currently for RoomPlan.

---
> ####  Aside from the recommendation of a 30’ x 30’ room maximum, is there an imposed maximum area size limit with how RoomPlan can capture new objects and surfaces?


|U03HHNW0MRR|:
No, there is nothing that will stop you from scanning a large area

|U03HHNW0MRR|:
While you will be able to scan larger areas, there is still impact that will happen with drift and device heat for larger areas

|U03K6CZC8LQ|:
makes sense, thank you <@U03HHNW0MRR>!

---
> ####  Can we store/export the original mesh while using the RoomPlan API alongside the RoomPlan CapturedRoom? Also, can RoomPlan be run concurrently while also capturing a mesh with Lidar?


|U03HAQGCXC7|:
<@U03J98VF2K1>: RoomPlan doesn't export mesh data. You'll have to run reconstruction mesh while RoomCaptureSession isn't active.

|U03J98VF2K1|:
Would we still be able to extract the depth map, color image, color camera position, lidar camera position?

|U03HAQGCXC7|:
<@U03J98VF2K1>: Yes, you can still assign yourself as a ARSessionDelegate and get those ARKit's information.

|U03J98VF2K1|:
Awesome thank you :slightly_smiling_face:

|U03J98VF2K1|:
I’m currently able to read the color camera position, is there lidar camera position information as well?

|U03J98VF2K1|:
<@U03HAQGCXC7> Additionally, which camera position is being returned when doing a Room Scan with lidar?

|U03HAQGCXC7|:
<@U03J98VF2K1>: The camera position is from the wide angled lens.

|U03J98VF2K1|:
Is that the only camera position we can receive? No lidar camera position information is being exposed?

|U03HAQGCXC7|:
<@U03J98VF2K1>: Yes, to the best of my knowledge, you only get the position from the camera that you are seeing on the screen. Since this is intrinsically ARKit information, you could confirm it in ARKit channel.

|U03J98VF2K1|:
Ah thanks, I’ll give that a try tomorrow :slightly_smiling_face:

---
> ####  Is it possible to get the underlying mesh information during or after a RoomPlan scan?


|U03HWCN4S0Z|:
It’s not possible to get underlying mesh information while RoomCaptureSession is active.

|U03J98VF2K1|:
Thank you :slightly_smiling_face:

---
> ####  I played around with the configurations during a RoomPlan, namely RoomBuilder. What does `.beautifyObjects` actually do? I exported two captured rooms with the option on and off. The results looked the same.


|U03HWCN8AL9|:
.beautifyObjects on vs off is to align chairs along with table.

|U03J98VF2K1|:
Thanks!

|U03H368GAB1|:
yes... if you try a scene with chairs around a table (like a dining set), you can see the difference.. the beautified chairs is shorter than table, and evenly distributed and align along table.

|U03J98VF2K1|:
Ah that’s why I didn’t see any difference, I only had one chair and it was pushed into my desk

---
> ####  Is it possible to leverage the object detection data separately from the room measurements (walls, doors, windows)? We have an app that uses ArKit + LiDAR to capture precise measurements, but would like to augment that with RoomPlan.


|U03HAQGCXC7|:
<@U03K881J7DX>: Object detection results are presented in the array property `objects` in `CapturedRoom` struct. You can just fish them out separately.

---
> ####  I’ve see that y’all recommend a custom visualizer. Would it be possible to add AR objects at the same time the user is taking measurements? Say, real-time substituting chairs, beds, walls, etc?


|U03HHNW0MRR|:
Yeah, using a custom visualizer it would totally be possible to substitute objects. A great opportunity for developers to explore!

|U03J2004PGT|:
I probably wouldn't be able to use RealityKit to place those objects with my custom visualizer correct? Because it would be busy with the RoomPlan?

|U03HHNW0MRR|:
Correct, the custom visualizer would replace `RoomCaptureView` and utilize the data only  API to perform the visualizations.

---
> ####  Can an app access the ARSession.currentFrame property on the RoomCaptureSession.arSession (for instance to obtain the RGB frame, depth, or camera pose) as usual? If so, is there any way to obtain the latest frame when available other than just... polling at 60Hz? Since the RoomCaptureSession owns the ARSessionDelegate, which is usually how the latest frame would be obtained.


|U03HAQGCXC7|:
<@U03HVDET8S2>: Yes, you can still get all of ARKit's information through ARSessionDelegate. RoomCaptureSession doesn't prevent client from becoming the delegate.

|U03HVDET8S2|:
Oh, nice! I just assumed it did -- thanks!

---
> ####  Do you plan on supporting additional ceiling types beyond flat? Sloped, peaked, tray/coffered?


|U03J9L3CG2X|:
Good question! We only support flat ceiling. It would be a great direction for developers to explore.

|U03JRR54X7S|:
+1 to this. My ceiling is sloped about 1" over 10' to allow draining in a common direction. I’m sure it would confuse any attempt to clean up angles to 90°, etc.

|U03HWCN4S0Z|:
Thanks for the feedback. We will take it into consideration

---
> ####  Can you please let us know if this RoomPlan is built on top of ARKit planes mesh?


|U03HWCNKLRF|:
Room Plan is built upon various ARKit technologies

|U03J98VF2K1|:
Can I assume that it using ARKit planes information?

|U03HWCN4S0Z|:
No, sorry we can not comment on implementation details and they are also subject to change

---
> ####  What is the smallest wall surface length that is supported? We're had difficulty capturing walls that less than 1 foot in length (not height).


|U03J9L3CG2X|:
We don't set an explicit limit on the length of the smallest walls. But thanks for the feedback. We will take this into consideration.

---
> ####  Are the RoomPlan models as accurate as the ARMeshAnchors or is accuracy being sacrificed for clean geometry?


|U03HWCN4S0Z|:
We haven’t compared the accuracy between RoomPlan models with ARMeshAnchors. A RoomPlan model is intended to be a simplified parametric representation of the room. We are trying to make the final model to be as close to the real world as possible. But there are some abstractions or simplifications such as representing wall/window/door/opening as planes without thickness.

|U03K6CZC8LQ|:
Thank you <@U03HWCN4S0Z>!

---
> ####  Is there any documentation for extracting wall dimensions from the USDZ file from within swift on iOS? I can import the USDZ into Xcode and easily see any manipulate the data, but I’m not seeing how to access that programmatically.


|U03HHNW0MRR|:
Using RealityKit you can load the USDZ as an Entity and then perform evaluations on it. Please note that there is a known issue with Beta 1 on the scale of dimensions in a USDZ file.
<https://developer.apple.com/documentation/realitykit/loading-entities-from-a-file>
Additionally, you are able to obtain the dimension values directly from the `CapturedRoom` surfaces and objects without needing to export and load the USDZ

|U03K2SE36N5|:
Thank you, are there any particular resources within RealityKit that I should start looking into?

|U03HHNW0MRR|:
Yeah that link above provides documentation on how to load a USDZ file as an Entity, and then you can walk through the child Entities to find your desired objects and surfaces.
Here is the documentation on an `Entity`
<https://developer.apple.com/documentation/realitykit/entity>

|U03K2SE36N5|:
Perfect, thank you!

---
> ####  As might be a common use case, do you have any guidance for how one might implement a "measurement" tool to measure from one point to another (I presume using a raycast to determine those points, but not sure where I'd go from there to measure in meters in feet the distance between two points)?


|U03HWCN4S0Z|:
Good question! Looks like you already have some ideas. Feel free to explore the possibilities.

---
> ####  Can the ARSession provided by RoomCaptureSession be supplied to the new NearbyInteraction NISession.setARSession? This would be useful to, say, localize a UWB beacon within a room being scanned.


|U03HAQGCXC7|:
<@U03HVDET8S2>: You can certainly supply the ARSession provided by RoomCaptureSession to other object. The intention is for users to get existing ARKit supplied information out of it. But if the ARSession is used to run ARConfiguration, RoomCaptureSession will stop automatically.

|U03HVDET8S2|:
That makes sense; just wanted to clarify. Thanks again <@U03HAQGCXC7>

---
> ####  Would it be possible to apply textures to the CapturedRoom?


|U03HWCN4S0Z|:
Great question! That will be a good use case for data-only API with custom visualization.

|U03J98VF2K1|:
I have turned off different things in the configurations, like instructions. Is that what data-only API means or is that something else?

|U03HWCN4S0Z|:
Oh, sorry for the confusion. The data-only API is mainly referring to `RoomCaptureSession` and `RoomCaptureSessionDelegate` protocol, with which you can get `CapturedRoom` during the scan and apply textures yourself. For final model, you will also get CapturedRoom from `RoomBuilder.capturedRoom(from:)` to apply textures.

|U03J98VF2K1|:
Ah that makes sense. thanks :slightly_smiling_face:

|U03K7271MEC|:
So when applying the textures, is that the same as grabbing an ARFrame and aligning to some timestamp to apply?

---
> ####  Is there a 3D processing library coming to iOS any time soon (make meshes watertight). Is that anything you guys have on your radar? 


|U03J7GGBZDW|:
Thanks, we don’t discuss future releases of Apple products. But we’d love your feedback and suggestion. Please file your feedback here to get it into our system. <https://developer.apple.com/bug-reporting/>

|U03J22LUULS|:
I am aware. Will file this to put it on your teams radar 

|U03J7GGBZDW|:
Thank you!

---
> ####  On Full Reconstruction Displacement maps always turn out Red. They are about 100MB large but contain no useable data. I checked all the different Color channels


|U03J7GGBZDW|:
In the _full_ detail level we provide the following texture maps: _diffuse, normal, AO, roughness, displacement_. Displacement is useful for some use cases.

In case you do not need some of the texture maps you could try to output uncompressed USDA and individual texture maps in a folder (by providing a folder URL). That makes it easier to choose a subset of the texture maps.

|U03J22LUULS|:
Thanks <@U03J7GGBZDW>. We would actually like to use displacement for rendering with NIRA but the displacement map we get from Full Quality doesn't have any useable information. 

|U03J7GGBZDW|:
The displacement map is a single channel output. We can look at your data if you share it via feature request.

|U03J22LUULS|:
Yeah I thought it is in the red channel but checked all channels and couldn't find any output. We filed a bug for it FB10124152

---
> ####  Can we use a Bounding Box on the PointCloud to pass to PhotogrammetrySession before starting mesh reconstruction to cut down on time for reconstruction? 


|U03HL4SJY5S|:
The `.bounds` `Request` should be used to get the initial reconstruction bounding box.  This can be used in an GUI iterative workflow as shown in the WWDC 21 Object Capture video to then send the desired reconstruction volume box for the reconstruction to use by adding it into the `Geometry` property of the `Request`.

Note that this year we added `OrientedBoundingBox` to allow more general reconstruction boxes (not axis-aligned).  We recommend to use the `.preview` `Detail` along with `.bounds` for this, but it should also be possible to use the point cloud as well.  The key is that the volume specified in `Geometry` is relative to the initial coordinate system defined by the `.bounds` `Request`.

Intermediate results in the session are cached, so subsequent requests should be faster than the initial ones as well.

---
> ####  For the PhotogrammetrySession API, is there a way to Geo-reference the results? So that we can put generated 3d models on a map, especially with drone captured imagery.


|U03HL4SJY5S|:
We do not provide automatic geo-referencing functionality in the API.

If this is a feature you would like to see, please submit to us a request in the Feedback Assistant.

|U03J21CNQ1G|:
Alright! Will submit a feedback!

|U03K881J7DX|:
You could just get the location using existing APIs though, right?

|U03K881J7DX|:
<https://developer.apple.com/documentation/corelocation>

|U03J21CNQ1G|:
Well you could get the locations of the images yes, but not the generated model right?

|U03K881J7DX|:
I was thinking of capturing the location of the device as it captures the room. Obviously, GPS inside doesn't always work well...

|U03K881J7DX|:
Sorry, I was focused on RoomPlan. You're looking at ObjectCapture :)

|U03J21CNQ1G|:
Yeah, the last time I played with PhotogrammetrySession, I used a bunch of drone imagery of a building, and the results were great but sadly no option to geo reference it.

|U03K881J7DX|:
My team used to do a lot with drone capture - using drone images to create accurate 3D roof measurements. I'd be happy to see if anyone has ideas that could help short of re-creating what we did. I don't think they want us sharing contact info on here, but you can look me up on LinkedIn - Jeff Lewis w/Verisk

|U03J7GGBZDW|:
Drone imagery is a great use case for photogrammetry.:+1::skin-tone-2:

|U03J21CNQ1G|:
<@U03K881J7DX> I work in a local startup that works on photogrammetry hence my interest in this subject. Don’t use LinkedIn much but I think I found your profile

|U03JSLCQU04|:
I have used a drone for photogrammetry that includes GPS and altitude in it's metadata. The resulting models are always reconstructed high in the Y axis as if it is using the altitude data. Does the API use altitude?

|U03JSLCQU04|:
This also seems to suggest that the APA can use GPS data to aid in reconstruction, but not return it. <https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata>

|U03J21CNQ1G|:
Btw <@U03JSLCQU04> you might want to use the workflow feature for your question as the Apple Engineers may not be monitoring this thread

|U03JSLCQU04|:
Done, thank you <@U03J21CNQ1G>

---
> ####  Hello all! I'm checking the PhotogrammetrySession.process(request:) new doc and it seems that now we can use it on iOS devices (<https://developer.apple.com/documentation/realitykit/photogrammetrysession/process(requests:)?changes=latest_major).|https://developer.apple.com/documentation/realitykit/photogrammetrysession/process(requests:)?changes=latest_major).> Will I be able to use Object Capture on an iPad? Is there some limitations like needed M1? Thanks!!!


|U03HHNPM8M9|:
This is an error in the documentation that will be addressed. Object Capture is only available on macOS.

---
> ####  I'd love to create both images for Object Capture and an ARReferenceObject so that I can later show virtual content registered on both the real object and the USDZ from Object Capture. After all, both give me access to a raw point cloud.  While there doesn't seem to be an official API for such a use-case I was wondering if you have anything you can point me to do register these point clouds. Are there any assumptions I can make, e.g. on the origin of the created USDZ model?


|U03J7GGBZDW|:
It is an interesting idea. The two point clouds are different data structures and come from different algorithms.
We do not provide currently algorithms for point cloud alignment but there are a several open source packages for this.
We are happy about feedback, please submit also a feature request for it.

|U03JEM7CE2D|:
Thanks! Does anyone knows particular algorithms or search queries that could point me in the right direction?

|U03JEM7CE2D|:
FB10160265

---
> ####  On Full Reconstruction and large image data sets (300 images+) we experienced timeouts and failed jobs and when a job finished it sometimes produced no Albedo Texture     - is it possible the gpu runs out of memory and cannot produce the high resolution texture (8K)


|U03J7GGBZDW|:
It is difficult to diagnose and depends also on the other workload which you have on the GPU. Please provide feedback with your data. That will allow us to look into this.

|U03J22LUULS|:
FB10124598

---
> ####  I've noticed when I set my phone down on the table with the camera facing down, didEndWith gets called automatically. Is there a way to prevent this from happening?


|U03HL4SJY5S|:
Since `RoomCaptureSession` relies on `ARSession`, this cannot be prevented since if tracking is failing, `didEnd` will automatically be called. However, you can take action based on the error provided in the `didEnd` delegate.

---
> ####  There is an option to add EXIF metadata to samples in object capture (like focal length, GPS position, etc), but there is no example or documentation on how this data should be formatted, and no feedback from the API that this data is successfully being used. Is it possible to provide an example of this?


|U03J7GGBZDW|:
ImageIO is a great framework to read and write image metadata, including EXIF metadata. Please see <https://developer.apple.com/documentation/imageio>

|U03JSLCQU04|:
Thank you for the advice! I may need to clarify better. If I am creating a photogrammetry sample manually with CVPixelBuffers, there is an option to add metadata as a [String : any] dictionary. Should the metadata be a float, int, string?

|U03JSLCQU04|:
<https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata>

|U03J7GGBZDW|:
That is good feedback for our documentation, thanks

---
> ####  I am using the photogrammetry cli application called "Hello Photogrammetry" that you have in your documentation. It all starts well by producing messages like the following to indicate progress "2022-06-09 22:30:24.675274-0500 HelloPhotogrammetry[42340:2622322] [HelloPhotogrammetry] Progress(request = modelFile(url: file:///Users/Merficius/Downloads/Test/Food/food-high-heic-meat.usdz, detail: RealityFoundation.PhotogrammetrySession.Request.Detail.full, geometry: nil) = 0.013281" at a good constant rate, but then it stops the progress and starts spamming this kind of message "2022-06-09 22:43:41.325189-0500 HelloPhotogrammetry[42340:2663624] VPA info: plugin is INTEL, AVD_id = 1080020, AVD_api.Create:0x24c2bdf2c", after a while it resumes to the first kind of message and then goes back to the second message, then to the first message, etc. Is this normal behavior?


|U03HHNPM8M9|:
Thanks for the feedback. Please provide data (images, logs, machine info etc.) to developer forum <https://developer.apple.com/bug-reporting/> or Feedback Assistant <https://feedbackassistant.apple.com> for us to look into.

---
> ####  Is there a way to feed a folder of images and masks to the Photogrammetry API, or will I need to instantiate each image and mask individually?  I know I can provide a folder, I just wasn’t sure if there is a way for the API to know which images are masks.


|U03HL4SJY5S|:
The `PhotogrammetrySession` when provided with a folder will assume _all_ image files in the folder are RGB images of the object to be reconstructed, so you cannot mix them all in one folder if you use the folder input init.  There is currently not a way to specify masks using the folder input to a session.

To provide masks, you will need to use the custom input `PhotogrammetrySession` that takes a lazy sequence of `PhotogrammetrySample`objects that you create yourself in your app.  You will have to load the images yourself in this case and they can be stored whereever you wish, in the same folder, in a database, etc.

|U03J20E7UBV|:
Got it, thank you, <@U03HL4SJY5S>!

---
> ####  (Reposted/clarified from threads) This seems to suggest that GPS metadata can be added to a sample to aid in reconstruction. <https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata|https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata> Does it help with the scale and altitude of a reconstruction? The dictionary key should be a string, but what data type should the value be added in? Float, double, int?


|U03J7GGBZDW|:
Yes providing GPS information is useful and can help with the reconstruction, especially for drone use cases.
We use the same formatting as ImageIO.

---
> ####  Is there a way to force the API to use a specific GPU?


|U03HHNPM8M9|:
The API does not support to specify GPUs. If you have this feature request in your use case, please consider filing a request in Feedback Assistant <https://feedbackassistant.apple.com> or developer forum <https://developer.apple.com/bug-reporting/>

---
> ####  If "object masking" is enabled when processing a Photogrammetry model, but there are no object masks provided, does the algorithm create its own mask?


|U03J7GGBZDW|:
yes if this value is enabled, but the samples don’t contain object masks, Object Capture attempts to automatically create a mask algorithmically. If it’s unable to create a mask, Object Capture reverts to reconstructing the object using the entire image.

|U03HL4SJY5S|:
More information can be found in the documentation here: <https://developer.apple.com/documentation/realitykit/photogrammetrysession/configuration-swift.struct/isobjectmaskingenabled>

|U03J20E7UBV|:
Got it, thank you <@U03J7GGBZDW> <@U03HL4SJY5S>!

---
> ####  Our App captures rooms. We use SceneKit to draw lines representing Wall edges. We want to stay with our wall capture method. But auto capturing everything else is enticing. I’ve plugged in a RoomCaptureSession into the app. I want to use point data to draw lines with SCNNodes. But when the callbacks get called(e.g. didUpdate), there does not seem to be any useful point data for a a door. Even with a high confidence level, in the didUpdate method when I look inside the CapturedRoom, I’m seeing 0 values for CompletedEdges, PolygonCorners and PolygonEdges. Is there a particular callBack method I should be using? It looks like didUpdate and didChange are the main methods that get called. But I’m not seeing any valuable point data in them.


|U03HHNVCZ19|:
You can use `transform` and `dimensions` paremeters to draw lines. The 4 corners can be inferred from those 2 parameters: the first column of the transform is the "right" vector, and second is the "up" vector. The fourth column is the position of the wall/door/opening/window/object Combining those unit vectors with the dimensions vector will give you the corners

---
> ####  Is there still a 1000 image hard limit in photogrammetry sessions? Previously the API would stop processing images after 1000 regardless of their resolution, or how much memory was in the system. I have tests both 2 megapixel up to 12 megapixel datasets, on Intel, M1, and M1 Max chips.


|U03HHNPM8M9|:
Thanks for the feedback. What is the desired upper limit for your use case? It would be great if you can provide more info about your use case if the current set-up is limiting, at Feedback Assistant <https://feedbackassistant.apple.com> or developer forum <https://developer.apple.com/bug-reporting/>.
