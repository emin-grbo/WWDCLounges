{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"WWDCLounges An archive of questions asked during WWDC Lounges. \ud83d\ude4c You are welcome to contribute via GitHub","title":"Welcome"},{"location":"index.html#wwdclounges","text":"An archive of questions asked during WWDC Lounges. \ud83d\ude4c You are welcome to contribute via GitHub","title":"WWDCLounges"},{"location":"template.html","text":"Template Lounge QAs by USER Question Answer","title":"Template Lounge QAs"},{"location":"template.html#template-lounge-qas","text":"","title":"Template Lounge QAs"},{"location":"template.html#by-user","text":"","title":"by USER"},{"location":"template.html#question","text":"Answer","title":"Question"},{"location":"wwdc21/DevTools.html","text":"DevTools Lounge QAs by paul \ud83d\uddd3 Tuesday Hi, thank you for organizing this! I have a question about CI, in regards to Xcode Cloud. I remember having troubles (no \"std output\" was printed anywhere) running certain scripts in Pre-build and post-build actions in Scheme editor when I was using Xcode Server & Bots back in 2015. The question is shall we rely on pre-build action for, say, running things like pod install and other pre-build setup w/o which the project cannot be run? Xcode Cloud supports custom CI-specific build scripts that are distinct from pre-build action scripts in your Xcode scheme. The logging from these scripts is captured and made available in your workflow results. See some documentation about this here: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts Will XCode Cloud allow you to work with a locally hosted (on my personal network) git repo? Xcode Cloud has first-class support and deep integration with both cloud-based source control providers \u2014 GitHub, Bitbucket, and GitLab \u2014 and on-premise source code providers; GitHub Enterprise, Bitbucket Server, and GitLab Self-managed. https://developer.apple.com/documentation/xcode/source-code-management-setup First thanks for creating Xcode Cloud it seems so promising! I watched all the videos so far, I was curious to know if there would be a way to retry particular failed test or not? Xcode Cloud will allow you to re-run a failed job. Which tests get run will depend on how you configured your test plan, and if you enabled a test repetition mode (New in Xcode 13!) in your test plans. Xcode Cloud always respects your test plan/scheme and workflow configuration Would it be possible to get the dSYM file from XCode Cloud builds? Download dSYM option is not available on App Store Connect if we build without bitcode enabled. It would be nice to have Testflight crash reports be symbolicated, and still have access to the dSYM file to use with other tools. What I found is that as long as Xcode outputs a dSYM, it will be included in the artifacts. For more advanced workflows, you could use Xcode Cloud\u2019s environment variables with a custom build script to extract symbols from an archive and upload them to a place of your choosing. More specifically, take a look at CI_ARCHIVE_PATH and ci_post_xcodebuild.sh . Will Xcode Cloud be available to teams enrolled in the Apple Developer Enterprise Program? No. Xcode Cloud is initially supported for Apple Developer Program teams. You can read more about requirements on this page: https://developer.apple.com/documentation/xcode/requirements-for-using-xcode-cloud Will there be webhooks available with XCode Cloud for build, test, and distribute status updates? You can find documentation about Xcode Cloud\u2019s available webhooks below: https://developer.apple.com/documentation/xcode/configuring-webhooks-in-xcode-cloud I have a question around testing - I was trying to figure out a way to test scenarios around IAP family sharing, either in the simulator using App Store Config files, or even in Sandbox on device. I can't seem to find a way to simulate something like having a purchase made available via a family purchase made by someone else, or to simulate having a purchase revoked via the head of family disabling family sharing (or someone leaving a family). Would testing in sandbox on two devices that were set up with family sharing be able to test these scenarios? Was also wondering about generating server to server notifications for those events as well, but even just being able to test in Xcode would be helpful. Have you been using StoreKit to test this? This documentation seems related to what you're asking for: https://developer.apple.com/documentation/storekit/original_api_for_in-app_purchase/supporting_family_sharing_in_your_app There's also a great session from WWDC2020 around StoreKit Testing with Xcode: https://developer.apple.com/videos/play/wwdc2020/10659/ Thanks, I've develop my code to handle the scenarios outlined in that document, I'm more wondering how I can trigger those in simulator testing... for example, if I'm running in the Xcode simulator, is there any StoreKit testing feature that would trigger the paymentQueue(_:didRevokeEntitlementsForProductIdentifiers:) method of SKPaymentTransactionObserver ? I'd like to be able to test my code to make sure my handling of that method works correctly. That document also mentions a REVOKE server to server notification, when testing in Sandbox is there any way to generate that event? Thanks for the followup. For the server to server notification, unfortunately not. Please file a feedback ticket for us to add that functionality (and add a followup here with the FB# if you have time to do so now). For the first case, yes! After a purchase is made, open the StoreKit Transaction Manager (Debug > StoreKit > Manage Transactions), you can select a transaction and refund it. This will cause paymentQueue(_:didRevokeEntitlementsForProductIdentifiers:) to be called within your app. Hi! Congrats for your work! Demo looks awesome can't wait to test it myself. Will Xcode Cloud allow us to use classic open source tools in the Swift ecosystem (like swiftlint or sourcery) in our workflows ? How will version dependency be managed ? Yes, Xcode Cloud has ways for you to use open source tools in your workflows, so you can do things like lint or statically generate code in the cloud if you wanted to. Check out this documentation on writing custom scripts for Xcode Cloud to see how to get started! As for how your versioned dependencies will be managed, that would be handled by whatever dependency manager your project uses. So if you use Swift Package Manager, then your build step would resolve the dependencies and versions according to how you wrote your Package.swift file. Will Xcode Cloud support building projects with third party dependencies and if so which dependency managers will be supported (Cocoapods, Carthage and Swift Package Manager)? I believe you can use dependency managers using custom build scripts. Here is some documentation for you! https://developer.apple.com/documentation/xcode/writing-custom-build-scripts See also: https://developer.apple.com/documentation/xcode/making-dependencies-available-to-xcode-cloud Could Xcode Cloud be used as a platform to eventually add distributed build/cache systems? We're very interested in the ability to do use this to speed up developer builds. Build systems like Bazel have powerful capabilities here, but with poor Xcode integration and increased complexity for developers. We would love to see this problem solved natively using Xcode and is a major pain point. Thank you for the feedback, please feel free to file this with Feedback Assistant as well. Can Xcode Cloud run UI-tests in parallel? Xcode Cloud can parallelize tests among all the test destinations configured in your workflow, so yes, it can run UI tests in parallel Will it be possible to quickly download all the screenshots from the UI tests? This can be useful for submitting screenshots to the App Store listing. Perhaps this can even be automated right from the flow. There are several ways to accomplish that: You can download the result bundle from Xcode Cloud and open the result bundle locally, or open it directly in Xcode. 1. From the familiar outline mode you can export the images you want by selecting them and clicking on the paperclip icon in that activity. 2. In gallery mode you can easily select multiple images and drag them to Finder or elsewhere. 3. You can automate this using xcresulttool. For an overview of xcresulttool see https://help.apple.com/xcode/mac/current/#/devc38fc7392 Will Xcode Cloud have the concept of permissions? I.e Restrictions on who can create, edit & trigger workflows. Depending on a user\u2019s role in App Store Connect, they may be able to put restrictions on Workflows. You can find the documentation about that here: https://developer.apple.com/documentation/xcode/developing-a-workflow-strategy-for-xcode-cloud Here is some information about App Store Connect role permissions: https://help.apple.com/app-store-connect/#/deve5f9a89d7 When I'm working directly on a Swift package without a project file, creating a new unit test case file always uses the ObjC template, and it appears this is still true in Xcode 13. Is this something I've done wrong, or should I file a feedback? Please do file a feedback about this issue on https://feedbackassistant.apple.com. Thank you for bringing this to our attention! One workaround is to create new test case files from the Test Navigator. Use the \u201c+\u201d button at the bottom-left corner of the Test Navigator, select \u201cNew Unit Test Class\u2026\u201c, and then select Swift from the Language selector dropdown. Another workaround would be to create the file in the Tests directory using the basic \u201cSwift File\u201d template, and then add the code for the new XCTestCase subclass yourself. Will Xcode Cloud integrate with Gitlab? Xcode Cloud has first-class support and deep integration with both cloud-based source control providers \u2014 GitHub, Bitbucket, and GitLab \u2014 and on-premise source code providers; GitHub Enterprise, Bitbucket Server, and GitLab Self-managed. https://developer.apple.com/documentation/xcode/source-code-management-setup First of all, Xcode Cloud looks very promising! Thanks for all the great work that has been done around it. I have one question that I couldn't resolve by going through the docs though (sorry if it's already answered there). Basically, I'd like to know if workflows can be triggered on demand through an API the same way it can be done with some App Store Connect actions. Yes, Xcode Cloud will be fully integrated into the App Store Connect public REST API. You will be able to start new builds programmatically. The public REST API integration is not immediately available today, but will be released during the summer. For context on one typical use case: apps that have not only a frontend component but also have backend server components require integration testing when the backend systems change. When the backend system has passed server side tests, you can set up programmatic triggers using the API to invoke Xcode Cloud builds and tests from an upstream pipeline using a specified staging server. Will a workflow be always tied to an App Store listing? Can a project be built (using REST API) without its app ID being registered in App Store Connect? Yes a workflow is always associated with an App listing in App Store Connect. Creating a workflow in Xcode streamlines the creation of App Store Connect records. If the App Store Connect record already exists, then these will be associated automatically, based on the bundle ID in your project. For XCode Cloud and releasing to TestFlight, how do you capture \"What to Test\" for what's released to the user? Once Xcode Cloud has successfully uploaded your build to TestFlight, you can go to the TestFlight tab in App Store Connect to enter that information Xcode 12.5 has a serious bug. When I run app via cmd+r, connecting debugger to a simulator takes about 1-2 minutes and debugserver process use 100% CPU. Do you plan to update Xcode 12.5 to 12.5.1 or 12.6 with fixing this issue? Xcode 13 doesn\u2019t have this issue but I am unable to use Xcode 13. Xcode 13 Beta should address this performance problem seen when running on recent versions of macOS Big Sur. My question is this: could you please share some of the best practices for CI/CD at Xcode projects? Im sure Xcode Cloud will be my best friend in a future, but if we imagine we don\u2019t have Xcode cloud : ) For general, self maintained CI/CD xcodebuild is your best friend. The \"Testing in Xcode\" session from WWDC19 hits on some best practices that may be useful to you: https://developer.apple.com/videos/play/wwdc2019/413/ Some points I'd like to highlight from the session: - The testing pyramid, to help strike the right balance between unit, integration, and UI tests - Using test plans efficiently for the variables you want to change between test runs - What configuration works best for your needs when planning CI/CD set up. At times it is useful to have a separate builder machine from test runner, in which case you'll break your executions into build-for-testing and test-without-building, but it really depends on your use case Will Xcode Cloud support some sort of secret management? i.e third-Party tokens, AWS creds Yes it does! See the Custom Environment Variables section on this page: https://developer.apple.com/documentation/xcode/xcode-cloud-workflow-reference You can add secret environment variables to a workflow. They are \"write-only\", meaning that once set, secret environment variables cannot be read again from the workflow editor. Secret environment variables are encrypted in transit and at rest, and used only on the ephemeral build environment. As such they are suitable for AWS keys, passwords, and API tokens. You can also add \"non-secret\" environment variables, which can be read and edited from the workflow editor. Does Xcode Cloud support custom build tools? for instance, to use Xcode Cloud workflow to run Bazel build of iOS project? That\u2019s not currently supported, but you may want to give it a try to see if you could get it to work. That\u2019s great feedback though so please file a report on feedbackassistant.apple.com! How long will builds take on Xcode Cloud compared to building locally, for example using an M1 Mac Mini? Build times will depend on a number of factors, like the size of your project, build settings, workflow settings, custom build scripts, and much more. I would recommend reading these articles: https://developer.apple.com/documentation/xcode/developing-a-workflow-strategy-for-xcode-cloud https://developer.apple.com/documentation/xcode/xcode-cloud-workflow-reference What type of machines will be available to run builds on? Will there be tiers in terms of performance? There are currently no performance based tiers. How to join Xcode Cloud beta program now? we can try it ASAP :-) You can join the waitlist here: https://developer.apple.com/xcode-cloud/beta/ Is there any tools within Xcode and Xcode Cloud that would allow for an easy monorepo setup? My team has two internal libraries that are used in a tvOS and two iOS applications so we are looking to move them all into an easy repo for ease of maintenance but we are unsure if there is anything available to help ease the transition. I don\u2019t believe Xcode nor Xcode Cloud have a way to merge multiple repos into a single repo, but there are a number of tools available to make dependencies between your projects easier to work with. I would highly recommend this talk from WWDC 2019 on Swift Packages and also this guide on managing dependencies in Xcode Cloud . xcodebuild often returns error 65 without clear explanation. Sometimes it seems that it could be the simulator failing to boot, sometimes the test results fail to be collected, but the error messages are always a source of frustration as it's very hard to understand the problem. Furthermore, these errors are often sporadic and often happen on CI, where it's even harder to fix. What can we do to tackle this issues? Are there plans to improve xcodebuild error reporting? If you encounter xcodebuild errors that are unclear, or indicate that the failure is a system error rather than a problem with your own project, please file feedback on https://feedbackassistant.apple.com and attach the .xcresult bundle from the test run for investigation. In many cases, additional helpful diagnostics can be captured by running xcrun simctl diagnose while the simulator you were targeting is still booted. We are always looking for ways to make the errors produced by xcodebuild clearer and more helpful, and feedback from developers is a very important part of that process! Will there be a separate cost for Xcode Cloud? It seems like there is a lot of resources available. I already pay $140/mo for my own cloud-based build server. There is no cost to join the beta program. You can sign up for that here https://developer.apple.com/xcode-cloud/beta/. More details on pricing and availability of Xcode Cloud will be announced this fall I see that Xcode Cloud supports custom scripts for bootstrapping the temporary environment, but is/will there be a mechanism for caching some of the dependencies? For example a Ruby version installed via rbenv, and bundler dependencies. These are things we typically would never check into Git but could do with caching somewhere like we can on other platforms Caching build dependencies is not currently supported, but you might be able to do that yourself in a custom build script. You may want to give it a try to see if you could get it to work. That\u2019s great feedback though so please file a report on feedbackassistant.apple.com Does Xcode cloud require the new Macos 12? Or will it also work with code 13 and macoa 11? Xcode Cloud is part of Xcode 13, which will be compatible with macOS 11.3 \ud83d\ude00 Will we be able to use an http API to upload/connect a repository to start a workflow, i.e. not using Xcode at all? No. You must use Xcode Cloud to configure your initial workflow and connect your source code repositories. Will Xcode Cloud support pipelines that don't target the app store or testflight? Would love to be able to build .apps or even .pkg for my macOS apps. Yes! You can use a custom build script to upload artifacts to your own destinations. Please see the ci_post_xcodebuild.sh document in that linked article for information about copying artifacts outside of Xcode Cloud. xcodebuild has a long-standing difference (FB5531332) from Xcode's testing behaviour: it ignores the LocationScenarioReference (Location Simulation in the Xcode Scheme Run Options) in a scheme. This limits the kind of integration testing we can do on CI and requires workarounds to mock Core Location which is crucial to our app's functionality. Has location simulation been considered for UI testing with Xcode Cloud? Thank you for the report and the feedback reference number! This is a bug with xcodebuild that we will bring to the attention of the engineering team. Are the Xcode Cloud configurations stored in the Xcode project file, or is there a manifest that can be built up outside of the project file? Many other CI platforms have yml configurations that are easily human-readable. (I can't wait to try out Xcode Cloud; thanks to all involved!) Xcode Cloud workflow configuration is not stored in the Xcode project file; the configuration is persisted only in the cloud services that support the service. Being centralized in the cloud, the configuration can be accessed through the Xcode UI itself, the App Store Connect web UI, and also programmatically through the App Store Connect public REST API (available this summer). The API uses JSON format. Whereas power users are comfortable with editing YAML, integrating the configuration into an intuitive user interface makes the service more broadly accessible initially. Does it mean that we can edit/create/delete Xcode Cloud workflows via App Store Connect REST API with JSON objects? Yes, that\u2019s correct. Will testflight be available without Xcode cloud? Yes, TestFlight will be available on Mac separate from Xcode Cloud \ud83d\ude00 This session can give you more information about it as well: https://developer.apple.com/videos/play/wwdc2021/10170/ Will Xcode Cloud support golden/snapshot testing? (take a screenshot of a view or scene so we can know how a view should look like, and then this screenshot can be used to compare to following runs and see if the UI was modified. I currently use for this snapshotpointfreeco/swift-snapshot-testing from github) Xcode Cloud will run any tests authored using XCTest, which can be combined with that library to write XCTest-based snapshot tests. Sorry if i missed it, is it possible to use Xcode Cloud as a remote build server, so the artefacts are redownloaded to my local machine and i can run it on a sim locally? Cheers Yes, you can download the artifacts with a \u201cci_post_xcodebuild.sh\u201d script. See the docs here: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts Will there be support for fetching dependencies from other dependency managers that are not Swift Package Manager? I have currently a big legacy macOS project that uses git submodules, cocoapods and carthage. Yes, you can use Custom Build Scripts in Xcode Cloud to fetch dependencies from other systems which xcodebuild doesn\u2019t natively support such as those. See https://developer.apple.com/documentation/xcode/writing-custom-build-scripts There is additional documentation specifically about CocoaPods and Carthage available at https://developer.apple.com/documentation/xcode/making-dependencies-available-to-xcode-cloud Will Xcode Cloud support third-party static code analysis tools, like SonarQube? https://www.sonarqube.org You can integrate an Xcode Cloud workflow with services that are addressable through a custom script. Check out this documentation for specifics and how-to! I have several groups in TestFlight. Will Xcode Cloud have an option to automatically add specified Groups to a build or will it only add the 'AppStore Connect User' group? Yes, you can configure a workflow to select one or more TestFlight groups. Our macOS app targets pro users in the music industry, which isn't known for rapid adoption of OS updates\u2026 Will TestFlight for macOS be made available to testers on prior releases of macOS? Our internal QA tests all the way back to macOS 10.12, so it would be nice to (finally!) maintain the same kind of workflow that we have for our iPad and iPhone testing on prior releases. Mac TestFlight support begins with macOS 12.0, it cannot be used on older releases Are there any API requirements/limitations on the apps that are submitted to TestFlight? For example, it would make sense (given your response) that I would be required to build w/ Xcode 13 + macOS 12 SDKs to integrate with TestFlight for macOS, but will I be OK to continue using a deployment target of macOS 10.12 for my app binary? Yes, you can continue to use a deployment target older than macOS 12.0! The requirements are: - You must build and submit from Xcode 13 and use a provisioning profile - The TestFlight client supports macOS 12 and later, so builds aren't installable on older macOS versions. If we have our own build/test farm which use AWS mac instance, does Xcode Cloud support to use our own dedicated cloud infra? No, this is not supported. I'm concerned about the Homebrew aspect. Is there a chance that some dependency scripts could build and link with parts of Homebrew? Some of these open source libraries build with things like CMake and Ninja and can be very difficult to control include paths. Not having Homebrew installed, I don't have to worry about that, but I would with Xcode cloud. We include homebrew by default on the VMs in Xcode Cloud. You can read more about how dependencies can be made available to a workflow here. That said, if you're concerned about a supply-side attack, or dependencies in your application being bad actors, you can conceivably audit the environment in a post-clone script after Xcode Cloud clones your source code into its environment. I haven't tried removing homebrew from the VM, but theoretically you could remove it or prevent it from phoning home. Is TestFlight for Mac limited to just apps destined for the Mac App Store or does it also work with notarised apps outside of the store? Apps must be uploaded for distribution to App Store Connect to become eligible for Mac TestFlight. I'm incredibly excited to use async/await and actors. What OSes will those be limited to? On Apple platforms, we currently support iOS 15, macOS Monterey, etc. This is tied to runtime integration with improvements to Dispatch in those OS releases for better performance. We are exploring the possibility of backward deployment. Concurrency works on Linux. Still a WIP on Windows. On performance. I have a 4500-lines enum for emojis in Unicode, so I can order them and move them around. In the functions I declared (such as one providing the group, or subgroup), I'm damned either way: if I don't put a default: fatalError() at then end of it, I have a compiler too slow error. If I put it, I have a Default will never be executed. Best of all, I cannot disable the warning in the package, so I'm left with 4 warnings at all times. Anything I can do? Sorry you have to deal with that. When the compiler analyzes a switch statement for exhaustiveness, it has some heuristics for very large enums that are intended to keep compiler performance good even in the face of complex patterns with very large sets of enum cases, but it can lead to false-negative cases when that goes wrong. If you have a moment to try to extract some of your code into a test case, and submit an issue to bugs.swift.org, we might be able to use it to further refine these heuristics. Thanks for letting us know about the issue. I'm wondering if this enum might be better-represented as a struct, perhaps conforming to RawRepresentable if you need an underlying value? Will Swift Concurrency be available on iOS 14? I saw the following text in the Known Issues of Xcode 13 Release Notes \u2014 \"Swift Concurrency requires a deployment target of macOS 12, iOS 15, tvOS 15, and watchOS 8 or newer. (70738378)\" This question was answered earlier, but I'll answer it again. Right now the supported deployment target is iOS 15, macOS Monterey, etc., or later. This is because of enhancements to Dispatch to support Swift concurrency, of which the concurrency runtime uses. We are actively exploring the possibility of backward deployment, but are mindful of not compromising the future of concurrency performance by doing so. How lightweight are Actors? What should be considered when declaring an Actor in regard of overhead or cost? Creating an instance of an actor type is no more costly than creating an instance of a class type. It's only during the access of an actor's protected state that a task suspension may happen, to switch to that actor's executor. The implementation of that switching is lightweight, and is optimized by both the compiler and runtime system. You can find out more about how this works in Thursday's session Swift concurrency: Behind the scenes : https://developer.apple.com/wwdc21/10254 How to run async function from sync code on main thread? Do I need to use detached or there is something else? Yeah, you can launch either of the forms of unstructured task from sync code. In Xcode beta 1, there is async { } , which will start a new async task that runs on the same actor as the launching context (which can be important if that's the MainActor, for main-thread-bound UI work), and asyncDetached { } , which will start a completely independent task. If you've been following Swift open source, though, you may know that these names have been revised due to feedback from the open source community; they're known as Task and Task.detached currently. Are we likely to see Swift Argument Parser reach 1.0 in the near future? It's a really great package and I'm keen to use it more widely. As an open source project, probably the best place to ask this question is on the \"Related Projects\" section of the Swift Forums where you'll connect with the main authors of that package: https://forums.swift.org/c/related-projects/argumentparser/60 How async/await works under the hood? Could you give us some links? The \"Swift concurrency: Behind the scenes\" session on Thursday will provide a lot of details about this. What are recommended sessions for leveraging async/await? The main session is \"Meet async/await in Swift\", but you may also find tomorrow's \"Swift concurrency: Update a sample app\" session to be very helpful, since we go through an existing app and use async/await , and more, so it can leverage Swift Concurrency. How will async/await and SwiftNIO work together? If I understood it correctly, async/await works very similar to SwiftNIO\u2019s EventLoopFutures. Support for async/await is in the main branch for SwiftNIO, and will be available officially after 5.5 is released. How long do you expect things to live in standard library previewing before being considered fully baked? SE-0270 has lived there for quite some time and wants its turn in the limelight Based on my conversation with the proposal authors, some issues have turned up with the design during the preview period that we need to address before it graduates to the standard library. This will likely require a follow up review. For the long enum question, I created a github. https://github.com/Misoservices/MisoEmojiUnicode Will do the bugs asap. Ty for the answer! Thanks for the follow-up! This link is very helpful. I took a quick look and I think making this a struct would probably simplify things and eliminate the problems you're having with large switch statements. I'd recommend you sign up for one of our Swift labs later this week so you can get one-on-one support from our team. Will there be (or are there already) some \u201eguidelines\u201c on when to use Publishers (combine) and when to use async/await? E.g. for a network request, URLSession currently provides support for both but what\u2019s better/recommended? For URLSession the async/await API is preferred for single values and the AsyncSequence API is preferred for reading chunk-by-chunk. The language level support for structured concurrency is a great way to integrate URLSession downloads into your app. Will there be tools to help visualize task hierarchies in the future? Or \"cancellation reason\"s for debugging? Thanks for the suggestion, I think it's a good idea! Really like what I'm seeing on DocC - I've heard its possible to output HTML pages from a generated docset - will there be some kind of simplified mechanism to apply styling to those pages? Thinking about how an organization might want to apply their own brand coloring and such to published docs on web. Today there isn\u2019t any integrated affordance for theming the generated site. This is certainly something we can see as valuable, though. Also a great topic to bring up when we make the project open source later this year. The \"Host and Automate\" presentation mentions: \"DocC comes with a built-in clean design\". Will it be possible to theme DocC builds? Ah, already a popular request. Today, there is no integrated support for theming the generated site, but would love for you to file a feedback request so we can track your interest. Is there a detailed spec for the .doccarchive output, and/or alternative output formats? You can get details about how you can host a .doccarchive on your website in the following session: https://developer.apple.com/wwdc21/10236. We\u2019ll have more details about the structure of the .doccarchive when we open source. You can post a followup question with details if you have something specific you\u2019re looking to do. A member of my team is building a tool to create unified output from several disparate docs generators that we use across different language-specific SDKs. (Currently Jazzy, Javadoc, JSDoc, etc.) We're trying to unify the reference manual experience for devs who use our docs without disrupting the workflows of our SDK teams, so we're trying to integrate with existing docs generators. I'd love to use DocC for the in-Xcode documentation, if I could also then feed DocC's output into our custom tool to unify reference docs on our docs site for our users. Having a spec for DocC's output, or being able to theme it, would go a long way toward enabling that unified experience. I'll keep my eye out for more details about the structure of the .doccarchive when you open source. Thanks for this - it looks awesome! Interesting details, thanks for that! Yes, keep an eye out for the open source details. As you explore using DocC, if there is something that would help or that you can't accomplish, please do file Feedback Assistant requests with details. Are there any plans for DocC to support Objective-C documentation builds? My org currently uses a docs generator to build both Swift and Objective-C doc sets for our SDKs, and we would likely have to continue using that if DocC can't build Objective-C docs sets. We\u2019ve definitely heard about the importance of Objective-C, and feel it ourselves. It is indeed a priority. Thank you for the feedback! What\u2019s the best way (via CLI/CI) to generate a documentation archive from a library that\u2019s just Package.swift , no wrapping Xcode project? xcodebuild docbuild works with a Package.swift file. You'll need to add a -scheme argument to tell xcodebuild which target to build; you can run xcodebuild -list to see the list. For example, you can run xcodebuild docbuild -scheme MyPackage to build a DocC Archive for the MyPackage scheme. The archive bundle will appear in your Derived Data directory (for example, ~/Library/Developer/Xcode/DerivedData/MyPackage-[hash]/Build/Products/Debug/MyPackage.doccarchive ). Is there built-in integration with Xcode Cloud workflows? Or would we need to implement it via a script? You\u2019re correct, the best way to support documentation builds on Xcode Cloud would be via a script that invokes xcodebuild docbuild . There\u2019s a session here that is a great reference for a use case like that: https://developer.apple.com/videos/play/wwdc2021/10236/ Does the hosted webapp have a way to download the archive so devs can add the docs to Xcode on their local machines? The primary use case we\u2019ve focused on is for other developers to import your package into their project as a dependency. That lets them use Build Documentation to build the docs locally and view the docs in Xcode\u2019s Documentation window. For other workflows, you can make a .doccarchive downloadable from your website. For example, you could zip the archive and have it as a downloadable resource on your site. If you have a different workflow in mind, post a followup and we can look at that specifically, or if you have other feedback we\u2019d love to get your specific use case in a Feedback Assistant. Are there any versioning capabilities in DocC? For example, being able to view the documentation for older versions of a package for users that can't or don't want to update? If a developer imports a specific version of your package into their project as a dependency and uses Build Documentation, they\u2019ll be able to read the documentation for that version in Xcode\u2019s documentation window. If you have a different workflow in mind, please post a followup and we can look at that specifically, or if you have other feedback we\u2019d love to get your specific use case in a Feedback Assistant. The \"Host and Automate DocC Documentation\" presentation mentions: \"The main page groups important symbols into topics related to higher-level tasks. Each of those group related symbols into more specific topics.\" Can you talk more about how those groupings happen? Is that something we dictate by the way we organize and present content in the framework, or can we manually specify groupings? Organizing your pages into groups is a great way to improve your documentation! To do that, you create a Topics section with a second-level heading called \"Topics\", followed by a group title in a third-level heading, and then a bulleted list of links. We describe the syntax in more detail here https://developer.apple.com/documentation/xcode/adding-structure-to-your-documentation-pages, under \"Arrange Top-Level Symbols Using Topic Groups\". We also have a session tomorrow that shows how to do this: https://developer.apple.com/videos/play/wwdc2021/10167/ along with some other ways you can improve your documentation. Usually a problem of documentation is that it becomes obsolete with time. Is there any way to prevent this by using docC? Is there any validation in the code that can be autocompleted? We think so too! It's important to keep documentation up to date with the changes you make in your code. Like a code compiler, DocC can emit warnings and errors when it finds something that doesn't quite look right. For example, DocC checks all of the links in your documentation you've written using double-backtick syntax, like MyType . If you remove or change the name of an API in your code, Xcode will display a warning on the line containing the documentation letting you know that you should update your docs. If you're interested in other ways to validate the documentation, we'd be interested to hear more via Feedback Assistant. Is there any way to know the documentation coverage of the public symbols of a framework? It would help a lot with adoption and finding blind spots on huge code bases. Hi Mauro! DocC does not currently expose documentation coverage but this is great feedback. Could you please file a Feedback Assistant request with details about how you'd want this to work? These types of enhancement requests are super helpful. Wonderful work. Thank you! Thinking ahead, is there any limitation that would inhibit the ability to extract docs for symbols other than public or open? Seems like it would be useful for internal use, and as a way to collect data on undocumented symbols. Thanks for the feedback! This is something that a lot of people have been interested in over the last few days :). We built the DocC integration in Xcode first and foremost to support public-facing docs. We know that this is important for teams working on a framework and will keep this in mind in the future\u2014thanks again for the question! Will the DocC compiler run only over source code dependencies or will it also generate the docs for the public interfaces of consumed xcframeworks? DocC integrates with the Swift compiler to extract information about the public symbols and their documentation during the build. The author of an xcframework can build and export that framework\u2019s documentation and distribute it alongside the xcframework to enable consumers to read the framework's documentation in Xcode. Do you have any tips or references on how to best distribute them together with the xcframework so that they are detected by Xcode? Is there any SPM tooling around this or is your answer more about a context of a manual import? Thank you very much! That's a great question and something that we want to explore more. For now, this can be accomplished if the framework author exports and uploads a separate zip file of the DocC Archive. Consumers of the xcframework can then download the documentation archive, and double-click it to import that documentation into Xcode. If you have more questions or want to follow up, you can request a DocC Lab appointment. When building docs locally, does the archive get stored in derived data? i.e. are devs going to have to rebuild docs every time they clear derived data to fix weird issues? When you use the Build Documentation menu in Xcode, the doccarchive does go into the derived data directory, so if you clear that, you would have to rebuild the docs. If you want to keep a version of the built docs in the Documentation window, you can export the doccarchive, and then you can double click it which imports it back into the Documentation window. When you import a .doccarchive , Xcode puts it into an \u201cImported Documentation\u201d section of the Documentation window, and that doesn't get removed when cleaning the derived data. In some of our internal modules we have some utility methods in extensions of types that are part of another consumed frameworks (for example: Foundation). We're working towards removing the majority of them in favor of other solutions that avoid extensions. In the meantime though, I haven't seen them in our generated docs and would like to know if there's a way to include them so that they are discoverable while we deprecate/move them. Thanks! DocC in Xcode 13 supports documentation for APIs defined in your module, but we know that documenting extension symbols is also very important. This is something that would also be a great topic for us to discuss when we open source later this year. I\u2019m loving DocC, thanks for all the hard work! Is there a way to omit some public symbols from the generated docs? For example, my UIView subclass overrides a series of methods like layoutSubviews that I don\u2019t want to be documented but they currently show in the documentation archive. I had a scan through the docs but couldn\u2019t seem to see a way to opt out If you prefix a public symbol with an underscore, DocC will automatically hide it from the built documentation. In your particular case, because you can't change the name of the method, that won't work. We're interested to hear more about this scenario and how it impacts your documentation via Feedback Assistant. I've checked the session in which you cover linking to types or methods. What I'd like to know is if we reference between documentations/symbols of 2 different frameworks. For example referencing in a class of package A a public type of package B in which A depends and uses as an input. We built DocC with a focus on having a really great documentation experience for individual frameworks and we don\u2019t currently support linking across different frameworks. We know this use case is especially important for developers who ship a collection of frameworks (maybe in a single repo) that work together. It\u2019s good to know that you\u2019re interested in this and it\u2019s (another) really great topic for discussion when we open source later this year. Is it possible to use DocC for local Swift packages? We're writing a lot of documentation for local modules (mostly for services and core API), and it would be really helpful to use DocC for them too. Yes, DocC works with both local and remote Swift packages. If you open the Swift package in Xcode, and use the Product > Build Documentation menu item, Xcode will build documentation for both that package and its dependencies (both local and remote). Note: if you use a documentation catalog in your Swift package, make sure that the Package manifest\u2019s swift-tools-version is set to 5.5 For more information about documenting a Swift package, see https://developer.apple.com/documentation/xcode/documenting-a-swift-framework-or-package I noticed that the website that gets generated in the .doccarchive doesn't have search functionality. Is there any plan to add this feature? This is a great question, and a really good feature request. You\u2019re correct- Today, search is supported when viewing documentation archives in Xcode\u2019s documentation window and it would be great to have search available on the web as well. This is something we\u2019ll definitely be interested in discussing further with the community when we open source DocC later this year. \ud83d\uddd3 Wednesday Simulator Recordings were added in Xcode 12.5. Is it possible to record audio with Simulator recordings? You cannot record audio with Simulator right now. If this is a feature you would like, please file a Feedback Request. xcrun simctl allows recording a video from the Simulator within the terminal. Can we progressively read out the recorded video while the recording is still running? E.g., start converting the video to a different video size? Hi! You cannot do this with simctl . We begin recording right when the command is executed and when you press Control-C, that ends the recording. Throughout this process data is written to the file. We do not support \u201cpiping\u201d the video or performing operations on it at the present time. If this is something you would like to have, please file a feature request using Feedback Assistant. Is it possible yet to run a native iOS app from Xcode on an M1 mac, instead of launching in a simulator? Yes! You can learn more about how to run these apps from the WWDC 2020 session here: https://developer.apple.com/videos/play/wwdc2020/10114/ Hi. Do you plan to release simulators for older iOS versions (13, 12) compatible with Apple Silicon? The legacy iOS and tvOS simulator runtimes are compatible with Apple Silicon Macs via Rosetta. Older watchOS simulator runtimes are i386 and are thus not compatible with Apple Silicon macs. I'm noticing when I try to store an item in the keychain on the iOS simulator with whenPasscodeSetThisDeviceOnly and biometryCurrentSet that it is failing. Is there a way to make this work on the simulators now? Is there somewhere I can learn more about changes here? I believe Keychain requires the Secure Enclave which is a hardware feature on iOS devices and is not present in Simulators, so I don't believe Keychain testing on Simulator will work, sorry. Physical hardware will be necessary for testing Keychain features. In watchOS, when i try to activate the session it fails all the time, but succeeds in second attempt. (on device) Thanks for the question! I think we'll need some more information from you to help triage this further. Could you please submit a bug report via http://feedbackassistant.apple.com so that we can investigate this. Also, please run xcrun simctl diagnose and attach the output to the feedback issue you report. Thank you! Is it possible to use simctl to connect & disconnect the hardware keyboard? It is not possible to connect/disconnect the hardware keyboard using simctl currently. If you could file a Feedback request with details about your use case that would be really helpful. Similar question to video, Is it possible to record video with Simulator recordings? Hi there! If you are referring to recording audio, we do not support that at the present time. Please use Feedback Assistant to file this request with us if this is important to you. I mean to record a video? It is possible to record video both from the simctl command line tool and the Simulator app. Both features work the same way, but the simctl command line tool gives you a few more options. Is there a technical reason why Xcode may at times not be able to launch an app properly in the simulator? Quitting Xcode and the simulator and then trying again will resolve the issue. This has been a problem for many years. Thanks for the question! I think we'll need some more information from you to help triage this further. Could you please submit a bug report via http://feedbackassistant.apple.com so that we can investigate this. It'd be really helpful to get some diagnostics, captured after you've reproduced the issue: A sysdiagnose bundle from your host Mac (run sudo sysdiagnose and attach the resulting archive) A diagnostics bundle from the simulator. Please run xcrun simctl diagnose With the new announcement of WebAuthn and security key improvements in ASAuthorization, will the Simulator with Xcode 13 support hardware security keys (so developers won't need to build and debug their app to on physical devices)? Hardware security keys are not supported in the Simulator on iOS 15. It is important to test features on-device because they take advantage of functionality that's built into iPhone, but which is not always possible to simulate. If this is important for your workflow, I recommend you let us know via the Feedback Assistant at https://feedbackassistant.apple.com. Is there a way to test apps using GroupActivities framework on Simulators? What would be the easiest approach to test such apps? The Group Activities APIs (for SharePlay) requires FaceTime. Unfortunately, there is no FaceTime support in the simulator, so you will need to test this aspect of your app on device. Enterprise apps from multiple companies don't run in iOS 15 (but are fine on 14) and claim they need to be updated - can you shed some light on what's actually out of date here and what needs to be changed? -[IXSErrorPresenter presentErrorForBundleIDs:code:underlyingError:errorSource:]: Asked to present alert for error 17 source MobileInstallation underlying error Error Domain=MIInstallerErrorDomain Code=13 \"Failed to verify code signature of /var/installd/Library/Caches/com.apple.mobile.installd.staging/temp.0Ti39V/extracted/Payload/App.app : 0xe8008029 (The code signature version is no longer supported.)\" These applications use an older code signature type that is no longer supported. Apps from the App Store have been automatically re-signed, however enterprise applications cannot be automatically signed by Apple. Please contact the developers for these apps; they will need to re-sign their apps with an up-to-date certificate and profile. For more information, I recommend you visit the Security lab on Friday. What is the currently fastest recommend setup to deploy a debug build onto a watch device? Especially with regards to the file transfer time. iOS and watchOS device connected via BLE and both in the same 5 GHz hotspot? Mac (Xcode device) in the same wifi? Other influences like power connected? At present, the most reliable way to test on watch is to ensure you have a wired connection to your phone and to ensure that your phone and watch are in close proximity to eachother on the same wireless network. It is also best to limit interference from other wireless devices and ensure that you have a wireless access point close enough to maintain a constant strong signal with both devices. I have a set of test simulators that I frequently use. As part of their initial setup I will often install a network profile, and for another project I require to login to iCloud for CloudKit testing. I notice between Xcode upgrades these changes are always wiped. Is there a way to keep them in place between iOS/Xcode upgrades, or is their a quick way to script provisioning them so I don't have to login to iCloud or install a profile for each simulator? Thanks for the great question! I think this might be an excellent opportunity to request a lab appointment with one of our Simulator engineers. It would be good to have time to talk through what your use case is, what you're seeing go wrong and when, and what options we can come up with together to make this work better for you. Please visit https://developer.apple.com/wwdc21/labs , search for Simulator in the \"Search WWDC21 labs...\" field, and request an appointment. Thank you!! Why is the Xcode wireless connection to a device so unreliable? I basically have no idea when I will be able to use wireless debugging. We hear your concerns about the reliability of wireless debugging, especially around needing to maintain a persistent connection to the device for a debug session. While we don't have significant improvements in this area with Xcode 13, it is an issue that we are actively investigating as we look to improve the wireless development experience. I noticed that compiling to a simulator in M1 still uses x86_64 arch even in Xcode 13, is there any reason for that? why the simulators arent arm64? Or maybe I am missing some config, I tried with latest iOS 15 deployment target Hi, the architectures that get built depend on the simulator runtime that you are targeting. Which runtime are you targeting? Do you see the same behavior when targeting a newer simulator runtime? I am targeting iOS 15 in this case, xcrun simctl list gives me: == Runtimes == iOS 15.0 (15.0 - 19A5261u) - com.apple.CoreSimulator.SimRuntime.iOS-15-0 And this is the flag that the swift compiler is using: -sdk /Users/omarzunigalagunas/Downloads/Xcode-beta.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator15.0.sdk I tested with an empty project and it compiles to arm64 but in our project it doesn't Can you check whether your project's configuration is forcing x86 output by overriding the ARCHS or EXCLUDED_ARCHS build settings? Is there a way to hide the top bar on a simulator (that says \"iPad 8th generation etc\") so you only get the content, kind of like the QuickTime Player is \"all window\"? You can change the appearance of the Simulator window by opening the Window menu and selecting Show Device Bezels. Thank you, but what I'd like is no bezel and no top bar. Is that possible? There is no configuration option like that right now. If you're trying to record or screenshot content, you can crop your recording area in the Screenshot app before capturing the content. If that's not sufficient for your use case, I'd encourage you to let us know via the Feedback Assistant at https://feedbackassistant.apple.com. Is it possible to write to the app's Documents directory on the Simulator, outside of the app? For example if we want to set up a specific state, by writing data on the documents folder, before running UI Tests. You can get the path to an application\u2019s data container for a given device with simctl: xcrun simctl get_app_container <device-id> <app-bundle-id> data The data container will contain the Documents directory. If you only have one simulator booted, you can use the device alias \"booted\" instead of specifying the device's ID. Are there any Xcode 12.x versions that support device debugging when a device is running iOS 14.6? Hi Jared! Xcode 12.5 supports devices running iOS 14.6. For App Store screenshots, I would like to use the simulator tool to take a picture with the device bezels embedded with the image. But when I do this with 12.5, I only get an screenshot with a 'cutout' of the device shape. How should I enable it to capture both the screen and the device bezel together? Currently we do not support this. The screenshot tool in Simulator can create images which are suitable for a variety of media. If you are trying to capture screenshots of your app for marketing materials, I suggest that you speak to the fine people over at the App Store for advice on how to take screenshots of your app for the store. Is there any way to view the XML file of Info.plist in Xcode 13? Because it's no longer a separate file, and inside the project target instead. Hi Andrew! You should still be able to view your Info.plist file by opening it in the project navigator on the left-hand side of the Xcode window. If you want to view the Info.plist file as it appears after building your app, select your .app bundle, Control-click or right-click it, and select Show Package Contents. The Info.plist file should be in there. If this is a traditional macOS app, it may be under the Contents folder. Thanks! But when you create a new project in Xcode 13, for example in the SwiftUI App template, there's no more Info.plist in the project navigator. This is an intentional design change in Xcode 13. I know the Xcode team would love to hear what you think. Please let them know by using the Feedback Assistant at https://feedbackassistant.apple.com. Is there a recommended way to control simulator setup when running UI tests? For example, for ui testing I need to disable the hardware keyboard but for regular development, it\u2019s not practical to do that. It\u2019s a bit disruptive having to enable/disable it each time so I\u2019m wondering if i can control this kind of thing somehow? Thanks for the great question! We have a helpful command line tool for Simulators that you can run via xcrun simctl that can do a lot of useful automation and setup. Here's a video from a previous WWDC session for more information: https://developer.apple.com/videos/play/wwdc2019/418 . If you find functionality missing from simctl, please do submit feedback through Feedback Assistant!! Also, one thing that you might try if you're doing this often... You can run all of your setup steps on a Simulator device, shut it down, then make a clone of that device with xcrun simctl clone , and then use that cloned device for testing. This might also be a good question to discuss with one of our Simulator engineers in our labs, so if that sounds like it would be helpful to you, please visit https://developer.apple.com/wwdc21/labs , search for Simulator in the \"Search WWDC21 labs...\" field, and request an appointment. Thank you!! I'll try again - sorry for not being clear: In Xcode 13, will we be able to launch our iPhone/iOS apps natively on our M1 Macs, rather than selecting a simulator? I do not believe this is currently possible. I am asking if Xcode 13 will support it. Thank you! iOS and iPadOS apps can run natively on macOS 11.0 and later running on Apple Silicon. To run such an app from within Xcode 12.0 or later, select the scheme for your iOS or iPadOS app and choose the 'My Mac (Designed for iPad)' run destination. This run destination is suitable for all iOS and iPadOS apps. Is it possible to run the iOS 14/15 simulators under Rosetta while running Xcode natively on the M1 Mac? A Hi Michael! I'm sorry, but we don't support this configuration. If you need this for your workflow, please let us know via the Feedback Assistant at https://feedbackassistant.apple.com. Any recommendations on using simulator or writing tests for AR apps? I couldn't find a way to effectively test my app using simulators. We have both 2D UI written in SwiftUI and UIKit, as well as 3D UI in Scenekit. Would be nice to be able to still iterate on the 2D UI using simulators! thanks :) AR requires the use of specialized camera hardware that is not present in the simulator, so AR cannot be simulated. You will need to test your AR-based UI on-device. When we changed our CI to GitHub Actions we started having issues with some simulators while running tests. Apparently sometimes (it seemed at random) when launching the simulator this would try to do some kind of \"synchronisation\", and it would stay idle for a few minutes, this, before running tests. Is there a way to avoid this 'synchronisation' to happen? PS: apologies in advance if you think this is related to the CI provider and not the simulator itself. Hi there and thanks for the question! I think we'll need some more information from you to help triage this further. Could you please submit a bug report via http://feedbackassistant.apple.com so that we can investigate this. Please run xcrun simctl diagnose and attach the output to the feedback issue you report. Also, I think this might be an excellent opportunity to request a lab appointment with one of our Simulator engineers. It would be good to have time to go through what you're seeing go wrong and when, and what options we can come up with together to make this work better for you. Please visit https://developer.apple.com/wwdc21/labs , search for Simulator in the \"Search WWDC21 labs...\" field, and request an appointment. It would be really helpful if you mention the feedback number that you filed in the notes for the lab request. Thank you!! I believe issues like this occur in CI systems where \"fresh\" hosts are instantiated for each job. I've observed performance issues due to update_dyld_sim_shared_cache running after booting a simulator for the first time \u2014 which on CI tends to coincide when attempting to run a test suite. Thanks for the follow-up! I think it might still be helpful to request a lab appointment with one of our Simulator engineers to talk through the issues here. It\u2019s difficult to recommend a good solution without having a better understanding of all the issues. :) If the bottleneck you\u2019re seeing is resource constraints because the dyld shared cache is being built at the same time as your tests are running, you could try adding to your CI scripting to build the dyld shared cache first, allow that to complete, and then run your UI tests after. You can do this by running xcrun simctl runtime update_dyld_shared_cache . Please note that this command is not currently documented externally and is subject to change. If this command is helpful to you, please file a Feedback Report and ask for this functionality to be made more broadly available. If it would be helpful to have a flag for xcrun simctl boot which changed the behavior to wait until the shared cache is generated before booting the device, please file a Feedback Report for that as well. Again, it\u2019s hard to make a good recommendation without a full understanding of the problem you\u2019re trying to solve, and the best way to do that would be to request a lab appointment as mentioned above. Hope this helps!! The Simulator menu offers an entry to set a Custom Location (Features > Location > Custom Location\u2026) Is there an equivalent for this in simctl ? Unfortunately, there is not a way to do this via simctl at this time. Please file a feedback assistant report via http://bugreport.apple.com as this would be a great addition to make in the future. I remember from some past WWDC session that the Simulator uses macOS's version of Core Animation, not the same CA stack that runs on the device. Is this still true, and if so, are there any documented differences we should be aware of? All recent versions (definitely iOS 5+ but possibly back to iOS 4) of the iOS simulator runtime have provided a version of CoreAnimation built from the same sources as the corresponding version for devices. Prior to the introduction of Metal support in the simulator, CoreAnimation went down a software rendering codepath. Since the introduction of Metal in the Simulator, CoreAnimation now uses the Metal rendering codepath for improved performance. Any difference in behavior would be considered a bug, so if you notice anything unexpected, please file a report via Feedback Assistant at http://bugreport.apple.com I started playing with an M1 recently, I haven't try it but is it possible to compile the app for a device (arm64) and use that build to run it in a simulator (arm64)? This could give us some advantages as being able to distribute the app to devs iphones and at the same time run the tests in a simulator by building only once Thanks for the question! The answer is no; this isn\u2019t possible. Even though in your case both the device and the simulator have the same architecture, they are different platforms because Simulator runs in macOS. As a result the builds are not mutually compatible. Now that we have Apple Silicon, are the iOS binaries running on the Simulator (e.g. launchd, Springboard) the exact same binaries as on the device? No, applications built for the Simulator are built against the Simulator SDK, which does not support exactly the same set of features and functionality as our devices support. So apps built for Simulator can't run on a device, nor the other way around. I often get this error \"Could not find module 'RealmSwift' for target 'x86_64-apple-ios-simulator'; found: arm64, arm64-apple-ios-simulator\" whenever I use Cocoapods. I'm on an M1 mac. I've tried solutions like this one: (https://stackoverflow.com/a/58192079/14351818), but they sometimes work and sometimes don't. I've added arm64 under Excluded Architectures too, and added a script to the Podfile from here (https://stackoverflow.com/a/63955114/14351818). Any tips? It is likely the pods you're trying to use are not configured to work with the simulator you want to use, or the CocoaPods technology itself is not simulator-aware. I would recommend reaching out to the developers of the specific pods you want to use. Do we have any simulators for macOS testing? (to cover scenarios like touchID or clamshell for automation) No, the Simulator does not support simulating macOS or Mac hardware features. You may find that virtualization software from third parties can help you with your testing. So Xcode cloud also will use actual devices where we would run our CI? Xcode Cloud uses the same macOS build worker to run your tests as the build worker that builds your code Is Xcode Cloud built on the \"configuration as code\" principle? Meaning: Will we be able to have different configurations in different branches, akin to GitHub actions or Azure DevOps YAML-based approaches? Hi there - All of your workflow configuration is stored on the server and not checked into your repository. That being said, you can create workflows that will only start builds for specific branches or branch patterns and use specific settings (macOS & Xcode version, environment variables, etc.). What Xcode cloud uses, actual devices/simulator/virtual machines? Great question! Xcode Cloud runs your iOS, tvOS, and watchOS tests on simulators, and your macOS tests on the same build worker used to build your code. Hi, nice presentations all! Can you confirm that Xcode Cloud will ONLY support Bitbucket, GitHub, and GitLab, both in cloud and on prem, but nothing else? Specifically, I have a bare git repo on my local network that I would like to use as my source code repo for Xcode Cloud. Is this possible? Yes - Xcode Cloud only supports cloud and on premise instances of GitHub, Bitbucket, and GitLab. It doesn\u2019t support bare git repos on local networks. Hello! In our current CI we check what changes the pr is doing and analyze our dependecy tree to find out which test bundles should run, then we use -only-testing args in xcodebuild cmd to filter them, is it possible to do so with Xcode Cloud? Hi! You can probably get something like that working by using a combination of a post-clone script, which does the analysis of the changes in the git commit, and then generate a scheme/test plan that dynamically has only the tests you want to run Hey! Yesterday on a previous question I asked about API availability, Richie mentioned that we\u2019ll have a REST API during this summer :partying_face: I have a follow up question about this though, will we be able to trigger a certain workflow from an API call while this workflow doesn\u2019t run on any git event (pr, tag change, commit, scheduled event, etc.). Basically, we want to control part of our release process from our custom Jenkins orchestration (that we have today) and trigger some of the workflow tasks on demand from there. Thanks! While we can't commit to the shape of the API right now, I think it will be a pretty common use case and a valid expectation to have w.r.t Xcode Cloud's public API. Can Xcode Cloud provide any support for Developer ID workflows? Either complete products or re-signable ones? Hi! Xcode Cloud will code sign macOS builds with Developer ID, and for the Mac App Store When can we expect invites to Xcode Cloud after submitting a request? Thanks for the question Edward! We\u2019re all excited about Xcode Cloud and we want to make using it the best possible experience. We'll be gradually adding users throughout the summer and fall. Account holders will be notified by email when your team is accepted. Beyond the obvious fact that Apple is handling the server setup and maintenance, are there any features of Xcode Cloud not available on Xcode Server? Hi! Xcode Cloud is a totally different product from Xcode Server, with many differences and improvements over Xcode Server. I'd recommend looking at the documentation if you're interested in specific features. Does Xcode cloud also support dependencies from private repositories? I.e. the project having private Swift framework dependencies that are hosted on GitHub privately or even on a local GitHub server. Definitely! This is covered by today's session \"Customize your advanced Xcode Cloud workflows\": https://developer.apple.com/videos/play/wwdc2021/10269/ In our current CI we need to use internal metrics endpoints behind a vpn so our secops team exposed en external endpoint and ip whitelisted it so we can access it, is Xcode Cloud infra will have a fixed public ip range so we can do the same? Hello! Yes, there will be a fixed IP range for all egress from Xcode Cloud. Over the summer details will be published in the documentation. If your git repo is in GitLab self hosted, because you want the source code to be secure and not hosted in the cloud, what happens to the source code during the Xcode Cloud CI/CD a process - is it transmitted to your servers encrypted? How long is the source kept on your servers after compilation and deployment? Xcode Cloud will securely clone your repo from your GitLab instance. Your source code is kept on the build worker for the duration of the build, then the build worker (and your source code) is immediately discarded Hi! I love what I have seen so far. Thanks for the hard work. In our company we rely heavily on Artifatory for our mobile builds. Some of the interactions we have require authentication via .netrc file configuration. Is this something that we will be able to achieve in Xcode Cloud ? Thank you! Glad you like it! I'd recommend looking into the custom scripting and environment variables documentation (and the Advanced Skywagon session). You could generate a .netrc file in a script, populate an access token from a secure environment variable. We have precompiled binaries (XCFrameworks) that swift package manager downloads from an private server. The package manager authenticates using a .netrc file. Would I be able to securely provide a .netrc file with the proper credentials to Xcode Cloud so it can fetch these packages? Xcode Cloud will no generate .netrc files for you, but you can create your own as part of a custom build script. For example, you could create the file in a ci_postclone.sh file and use a secret environment variable to store the token you want to use. Both custom scripts and (secret) environment variables are covered in today's session: https://developer.apple.com/videos/play/wwdc2021/10269/ Will Xcode cloud require that we push the app including the framework dependencies added by swift package manager or cocoapods? Hi! Xcode Cloud requires the source of you app, and any non-binary dependencies to be available in the build environment. Xcode cloud is great for CI, would be eventually be able to avoid building locally and get the built products from the build servers? Xcode Cloud is built specifically with CI in mind. Having said that, you can download all artifacts produced from each build. We are currently building in our CI using Macstadium + ORKA, which adds a virtualisation layer. We can deploy VMs with different CPU configurations depending how time critical the task we need to perform is (e.g. PRs have faster machines for better developer feedback, release builds work in slower VMs because we are not in a rush). Will something similar be possible with Xcode Cloud? Great question! Xcode Cloud was designed so that developers no longer have to maintain a CI build environment configuration Is it possible to configure a workflow to skip certain commits (e.g. we use [NO-CI] prefix in the commit message to have our Jenkins ignore a it) That's not something that's supported right now but that's a great idea! Can you please file a feedback about this? We will able to select the stack? For example M1 over Intel computers, we noticed M1 has a huge impact in compilation time Xcode Cloud was designed so that developers no longer have to maintain a CI build environment configuration. In our CI we do commits (e.g. to bump the version and build number of the app) and push it to the repo. Will we be able to commit and push from Xcode Cloud from the scripts? Xcode Cloud will actually manage the build number (CFBundleVersion) automatically for you - so likely you won't need to do those kinds of pushes. However if you do need to, you can add a custom script that calls various git commands - which will then trigger another workflow I have a follow up question for this one. Will Xcode Cloud provide an option to somehow format the build number? That's not something that's supported right now but that's a great idea! Can you please file a feedback about this? Does Xcode Cloud run tests on devices or simulators? Thanks for your question. Xcode Cloud runs tests on simulators. Can we set baseline metrics for performance tests from Xcode cloud? You\u2019re not able to set baseline metrics for performance test, but please file a request through feedback assistant! Would Xcode Cloud support beta versions of Xcode ? Totally! You can configure a workflow to use a specific version of Xcode (including betas), or always use the latest version available (including betas). Does Xcode Cloud work for Enterprise Accounts? To use Xcode Cloud, you need to be enrolled in the Apple Developer Program. https://developer.apple.com/documentation/xcode/requirements-for-using-xcode-cloud Would Xcode cloud support build systems like bazel? Xcode Cloud supports custom build scripts, so you can run bazel commands if you need to build additional tools as part of your build process. Currently we create alpha builds for QA on a regularly basis using an Enterprise certificate. We distribute the alpha builds using Bitrise. On every alpha release we notify QA via a slack channel and they get a short description of the release, a download url as text and QR code (to ease installation from device). Would I be able to have the same flow using Xcode Cloud? And would I be able to distribute enterprise builds via TestFlight? Hi! You can definitely achieve this workflow with Xcode Cloud - in fact we expect it would be significantly easier. You can set up a workflow that deploys to an internal TestFlight group representing your QA team (and also notifies in slack). They can then install the build through the TestFlight app. You don't need to worry about Enterprise signing to do this Will app extensions be supported in any capacity when building apps on iPad? Swift Playgrounds 4 supports making apps for iPhone and iPad, in addition to the playgrounds and playground books already supported by Swift Playgrounds. Thanks! Specifically, I'm curious about whether any app extensions, action or intent extensions for example, might be available to include in the initial version of the new project type, or if it will be limited to just \u2018standard\u2019 apps without these kind of features for now? :slightly_smiling_face: Those are some awesome ideas! We don't have any specific plans to support that this year but we encourage you to file a Feedback to state your support for those features. When building on iPad, will it be possible to install apps to the iPad homescreen locally? Or might this require a baseline of submitting through Testflight? Great question! If you want to install the app you're making in Swift Playgrounds to your device, you can sign into your developer account and submit the app to TestFlight and then install it on your iPad or other devices using the TestFlight app. Is Swift Playgrounds expected to only support SwiftUI, or will it be open to other frameworks like UIKit and 3rd party frameworks? Swift Playgrounds supports both UIKit and SwiftUI. SwiftUI Previews requires a SwiftUI app, but you can always use UIViewRepresentable to show UIKit content in SwiftUI. Thank you! To follow up on this, what is the planned support for 3rd party frameworks? Will it be possible to use say, Snapkit? Playgrounds supports working with Swift Packages. SwiftUI Previews (which is in Swift Playground 4) allows you to preview SwiftUI content. To use something like SnapKit you'd have to use it with UIKit and use UIViewRepresentable I'm very excited for the upcoming features to enable building apps on iPad, but details in the main Keynote were very light. Is there anywhere I can learn more, and are you able to answer questions on it? At this moment, the available details are available at the State of The Union video https://developer.apple.com/wwdc21/102 at 0:33 I\u2019m sure you\u2019re getting this question a lot - any word on when a Swift Playgrounds 4 beta will be available? :smile: So excited to try it out! Swift Playgrounds 4 will be available later this year. Is it possible to import SPM packages into Swift Playgrounds? That would be really helpful in using Playgrounds as a tool to build more richly featured apps without having to traverse over to Xcode. Yes! The apps you build can depend on publicly-available Swift Packages. Hmm.. so I won't be able to use my internal (non-public) Swift packages when building on iPad? At this time, we do not plan to support packages that require authentication Will iPhone and iPad be the only devices supported in the next version of Swift Playgrounds when building Apps, or will developers be able to build for watchOS and macOS as well? Apps built using Swift Playgrounds are universal, so when they\u2019re distributed using TestFlight or the App Store, they\u2019ll run on iPhone, iPad, and Apple Silicon Macs. To build and deploy to the AppStore from iPadOS using Swift Playgrounds, do you need to be a subscriber of Xcode Cloud? Nope! Anyone with a developer account will be able to submit their apps to TestFlight and the App Store. Hi! Will Swift Playgrounds 4 for iPad have the ability to add dependencies on third party Swift Packages? Yes! The apps you build can depend on publicly-available Swift Packages. What does the workflow look like if you wanted to work from Mac to iPad or vice versa? Is it all iCloud synced or will it connect to Github/Gitlab/other Git syncing services? Swift Playgrounds already works great with iCloud Drive (or any other file provider extension) and will continue to do so. We don't have anything to announce at this time about other mechanisms. Will I be able to see what my app looks like on different device sizes and orientations from within Swift Playgrounds? Yes! You can take your app preview fullscreen, and put Swift Playgrounds into split-view multitasking to test a bunch of different configurations. You can also write custom Preview Providers to try even more layouts and configurations. Will folks be able to open existing Xcode projects in Swift Playgrounds 4, or will a project have to be migrated to some sort of .playgroundbook ? Swift Playgrounds 4 has a new project format based on the Swift Package format. Can you edit this format in Xcode without conversion? So you can start working on a app in Xcode and then continue on iPad and vice versa? Yes! App projects can be moved seamlessly between Swift Playgrounds and Xcode, so you can work on whichever device you prefer. It was mentioned I think that Playgrounds apps destined for the app store could be brought over to Xcode, can it go round trip back to Playgrounds? Especially wondering if you could use Xcode to define storyboards or data models to use, and bring those into Playgrounds. Thanks for the great question! You will be able to open Swift Playgrounds projects in Xcode since they're based on the Swift package format! Note that Xcode has more tools than Playgrounds will be able to support. If you end up editing the Playgrounds project in Xcode in such a way that won't be supported by Playgrounds, you'll see a warning in Xcode. Any plans for further playground books like Cipher? The existing content is brilliant, as were the sessions on Swan's Quest last year. The additions like Sensor Arcade and the accompanying templates are inspirational, especially in the classroom... We\u2019re so glad you like the content! We haven\u2019t announced anything about that. Stay tuned for future updates! Will Swift Playgrounds have any kind of simulator, or are SwiftUI projects running as previews? Howdy! Swift Playgrounds projects will allow you to make iOS apps with the SwiftUI lifecycle \u2014 you'll be able to see your app running in Playgrounds, and you'll be able to create preview providers to preview specific views. What frameworks are unavailable in Swift Playgrounds created apps? Howdy! While we can't go into details about what frameworks Swift Playgrounds will or will not support \u2014 I'm curious about what you're hoping to achieve! Feel free to respond in a followup or please send us feedback for your hopes, dreams, and expectations! Is the MacOS version of the Playgrounds app actively being maintained? Should we expect it to be able to build apps as well? For this year, we focused on bringing support for building apps to iPad. Does Swift Playgrounds 4 support multi-window/scenes? i.e. Can I have my editor as one full screen scene and use slide over for my preview? Additionally, does it have support for a secondary display (AirPlay/USB) to go along with multiple scenes/windows? We're excited to hear about the ways you want to use Swift Playgrounds! We'd love to have folks send us their desires and expectations via feedback! :) What are the requirements for Swift Playgrounds? Can you build apps on a 2018 (A12) iPad Pro for example? You'll be able to build apps in Swift Playgrounds on any iPad running iPadOS 15! Will it he possible for a xcode project to be opened in swift playgrounds? And for a swift playground in xcode? Projects in Swift Playgrounds 4 are built on top of the Swift Package format, and they work great in Xcode. If you make a change to your project in Xcode, and Xcode knows that it won\u2019t be compatible with Swift Playgrounds, it will show a warning. Are there any common app capabilities that I could create using SwiftUI & Swift on Xcode that I would not be able to create in Swift Playgrounds? Or Apple Frameworks that I could not use in Swift Playgrounds? Swift Playgrounds 4 will have wide support for Apple\u2019s SDKs. We believe some truly powerful apps will be built with Swift Playgrounds 4, and it will be easy to bring your projects into Xcode when you need more power. The State of the Union discussed moving an app project from Swift Playgrounds 4 on iPad over to Xcode on the Mac. Will this be possible in the reverse? Can I edit the project in Xcode and then bring it back to Swift Playgrounds? Hi Greg. Yes the new project format will allow for editing the project in Xcode and bringing it back into Swift Playgrounds! Can we release to the app store from playgrounds today? To adhoc distribution? Do we need to wait until something comes out of beta before this flow works? Thank you. Building apps, including submitting them to the App Store, is new in Swift Playgrounds 4. Swift Playgrounds 4 will be available later this year. What are some neat benefits of writing and running full iOS/iPadOS apps on iPad that may not be immediately obvious without having experienced using the app? This is an incredible question! Making iPadOS apps right on the iPad is an incredible experience. It's easy to get a feel for how your app will behave as a part of the amazing iPadOS system experience like when in fullscreen, a multitasking split size, and in slide over. You can test out how your app responds to a software keyboard present and how it changes when you attach a hardware keyboard. You can check how your UI responds to hover effects with the trackpad, and you can test out great gestures right in the canvas with your finger(s)! We're so excited to see what y'all come up with when working in Swift Playgrounds 4. If any of these things spark inspiration or excitement about Playgrounds, we'd love to hear your desires and expectations via feedback! Might external display support be considered for preview rendering at various device sizes, to reserve more screen space for code? That's an awesome idea! Please file an enhancement request through Feedback Assistant: https://developer.apple.com/bug-reporting/ Might the upcoming Playgrounds release include support for find & replace, or refactoring? Renaming variables, or indeed any declaration, can be quite painful without it. Or if you can't comment - are there any good workarounds for this currently? These are some awesome feature ideas! While we can't comment on plans for the future, we highly encourage you to submit an enhancement request through the Feedback Assistant: https://developer.apple.com/bug-reporting/ Will Swift Playgrounds support all the latest new technologies Swift 5.5 is introducing ? (Async/Away, Actors...etc) Yes! Swift Playgrounds 4 will ship with the Swift 5.5 compiler, including support for structured concurrency. Does Swift Playgrounds support using packages with Swift Package Manager? For example, using a networking library when building a SwiftUI app on the iPad. Yes! The apps you build can depend on any publicly-available Swift Package. You are specifically saying \u201cpublicly available\u201d. So I assume there will be no support for signing into a GitHub account for example, or to provide SSH credentials for a non-public repository at the beginning, right? That's correct\u2014at this time, we do not plan to support packages that require authentication. What kind of apps can be created and submitted to the App Store? iPad and iPhone only? Or macOS apps as well, even if only through Catalyst? Apps built using Swift Playgrounds are built universal against the iOS SDK, so when they\u2019re distributed using TestFlight or the App Store, they\u2019ll run on iPhone, iPad, and Apple Silicon Macs. \ud83d\uddd3 Thursday What are best practices for including and managing a lot of Swift Package dependencies in a project? Adding them to the project file doesn't scale well after more than a handful. I believe this WWDC talk is a great resource for Swift Package Management Best Practices: https://developer.apple.com/videos/play/wwdc2019/408/ So excited about the Vim keybindings (I have waited more than a decade for it!). Does the team have a plan for supporting a richer set (repeat command (.), replace command \u00ae, macros, \u2026) Good morning, and thanks for taking the time to ask this question! If there are suggestions or features your would like please submit them using the feedback assistant on https://developer.apple.com/bug-reporting/ I am seeing that sometimes Xcode 13 cant opt+Click on symbols can't find quick help. It works on some frameworks methods, but not on class properties and methods, and frameworks from cocoapods most of the time. The same click worked in Xcode 12. I can't attach a screenshot to the question haha. That sounds like an issue we'd like to investigate further. If you can reproduce it, would you mind filing a bug report using feedback assistant? If you can run Xcode with some logging enabled, it would help us understand it better. Try running Xcode from Terminal like this, env SOURCEKIT_LOGGING=3 /path/to/Xcode.app/Contents/MacOS/Xcode , and then reproduce the problem by opt+clicking the symbol. If you can also provide a screen recording demonstrating the problem, it would help. Are you taking xcodebuild questions? :) If I try to create an xcframework ( xcodebuild -create-xcframework ... ) that contains both Intel and ARM simulator frameworks I get the error Both ios-arm64-simulator and ios-x86_64-simulator represent two equivalent library definitions. Has this been fixed in Xcode 13? It sounds like you have two separate thin (single-architecture) binaries, one for each arch, but both for the simulator. XCFrameworks don't support that; there needs to be only a single multi-arch binary. You can use the lipo tool (which is what Xcode uses) to combine your binaries if you're not using Xcode to build the product already. Does the new Xcode PR review feature supports a strategy with forking? So far I can only create a PR that points my fork's main branch, not the original repo of my fork. Xcode\u2019s pull request feature doesn\u2019t work with forked repositories. It doesn't support submitting pull requests to an upstream repository. When you create a new project using the iOS App template, and select SwiftUI for the interface, there is no more Info.plist file. Instead, I can find it inside Project -> Targets -> Info. However, how can I access the file's XML source code? You should still be able to find the Info.plist file in the project directory under: <Project Root Directory>/<Project Name>/Info.plist . From there, it can be edited with your favorite editor, including Xcode. It can also be opened quickly in Xcode by using Cmd + Shift + O and searching for Info.plist. Our repo doesn't commit in the .xcodeproj but generates it locally on the fly. Does Xcode Cloud support a case when a project is generated or modified in the post-clone script? Yes. Bear in mind that creating your first workflow with Xcode Cloud happens from Xcode. It analyzes your project locally, suggests a configuration to quickly create you first workflow. In the scenario where you do not check in your Xcode project file, then these are the adjusted setup steps: You need to generate the Xcode project file locally. To exclude it from your repo, you could add a .gitignore rule. Add the add Xcode project generation to post-clone script. Create first workflow from Xcode. Create first workflow from Xcode: https://developer.apple.com/documentation/xcode/configuring-your-first-xcode-cloud-workflow Post clone script: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts Is there a way to quickly mark a Swift (SPM) Package for editing in Xcode 13? (Similar to how you can swift package edit at the command-line), or is it still required that I separately clone the package, and drag it into my project? The workflow for this remains unchanged from previous versions of Xcode. How did the team prioritize which new features to build vs which features to work on later? One of the many considerations we take into account would be what we receive from Developers via the Feedback application. Please send us your suggestions \u2014you can send those with the Feedback Assistant. https://developer.apple.com/bug-reporting/ Creating a new project with Xcode 13 doesn't show up the Products directory in the Project Navigator, is it possible to add it back per project or globally? The Products directory will no longer be shown by default in newly created projects, as you noticed. You can still get to the directory with your build products by using the new Product -> Show Build Folder in Finder menu item. If there is a reason why you would want to always show the products directory in the project navigator, we'd love for you to file a Feedback via Feedback Assistant to explain your use case. https://developer.apple.com/bug-reporting/ What powers Xcode's new Swift syntax highlighting? It looks very similar to the semantic highlighting recently merged into sourcekit-lsp! If sourcekit-lsp is being used, will future improvements be available in Xcode too, and will Xcode's features (like live issues) be available in sourcekit-lsp at some point? Xcode 13 has a brand new syntax highlighting implementation for Swift, which is optimized for performance. This is unrelated to sourcekit-lsp. If you see any issues, please report feedback: https://developer.apple.com/bug-reporting/ What is recommended option of adding SPM dependency from private GitHub repo? Am I right, that for Xcode it is https:// url accompanied by Xcode GitHub account? Question: if I need to connect two private repos from GitHub, and one uses account1, another uses account2. How to specify, what account to use with what repository? You're correct that when using privately hosted packages in Xcode you'll want to be signed into your source control account(s) (in Preferences \u2192 Accounts). Authentication should be handled automatically and should choose the correct account for each package. If you run in to any issues, we'd really love to hear about it via Feedback Assistant. :bow: What is the simplest way to view a diff of changes to the documentation? Sometimes I'll find on Twitter from an Apple engineer or writer that particular docs have been updated, but I'm curious if there's a more consolidated and comprehensive way to find docs updates. I checked in with the documentation team on this \u2013 while it's possible to view a diff to see what changed for individual APIs, there is not currently an overarching view that would show all the diffs between two versions. If that's something you'd find useful, please do submit that feedback via feedbackassistant.apple.com for the documentation team to look into! Given that the legacy build system is deprecated, we're still noticing that the new build system is slower to run a build than the old one. We're very sensitive to build performance -- currently a roughly no-op build with the legacy build system is roughly 60s on a recent Mac Pro with our codebase and considerably less than that on legacy. Any optimization tips would be greatly appreciated! Also - a lot of this is spent in the build planning step -- are there plans to cache or reduce build graph evaluation in the future? Let me know if filing a radar/feedback is useful. Task planning should only re-run if something in the project structure has changed between builds; when doing (for example) an immediate no-op rebuild, this shouldn't happen. Please file a bug report. If you can attach a sample project which reproduces the problem, that would help, as I suspect there's something in the project structure which is triggering this. Do you have any general guidance for writing useful Feedback Assistant reports for Xcode? For example, is it a good idea to make bug reports isolated and minimal in an Xcode playground? We have some great tips for filing bug reports using Feedback Assistant here: https://developer.apple.com/news/?id=vvrgkboh Yes, it is an excellent idea to create a minimal scenario that shows your issue. Screenshots and a sysdiagnose also help us investigate the issue. We really do read every one of your bug reports too so as one of the folks that reads those, I just want to say thanks for asking this question! We really appreciate those reproducible, actionable bug reports! How is your experience using Xcode to work on Xcode? It\u2019s definitely a very \u201cmeta\u201d experience to write code for new functionality in the Source Editor \u2026 in the Source Editor. Or to debug Xcode using Xcode. Honestly, you sometimes loose track of which one you are working in and which one your are debugging. We can also profile Instruments with Instruments by the way. So it\u2019s definitely fun to build the tools you are working with every day. It\u2019s not as fun to break the tools you are working with every day. Luckily, we\u2019ve been using Xcode Cloud with the Xcode project to make this less likely! Why doesn't the new HTTP Traffic Instrument work with the simulator. Is this a limitation that might be fixed in a future beta? Thank you for your feedback; it's a known issue in Beta-1 that we are investigating, as the feature is is intended to work in simulator. How does the team decide if a particular subsystem of Xcode should be open source for community contributions? Also, do you have any tips for getting started contributing to aspects to tooling on Apple's GitHub (perhaps low-hanging fruit)? For the Swift project, we have some bugs tagged as \"starter bugs\". Suitable for new contributors, some even if you have no experience with compilers! https://bugs.swift.org/browse/TF-67?jql=status%20%3D%20Open%20AND%20labels%20%3D%20StarterBug%20AND%20assignee%20in%20(EMPTY) Is there any way to cache Swift Package Manager dependency builds? This is one of the longest steps in our current build, especially on CI. We cache the checkouts for them, but being able to cache SPM builds would save a lot of time! Adding clonedSourcePackagesDirPath to your xcodebuild flags should help you point CI systems to already downloaded packages. There are Swift Package Manager Office Hours starting today at 3pm and it might be a better forum for this question. What does ALLOW_TARGET_PLATFORM_SPECIALIZATION + SUPPORTED_PLATFORMS actually do? Can I build XCFramework fro macOS/iOS/tvOS with one xcodebuild call? SUPPORTED_PLATFORMS defines the platforms a target supports building for. For example a framework target which can be built for either iOS or watchOS. ALLOW_TARGET_PLATFORM_SPECIALIZATION allows a target to build for multiple platforms in a single build. For example, building an iOS app with an included watchOS app, both of which depend on that framework, so the framework can be built for both platforms. But XCFrameworks still need to be built with a separate xcodebuild invocation. Is there a way to create our own file templates for adding a new file to a Swift Package Manager module? The existing templates don't include substitution templates, so they require modification (i.e. replacing SwiftUIView in both the View and the PreviewProvider) before they're as useful as their standard Xcode New File equivalent templates. There's currently no way to customize new file templates (but, as ever, if you'd like to see this happen please file Feedback!). Generally, creating a new file in a package from within Xcode (e.g., \u2318n while your package is selected in the Project Navigator) should bring up the same New File template pane as you would see for a project. If you're seeing something else or if this doesn't meet your expectations, we'd love to hear about it. Is there some way to download Xcode using the App Store Connect API and the keys that it generates ? This would be super useful in automated workflows to setup CI machines with Xcode. While there are solutions like Configurator and Remote Desktop that allow you to install apps from the Mac App Store, including Xcode, there is not a way to install Xcode through the App Store Connect API. It sounds like the best solution for your situation would likely be to utilize Xcode available as a download in XIP format from developer.apple.com/download How does Xcode Cloud work for projects that make use of multiple developer accounts? For example, we have one enterprise account for development and beta builds, and then a separate developer account for our App Store builds. Would we be able to share configuration between these two accounts or will they have to be separate? You cannot share data between different Developer Program accounts, all accounts are private and secure as you would expect. Also Apple Developer Enterprise Program accounts are not supported in Xcode Cloud at this point in time. In ObjC projects where headers haven't changed, how can we debug unexpected full/near-full rebuilds for Debug? If you think this could be a bug and you can attach a reproducible case, please file a bug report. That said, you can set a user default: defaults write com.apple.dt.XCBuild EnableBuildDebugging -bool YES which will dump Intermediates.noindex/XCBuildData/BuildDebugging- directories containing trace files (which are text files despite the file extension) which have the low-level dependency analysis that the build performs. There is not tooling for processing these files, but they might provide some insight into your situation. Important: You should delete this user default when you've collected the data you want, as enabling build debugging can affect build times. It's not intended to be left enabled all the time. What was the reasoning behind having the HTTP Request monitoring in Instruments vs the Activity Monitor in Xcode? It would've been cool to not have to intentionally run and setup Instruments to have that working. I assume you are referring to the debug gauges in the Debug navigator. We do already show the \u201cNetwork\u201d activity. However, the HTTP Traffic instrument shows very detailed information that wouldn\u2019t all fit into the the existing debug gauges. There are many different ways to visualizes network activity and even the HTTP Traffic instruments provides several ways to visualize your activity. However, if a specific summary view about HTTP traffic would be really helpful to you in the Debug Navigator within Xcode, please file a Feedback to describe what you would like to see there. https://developer.apple.com/bug-reporting/ Has the team ever considered allowing Xcode to send macOS notifications - for example when tests pass / fail ? After tests finish, if Xcode is not the frontmost application, it posts a macOS notification describing whether testing succeeded or failed. Beyond testing, Xcode includes a flexible feature called Behaviors which allows you to opt-in to receiving notifications when various events occur during your development workflow. For more details, see https://developer.apple.com/library/archive/documentation/ToolsLanguages/Conceptual/Xcode_Overview/CustomizingYourWorkflow.html MacOS build worked on xcode cloud, are these Apple silicon devices or Intel based? or can we select? Xcode Cloud builds universal binaries, just as Xcode does on your local Mac, that are compatible with all Macs. For more information about Universal Binaries, please check the developer documentation here: https://developer.apple.com/documentation/apple-silicon/building-a-universal-macos-binary My team produces a number of software components, and we've recently switched from packaging them as static frameworks to XCFrameworks. Xcode doesn't have a target type for producing XCFrameworks (please tell me that I'm wrong!), so that packaging happens in a post-archive script. When we were just producing static frameworks, we could drop several of our projects into a common workspace and it was easy to debug right into each of those frameworks -- Xcode figures out that since the Foo project builds the Foo framework, other projects in the workspace should link against that. We'd really like to do the same thing with our XCFrameworks, but they don't seem to be supported as well. Is there something we can do to debug into our XCFrameworks when their respective projects are all in a workspace together? Thanks for reaching out! XCFrameworks are not intended to be used during development if you're a team that produces them. From your perspective, they should be used only for distribution. For development, you should use dynamic framework targets and then link them into your executable that you'd like to debug. For more information on multi-platform frameworks, we suggest to watch \"Explore advanced project configuration in Xcode\" WWDC Session, which is set to be released on Friday: https://developer.apple.com/wwdc21/10210 Please let us know if you have any follow up questions or thoughts! With Vim mode, can you support \u201cV\u201d to visual select lines? FB9145839 It looks like you already submitted the request via feedback assistant. Thanks for doing that, it's the best way to let us know of any enhancements you'd like to see. When are we going to see TestFlight beta user crash report with the user information? It helps a lot when we receive a bug report from the user and find the stacktrace of the crash. In Xcode 13, the Crashes Organizer will show you feedback from TestFlight users of your app, including user information like name and email address if the user chose to provide it. In addition, you can view feedback and download the crash log directly from App Store Connect. Xcode Cloud looks great. Has the team (or others in-channel) tested it with Fastlane-based CI workflows to date ? Xcode Cloud is designed to automatically build, test, and prepare your app for TestFlight and the App Store. And should you need to install additional third-party tools to accomplish other tasks, then this can be done using Custom Build Scripts. You can learn more about extending Xcode Cloud here: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts We built SDK for our clients, not an app and have several semi-independent components. I want to understand what would be ideal way to distribute our SDK as XCFramework where it depends on our two other internal static libraries. But out of one we need to distribute one dependency as dynamic framework mainly as XCFramework moving forward. SDK is mix of Objc and Swift. Existing Structure SDK - Distribution as Dynamic FAT Framework. SDK has dependency on StaticLibrary1 and resources which are brought using cocoapods - whose public API needs to be available as part of SDK public interface. SDK contains StaticLibrary2 - whose public API needs to be available as part of SDK public interface. End goal is to distribute SDK as XCFramework. Should we distribute other dependency as XCFramework as well and integrator needs to bring all those three in order to utilize our SDK An XCFramework can be used to distribute a framework for multiple platforms in a single bundle, but XCFrameworks don\u2019t provide any support for distributing an SDK; you still need to distribute a single XCFramework for each framework you\u2019re distributing. Xcode doesn\u2019t provide support for constructing SDKs like you\u2019re distributing; you would need to provide guidance to your users to use the frameworks you're providing to them. Xcode Cloud: can it produce signed Developer ID Mac apps? Two answers in this channel conflict (one said only App Store, the other said yes Developer ID) Great question! Xcode Cloud does not support Developer ID-signed Mac apps. It supports Development and Mac App Store signing. I encourage you to submit a feedback request for any enhancements you'd like to see in the future! Will it be possible to be notified by email when a user submits a feedback(with a crash)? That is great feedback! Please submit an enhancement request with details about what you'd like to see via Feedback Assistant. In Xcode Organizer, is there anyway to: See watchdog crashes Search by the stack trace content of a crash See a version's all crashes and not only the top ones There is not a way to see individual watchdog crashes in the Xcode Organizer. However the new Terminations metric in Xcode 13 will show you aggregated data about all types of terminations that customers are experiencing in your app. Searching by the content of a stack trace is not available in the Organizer, but this would be a great enhancement to request via Feedback Assistant! In Xcode 13, you can see up to 1000 of your app's crashes for the past year, so you can view and investigate far more than the top 25 crashes. Do you think Xcode Cloud has value for individual developers, or just teams? Xcode Cloud can greatly add value, even for individual developers. It can easily help you validate the code you write by starting integration jobs that will also run your tests on multiple run destinations (i.e. iOS/iPadOS simulator, Mac Catalyst) in addition to building. This way, you will feel more confident about your work when shipping updates of your App (it also handles code-signing for you). With Xcode Cloud, your project is ready to accept outside contributions without needing to maintain a CI infrastructure. What are some of your favorite tips and tricks for triaging crashes? When I am looking at a new crash in an area I am not familiar with, one of the first things I do is try to understand why it is crashing. Asking yourself questions can also help you get a quick understanding of what kind of crash it is. \"Is it crashing because I made an assumption I shouldn't be?\" \"Is this framework I am using expecting something different than what I am giving it?\" \"Does this only happen sometimes?\" Answering those questions can get you started on understand the crash and then working towards a fix! Using the Organizer's inspector allows me to see how big of a problem it is. It can tell me all about how many devices this crash is happening on, what versions of the OS, and new this year,: where is it crashing (App Store or TestFlight) and which app versions? Our project has multiple build configurations, not just release and debug. Currently with Cocoapods we can have different dependencies for each build configuration. Last time we checked this was not possible with SPM. Has this changed? If not, do you expect this to change in the future? There is a proposal for conditional target dependencies (see https://github.com/apple/swift-evolution/blob/main/proposals/0273-swiftpm-conditional-target-dependencies.md) but this has not yet been fully implemented. Is there a way to only update a single package? If I have multiple packages specifying \"Up to next major\", Xcode will try to update all of them when I run \"Update to Latest Package Versions\" but sometimes I might only want to update a specific one. Not at the moment, but it is something we are looking to make available on both the CLI and Xcode. Hi, SPM is incredible! Does the presence of Playgrounds for iPad mean that we can replace xcodeproj-based projects with SPM-based projects even for non-console apps? Thanks! We're glad you like it! Because projects in Swift Playgrounds are built on top of the Swift Package format, they'll also work great in Xcode when making apps for iPadOS. What's the best method for simultaneously develop an app and its dependent Swift packages? (local development with submodule?) Check out https://developer.apple.com/documentation/swift_packages/editing_a_package_dependency_as_a_local_package If I've got private dependencies from, let's say GitHub via ssh. If they require different credentials, I can use ~/.ssh/config to tie ssh keys to specific configurations. SPM uses that perfectly. But if I put same Package.swift file into Xcode, it does not. Is it possible to make Xcode behave like swift package commands in this case? Thanks! You can configure Xcode to use the system git instead of the builtin one, which is what SwiftPM uses on the command line. See \"Use Your System\u2019s Git Tooling\" in https://developer.apple.com/documentation/swift_packages/building_swift_packages_or_apps_that_use_them_in_continuous_integration_workflows Have there been improvements to checkout times in Xcode 13? We've had to fork many of our dependencies and remove the git history to allow for reasonable checkout times. Starting Swift 5.4, Swift Package Manager caches package dependencies on a per-user basis, which reduces the amount of network traffic and increases performance of dependency resolution for subsequent uses of the same package. This means that while the initial clone may still be slow depending on the repository size, subsequent clones are much faster. This can also be used in CI by configuring the cache location. See https://swift.org/blog/swift-5-4-released/ Is there a way to check to see if there are updates available without installing them, equivalent to pod outdated ? There's not a convenient command to do this yet. As a workaround, you could perform an update, then revert your Package.resolved file to its prior state with your source control system to go back to your old dependency versions. \ud83d\uddd3 Friday What is the recommended practice to run a test case only on the specific OS version? Adding @available(iOS 14, *) directive on the top of class declarations seems not working somehow. The recommended way for this is to use one of the XCTest APIs to skip the test if the required APIs are not available. There is more information at https://developer.apple.com/documentation/xctest/methods_for_skipping_tests Maybe this has been asked, but with the new TestFlight crash logging in Xcode Organizer, will there be a way to integrate this with issue trackers like Jira? Sorry, we don't have the answer to your question in this lab, but, if you haven't already seen it, I would recommend checking out the Triage TestFlight crashes in Xcode Organizer video at https://developer.apple.com/wwdc21/10203 Are there any recommended workflows for using run scripts inside of a Package.swift file? There have definitely been a few times where I've wanted to have my Package.swift file generate things like compiled protobuf models. Thanks! SE-0303 defines new functionality in SwiftPM enabling the creation of structured plugins for code generation. Can Static Analyzer in Xcode 13 find dead code like unused variables and methods? Xcode provides compiler warnings for unused local variables and unused static functions and unused C++ private methods. There are no warnings for unused Objective-C methods because in Objective-C it's a fairly hard technical problem as it's a very dynamic language. The static analyzer provides additional warnings for problems that require deeper analysis such as unused initializations or assignments that get immediately overwritten (\"dead stores\") and redundant if-clauses and operands in expressions (new in this release! - off by default, see the \"Unused Code\" section in build settings). Do Static Analyzer works with Swift? The static analyzer can detect bugs in C/C++ and ObjC code. It doesn\u2019t detect bugs in Swift code. However, you can use the analyzer on mixed Swift-ObjC projects. We recommend running the analyzer on your project if it has C/C++ or ObjC code as some issues the analyzer can detect on ObjC code can manifest as hard-to-find bugs when used from Swift. How big would be the time impact on building with Xcode if I turn ON the \u201cAnalyze During build flag?\u201d Usually we expect 2-5x slowdown. You can set mode for analysis to \"Shallow\", which is generally faster (2-3x slowdown), but finds fewer bugs. Does the Static Analyzer, analyzes the full source code of my project including third party libraries and frameworks? If so, can I configure which frameworks to exclude? This is a great question! The static analyzer will analyze all third party code as long as it's getting compiled when you're building your app (i.e., binary frameworks aren't analyzed) and it's written in a language supported by the static analyzer (i.e. C/C++/Objective-C but not Swift). There is no way to exclude individual frameworks but given that the source code is available, you may have a chance to edit it to suppress unwanted warnings. Is there any point in running on the static analyzer on pure Swift code? If your app has only Swift code, the analyzer wouldn\u2019t find any issues. However, if your project has ObjC code as well, we recommend running the analyzer.","title":"DevTools"},{"location":"wwdc21/DevTools.html#devtools-lounge-qas","text":"","title":"DevTools Lounge QAs"},{"location":"wwdc21/DevTools.html#by-paul","text":"","title":"by paul"},{"location":"wwdc21/DevTools.html#tuesday","text":"","title":"\ud83d\uddd3 Tuesday"},{"location":"wwdc21/DevTools.html#hi-thank-you-for-organizing-this-i-have-a-question-about-ci-in-regards-to-xcode-cloud-i-remember-having-troubles-no-std-output-was-printed-anywhere-running-certain-scripts-in-pre-build-and-post-build-actions-in-scheme-editor-when-i-was-using-xcode-server-bots-back-in-2015-the-question-is-shall-we-rely-on-pre-build-action-for-say-running-things-like-pod-install-and-other-pre-build-setup-wo-which-the-project-cannot-be-run","text":"Xcode Cloud supports custom CI-specific build scripts that are distinct from pre-build action scripts in your Xcode scheme. The logging from these scripts is captured and made available in your workflow results. See some documentation about this here: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts","title":"Hi, thank you for organizing this! I have a question about CI, in regards to Xcode Cloud. I remember having troubles (no \"std output\" was printed anywhere) running certain scripts in Pre-build and post-build actions in Scheme editor when I was using Xcode Server &amp; Bots back in 2015. The question is shall we rely on pre-build action for, say, running things like pod install and other pre-build setup w/o which the project cannot be run?"},{"location":"wwdc21/DevTools.html#will-xcode-cloud-allow-you-to-work-with-a-locally-hosted-on-my-personal-network-git-repo","text":"Xcode Cloud has first-class support and deep integration with both cloud-based source control providers \u2014 GitHub, Bitbucket, and GitLab \u2014 and on-premise source code providers; GitHub Enterprise, Bitbucket Server, and GitLab Self-managed. https://developer.apple.com/documentation/xcode/source-code-management-setup","title":"Will XCode Cloud allow you to work with a locally hosted (on my personal network) git repo?"},{"location":"wwdc21/DevTools.html#first-thanks-for-creating-xcode-cloud-it-seems-so-promising","text":"I watched all the videos so far, I was curious to know if there would be a way to retry particular failed test or not? Xcode Cloud will allow you to re-run a failed job. Which tests get run will depend on how you configured your test plan, and if you enabled a test repetition mode (New in Xcode 13!) in your test plans. Xcode Cloud always respects your test plan/scheme and workflow configuration","title":"First thanks for creating Xcode Cloud it seems so promising!"},{"location":"wwdc21/DevTools.html#would-it-be-possible-to-get-the-dsym-file-from-xcode-cloud-builds-download-dsym-option-is-not-available-on-app-store-connect-if-we-build-without-bitcode-enabled-it-would-be-nice-to-have-testflight-crash-reports-be-symbolicated-and-still-have-access-to-the-dsym-file-to-use-with-other-tools","text":"What I found is that as long as Xcode outputs a dSYM, it will be included in the artifacts. For more advanced workflows, you could use Xcode Cloud\u2019s environment variables with a custom build script to extract symbols from an archive and upload them to a place of your choosing. More specifically, take a look at CI_ARCHIVE_PATH and ci_post_xcodebuild.sh .","title":"Would it be possible to get the dSYM file from XCode Cloud builds? Download dSYM option is not available on App Store Connect if we build without bitcode enabled. It would be nice to have Testflight crash reports be symbolicated, and still have access to the dSYM file to use with other tools."},{"location":"wwdc21/DevTools.html#will-xcode-cloud-be-available-to-teams-enrolled-in-the-apple-developer-enterprise-program","text":"No. Xcode Cloud is initially supported for Apple Developer Program teams. You can read more about requirements on this page: https://developer.apple.com/documentation/xcode/requirements-for-using-xcode-cloud","title":"Will Xcode Cloud be available to teams enrolled in the Apple Developer Enterprise Program?"},{"location":"wwdc21/DevTools.html#will-there-be-webhooks-available-with-xcode-cloud-for-build-test-and-distribute-status-updates","text":"You can find documentation about Xcode Cloud\u2019s available webhooks below: https://developer.apple.com/documentation/xcode/configuring-webhooks-in-xcode-cloud","title":"Will there be webhooks available with XCode Cloud for build, test, and distribute status updates?"},{"location":"wwdc21/DevTools.html#i-have-a-question-around-testing-i-was-trying-to-figure-out-a-way-to-test-scenarios-around-iap-family-sharing-either-in-the-simulator-using-app-store-config-files-or-even-in-sandbox-on-device-i-cant-seem-to-find-a-way-to-simulate-something-like-having-a-purchase-made-available-via-a-family-purchase-made-by-someone-else-or-to-simulate-having-a-purchase-revoked-via-the-head-of-family-disabling-family-sharing-or-someone-leaving-a-family-would-testing-in-sandbox-on-two-devices-that-were-set-up-with-family-sharing-be-able-to-test-these-scenarios-was-also-wondering-about-generating-server-to-server-notifications-for-those-events-as-well-but-even-just-being-able-to-test-in-xcode-would-be-helpful","text":"Have you been using StoreKit to test this? This documentation seems related to what you're asking for: https://developer.apple.com/documentation/storekit/original_api_for_in-app_purchase/supporting_family_sharing_in_your_app There's also a great session from WWDC2020 around StoreKit Testing with Xcode: https://developer.apple.com/videos/play/wwdc2020/10659/ Thanks, I've develop my code to handle the scenarios outlined in that document, I'm more wondering how I can trigger those in simulator testing... for example, if I'm running in the Xcode simulator, is there any StoreKit testing feature that would trigger the paymentQueue(_:didRevokeEntitlementsForProductIdentifiers:) method of SKPaymentTransactionObserver ? I'd like to be able to test my code to make sure my handling of that method works correctly. That document also mentions a REVOKE server to server notification, when testing in Sandbox is there any way to generate that event? Thanks for the followup. For the server to server notification, unfortunately not. Please file a feedback ticket for us to add that functionality (and add a followup here with the FB# if you have time to do so now). For the first case, yes! After a purchase is made, open the StoreKit Transaction Manager (Debug > StoreKit > Manage Transactions), you can select a transaction and refund it. This will cause paymentQueue(_:didRevokeEntitlementsForProductIdentifiers:) to be called within your app.","title":"I have a question around testing - I was trying to figure out a way to test scenarios around IAP family sharing, either in the simulator using App Store Config files, or even in Sandbox on device.  I can't seem to find a way to simulate something like having a purchase made available via a family purchase made by someone else, or to simulate having a purchase revoked via the head of family disabling family sharing (or someone leaving a family).  Would testing in sandbox on two devices that were set up with family sharing be able to test these scenarios?  Was also wondering about generating server to server notifications for those events as well, but even just being able to test in Xcode would be helpful."},{"location":"wwdc21/DevTools.html#hi-congrats-for-your-work-demo-looks-awesome-cant-wait-to-test-it-myself","text":"Will Xcode Cloud allow us to use classic open source tools in the Swift ecosystem (like swiftlint or sourcery) in our workflows ? How will version dependency be managed ? Yes, Xcode Cloud has ways for you to use open source tools in your workflows, so you can do things like lint or statically generate code in the cloud if you wanted to. Check out this documentation on writing custom scripts for Xcode Cloud to see how to get started! As for how your versioned dependencies will be managed, that would be handled by whatever dependency manager your project uses. So if you use Swift Package Manager, then your build step would resolve the dependencies and versions according to how you wrote your Package.swift file.","title":"Hi! Congrats for your work! Demo looks awesome can't wait to test it myself."},{"location":"wwdc21/DevTools.html#will-xcode-cloud-support-building-projects-with-third-party-dependencies-and-if-so-which-dependency-managers-will-be-supported-cocoapods-carthage-and-swift-package-manager","text":"I believe you can use dependency managers using custom build scripts. Here is some documentation for you! https://developer.apple.com/documentation/xcode/writing-custom-build-scripts See also: https://developer.apple.com/documentation/xcode/making-dependencies-available-to-xcode-cloud","title":"Will Xcode Cloud support building projects with third party dependencies and if so which dependency managers will be supported (Cocoapods, Carthage and Swift Package Manager)?"},{"location":"wwdc21/DevTools.html#could-xcode-cloud-be-used-as-a-platform-to-eventually-add-distributed-buildcache-systems-were-very-interested-in-the-ability-to-do-use-this-to-speed-up-developer-builds-build-systems-like-bazel-have-powerful-capabilities-here-but-with-poor-xcode-integration-and-increased-complexity-for-developers-we-would-love-to-see-this-problem-solved-natively-using-xcode-and-is-a-major-pain-point","text":"Thank you for the feedback, please feel free to file this with Feedback Assistant as well.","title":"Could Xcode Cloud be used as a platform to eventually add distributed build/cache systems? We're very interested in the ability to do use this to speed up developer builds. Build systems like Bazel have powerful capabilities here, but with poor Xcode integration and increased complexity for developers. We would love to see this problem solved natively using Xcode and is a major pain point."},{"location":"wwdc21/DevTools.html#can-xcode-cloud-run-ui-tests-in-parallel","text":"Xcode Cloud can parallelize tests among all the test destinations configured in your workflow, so yes, it can run UI tests in parallel","title":"Can Xcode Cloud run UI-tests in parallel?"},{"location":"wwdc21/DevTools.html#will-it-be-possible-to-quickly-download-all-the-screenshots-from-the-ui-tests-this-can-be-useful-for-submitting-screenshots-to-the-app-store-listing-perhaps-this-can-even-be-automated-right-from-the-flow","text":"There are several ways to accomplish that: You can download the result bundle from Xcode Cloud and open the result bundle locally, or open it directly in Xcode. 1. From the familiar outline mode you can export the images you want by selecting them and clicking on the paperclip icon in that activity. 2. In gallery mode you can easily select multiple images and drag them to Finder or elsewhere. 3. You can automate this using xcresulttool. For an overview of xcresulttool see https://help.apple.com/xcode/mac/current/#/devc38fc7392","title":"Will it be possible to quickly download all the screenshots from the UI tests? This can be useful for submitting screenshots to the App Store listing. Perhaps this can even be automated right from the flow."},{"location":"wwdc21/DevTools.html#will-xcode-cloud-have-the-concept-of-permissions-ie-restrictions-on-who-can-create-edit-trigger-workflows","text":"Depending on a user\u2019s role in App Store Connect, they may be able to put restrictions on Workflows. You can find the documentation about that here: https://developer.apple.com/documentation/xcode/developing-a-workflow-strategy-for-xcode-cloud Here is some information about App Store Connect role permissions: https://help.apple.com/app-store-connect/#/deve5f9a89d7","title":"Will Xcode Cloud have the concept of permissions? I.e Restrictions on who can create, edit &amp; trigger workflows."},{"location":"wwdc21/DevTools.html#when-im-working-directly-on-a-swift-package-without-a-project-file-creating-a-new-unit-test-case-file-always-uses-the-objc-template-and-it-appears-this-is-still-true-in-xcode-13-is-this-something-ive-done-wrong-or-should-i-file-a-feedback","text":"Please do file a feedback about this issue on https://feedbackassistant.apple.com. Thank you for bringing this to our attention! One workaround is to create new test case files from the Test Navigator. Use the \u201c+\u201d button at the bottom-left corner of the Test Navigator, select \u201cNew Unit Test Class\u2026\u201c, and then select Swift from the Language selector dropdown. Another workaround would be to create the file in the Tests directory using the basic \u201cSwift File\u201d template, and then add the code for the new XCTestCase subclass yourself.","title":"When I'm working directly on a Swift package without a project file, creating a new unit test case file always uses the ObjC template, and it appears this is still true in Xcode 13. Is this something I've done wrong, or should I file a feedback?"},{"location":"wwdc21/DevTools.html#will-xcode-cloud-integrate-with-gitlab","text":"Xcode Cloud has first-class support and deep integration with both cloud-based source control providers \u2014 GitHub, Bitbucket, and GitLab \u2014 and on-premise source code providers; GitHub Enterprise, Bitbucket Server, and GitLab Self-managed. https://developer.apple.com/documentation/xcode/source-code-management-setup","title":"Will Xcode Cloud integrate with Gitlab?"},{"location":"wwdc21/DevTools.html#first-of-all-xcode-cloud-looks-very-promising-thanks-for-all-the-great-work-that-has-been-done-around-it","text":"I have one question that I couldn't resolve by going through the docs though (sorry if it's already answered there). Basically, I'd like to know if workflows can be triggered on demand through an API the same way it can be done with some App Store Connect actions. Yes, Xcode Cloud will be fully integrated into the App Store Connect public REST API. You will be able to start new builds programmatically. The public REST API integration is not immediately available today, but will be released during the summer. For context on one typical use case: apps that have not only a frontend component but also have backend server components require integration testing when the backend systems change. When the backend system has passed server side tests, you can set up programmatic triggers using the API to invoke Xcode Cloud builds and tests from an upstream pipeline using a specified staging server. Will a workflow be always tied to an App Store listing? Can a project be built (using REST API) without its app ID being registered in App Store Connect? Yes a workflow is always associated with an App listing in App Store Connect. Creating a workflow in Xcode streamlines the creation of App Store Connect records. If the App Store Connect record already exists, then these will be associated automatically, based on the bundle ID in your project.","title":"First of all, Xcode Cloud looks very promising! Thanks for all the great work that has been done around it."},{"location":"wwdc21/DevTools.html#for-xcode-cloud-and-releasing-to-testflight-how-do-you-capture-what-to-test-for-whats-released-to-the-user","text":"Once Xcode Cloud has successfully uploaded your build to TestFlight, you can go to the TestFlight tab in App Store Connect to enter that information Xcode 12.5 has a serious bug. When I run app via cmd+r, connecting debugger to a simulator takes about 1-2 minutes and debugserver process use 100% CPU. Do you plan to update Xcode 12.5 to 12.5.1 or 12.6 with fixing this issue? Xcode 13 doesn\u2019t have this issue but I am unable to use Xcode 13. Xcode 13 Beta should address this performance problem seen when running on recent versions of macOS Big Sur.","title":"For XCode Cloud and releasing to TestFlight, how do you capture \"What to Test\" for what's released to the user?"},{"location":"wwdc21/DevTools.html#my-question-is-this-could-you-please-share-some-of-the-best-practices-for-cicd-at-xcode-projects-im-sure-xcode-cloud-will-be-my-best-friend-in-a-future-but-if-we-imagine-we-dont-have-xcode-cloud","text":"For general, self maintained CI/CD xcodebuild is your best friend. The \"Testing in Xcode\" session from WWDC19 hits on some best practices that may be useful to you: https://developer.apple.com/videos/play/wwdc2019/413/ Some points I'd like to highlight from the session: - The testing pyramid, to help strike the right balance between unit, integration, and UI tests - Using test plans efficiently for the variables you want to change between test runs - What configuration works best for your needs when planning CI/CD set up. At times it is useful to have a separate builder machine from test runner, in which case you'll break your executions into build-for-testing and test-without-building, but it really depends on your use case","title":"My question is this: could you please share some of the best practices for CI/CD at Xcode projects? Im sure Xcode Cloud will be my best friend in a future, but if we imagine we don\u2019t have Xcode cloud : )"},{"location":"wwdc21/DevTools.html#will-xcode-cloud-support-some-sort-of-secret-management-ie-third-party-tokens-aws-creds","text":"Yes it does! See the Custom Environment Variables section on this page: https://developer.apple.com/documentation/xcode/xcode-cloud-workflow-reference You can add secret environment variables to a workflow. They are \"write-only\", meaning that once set, secret environment variables cannot be read again from the workflow editor. Secret environment variables are encrypted in transit and at rest, and used only on the ephemeral build environment. As such they are suitable for AWS keys, passwords, and API tokens. You can also add \"non-secret\" environment variables, which can be read and edited from the workflow editor.","title":"Will Xcode Cloud support some sort of secret management? i.e third-Party tokens, AWS creds"},{"location":"wwdc21/DevTools.html#does-xcode-cloud-support-custom-build-tools-for-instance-to-use-xcode-cloud-workflow-to-run-bazel-build-of-ios-project","text":"That\u2019s not currently supported, but you may want to give it a try to see if you could get it to work. That\u2019s great feedback though so please file a report on feedbackassistant.apple.com!","title":"Does Xcode Cloud support custom build tools? for instance, to use Xcode Cloud workflow to run Bazel build of iOS project?"},{"location":"wwdc21/DevTools.html#how-long-will-builds-take-on-xcode-cloud-compared-to-building-locally-for-example-using-an-m1-mac-mini","text":"Build times will depend on a number of factors, like the size of your project, build settings, workflow settings, custom build scripts, and much more. I would recommend reading these articles: https://developer.apple.com/documentation/xcode/developing-a-workflow-strategy-for-xcode-cloud https://developer.apple.com/documentation/xcode/xcode-cloud-workflow-reference What type of machines will be available to run builds on? Will there be tiers in terms of performance? There are currently no performance based tiers.","title":"How long will builds take on Xcode Cloud compared to building locally, for example using an M1 Mac Mini?"},{"location":"wwdc21/DevTools.html#how-to-join-xcode-cloud-beta-program-now-we-can-try-it-asap-","text":"You can join the waitlist here: https://developer.apple.com/xcode-cloud/beta/","title":"How to join Xcode Cloud beta program now? we can try it ASAP :-)"},{"location":"wwdc21/DevTools.html#is-there-any-tools-within-xcode-and-xcode-cloud-that-would-allow-for-an-easy-monorepo-setup-my-team-has-two-internal-libraries-that-are-used-in-a-tvos-and-two-ios-applications-so-we-are-looking-to-move-them-all-into-an-easy-repo-for-ease-of-maintenance-but-we-are-unsure-if-there-is-anything-available-to-help-ease-the-transition","text":"I don\u2019t believe Xcode nor Xcode Cloud have a way to merge multiple repos into a single repo, but there are a number of tools available to make dependencies between your projects easier to work with. I would highly recommend this talk from WWDC 2019 on Swift Packages and also this guide on managing dependencies in Xcode Cloud .","title":"Is there any tools within Xcode and Xcode Cloud that would allow for an easy monorepo setup? My team has two internal libraries that are used in a tvOS and two iOS applications so we are looking to move them all into an easy repo for ease of maintenance but we are unsure if there is anything available to help ease the transition."},{"location":"wwdc21/DevTools.html#xcodebuild-often-returns-error-65-without-clear-explanation-sometimes-it-seems-that-it-could-be-the-simulator-failing-to-boot-sometimes-the-test-results-fail-to-be-collected-but-the-error-messages-are-always-a-source-of-frustration-as-its-very-hard-to-understand-the-problem-furthermore-these-errors-are-often-sporadic-and-often-happen-on-ci-where-its-even-harder-to-fix-what-can-we-do-to-tackle-this-issues-are-there-plans-to-improve-xcodebuild-error-reporting","text":"If you encounter xcodebuild errors that are unclear, or indicate that the failure is a system error rather than a problem with your own project, please file feedback on https://feedbackassistant.apple.com and attach the .xcresult bundle from the test run for investigation. In many cases, additional helpful diagnostics can be captured by running xcrun simctl diagnose while the simulator you were targeting is still booted. We are always looking for ways to make the errors produced by xcodebuild clearer and more helpful, and feedback from developers is a very important part of that process!","title":"xcodebuild often returns error 65 without clear explanation. Sometimes it seems that it could be the simulator failing to boot, sometimes the test results fail to be collected, but the error messages are always a source of frustration as it's very hard to understand the problem. Furthermore, these errors are often sporadic and often happen on CI, where it's even harder to fix. What can we do to tackle this issues? Are there plans to improve xcodebuild error reporting?"},{"location":"wwdc21/DevTools.html#will-there-be-a-separate-cost-for-xcode-cloud-it-seems-like-there-is-a-lot-of-resources-available-i-already-pay-140mo-for-my-own-cloud-based-build-server","text":"There is no cost to join the beta program. You can sign up for that here https://developer.apple.com/xcode-cloud/beta/. More details on pricing and availability of Xcode Cloud will be announced this fall","title":"Will there be a separate cost for Xcode Cloud? It seems like there is a lot of resources available. I already pay $140/mo for my own cloud-based build server."},{"location":"wwdc21/DevTools.html#i-see-that-xcode-cloud-supports-custom-scripts-for-bootstrapping-the-temporary-environment-but-iswill-there-be-a-mechanism-for-caching-some-of-the-dependencies-for-example-a-ruby-version-installed-via-rbenv-and-bundler-dependencies-these-are-things-we-typically-would-never-check-into-git-but-could-do-with-caching-somewhere-like-we-can-on-other-platforms","text":"Caching build dependencies is not currently supported, but you might be able to do that yourself in a custom build script. You may want to give it a try to see if you could get it to work. That\u2019s great feedback though so please file a report on feedbackassistant.apple.com","title":"I see that Xcode Cloud supports custom scripts for bootstrapping the temporary environment, but is/will there be a mechanism for caching some of the dependencies? For example a Ruby version installed via rbenv, and bundler dependencies. These are things we typically would never check into Git but could do with caching somewhere like we can on other platforms"},{"location":"wwdc21/DevTools.html#does-xcode-cloud-require-the-new-macos-12-or-will-it-also-work-with-code-13-and-macoa-11","text":"Xcode Cloud is part of Xcode 13, which will be compatible with macOS 11.3 \ud83d\ude00","title":"Does Xcode cloud require the new Macos 12? Or will it also work with code 13 and macoa 11?"},{"location":"wwdc21/DevTools.html#will-we-be-able-to-use-an-http-api-to-uploadconnect-a-repository-to-start-a-workflow-ie-not-using-xcode-at-all","text":"No. You must use Xcode Cloud to configure your initial workflow and connect your source code repositories.","title":"Will we be able to use an http API to upload/connect a repository to start a workflow, i.e. not using Xcode at all?"},{"location":"wwdc21/DevTools.html#will-xcode-cloud-support-pipelines-that-dont-target-the-app-store-or-testflight-would-love-to-be-able-to-build-apps-or-even-pkg-for-my-macos-apps","text":"Yes! You can use a custom build script to upload artifacts to your own destinations. Please see the ci_post_xcodebuild.sh document in that linked article for information about copying artifacts outside of Xcode Cloud.","title":"Will Xcode Cloud support pipelines that don't target the app store or testflight? Would love to be able to build .apps or even .pkg for my macOS apps."},{"location":"wwdc21/DevTools.html#xcodebuild-has-a-long-standing-difference-fb5531332-from-xcodes-testing-behaviour-it-ignores-the-locationscenarioreference-location-simulation-in-the-xcode-scheme-run-options-in-a-scheme-this-limits-the-kind-of-integration-testing-we-can-do-on-ci-and-requires-workarounds-to-mock-core-location-which-is-crucial-to-our-apps-functionality-has-location-simulation-been-considered-for-ui-testing-with-xcode-cloud","text":"Thank you for the report and the feedback reference number! This is a bug with xcodebuild that we will bring to the attention of the engineering team.","title":"xcodebuild has a long-standing difference (FB5531332)  from Xcode's testing behaviour: it ignores the LocationScenarioReference (Location Simulation in the Xcode Scheme Run Options) in a scheme.  This limits the kind of integration testing we can do on CI and requires workarounds to mock Core Location which is crucial to our app's functionality.  Has location simulation been considered for UI testing with Xcode Cloud?"},{"location":"wwdc21/DevTools.html#are-the-xcode-cloud-configurations-stored-in-the-xcode-project-file-or-is-there-a-manifest-that-can-be-built-up-outside-of-the-project-file-many-other-ci-platforms-have-yml-configurations-that-are-easily-human-readable-i-cant-wait-to-try-out-xcode-cloud-thanks-to-all-involved","text":"Xcode Cloud workflow configuration is not stored in the Xcode project file; the configuration is persisted only in the cloud services that support the service. Being centralized in the cloud, the configuration can be accessed through the Xcode UI itself, the App Store Connect web UI, and also programmatically through the App Store Connect public REST API (available this summer). The API uses JSON format. Whereas power users are comfortable with editing YAML, integrating the configuration into an intuitive user interface makes the service more broadly accessible initially. Does it mean that we can edit/create/delete Xcode Cloud workflows via App Store Connect REST API with JSON objects? Yes, that\u2019s correct.","title":"Are the Xcode Cloud configurations stored in the Xcode project file, or is there a manifest that can be built up outside of the project file? Many other CI platforms have yml configurations that are easily human-readable. (I can't wait to try out Xcode Cloud; thanks to all involved!)"},{"location":"wwdc21/DevTools.html#will-testflight-be-available-without-xcode-cloud","text":"Yes, TestFlight will be available on Mac separate from Xcode Cloud \ud83d\ude00 This session can give you more information about it as well: https://developer.apple.com/videos/play/wwdc2021/10170/","title":"Will testflight be available without Xcode cloud?"},{"location":"wwdc21/DevTools.html#will-xcode-cloud-support-goldensnapshot-testing-take-a-screenshot-of-a-view-or-scene-so-we-can-know-how-a-view-should-look-like-and-then-this-screenshot-can-be-used-to-compare-to-following-runs-and-see-if-the-ui-was-modified-i-currently-use-for-this-snapshotpointfreecoswift-snapshot-testing-from-github","text":"Xcode Cloud will run any tests authored using XCTest, which can be combined with that library to write XCTest-based snapshot tests.","title":"Will Xcode Cloud support golden/snapshot testing? (take a screenshot of a view or scene so we can know how a view should look like, and then this screenshot can be used to compare to following runs and see if the UI was modified. I currently use for this snapshotpointfreeco/swift-snapshot-testing from github)"},{"location":"wwdc21/DevTools.html#sorry-if-i-missed-it-is-it-possible-to-use-xcode-cloud-as-a-remote-build-server-so-the-artefacts-are-redownloaded-to-my-local-machine-and-i-can-run-it-on-a-sim-locally-cheers","text":"Yes, you can download the artifacts with a \u201cci_post_xcodebuild.sh\u201d script. See the docs here: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts","title":"Sorry if i missed it, is it possible to use Xcode Cloud as a remote build server, so the artefacts are redownloaded to my local machine and i can run it on a sim locally? Cheers"},{"location":"wwdc21/DevTools.html#will-there-be-support-for-fetching-dependencies-from-other-dependency-managers-that-are-not-swift-package-manager-i-have-currently-a-big-legacy-macos-project-that-uses-git-submodules-cocoapods-and-carthage","text":"Yes, you can use Custom Build Scripts in Xcode Cloud to fetch dependencies from other systems which xcodebuild doesn\u2019t natively support such as those. See https://developer.apple.com/documentation/xcode/writing-custom-build-scripts There is additional documentation specifically about CocoaPods and Carthage available at https://developer.apple.com/documentation/xcode/making-dependencies-available-to-xcode-cloud","title":"Will there be support for fetching dependencies from other dependency managers that are not Swift Package Manager? I have currently a big legacy macOS project that uses git submodules, cocoapods and carthage."},{"location":"wwdc21/DevTools.html#will-xcode-cloud-support-third-party-static-code-analysis-tools-like-sonarqube-httpswwwsonarqubeorg","text":"You can integrate an Xcode Cloud workflow with services that are addressable through a custom script. Check out this documentation for specifics and how-to!","title":"Will Xcode Cloud support third-party static code analysis tools, like SonarQube? https://www.sonarqube.org"},{"location":"wwdc21/DevTools.html#i-have-several-groups-in-testflight-will-xcode-cloud-have-an-option-to-automatically-add-specified-groups-to-a-build-or-will-it-only-add-the-appstore-connect-user-group","text":"Yes, you can configure a workflow to select one or more TestFlight groups.","title":"I have several groups in TestFlight. Will Xcode Cloud have an option to automatically add specified Groups to a build or will it only add the 'AppStore Connect User' group?"},{"location":"wwdc21/DevTools.html#our-macos-app-targets-pro-users-in-the-music-industry-which-isnt-known-for-rapid-adoption-of-os-updates","text":"Will TestFlight for macOS be made available to testers on prior releases of macOS? Our internal QA tests all the way back to macOS 10.12, so it would be nice to (finally!) maintain the same kind of workflow that we have for our iPad and iPhone testing on prior releases. Mac TestFlight support begins with macOS 12.0, it cannot be used on older releases Are there any API requirements/limitations on the apps that are submitted to TestFlight? For example, it would make sense (given your response) that I would be required to build w/ Xcode 13 + macOS 12 SDKs to integrate with TestFlight for macOS, but will I be OK to continue using a deployment target of macOS 10.12 for my app binary? Yes, you can continue to use a deployment target older than macOS 12.0! The requirements are: - You must build and submit from Xcode 13 and use a provisioning profile - The TestFlight client supports macOS 12 and later, so builds aren't installable on older macOS versions.","title":"Our macOS app targets pro users in the music industry, which isn't known for rapid adoption of OS updates\u2026"},{"location":"wwdc21/DevTools.html#if-we-have-our-own-buildtest-farm-which-use-aws-mac-instance-does-xcode-cloud-support-to-use-our-own-dedicated-cloud-infra","text":"No, this is not supported.","title":"If we have our own build/test farm which use AWS mac instance, does Xcode Cloud support to use our own dedicated cloud infra?"},{"location":"wwdc21/DevTools.html#im-concerned-about-the-homebrew-aspect-is-there-a-chance-that-some-dependency-scripts-could-build-and-link-with-parts-of-homebrew-some-of-these-open-source-libraries-build-with-things-like-cmake-and-ninja-and-can-be-very-difficult-to-control-include-paths-not-having-homebrew-installed-i-dont-have-to-worry-about-that-but-i-would-with-xcode-cloud","text":"We include homebrew by default on the VMs in Xcode Cloud. You can read more about how dependencies can be made available to a workflow here. That said, if you're concerned about a supply-side attack, or dependencies in your application being bad actors, you can conceivably audit the environment in a post-clone script after Xcode Cloud clones your source code into its environment. I haven't tried removing homebrew from the VM, but theoretically you could remove it or prevent it from phoning home.","title":"I'm concerned about the Homebrew aspect. Is there a chance that some dependency scripts could build and link with parts of Homebrew? Some of these open source libraries build with things like CMake and Ninja and can be very difficult to control include paths. Not having Homebrew installed, I don't have to worry about that, but I would with Xcode cloud."},{"location":"wwdc21/DevTools.html#is-testflight-for-mac-limited-to-just-apps-destined-for-the-mac-app-store-or-does-it-also-work-with-notarised-apps-outside-of-the-store","text":"Apps must be uploaded for distribution to App Store Connect to become eligible for Mac TestFlight.","title":"Is TestFlight for Mac limited to just apps destined for the Mac App Store or does it also work with notarised apps outside of the store?"},{"location":"wwdc21/DevTools.html#im-incredibly-excited-to-use-asyncawait-and-actors-what-oses-will-those-be-limited-to","text":"On Apple platforms, we currently support iOS 15, macOS Monterey, etc. This is tied to runtime integration with improvements to Dispatch in those OS releases for better performance. We are exploring the possibility of backward deployment. Concurrency works on Linux. Still a WIP on Windows.","title":"I'm incredibly excited to use async/await and actors. What OSes will those be limited to?"},{"location":"wwdc21/DevTools.html#on-performance-i-have-a-4500-lines-enum-for-emojis-in-unicode-so-i-can-order-them-and-move-them-around-in-the-functions-i-declared-such-as-one-providing-the-group-or-subgroup-im-damned-either-way-if-i-dont-put-a-default-fatalerror-at-then-end-of-it-i-have-a-compiler-too-slow-error-if-i-put-it-i-have-a-default-will-never-be-executed-best-of-all-i-cannot-disable-the-warning-in-the-package-so-im-left-with-4-warnings-at-all-times-anything-i-can-do","text":"Sorry you have to deal with that. When the compiler analyzes a switch statement for exhaustiveness, it has some heuristics for very large enums that are intended to keep compiler performance good even in the face of complex patterns with very large sets of enum cases, but it can lead to false-negative cases when that goes wrong. If you have a moment to try to extract some of your code into a test case, and submit an issue to bugs.swift.org, we might be able to use it to further refine these heuristics. Thanks for letting us know about the issue. I'm wondering if this enum might be better-represented as a struct, perhaps conforming to RawRepresentable if you need an underlying value?","title":"On performance. I have a 4500-lines enum for emojis in Unicode, so I can order them and move them around. In the functions I declared (such as one providing the group, or subgroup), I'm damned either way: if I don't put a default: fatalError() at then end of it, I have a compiler too slow error. If I put it, I have a Default will never be executed. Best of all, I cannot disable the warning in the package, so I'm left with 4 warnings at all times. Anything I can do?"},{"location":"wwdc21/DevTools.html#will-swift-concurrency-be-available-on-ios-14-i-saw-the-following-text-in-the-known-issues-of-xcode-13-release-notes-swift-concurrency-requires-a-deployment-target-of-macos-12-ios-15-tvos-15-and-watchos-8-or-newer-70738378","text":"This question was answered earlier, but I'll answer it again. Right now the supported deployment target is iOS 15, macOS Monterey, etc., or later. This is because of enhancements to Dispatch to support Swift concurrency, of which the concurrency runtime uses. We are actively exploring the possibility of backward deployment, but are mindful of not compromising the future of concurrency performance by doing so.","title":"Will Swift Concurrency be available on iOS 14? I saw the following text in the Known Issues of Xcode 13 Release Notes \u2014 \"Swift Concurrency requires a deployment target of macOS 12, iOS 15, tvOS 15, and watchOS 8 or newer. (70738378)\""},{"location":"wwdc21/DevTools.html#how-lightweight-are-actors-what-should-be-considered-when-declaring-an-actor-in-regard-of-overhead-or-cost","text":"Creating an instance of an actor type is no more costly than creating an instance of a class type. It's only during the access of an actor's protected state that a task suspension may happen, to switch to that actor's executor. The implementation of that switching is lightweight, and is optimized by both the compiler and runtime system. You can find out more about how this works in Thursday's session Swift concurrency: Behind the scenes : https://developer.apple.com/wwdc21/10254","title":"How lightweight are Actors? What should be considered when declaring an Actor in regard of overhead or cost?"},{"location":"wwdc21/DevTools.html#how-to-run-async-function-from-sync-code-on-main-thread-do-i-need-to-use-detached-or-there-is-something-else","text":"Yeah, you can launch either of the forms of unstructured task from sync code. In Xcode beta 1, there is async { } , which will start a new async task that runs on the same actor as the launching context (which can be important if that's the MainActor, for main-thread-bound UI work), and asyncDetached { } , which will start a completely independent task. If you've been following Swift open source, though, you may know that these names have been revised due to feedback from the open source community; they're known as Task and Task.detached currently.","title":"How to run async function from sync code on main thread? Do I need to use detached or there is something else?"},{"location":"wwdc21/DevTools.html#are-we-likely-to-see-swift-argument-parser-reach-10-in-the-near-future-its-a-really-great-package-and-im-keen-to-use-it-more-widely","text":"As an open source project, probably the best place to ask this question is on the \"Related Projects\" section of the Swift Forums where you'll connect with the main authors of that package: https://forums.swift.org/c/related-projects/argumentparser/60","title":"Are we likely to see Swift Argument Parser reach 1.0 in the near future? It's a really great package and I'm keen to use it more widely."},{"location":"wwdc21/DevTools.html#how-asyncawait-works-under-the-hood-could-you-give-us-some-links","text":"The \"Swift concurrency: Behind the scenes\" session on Thursday will provide a lot of details about this.","title":"How async/await works under the hood? Could you give us some links?"},{"location":"wwdc21/DevTools.html#what-are-recommended-sessions-for-leveraging-asyncawait","text":"The main session is \"Meet async/await in Swift\", but you may also find tomorrow's \"Swift concurrency: Update a sample app\" session to be very helpful, since we go through an existing app and use async/await , and more, so it can leverage Swift Concurrency.","title":"What are recommended sessions for leveraging async/await?"},{"location":"wwdc21/DevTools.html#how-will-asyncawait-and-swiftnio-work-together-if-i-understood-it-correctly-asyncawait-works-very-similar-to-swiftnios-eventloopfutures","text":"Support for async/await is in the main branch for SwiftNIO, and will be available officially after 5.5 is released.","title":"How will async/await and SwiftNIO work together? If I understood it correctly, async/await works very similar to SwiftNIO\u2019s EventLoopFutures."},{"location":"wwdc21/DevTools.html#how-long-do-you-expect-things-to-live-in-standard-library-previewing-before-being-considered-fully-baked-se-0270-has-lived-there-for-quite-some-time-and-wants-its-turn-in-the-limelight","text":"Based on my conversation with the proposal authors, some issues have turned up with the design during the preview period that we need to address before it graduates to the standard library. This will likely require a follow up review.","title":"How long do you expect things to live in standard library previewing before being considered fully baked? SE-0270 has lived there for quite some time and wants its turn in the limelight"},{"location":"wwdc21/DevTools.html#for-the-long-enum-question-i-created-a-github-httpsgithubcommisoservicesmisoemojiunicode","text":"Will do the bugs asap. Ty for the answer! Thanks for the follow-up! This link is very helpful. I took a quick look and I think making this a struct would probably simplify things and eliminate the problems you're having with large switch statements. I'd recommend you sign up for one of our Swift labs later this week so you can get one-on-one support from our team.","title":"For the long enum question, I created a github. https://github.com/Misoservices/MisoEmojiUnicode"},{"location":"wwdc21/DevTools.html#will-there-be-or-are-there-already-some-guidelines-on-when-to-use-publishers-combine-and-when-to-use-asyncawait-eg-for-a-network-request-urlsession-currently-provides-support-for-both-but-whats-betterrecommended","text":"For URLSession the async/await API is preferred for single values and the AsyncSequence API is preferred for reading chunk-by-chunk. The language level support for structured concurrency is a great way to integrate URLSession downloads into your app.","title":"Will there be (or are there already) some \u201eguidelines\u201c on when to use Publishers (combine) and when to use async/await? E.g. for a network request, URLSession currently provides support for both but what\u2019s better/recommended?"},{"location":"wwdc21/DevTools.html#will-there-be-tools-to-help-visualize-task-hierarchies-in-the-future-or-cancellation-reasons-for-debugging","text":"Thanks for the suggestion, I think it's a good idea!","title":"Will there be tools to help visualize task hierarchies in the future? Or \"cancellation reason\"s for debugging?"},{"location":"wwdc21/DevTools.html#really-like-what-im-seeing-on-docc-ive-heard-its-possible-to-output-html-pages-from-a-generated-docset-will-there-be-some-kind-of-simplified-mechanism-to-apply-styling-to-those-pages-thinking-about-how-an-organization-might-want-to-apply-their-own-brand-coloring-and-such-to-published-docs-on-web","text":"Today there isn\u2019t any integrated affordance for theming the generated site. This is certainly something we can see as valuable, though. Also a great topic to bring up when we make the project open source later this year.","title":"Really like what I'm seeing on DocC - I've heard its possible to output HTML pages from a generated docset - will there be some kind of simplified mechanism to apply styling to those pages? Thinking about how an organization might want to apply their own brand coloring and such to published docs on web."},{"location":"wwdc21/DevTools.html#the-host-and-automate-presentation-mentions-docc-comes-with-a-built-in-clean-design-will-it-be-possible-to-theme-docc-builds","text":"Ah, already a popular request. Today, there is no integrated support for theming the generated site, but would love for you to file a feedback request so we can track your interest.","title":"The \"Host and Automate\" presentation mentions: \"DocC comes with a built-in clean design\". Will it be possible to theme DocC builds?"},{"location":"wwdc21/DevTools.html#is-there-a-detailed-spec-for-the-doccarchive-output-andor-alternative-output-formats","text":"You can get details about how you can host a .doccarchive on your website in the following session: https://developer.apple.com/wwdc21/10236. We\u2019ll have more details about the structure of the .doccarchive when we open source. You can post a followup question with details if you have something specific you\u2019re looking to do. A member of my team is building a tool to create unified output from several disparate docs generators that we use across different language-specific SDKs. (Currently Jazzy, Javadoc, JSDoc, etc.) We're trying to unify the reference manual experience for devs who use our docs without disrupting the workflows of our SDK teams, so we're trying to integrate with existing docs generators. I'd love to use DocC for the in-Xcode documentation, if I could also then feed DocC's output into our custom tool to unify reference docs on our docs site for our users. Having a spec for DocC's output, or being able to theme it, would go a long way toward enabling that unified experience. I'll keep my eye out for more details about the structure of the .doccarchive when you open source. Thanks for this - it looks awesome! Interesting details, thanks for that! Yes, keep an eye out for the open source details. As you explore using DocC, if there is something that would help or that you can't accomplish, please do file Feedback Assistant requests with details.","title":"Is there a detailed spec for the .doccarchive output, and/or alternative output formats?"},{"location":"wwdc21/DevTools.html#are-there-any-plans-for-docc-to-support-objective-c-documentation-builds-my-org-currently-uses-a-docs-generator-to-build-both-swift-and-objective-c-doc-sets-for-our-sdks-and-we-would-likely-have-to-continue-using-that-if-docc-cant-build-objective-c-docs-sets","text":"We\u2019ve definitely heard about the importance of Objective-C, and feel it ourselves. It is indeed a priority. Thank you for the feedback!","title":"Are there any plans for DocC to support Objective-C documentation builds? My org currently uses a docs generator to build both Swift and Objective-C doc sets for our SDKs, and we would likely have to continue using that if DocC can't build Objective-C docs sets."},{"location":"wwdc21/DevTools.html#whats-the-best-way-via-clici-to-generate-a-documentation-archive-from-a-library-thats-just-packageswift-no-wrapping-xcode-project","text":"xcodebuild docbuild works with a Package.swift file. You'll need to add a -scheme argument to tell xcodebuild which target to build; you can run xcodebuild -list to see the list. For example, you can run xcodebuild docbuild -scheme MyPackage to build a DocC Archive for the MyPackage scheme. The archive bundle will appear in your Derived Data directory (for example, ~/Library/Developer/Xcode/DerivedData/MyPackage-[hash]/Build/Products/Debug/MyPackage.doccarchive ).","title":"What\u2019s the best way (via CLI/CI) to generate a documentation archive from a library that\u2019s just Package.swift, no wrapping Xcode project?"},{"location":"wwdc21/DevTools.html#is-there-built-in-integration-with-xcode-cloud-workflows-or-would-we-need-to-implement-it-via-a-script","text":"You\u2019re correct, the best way to support documentation builds on Xcode Cloud would be via a script that invokes xcodebuild docbuild . There\u2019s a session here that is a great reference for a use case like that: https://developer.apple.com/videos/play/wwdc2021/10236/","title":"Is there built-in integration with Xcode Cloud workflows? Or would we need to implement it via a script?"},{"location":"wwdc21/DevTools.html#does-the-hosted-webapp-have-a-way-to-download-the-archive-so-devs-can-add-the-docs-to-xcode-on-their-local-machines","text":"The primary use case we\u2019ve focused on is for other developers to import your package into their project as a dependency. That lets them use Build Documentation to build the docs locally and view the docs in Xcode\u2019s Documentation window. For other workflows, you can make a .doccarchive downloadable from your website. For example, you could zip the archive and have it as a downloadable resource on your site. If you have a different workflow in mind, post a followup and we can look at that specifically, or if you have other feedback we\u2019d love to get your specific use case in a Feedback Assistant.","title":"Does the hosted webapp have a way to download the archive so devs can add the docs to Xcode on their local machines?"},{"location":"wwdc21/DevTools.html#are-there-any-versioning-capabilities-in-docc-for-example-being-able-to-view-the-documentation-for-older-versions-of-a-package-for-users-that-cant-or-dont-want-to-update","text":"If a developer imports a specific version of your package into their project as a dependency and uses Build Documentation, they\u2019ll be able to read the documentation for that version in Xcode\u2019s documentation window. If you have a different workflow in mind, please post a followup and we can look at that specifically, or if you have other feedback we\u2019d love to get your specific use case in a Feedback Assistant.","title":"Are there any versioning capabilities in DocC? For example, being able to view the documentation for older versions of a package for users that can't or don't want to update?"},{"location":"wwdc21/DevTools.html#the-host-and-automate-docc-documentation-presentation-mentions-the-main-page-groups-important-symbols-into-topics-related-to-higher-level-tasks-each-of-those-group-related-symbols-into-more-specific-topics","text":"Can you talk more about how those groupings happen? Is that something we dictate by the way we organize and present content in the framework, or can we manually specify groupings? Organizing your pages into groups is a great way to improve your documentation! To do that, you create a Topics section with a second-level heading called \"Topics\", followed by a group title in a third-level heading, and then a bulleted list of links. We describe the syntax in more detail here https://developer.apple.com/documentation/xcode/adding-structure-to-your-documentation-pages, under \"Arrange Top-Level Symbols Using Topic Groups\". We also have a session tomorrow that shows how to do this: https://developer.apple.com/videos/play/wwdc2021/10167/ along with some other ways you can improve your documentation.","title":"The \"Host and Automate DocC Documentation\" presentation mentions: \"The main page groups important symbols into topics related to higher-level tasks. Each of those group related symbols into more specific topics.\""},{"location":"wwdc21/DevTools.html#usually-a-problem-of-documentation-is-that-it-becomes-obsolete-with-time-is-there-any-way-to-prevent-this-by-using-docc-is-there-any-validation-in-the-code-that-can-be-autocompleted","text":"We think so too! It's important to keep documentation up to date with the changes you make in your code. Like a code compiler, DocC can emit warnings and errors when it finds something that doesn't quite look right. For example, DocC checks all of the links in your documentation you've written using double-backtick syntax, like MyType . If you remove or change the name of an API in your code, Xcode will display a warning on the line containing the documentation letting you know that you should update your docs. If you're interested in other ways to validate the documentation, we'd be interested to hear more via Feedback Assistant.","title":"Usually a problem of documentation is that it becomes obsolete with time. Is there any way to prevent this by using docC? Is there any validation in the code that can be autocompleted?"},{"location":"wwdc21/DevTools.html#is-there-any-way-to-know-the-documentation-coverage-of-the-public-symbols-of-a-framework-it-would-help-a-lot-with-adoption-and-finding-blind-spots-on-huge-code-bases","text":"Hi Mauro! DocC does not currently expose documentation coverage but this is great feedback. Could you please file a Feedback Assistant request with details about how you'd want this to work? These types of enhancement requests are super helpful.","title":"Is there any way to know the documentation coverage of the public symbols of a framework? It would help a lot with adoption and finding blind spots on huge code bases."},{"location":"wwdc21/DevTools.html#wonderful-work-thank-you-thinking-ahead-is-there-any-limitation-that-would-inhibit-the-ability-to-extract-docs-for-symbols-other-than-public-or-open-seems-like-it-would-be-useful-for-internal-use-and-as-a-way-to-collect-data-on-undocumented-symbols","text":"Thanks for the feedback! This is something that a lot of people have been interested in over the last few days :). We built the DocC integration in Xcode first and foremost to support public-facing docs. We know that this is important for teams working on a framework and will keep this in mind in the future\u2014thanks again for the question!","title":"Wonderful work. Thank you! Thinking ahead, is there any limitation that would inhibit the ability to extract docs for symbols other than public or open? Seems like it would be useful for internal use, and as a way to collect data on undocumented symbols."},{"location":"wwdc21/DevTools.html#will-the-docc-compiler-run-only-over-source-code-dependencies-or-will-it-also-generate-the-docs-for-the-public-interfaces-of-consumed-xcframeworks","text":"DocC integrates with the Swift compiler to extract information about the public symbols and their documentation during the build. The author of an xcframework can build and export that framework\u2019s documentation and distribute it alongside the xcframework to enable consumers to read the framework's documentation in Xcode. Do you have any tips or references on how to best distribute them together with the xcframework so that they are detected by Xcode? Is there any SPM tooling around this or is your answer more about a context of a manual import? Thank you very much! That's a great question and something that we want to explore more. For now, this can be accomplished if the framework author exports and uploads a separate zip file of the DocC Archive. Consumers of the xcframework can then download the documentation archive, and double-click it to import that documentation into Xcode. If you have more questions or want to follow up, you can request a DocC Lab appointment.","title":"Will the DocC compiler run only over source code dependencies or will it also generate the docs for the public interfaces of consumed xcframeworks?"},{"location":"wwdc21/DevTools.html#when-building-docs-locally-does-the-archive-get-stored-in-derived-data-ie-are-devs-going-to-have-to-rebuild-docs-every-time-they-clear-derived-data-to-fix-weird-issues","text":"When you use the Build Documentation menu in Xcode, the doccarchive does go into the derived data directory, so if you clear that, you would have to rebuild the docs. If you want to keep a version of the built docs in the Documentation window, you can export the doccarchive, and then you can double click it which imports it back into the Documentation window. When you import a .doccarchive , Xcode puts it into an \u201cImported Documentation\u201d section of the Documentation window, and that doesn't get removed when cleaning the derived data.","title":"When building docs locally, does the archive get stored in derived data? i.e. are devs going to have to rebuild docs every time they clear derived data to fix weird issues?"},{"location":"wwdc21/DevTools.html#in-some-of-our-internal-modules-we-have-some-utility-methods-in-extensions-of-types-that-are-part-of-another-consumed-frameworks-for-example-foundation-were-working-towards-removing-the-majority-of-them-in-favor-of-other-solutions-that-avoid-extensions-in-the-meantime-though-i-havent-seen-them-in-our-generated-docs-and-would-like-to-know-if-theres-a-way-to-include-them-so-that-they-are-discoverable-while-we-deprecatemove-them-thanks","text":"DocC in Xcode 13 supports documentation for APIs defined in your module, but we know that documenting extension symbols is also very important. This is something that would also be a great topic for us to discuss when we open source later this year.","title":"In some of our internal modules we have some utility methods in extensions of types that are part of another consumed frameworks (for example: Foundation). We're working towards removing the majority of them in favor of other solutions that avoid extensions. In the meantime though, I haven't seen them in our generated docs and would like to know if there's a way to include them so that they are discoverable while we deprecate/move them. Thanks!"},{"location":"wwdc21/DevTools.html#im-loving-docc-thanks-for-all-the-hard-work","text":"Is there a way to omit some public symbols from the generated docs? For example, my UIView subclass overrides a series of methods like layoutSubviews that I don\u2019t want to be documented but they currently show in the documentation archive. I had a scan through the docs but couldn\u2019t seem to see a way to opt out If you prefix a public symbol with an underscore, DocC will automatically hide it from the built documentation. In your particular case, because you can't change the name of the method, that won't work. We're interested to hear more about this scenario and how it impacts your documentation via Feedback Assistant.","title":"I\u2019m loving DocC, thanks for all the hard work!"},{"location":"wwdc21/DevTools.html#ive-checked-the-session-in-which-you-cover-linking-to-types-or-methods-what-id-like-to-know-is-if-we-reference-between-documentationssymbols-of-2-different-frameworks-for-example-referencing-in-a-class-of-package-a-a-public-type-of-package-b-in-which-a-depends-and-uses-as-an-input","text":"We built DocC with a focus on having a really great documentation experience for individual frameworks and we don\u2019t currently support linking across different frameworks. We know this use case is especially important for developers who ship a collection of frameworks (maybe in a single repo) that work together. It\u2019s good to know that you\u2019re interested in this and it\u2019s (another) really great topic for discussion when we open source later this year.","title":"I've checked the session in which you cover linking to types or methods. What I'd like to know is if we reference between documentations/symbols of 2 different frameworks. For example referencing in a class of package A a public type of package B in which A depends and uses as an input."},{"location":"wwdc21/DevTools.html#is-it-possible-to-use-docc-for-local-swift-packages-were-writing-a-lot-of-documentation-for-local-modules-mostly-for-services-and-core-api-and-it-would-be-really-helpful-to-use-docc-for-them-too","text":"Yes, DocC works with both local and remote Swift packages. If you open the Swift package in Xcode, and use the Product > Build Documentation menu item, Xcode will build documentation for both that package and its dependencies (both local and remote). Note: if you use a documentation catalog in your Swift package, make sure that the Package manifest\u2019s swift-tools-version is set to 5.5 For more information about documenting a Swift package, see https://developer.apple.com/documentation/xcode/documenting-a-swift-framework-or-package","title":"Is it possible to use DocC for local Swift packages? We're writing a lot of documentation for local modules (mostly for services and core API), and it would be really helpful to use DocC for them too."},{"location":"wwdc21/DevTools.html#i-noticed-that-the-website-that-gets-generated-in-the-doccarchive-doesnt-have-search-functionality-is-there-any-plan-to-add-this-feature","text":"This is a great question, and a really good feature request. You\u2019re correct- Today, search is supported when viewing documentation archives in Xcode\u2019s documentation window and it would be great to have search available on the web as well. This is something we\u2019ll definitely be interested in discussing further with the community when we open source DocC later this year.","title":"I noticed that the website that gets generated in the .doccarchive doesn't have search functionality. Is there any plan to add this feature?"},{"location":"wwdc21/DevTools.html#wednesday","text":"Simulator Recordings were added in Xcode 12.5. Is it possible to record audio with Simulator recordings? You cannot record audio with Simulator right now. If this is a feature you would like, please file a Feedback Request.","title":"\ud83d\uddd3 Wednesday"},{"location":"wwdc21/DevTools.html#xcrun-simctl-allows-recording-a-video-from-the-simulator-within-the-terminal-can-we-progressively-read-out-the-recorded-video-while-the-recording-is-still-running-eg-start-converting-the-video-to-a-different-video-size","text":"Hi! You cannot do this with simctl . We begin recording right when the command is executed and when you press Control-C, that ends the recording. Throughout this process data is written to the file. We do not support \u201cpiping\u201d the video or performing operations on it at the present time. If this is something you would like to have, please file a feature request using Feedback Assistant.","title":"xcrun simctl allows recording a video from the Simulator within the terminal. Can we progressively read out the recorded video while the recording is still running? E.g., start converting the video to a different video size?"},{"location":"wwdc21/DevTools.html#is-it-possible-yet-to-run-a-native-ios-app-from-xcode-on-an-m1-mac-instead-of-launching-in-a-simulator","text":"Yes! You can learn more about how to run these apps from the WWDC 2020 session here: https://developer.apple.com/videos/play/wwdc2020/10114/","title":"Is it possible yet to run a native iOS app from Xcode on an M1 mac, instead of launching in a simulator?"},{"location":"wwdc21/DevTools.html#hi-do-you-plan-to-release-simulators-for-older-ios-versions-13-12-compatible-with-apple-silicon","text":"The legacy iOS and tvOS simulator runtimes are compatible with Apple Silicon Macs via Rosetta. Older watchOS simulator runtimes are i386 and are thus not compatible with Apple Silicon macs.","title":"Hi. Do you plan to release simulators for older iOS versions (13, 12) compatible with Apple Silicon?"},{"location":"wwdc21/DevTools.html#im-noticing-when-i-try-to-store-an-item-in-the-keychain-on-the-ios-simulator-with-whenpasscodesetthisdeviceonly-and-biometrycurrentset-that-it-is-failing-is-there-a-way-to-make-this-work-on-the-simulators-now-is-there-somewhere-i-can-learn-more-about-changes-here","text":"I believe Keychain requires the Secure Enclave which is a hardware feature on iOS devices and is not present in Simulators, so I don't believe Keychain testing on Simulator will work, sorry. Physical hardware will be necessary for testing Keychain features.","title":"I'm noticing when I try to store an item in the keychain on the iOS simulator with whenPasscodeSetThisDeviceOnly and biometryCurrentSet that it is failing. Is there a way to make this work on the simulators now? Is there somewhere I can learn more about changes here?"},{"location":"wwdc21/DevTools.html#in-watchos-when-i-try-to-activate-the-session-it-fails-all-the-time-but-succeeds-in-second-attempt-on-device","text":"Thanks for the question! I think we'll need some more information from you to help triage this further. Could you please submit a bug report via http://feedbackassistant.apple.com so that we can investigate this. Also, please run xcrun simctl diagnose and attach the output to the feedback issue you report. Thank you!","title":"In watchOS, when i try to activate the session it fails all the time, but succeeds in second attempt. (on device)"},{"location":"wwdc21/DevTools.html#is-it-possible-to-use-simctl-to-connect-disconnect-the-hardware-keyboard","text":"It is not possible to connect/disconnect the hardware keyboard using simctl currently. If you could file a Feedback request with details about your use case that would be really helpful.","title":"Is it possible to use simctl to connect &amp; disconnect the hardware keyboard?"},{"location":"wwdc21/DevTools.html#similar-question-to-video-is-it-possible-to-record-video-with-simulator-recordings","text":"Hi there! If you are referring to recording audio, we do not support that at the present time. Please use Feedback Assistant to file this request with us if this is important to you. I mean to record a video? It is possible to record video both from the simctl command line tool and the Simulator app. Both features work the same way, but the simctl command line tool gives you a few more options.","title":"Similar question to video,  Is it possible to record video with Simulator recordings?"},{"location":"wwdc21/DevTools.html#is-there-a-technical-reason-why-xcode-may-at-times-not-be-able-to-launch-an-app-properly-in-the-simulator-quitting-xcode-and-the-simulator-and-then-trying-again-will-resolve-the-issue-this-has-been-a-problem-for-many-years","text":"Thanks for the question! I think we'll need some more information from you to help triage this further. Could you please submit a bug report via http://feedbackassistant.apple.com so that we can investigate this. It'd be really helpful to get some diagnostics, captured after you've reproduced the issue: A sysdiagnose bundle from your host Mac (run sudo sysdiagnose and attach the resulting archive) A diagnostics bundle from the simulator. Please run xcrun simctl diagnose","title":"Is there a technical reason why Xcode may at times not be able to launch an app properly in the simulator? Quitting Xcode and the simulator and then trying again will resolve the issue. This has been a problem for many years."},{"location":"wwdc21/DevTools.html#with-the-new-announcement-of-webauthn-and-security-key-improvements-in-asauthorization-will-the-simulator-with-xcode-13-support-hardware-security-keys-so-developers-wont-need-to-build-and-debug-their-app-to-on-physical-devices","text":"Hardware security keys are not supported in the Simulator on iOS 15. It is important to test features on-device because they take advantage of functionality that's built into iPhone, but which is not always possible to simulate. If this is important for your workflow, I recommend you let us know via the Feedback Assistant at https://feedbackassistant.apple.com.","title":"With the new announcement of WebAuthn and security key improvements in ASAuthorization, will the Simulator with Xcode 13 support hardware security keys (so developers won't need to build and debug their app to on physical devices)?"},{"location":"wwdc21/DevTools.html#is-there-a-way-to-test-apps-using-groupactivities-framework-on-simulators-what-would-be-the-easiest-approach-to-test-such-apps","text":"The Group Activities APIs (for SharePlay) requires FaceTime. Unfortunately, there is no FaceTime support in the simulator, so you will need to test this aspect of your app on device.","title":"Is there a way to test apps using GroupActivities framework on Simulators? What would be the easiest approach to test such apps?"},{"location":"wwdc21/DevTools.html#enterprise-apps-from-multiple-companies-dont-run-in-ios-15-but-are-fine-on-14-and-claim-they-need-to-be-updated-can-you-shed-some-light-on-whats-actually-out-of-date-here-and-what-needs-to-be-changed","text":"-[IXSErrorPresenter presentErrorForBundleIDs:code:underlyingError:errorSource:]: Asked to present alert for error 17 source MobileInstallation underlying error Error Domain=MIInstallerErrorDomain Code=13 \"Failed to verify code signature of /var/installd/Library/Caches/com.apple.mobile.installd.staging/temp.0Ti39V/extracted/Payload/App.app : 0xe8008029 (The code signature version is no longer supported.)\" These applications use an older code signature type that is no longer supported. Apps from the App Store have been automatically re-signed, however enterprise applications cannot be automatically signed by Apple. Please contact the developers for these apps; they will need to re-sign their apps with an up-to-date certificate and profile. For more information, I recommend you visit the Security lab on Friday.","title":"Enterprise apps from multiple companies don't run in iOS 15 (but are fine on 14) and claim they need to be updated - can you shed some light on what's actually out of date here and what needs to be changed?"},{"location":"wwdc21/DevTools.html#what-is-the-currently-fastest-recommend-setup-to-deploy-a-debug-build-onto-a-watch-device-especially-with-regards-to-the-file-transfer-time-ios-and-watchos-device-connected-via-ble-and-both-in-the-same-5-ghz-hotspot-mac-xcode-device-in-the-same-wifi-other-influences-like-power-connected","text":"At present, the most reliable way to test on watch is to ensure you have a wired connection to your phone and to ensure that your phone and watch are in close proximity to eachother on the same wireless network. It is also best to limit interference from other wireless devices and ensure that you have a wireless access point close enough to maintain a constant strong signal with both devices.","title":"What is the currently fastest recommend setup to deploy a debug build onto a watch device? Especially with regards to the file transfer time. iOS and watchOS device connected via BLE and both in the same 5 GHz hotspot? Mac (Xcode device) in the same wifi? Other influences like power connected?"},{"location":"wwdc21/DevTools.html#i-have-a-set-of-test-simulators-that-i-frequently-use-as-part-of-their-initial-setup-i-will-often-install-a-network-profile-and-for-another-project-i-require-to-login-to-icloud-for-cloudkit-testing-i-notice-between-xcode-upgrades-these-changes-are-always-wiped-is-there-a-way-to-keep-them-in-place-between-iosxcode-upgrades-or-is-their-a-quick-way-to-script-provisioning-them-so-i-dont-have-to-login-to-icloud-or-install-a-profile-for-each-simulator","text":"Thanks for the great question! I think this might be an excellent opportunity to request a lab appointment with one of our Simulator engineers. It would be good to have time to talk through what your use case is, what you're seeing go wrong and when, and what options we can come up with together to make this work better for you. Please visit https://developer.apple.com/wwdc21/labs , search for Simulator in the \"Search WWDC21 labs...\" field, and request an appointment. Thank you!!","title":"I have a set of test simulators that I frequently use. As part of their initial setup I will often install a network profile, and for another project I require to login to iCloud for CloudKit testing. I notice between Xcode upgrades these changes are always wiped. Is there a way to keep them in place between iOS/Xcode upgrades, or is their a quick way to script provisioning them so I don't have to login to iCloud or install a profile for each simulator?"},{"location":"wwdc21/DevTools.html#why-is-the-xcode-wireless-connection-to-a-device-so-unreliable-i-basically-have-no-idea-when-i-will-be-able-to-use-wireless-debugging","text":"We hear your concerns about the reliability of wireless debugging, especially around needing to maintain a persistent connection to the device for a debug session. While we don't have significant improvements in this area with Xcode 13, it is an issue that we are actively investigating as we look to improve the wireless development experience.","title":"Why is the Xcode wireless connection to a device so unreliable? I basically have no idea when I will be able to use wireless debugging."},{"location":"wwdc21/DevTools.html#i-noticed-that-compiling-to-a-simulator-in-m1-still-uses-x86_64-arch-even-in-xcode-13-is-there-any-reason-for-that-why-the-simulators-arent-arm64-or-maybe-i-am-missing-some-config-i-tried-with-latest-ios-15-deployment-target","text":"Hi, the architectures that get built depend on the simulator runtime that you are targeting. Which runtime are you targeting? Do you see the same behavior when targeting a newer simulator runtime? I am targeting iOS 15 in this case, xcrun simctl list gives me: == Runtimes == iOS 15.0 (15.0 - 19A5261u) - com.apple.CoreSimulator.SimRuntime.iOS-15-0 And this is the flag that the swift compiler is using: -sdk /Users/omarzunigalagunas/Downloads/Xcode-beta.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator15.0.sdk I tested with an empty project and it compiles to arm64 but in our project it doesn't Can you check whether your project's configuration is forcing x86 output by overriding the ARCHS or EXCLUDED_ARCHS build settings?","title":"I noticed that compiling to a simulator in M1 still uses x86_64 arch even in Xcode 13, is there any reason for that? why the simulators arent arm64? Or maybe I am missing some config, I tried with latest iOS 15 deployment target"},{"location":"wwdc21/DevTools.html#is-there-a-way-to-hide-the-top-bar-on-a-simulator-that-says-ipad-8th-generation-etc-so-you-only-get-the-content-kind-of-like-the-quicktime-player-is-all-window","text":"You can change the appearance of the Simulator window by opening the Window menu and selecting Show Device Bezels. Thank you, but what I'd like is no bezel and no top bar. Is that possible? There is no configuration option like that right now. If you're trying to record or screenshot content, you can crop your recording area in the Screenshot app before capturing the content. If that's not sufficient for your use case, I'd encourage you to let us know via the Feedback Assistant at https://feedbackassistant.apple.com.","title":"Is there a way to hide the top bar on a simulator (that says \"iPad 8th generation etc\") so you only get the content, kind of like the QuickTime Player is \"all window\"?"},{"location":"wwdc21/DevTools.html#is-it-possible-to-write-to-the-apps-documents-directory-on-the-simulator-outside-of-the-app","text":"For example if we want to set up a specific state, by writing data on the documents folder, before running UI Tests. You can get the path to an application\u2019s data container for a given device with simctl: xcrun simctl get_app_container <device-id> <app-bundle-id> data The data container will contain the Documents directory. If you only have one simulator booted, you can use the device alias \"booted\" instead of specifying the device's ID.","title":"Is it possible to write to the app's Documents directory on the Simulator, outside of the app?"},{"location":"wwdc21/DevTools.html#are-there-any-xcode-12x-versions-that-support-device-debugging-when-a-device-is-running-ios-146","text":"Hi Jared! Xcode 12.5 supports devices running iOS 14.6.","title":"Are there any Xcode 12.x versions that support device debugging when a device is running iOS 14.6?"},{"location":"wwdc21/DevTools.html#for-app-store-screenshots-i-would-like-to-use-the-simulator-tool-to-take-a-picture-with-the-device-bezels-embedded-with-the-image-but-when-i-do-this-with-125-i-only-get-an-screenshot-with-a-cutout-of-the-device-shape-how-should-i-enable-it-to-capture-both-the-screen-and-the-device-bezel-together","text":"Currently we do not support this. The screenshot tool in Simulator can create images which are suitable for a variety of media. If you are trying to capture screenshots of your app for marketing materials, I suggest that you speak to the fine people over at the App Store for advice on how to take screenshots of your app for the store.","title":"For App Store screenshots, I would like to use the simulator tool to take a picture with the device bezels embedded with the image. But when I do this with 12.5, I only get an screenshot with a 'cutout' of the device shape. How should I enable it to capture both the screen and the device bezel together?"},{"location":"wwdc21/DevTools.html#is-there-any-way-to-view-the-xml-file-of-infoplist-in-xcode-13-because-its-no-longer-a-separate-file-and-inside-the-project-target-instead","text":"Hi Andrew! You should still be able to view your Info.plist file by opening it in the project navigator on the left-hand side of the Xcode window. If you want to view the Info.plist file as it appears after building your app, select your .app bundle, Control-click or right-click it, and select Show Package Contents. The Info.plist file should be in there. If this is a traditional macOS app, it may be under the Contents folder. Thanks! But when you create a new project in Xcode 13, for example in the SwiftUI App template, there's no more Info.plist in the project navigator. This is an intentional design change in Xcode 13. I know the Xcode team would love to hear what you think. Please let them know by using the Feedback Assistant at https://feedbackassistant.apple.com.","title":"Is there any way to view the XML file of Info.plist in Xcode 13? Because it's no longer a separate file, and inside the project target instead."},{"location":"wwdc21/DevTools.html#is-there-a-recommended-way-to-control-simulator-setup-when-running-ui-tests-for-example-for-ui-testing-i-need-to-disable-the-hardware-keyboard-but-for-regular-development-its-not-practical-to-do-that","text":"It\u2019s a bit disruptive having to enable/disable it each time so I\u2019m wondering if i can control this kind of thing somehow? Thanks for the great question! We have a helpful command line tool for Simulators that you can run via xcrun simctl that can do a lot of useful automation and setup. Here's a video from a previous WWDC session for more information: https://developer.apple.com/videos/play/wwdc2019/418 . If you find functionality missing from simctl, please do submit feedback through Feedback Assistant!! Also, one thing that you might try if you're doing this often... You can run all of your setup steps on a Simulator device, shut it down, then make a clone of that device with xcrun simctl clone , and then use that cloned device for testing. This might also be a good question to discuss with one of our Simulator engineers in our labs, so if that sounds like it would be helpful to you, please visit https://developer.apple.com/wwdc21/labs , search for Simulator in the \"Search WWDC21 labs...\" field, and request an appointment. Thank you!!","title":"Is there a recommended way to control simulator setup when running UI tests? For example, for ui testing I need to disable the hardware keyboard but for regular development, it\u2019s not practical to do that."},{"location":"wwdc21/DevTools.html#ill-try-again-sorry-for-not-being-clear","text":"In Xcode 13, will we be able to launch our iPhone/iOS apps natively on our M1 Macs, rather than selecting a simulator? I do not believe this is currently possible. I am asking if Xcode 13 will support it. Thank you! iOS and iPadOS apps can run natively on macOS 11.0 and later running on Apple Silicon. To run such an app from within Xcode 12.0 or later, select the scheme for your iOS or iPadOS app and choose the 'My Mac (Designed for iPad)' run destination. This run destination is suitable for all iOS and iPadOS apps.","title":"I'll try again - sorry for not being clear:"},{"location":"wwdc21/DevTools.html#is-it-possible-to-run-the-ios-1415-simulators-under-rosetta-while-running-xcode-natively-on-the-m1-mac","text":"","title":"Is it possible to run the iOS 14/15 simulators under Rosetta while running Xcode natively on the M1 Mac?"},{"location":"wwdc21/DevTools.html#a","text":"Hi Michael! I'm sorry, but we don't support this configuration. If you need this for your workflow, please let us know via the Feedback Assistant at https://feedbackassistant.apple.com.","title":"A"},{"location":"wwdc21/DevTools.html#any-recommendations-on-using-simulator-or-writing-tests-for-ar-apps-i-couldnt-find-a-way-to-effectively-test-my-app-using-simulators-we-have-both-2d-ui-written-in-swiftui-and-uikit-as-well-as-3d-ui-in-scenekit-would-be-nice-to-be-able-to-still-iterate-on-the-2d-ui-using-simulators-thanks","text":"AR requires the use of specialized camera hardware that is not present in the simulator, so AR cannot be simulated. You will need to test your AR-based UI on-device.","title":"Any recommendations on using simulator or writing tests for AR apps? I couldn't find a way to effectively test my app using simulators. We have both 2D UI written in SwiftUI and UIKit, as well as 3D UI in Scenekit. Would be nice to be able to still iterate on the 2D UI using simulators! thanks :)"},{"location":"wwdc21/DevTools.html#when-we-changed-our-ci-to-github-actions-we-started-having-issues-with-some-simulators-while-running-tests-apparently-sometimes-it-seemed-at-random-when-launching-the-simulator-this-would-try-to-do-some-kind-of-synchronisation-and-it-would-stay-idle-for-a-few-minutes-this-before-running-tests","text":"Is there a way to avoid this 'synchronisation' to happen? PS: apologies in advance if you think this is related to the CI provider and not the simulator itself. Hi there and thanks for the question! I think we'll need some more information from you to help triage this further. Could you please submit a bug report via http://feedbackassistant.apple.com so that we can investigate this. Please run xcrun simctl diagnose and attach the output to the feedback issue you report. Also, I think this might be an excellent opportunity to request a lab appointment with one of our Simulator engineers. It would be good to have time to go through what you're seeing go wrong and when, and what options we can come up with together to make this work better for you. Please visit https://developer.apple.com/wwdc21/labs , search for Simulator in the \"Search WWDC21 labs...\" field, and request an appointment. It would be really helpful if you mention the feedback number that you filed in the notes for the lab request. Thank you!! I believe issues like this occur in CI systems where \"fresh\" hosts are instantiated for each job. I've observed performance issues due to update_dyld_sim_shared_cache running after booting a simulator for the first time \u2014 which on CI tends to coincide when attempting to run a test suite. Thanks for the follow-up! I think it might still be helpful to request a lab appointment with one of our Simulator engineers to talk through the issues here. It\u2019s difficult to recommend a good solution without having a better understanding of all the issues. :) If the bottleneck you\u2019re seeing is resource constraints because the dyld shared cache is being built at the same time as your tests are running, you could try adding to your CI scripting to build the dyld shared cache first, allow that to complete, and then run your UI tests after. You can do this by running xcrun simctl runtime update_dyld_shared_cache . Please note that this command is not currently documented externally and is subject to change. If this command is helpful to you, please file a Feedback Report and ask for this functionality to be made more broadly available. If it would be helpful to have a flag for xcrun simctl boot which changed the behavior to wait until the shared cache is generated before booting the device, please file a Feedback Report for that as well. Again, it\u2019s hard to make a good recommendation without a full understanding of the problem you\u2019re trying to solve, and the best way to do that would be to request a lab appointment as mentioned above. Hope this helps!!","title":"When we changed our CI to GitHub Actions we started having issues with some simulators while running tests. Apparently sometimes (it seemed at random) when launching the simulator this would try to do some kind of \"synchronisation\", and it would stay idle for a few minutes, this, before running tests."},{"location":"wwdc21/DevTools.html#the-simulator-menu-offers-an-entry-to-set-a-custom-location-features-location-custom-location","text":"Is there an equivalent for this in simctl ? Unfortunately, there is not a way to do this via simctl at this time. Please file a feedback assistant report via http://bugreport.apple.com as this would be a great addition to make in the future.","title":"The Simulator menu offers an entry to set a Custom Location (Features &gt; Location &gt; Custom Location\u2026)"},{"location":"wwdc21/DevTools.html#i-remember-from-some-past-wwdc-session-that-the-simulator-uses-macoss-version-of-core-animation-not-the-same-ca-stack-that-runs-on-the-device-is-this-still-true-and-if-so-are-there-any-documented-differences-we-should-be-aware-of","text":"All recent versions (definitely iOS 5+ but possibly back to iOS 4) of the iOS simulator runtime have provided a version of CoreAnimation built from the same sources as the corresponding version for devices. Prior to the introduction of Metal support in the simulator, CoreAnimation went down a software rendering codepath. Since the introduction of Metal in the Simulator, CoreAnimation now uses the Metal rendering codepath for improved performance. Any difference in behavior would be considered a bug, so if you notice anything unexpected, please file a report via Feedback Assistant at http://bugreport.apple.com","title":"I remember from some past WWDC session that the Simulator uses macOS's version of Core Animation, not the same CA stack that runs on the device. Is this still true, and if so, are there any documented differences we should be aware of?"},{"location":"wwdc21/DevTools.html#i-started-playing-with-an-m1-recently-i-havent-try-it-but-is-it-possible-to-compile-the-app-for-a-device-arm64-and-use-that-build-to-run-it-in-a-simulator-arm64-this-could-give-us-some-advantages-as-being-able-to-distribute-the-app-to-devs-iphones-and-at-the-same-time-run-the-tests-in-a-simulator-by-building-only-once","text":"Thanks for the question! The answer is no; this isn\u2019t possible. Even though in your case both the device and the simulator have the same architecture, they are different platforms because Simulator runs in macOS. As a result the builds are not mutually compatible.","title":"I started playing with an M1 recently, I haven't try it but is it possible to compile the app for a device (arm64) and use that build to run it in a simulator (arm64)? This could give us some advantages as being able to distribute the app to devs iphones and at the same time run the tests in a simulator by building only once"},{"location":"wwdc21/DevTools.html#now-that-we-have-apple-silicon-are-the-ios-binaries-running-on-the-simulator-eg-launchd-springboard-the-exact-same-binaries-as-on-the-device","text":"No, applications built for the Simulator are built against the Simulator SDK, which does not support exactly the same set of features and functionality as our devices support. So apps built for Simulator can't run on a device, nor the other way around.","title":"Now that we have Apple Silicon, are the iOS binaries running on the Simulator (e.g. launchd, Springboard) the exact same binaries as on the device?"},{"location":"wwdc21/DevTools.html#i-often-get-this-error-could-not-find-module-realmswift-for-target-x86_64-apple-ios-simulator-found-arm64-arm64-apple-ios-simulator-whenever-i-use-cocoapods-im-on-an-m1-mac","text":"I've tried solutions like this one: (https://stackoverflow.com/a/58192079/14351818), but they sometimes work and sometimes don't. I've added arm64 under Excluded Architectures too, and added a script to the Podfile from here (https://stackoverflow.com/a/63955114/14351818). Any tips? It is likely the pods you're trying to use are not configured to work with the simulator you want to use, or the CocoaPods technology itself is not simulator-aware. I would recommend reaching out to the developers of the specific pods you want to use.","title":"I often get this error \"Could not find module 'RealmSwift' for target 'x86_64-apple-ios-simulator'; found: arm64, arm64-apple-ios-simulator\" whenever I use Cocoapods. I'm on an M1 mac."},{"location":"wwdc21/DevTools.html#do-we-have-any-simulators-for-macos-testing-to-cover-scenarios-like-touchid-or-clamshell-for-automation","text":"No, the Simulator does not support simulating macOS or Mac hardware features. You may find that virtualization software from third parties can help you with your testing. So Xcode cloud also will use actual devices where we would run our CI? Xcode Cloud uses the same macOS build worker to run your tests as the build worker that builds your code","title":"Do we have any simulators for macOS testing? (to cover scenarios like touchID or clamshell for automation)"},{"location":"wwdc21/DevTools.html#is-xcode-cloud-built-on-the-configuration-as-code-principle-meaning-will-we-be-able-to-have-different-configurations-in-different-branches-akin-to-github-actions-or-azure-devops-yaml-based-approaches","text":"Hi there - All of your workflow configuration is stored on the server and not checked into your repository. That being said, you can create workflows that will only start builds for specific branches or branch patterns and use specific settings (macOS & Xcode version, environment variables, etc.).","title":"Is Xcode Cloud built on the \"configuration as code\" principle? Meaning: Will we be able to have different configurations in different branches, akin to GitHub actions or Azure DevOps YAML-based approaches?"},{"location":"wwdc21/DevTools.html#what-xcode-cloud-uses-actual-devicessimulatorvirtual-machines","text":"Great question! Xcode Cloud runs your iOS, tvOS, and watchOS tests on simulators, and your macOS tests on the same build worker used to build your code.","title":"What Xcode cloud uses, actual devices/simulator/virtual machines?"},{"location":"wwdc21/DevTools.html#hi-nice-presentations-all-can-you-confirm-that-xcode-cloud-will-only-support-bitbucket-github-and-gitlab-both-in-cloud-and-on-prem-but-nothing-else-specifically-i-have-a-bare-git-repo-on-my-local-network-that-i-would-like-to-use-as-my-source-code-repo-for-xcode-cloud-is-this-possible","text":"Yes - Xcode Cloud only supports cloud and on premise instances of GitHub, Bitbucket, and GitLab. It doesn\u2019t support bare git repos on local networks.","title":"Hi, nice presentations all!  Can you confirm that Xcode Cloud will ONLY support Bitbucket, GitHub, and GitLab, both in cloud and on prem, but nothing else?  Specifically, I have a bare git repo on my local network that I would like to use as my source code repo for Xcode Cloud.  Is this possible?"},{"location":"wwdc21/DevTools.html#hello-in-our-current-ci-we-check-what-changes-the-pr-is-doing-and-analyze-our-dependecy-tree-to-find-out-which-test-bundles-should-run-then-we-use-only-testing-args-in-xcodebuild-cmd-to-filter-them-is-it-possible-to-do-so-with-xcode-cloud","text":"Hi! You can probably get something like that working by using a combination of a post-clone script, which does the analysis of the changes in the git commit, and then generate a scheme/test plan that dynamically has only the tests you want to run","title":"Hello! In our current CI we check what changes the pr is doing and analyze our dependecy tree to find out which test bundles should run, then we use -only-testing args in xcodebuild cmd to filter them, is it possible to do so with Xcode Cloud?"},{"location":"wwdc21/DevTools.html#hey-yesterday-on-a-previous-question-i-asked-about-api-availability-richie-mentioned-that-well-have-a-rest-api-during-this-summer-partying_face","text":"I have a follow up question about this though, will we be able to trigger a certain workflow from an API call while this workflow doesn\u2019t run on any git event (pr, tag change, commit, scheduled event, etc.). Basically, we want to control part of our release process from our custom Jenkins orchestration (that we have today) and trigger some of the workflow tasks on demand from there. Thanks! While we can't commit to the shape of the API right now, I think it will be a pretty common use case and a valid expectation to have w.r.t Xcode Cloud's public API.","title":"Hey! Yesterday on a previous question I asked about API availability, Richie mentioned that we\u2019ll have a REST API during this summer :partying_face:"},{"location":"wwdc21/DevTools.html#can-xcode-cloud-provide-any-support-for-developer-id-workflows-either-complete-products-or-re-signable-ones","text":"Hi! Xcode Cloud will code sign macOS builds with Developer ID, and for the Mac App Store","title":"Can Xcode Cloud provide any support for Developer ID workflows? Either complete products or re-signable ones?"},{"location":"wwdc21/DevTools.html#when-can-we-expect-invites-to-xcode-cloud-after-submitting-a-request","text":"Thanks for the question Edward! We\u2019re all excited about Xcode Cloud and we want to make using it the best possible experience. We'll be gradually adding users throughout the summer and fall. Account holders will be notified by email when your team is accepted.","title":"When can we expect invites to Xcode Cloud after submitting a request?"},{"location":"wwdc21/DevTools.html#beyond-the-obvious-fact-that-apple-is-handling-the-server-setup-and-maintenance-are-there-any-features-of-xcode-cloud-not-available-on-xcode-server","text":"Hi! Xcode Cloud is a totally different product from Xcode Server, with many differences and improvements over Xcode Server. I'd recommend looking at the documentation if you're interested in specific features.","title":"Beyond the obvious fact that Apple is handling the server setup and maintenance, are there any features of Xcode Cloud not available on Xcode Server?"},{"location":"wwdc21/DevTools.html#does-xcode-cloud-also-support-dependencies-from-private-repositories-ie-the-project-having-private-swift-framework-dependencies-that-are-hosted-on-github-privately-or-even-on-a-local-github-server","text":"Definitely! This is covered by today's session \"Customize your advanced Xcode Cloud workflows\": https://developer.apple.com/videos/play/wwdc2021/10269/","title":"Does Xcode cloud also support dependencies from private repositories? I.e. the project having private Swift framework dependencies that are hosted on GitHub privately or even on a local GitHub server."},{"location":"wwdc21/DevTools.html#in-our-current-ci-we-need-to-use-internal-metrics-endpoints-behind-a-vpn-so-our-secops-team-exposed-en-external-endpoint-and-ip-whitelisted-it-so-we-can-access-it-is-xcode-cloud-infra-will-have-a-fixed-public-ip-range-so-we-can-do-the-same","text":"Hello! Yes, there will be a fixed IP range for all egress from Xcode Cloud. Over the summer details will be published in the documentation.","title":"In our current CI we need to use internal metrics endpoints behind a vpn so our secops team exposed en external endpoint and ip whitelisted it so we can access it, is Xcode Cloud infra will have a fixed public ip range so we can do the same?"},{"location":"wwdc21/DevTools.html#if-your-git-repo-is-in-gitlab-self-hosted-because-you-want-the-source-code-to-be-secure-and-not-hosted-in-the-cloud-what-happens-to-the-source-code-during-the-xcode-cloud-cicd-a-process-is-it-transmitted-to-your-servers-encrypted-how-long-is-the-source-kept-on-your-servers-after-compilation-and-deployment","text":"Xcode Cloud will securely clone your repo from your GitLab instance. Your source code is kept on the build worker for the duration of the build, then the build worker (and your source code) is immediately discarded","title":"If your git repo is in GitLab self hosted, because you want the source code to be secure and not hosted in the cloud, what happens to the source code during the Xcode Cloud CI/CD a process - is it transmitted to your servers encrypted?  How long is the source kept on your servers after compilation and deployment?"},{"location":"wwdc21/DevTools.html#hi-i-love-what-i-have-seen-so-far-thanks-for-the-hard-work-in-our-company-we-rely-heavily-on-artifatory-for-our-mobile-builds-some-of-the-interactions-we-have-require-authentication-via-netrc-file-configuration-is-this-something-that-we-will-be-able-to-achieve-in-xcode-cloud","text":"Thank you! Glad you like it! I'd recommend looking into the custom scripting and environment variables documentation (and the Advanced Skywagon session). You could generate a .netrc file in a script, populate an access token from a secure environment variable.","title":"Hi! I love what I have seen so far. Thanks for the hard work. In our company we rely heavily on Artifatory for our mobile builds. Some of the interactions we have require authentication via .netrc file configuration. Is this something that we will be able to achieve in Xcode Cloud ?"},{"location":"wwdc21/DevTools.html#we-have-precompiled-binaries-xcframeworks-that-swift-package-manager-downloads-from-an-private-server-the-package-manager-authenticates-using-a-netrc-file-would-i-be-able-to-securely-provide-a-netrc-file-with-the-proper-credentials-to-xcode-cloud-so-it-can-fetch-these-packages","text":"Xcode Cloud will no generate .netrc files for you, but you can create your own as part of a custom build script. For example, you could create the file in a ci_postclone.sh file and use a secret environment variable to store the token you want to use. Both custom scripts and (secret) environment variables are covered in today's session: https://developer.apple.com/videos/play/wwdc2021/10269/","title":"We have precompiled binaries (XCFrameworks) that swift package manager downloads from an private server. The package manager authenticates using a .netrc file. Would I be able to securely provide a .netrc file with the proper credentials to Xcode Cloud so it can fetch these packages?"},{"location":"wwdc21/DevTools.html#will-xcode-cloud-require-that-we-push-the-app-including-the-framework-dependencies-added-by-swift-package-manager-or-cocoapods","text":"Hi! Xcode Cloud requires the source of you app, and any non-binary dependencies to be available in the build environment.","title":"Will Xcode cloud require that we push the app including the framework dependencies added by swift package manager or cocoapods?"},{"location":"wwdc21/DevTools.html#xcode-cloud-is-great-for-ci-would-be-eventually-be-able-to-avoid-building-locally-and-get-the-built-products-from-the-build-servers","text":"Xcode Cloud is built specifically with CI in mind. Having said that, you can download all artifacts produced from each build.","title":"Xcode cloud is great for CI, would be eventually be able to avoid building locally and get the built products from the build servers?"},{"location":"wwdc21/DevTools.html#we-are-currently-building-in-our-ci-using-macstadium-orka-which-adds-a-virtualisation-layer-we-can-deploy-vms-with-different-cpu-configurations-depending-how-time-critical-the-task-we-need-to-perform-is-eg-prs-have-faster-machines-for-better-developer-feedback-release-builds-work-in-slower-vms-because-we-are-not-in-a-rush-will-something-similar-be-possible-with-xcode-cloud","text":"Great question! Xcode Cloud was designed so that developers no longer have to maintain a CI build environment configuration","title":"We are currently building in our CI using Macstadium + ORKA, which adds a virtualisation layer. We can deploy VMs with different CPU configurations depending how time critical the task we need to perform is (e.g. PRs have faster machines for better developer feedback, release builds work in slower VMs because we are not in a rush). Will something similar be possible with Xcode Cloud?"},{"location":"wwdc21/DevTools.html#is-it-possible-to-configure-a-workflow-to-skip-certain-commits-eg-we-use-no-ci-prefix-in-the-commit-message-to-have-our-jenkins-ignore-a-it","text":"That's not something that's supported right now but that's a great idea! Can you please file a feedback about this?","title":"Is it possible to configure a workflow to skip certain commits (e.g. we use [NO-CI] prefix in the commit message to have our Jenkins ignore a it)"},{"location":"wwdc21/DevTools.html#we-will-able-to-select-the-stack-for-example-m1-over-intel-computers-we-noticed-m1-has-a-huge-impact-in-compilation-time","text":"Xcode Cloud was designed so that developers no longer have to maintain a CI build environment configuration.","title":"We will able to select the stack? For example M1 over Intel computers, we noticed M1 has a huge impact in compilation time"},{"location":"wwdc21/DevTools.html#in-our-ci-we-do-commits-eg-to-bump-the-version-and-build-number-of-the-app-and-push-it-to-the-repo-will-we-be-able-to-commit-and-push-from-xcode-cloud-from-the-scripts","text":"Xcode Cloud will actually manage the build number (CFBundleVersion) automatically for you - so likely you won't need to do those kinds of pushes. However if you do need to, you can add a custom script that calls various git commands - which will then trigger another workflow I have a follow up question for this one. Will Xcode Cloud provide an option to somehow format the build number? That's not something that's supported right now but that's a great idea! Can you please file a feedback about this?","title":"In our CI we do commits (e.g. to bump the version and build number of the app) and push it to the repo. Will we be able to commit and push from Xcode Cloud from the scripts?"},{"location":"wwdc21/DevTools.html#does-xcode-cloud-run-tests-on-devices-or-simulators","text":"Thanks for your question. Xcode Cloud runs tests on simulators.","title":"Does Xcode Cloud run tests on devices or simulators?"},{"location":"wwdc21/DevTools.html#can-we-set-baseline-metrics-for-performance-tests-from-xcode-cloud","text":"You\u2019re not able to set baseline metrics for performance test, but please file a request through feedback assistant!","title":"Can we set baseline metrics for performance tests from Xcode cloud?"},{"location":"wwdc21/DevTools.html#would-xcode-cloud-support-beta-versions-of-xcode","text":"Totally! You can configure a workflow to use a specific version of Xcode (including betas), or always use the latest version available (including betas).","title":"Would Xcode Cloud support beta versions of Xcode ?"},{"location":"wwdc21/DevTools.html#does-xcode-cloud-work-for-enterprise-accounts","text":"To use Xcode Cloud, you need to be enrolled in the Apple Developer Program. https://developer.apple.com/documentation/xcode/requirements-for-using-xcode-cloud","title":"Does Xcode Cloud work for Enterprise Accounts?"},{"location":"wwdc21/DevTools.html#would-xcode-cloud-support-build-systems-like-bazel","text":"Xcode Cloud supports custom build scripts, so you can run bazel commands if you need to build additional tools as part of your build process.","title":"Would Xcode cloud support build systems like bazel?"},{"location":"wwdc21/DevTools.html#currently-we-create-alpha-builds-for-qa-on-a-regularly-basis-using-an-enterprise-certificate-we-distribute-the-alpha-builds-using-bitrise-on-every-alpha-release-we-notify-qa-via-a-slack-channel-and-they-get-a-short-description-of-the-release-a-download-url-as-text-and-qr-code-to-ease-installation-from-device-would-i-be-able-to-have-the-same-flow-using-xcode-cloud-and-would-i-be-able-to-distribute-enterprise-builds-via-testflight","text":"Hi! You can definitely achieve this workflow with Xcode Cloud - in fact we expect it would be significantly easier. You can set up a workflow that deploys to an internal TestFlight group representing your QA team (and also notifies in slack). They can then install the build through the TestFlight app. You don't need to worry about Enterprise signing to do this","title":"Currently we create alpha builds for QA on a regularly basis using an Enterprise certificate. We distribute the alpha builds using Bitrise. On every alpha release we notify QA via a slack channel and they get a short description of the release, a download url as text and QR code (to ease installation from device). Would I be able to have the same flow using Xcode Cloud? And would I be able to distribute enterprise builds via TestFlight?"},{"location":"wwdc21/DevTools.html#will-app-extensions-be-supported-in-any-capacity-when-building-apps-on-ipad","text":"Swift Playgrounds 4 supports making apps for iPhone and iPad, in addition to the playgrounds and playground books already supported by Swift Playgrounds. Thanks! Specifically, I'm curious about whether any app extensions, action or intent extensions for example, might be available to include in the initial version of the new project type, or if it will be limited to just \u2018standard\u2019 apps without these kind of features for now? :slightly_smiling_face: Those are some awesome ideas! We don't have any specific plans to support that this year but we encourage you to file a Feedback to state your support for those features.","title":"Will app extensions be supported in any capacity when building apps on iPad?"},{"location":"wwdc21/DevTools.html#when-building-on-ipad-will-it-be-possible-to-install-apps-to-the-ipad-homescreen-locally-or-might-this-require-a-baseline-of-submitting-through-testflight","text":"Great question! If you want to install the app you're making in Swift Playgrounds to your device, you can sign into your developer account and submit the app to TestFlight and then install it on your iPad or other devices using the TestFlight app.","title":"When building on iPad, will it be possible to install apps to the iPad homescreen locally? Or might this require a baseline of submitting through Testflight?"},{"location":"wwdc21/DevTools.html#is-swift-playgrounds-expected-to-only-support-swiftui-or-will-it-be-open-to-other-frameworks-like-uikit-and-3rd-party-frameworks","text":"Swift Playgrounds supports both UIKit and SwiftUI. SwiftUI Previews requires a SwiftUI app, but you can always use UIViewRepresentable to show UIKit content in SwiftUI. Thank you! To follow up on this, what is the planned support for 3rd party frameworks? Will it be possible to use say, Snapkit? Playgrounds supports working with Swift Packages. SwiftUI Previews (which is in Swift Playground 4) allows you to preview SwiftUI content. To use something like SnapKit you'd have to use it with UIKit and use UIViewRepresentable","title":"Is Swift Playgrounds expected to only support SwiftUI, or will it be open to other frameworks like UIKit and 3rd party frameworks?"},{"location":"wwdc21/DevTools.html#im-very-excited-for-the-upcoming-features-to-enable-building-apps-on-ipad-but-details-in-the-main-keynote-were-very-light-is-there-anywhere-i-can-learn-more-and-are-you-able-to-answer-questions-on-it","text":"At this moment, the available details are available at the State of The Union video https://developer.apple.com/wwdc21/102 at 0:33","title":"I'm very excited for the upcoming features to enable building apps on iPad, but details in the main Keynote were very light. Is there anywhere I can learn more, and are you able to answer questions on it?"},{"location":"wwdc21/DevTools.html#im-sure-youre-getting-this-question-a-lot-any-word-on-when-a-swift-playgrounds-4-beta-will-be-available-smile-so-excited-to-try-it-out","text":"Swift Playgrounds 4 will be available later this year.","title":"I\u2019m sure you\u2019re getting this question a lot - any word on when a Swift Playgrounds 4 beta will be available? :smile: So excited to try it out!"},{"location":"wwdc21/DevTools.html#is-it-possible-to-import-spm-packages-into-swift-playgrounds-that-would-be-really-helpful-in-using-playgrounds-as-a-tool-to-build-more-richly-featured-apps-without-having-to-traverse-over-to-xcode","text":"Yes! The apps you build can depend on publicly-available Swift Packages. Hmm.. so I won't be able to use my internal (non-public) Swift packages when building on iPad? At this time, we do not plan to support packages that require authentication","title":"Is it possible to import SPM packages into Swift Playgrounds?  That would be really helpful in using Playgrounds as a tool to build more richly featured apps without having to traverse over to Xcode."},{"location":"wwdc21/DevTools.html#will-iphone-and-ipad-be-the-only-devices-supported-in-the-next-version-of-swift-playgrounds-when-building-apps-or-will-developers-be-able-to-build-for-watchos-and-macos-as-well","text":"Apps built using Swift Playgrounds are universal, so when they\u2019re distributed using TestFlight or the App Store, they\u2019ll run on iPhone, iPad, and Apple Silicon Macs.","title":"Will iPhone and iPad be the only devices supported in the next version of Swift Playgrounds when building Apps, or will developers be able to build for watchOS and macOS as well?"},{"location":"wwdc21/DevTools.html#to-build-and-deploy-to-the-appstore-from-ipados-using-swift-playgrounds-do-you-need-to-be-a-subscriber-of-xcode-cloud","text":"Nope! Anyone with a developer account will be able to submit their apps to TestFlight and the App Store.","title":"To build and deploy to the AppStore from iPadOS using Swift Playgrounds, do you need to be a subscriber of Xcode Cloud?"},{"location":"wwdc21/DevTools.html#hi-will-swift-playgrounds-4-for-ipad-have-the-ability-to-add-dependencies-on-third-party-swift-packages","text":"Yes! The apps you build can depend on publicly-available Swift Packages.","title":"Hi! Will Swift Playgrounds 4 for iPad have the ability to add dependencies on third party Swift Packages?"},{"location":"wwdc21/DevTools.html#what-does-the-workflow-look-like-if-you-wanted-to-work-from-mac-to-ipad-or-vice-versa-is-it-all-icloud-synced-or-will-it-connect-to-githubgitlabother-git-syncing-services","text":"Swift Playgrounds already works great with iCloud Drive (or any other file provider extension) and will continue to do so. We don't have anything to announce at this time about other mechanisms.","title":"What does the workflow look like if you wanted to work from Mac to iPad or vice versa? Is it all iCloud synced or will it connect to Github/Gitlab/other Git syncing services?"},{"location":"wwdc21/DevTools.html#will-i-be-able-to-see-what-my-app-looks-like-on-different-device-sizes-and-orientations-from-within-swift-playgrounds","text":"Yes! You can take your app preview fullscreen, and put Swift Playgrounds into split-view multitasking to test a bunch of different configurations. You can also write custom Preview Providers to try even more layouts and configurations.","title":"Will I be able to see what my app looks like on different device sizes and orientations from within Swift Playgrounds?"},{"location":"wwdc21/DevTools.html#will-folks-be-able-to-open-existing-xcode-projects-in-swift-playgrounds-4-or-will-a-project-have-to-be-migrated-to-some-sort-of-playgroundbook","text":"Swift Playgrounds 4 has a new project format based on the Swift Package format. Can you edit this format in Xcode without conversion? So you can start working on a app in Xcode and then continue on iPad and vice versa? Yes! App projects can be moved seamlessly between Swift Playgrounds and Xcode, so you can work on whichever device you prefer.","title":"Will folks be able to open existing Xcode projects in Swift Playgrounds 4, or will a project have to be migrated to some sort of .playgroundbook?"},{"location":"wwdc21/DevTools.html#it-was-mentioned-i-think-that-playgrounds-apps-destined-for-the-app-store-could-be-brought-over-to-xcode-can-it-go-round-trip-back-to-playgrounds-especially-wondering-if-you-could-use-xcode-to-define-storyboards-or-data-models-to-use-and-bring-those-into-playgrounds","text":"Thanks for the great question! You will be able to open Swift Playgrounds projects in Xcode since they're based on the Swift package format! Note that Xcode has more tools than Playgrounds will be able to support. If you end up editing the Playgrounds project in Xcode in such a way that won't be supported by Playgrounds, you'll see a warning in Xcode.","title":"It was mentioned I think that Playgrounds apps destined for the app store could be brought over to Xcode, can it go round trip back to Playgrounds?  Especially wondering if you could use Xcode to define storyboards or data models to use, and bring those into Playgrounds."},{"location":"wwdc21/DevTools.html#any-plans-for-further-playground-books-like-cipher-the-existing-content-is-brilliant-as-were-the-sessions-on-swans-quest-last-year-the-additions-like-sensor-arcade-and-the-accompanying-templates-are-inspirational-especially-in-the-classroom","text":"We\u2019re so glad you like the content! We haven\u2019t announced anything about that. Stay tuned for future updates!","title":"Any plans for further playground books like Cipher? The existing content is brilliant, as were the sessions on Swan's Quest last year. The additions like Sensor Arcade and the accompanying templates are inspirational, especially in the classroom..."},{"location":"wwdc21/DevTools.html#will-swift-playgrounds-have-any-kind-of-simulator-or-are-swiftui-projects-running-as-previews","text":"Howdy! Swift Playgrounds projects will allow you to make iOS apps with the SwiftUI lifecycle \u2014 you'll be able to see your app running in Playgrounds, and you'll be able to create preview providers to preview specific views.","title":"Will Swift Playgrounds have any kind of simulator, or are SwiftUI projects running as previews?"},{"location":"wwdc21/DevTools.html#what-frameworks-are-unavailable-in-swift-playgrounds-created-apps","text":"Howdy! While we can't go into details about what frameworks Swift Playgrounds will or will not support \u2014 I'm curious about what you're hoping to achieve! Feel free to respond in a followup or please send us feedback for your hopes, dreams, and expectations!","title":"What frameworks are unavailable in Swift Playgrounds created apps?"},{"location":"wwdc21/DevTools.html#is-the-macos-version-of-the-playgrounds-app-actively-being-maintained-should-we-expect-it-to-be-able-to-build-apps-as-well","text":"For this year, we focused on bringing support for building apps to iPad.","title":"Is the MacOS version of the Playgrounds app actively being maintained? Should we expect it to be able to build apps as well?"},{"location":"wwdc21/DevTools.html#does-swift-playgrounds-4-support-multi-windowscenes-ie-can-i-have-my-editor-as-one-full-screen-scene-and-use-slide-over-for-my-preview-additionally-does-it-have-support-for-a-secondary-display-airplayusb-to-go-along-with-multiple-sceneswindows","text":"We're excited to hear about the ways you want to use Swift Playgrounds! We'd love to have folks send us their desires and expectations via feedback! :)","title":"Does Swift Playgrounds 4 support multi-window/scenes? i.e. Can I have my editor as one full screen scene and use slide over for my preview? Additionally, does it have support for a secondary display (AirPlay/USB) to go along with multiple scenes/windows?"},{"location":"wwdc21/DevTools.html#what-are-the-requirements-for-swift-playgrounds-can-you-build-apps-on-a-2018-a12-ipad-pro-for-example","text":"You'll be able to build apps in Swift Playgrounds on any iPad running iPadOS 15!","title":"What are the requirements for Swift Playgrounds? Can you build apps on a 2018 (A12) iPad Pro for example?"},{"location":"wwdc21/DevTools.html#will-it-he-possible-for-a-xcode-project-to-be-opened-in-swift-playgrounds-and-for-a-swift-playground-in-xcode","text":"Projects in Swift Playgrounds 4 are built on top of the Swift Package format, and they work great in Xcode. If you make a change to your project in Xcode, and Xcode knows that it won\u2019t be compatible with Swift Playgrounds, it will show a warning.","title":"Will it he possible for a xcode project to be opened in swift playgrounds? And for a swift playground in xcode?"},{"location":"wwdc21/DevTools.html#are-there-any-common-app-capabilities-that-i-could-create-using-swiftui-swift-on-xcode-that-i-would-not-be-able-to-create-in-swift-playgrounds-or-apple-frameworks-that-i-could-not-use-in-swift-playgrounds","text":"Swift Playgrounds 4 will have wide support for Apple\u2019s SDKs. We believe some truly powerful apps will be built with Swift Playgrounds 4, and it will be easy to bring your projects into Xcode when you need more power.","title":"Are there any common app capabilities that I could create using SwiftUI &amp; Swift on Xcode that I would not be able to create in Swift Playgrounds? Or Apple Frameworks that I could not use in Swift Playgrounds?"},{"location":"wwdc21/DevTools.html#the-state-of-the-union-discussed-moving-an-app-project-from-swift-playgrounds-4-on-ipad-over-to-xcode-on-the-mac-will-this-be-possible-in-the-reverse-can-i-edit-the-project-in-xcode-and-then-bring-it-back-to-swift-playgrounds","text":"Hi Greg. Yes the new project format will allow for editing the project in Xcode and bringing it back into Swift Playgrounds!","title":"The State of the Union discussed moving an app project from Swift Playgrounds 4 on iPad over to Xcode on the Mac. Will this be possible in the reverse? Can I edit the project in Xcode and then bring it back to Swift Playgrounds?"},{"location":"wwdc21/DevTools.html#can-we-release-to-the-app-store-from-playgrounds-today-to-adhoc-distribution-do-we-need-to-wait-until-something-comes-out-of-beta-before-this-flow-works-thank-you","text":"Building apps, including submitting them to the App Store, is new in Swift Playgrounds 4. Swift Playgrounds 4 will be available later this year.","title":"Can we release to the app store from playgrounds today? To adhoc distribution? Do we need to wait until something comes out of beta before this flow works? Thank you."},{"location":"wwdc21/DevTools.html#_1","text":"What are some neat benefits of writing and running full iOS/iPadOS apps on iPad that may not be immediately obvious without having experienced using the app? This is an incredible question! Making iPadOS apps right on the iPad is an incredible experience. It's easy to get a feel for how your app will behave as a part of the amazing iPadOS system experience like when in fullscreen, a multitasking split size, and in slide over. You can test out how your app responds to a software keyboard present and how it changes when you attach a hardware keyboard. You can check how your UI responds to hover effects with the trackpad, and you can test out great gestures right in the canvas with your finger(s)! We're so excited to see what y'all come up with when working in Swift Playgrounds 4. If any of these things spark inspiration or excitement about Playgrounds, we'd love to hear your desires and expectations via feedback!","title":""},{"location":"wwdc21/DevTools.html#might-external-display-support-be-considered-for-preview-rendering-at-various-device-sizes-to-reserve-more-screen-space-for-code","text":"That's an awesome idea! Please file an enhancement request through Feedback Assistant: https://developer.apple.com/bug-reporting/","title":"Might external display support be considered for preview rendering at various device sizes, to reserve more screen space for code?"},{"location":"wwdc21/DevTools.html#might-the-upcoming-playgrounds-release-include-support-for-find-replace-or-refactoring-renaming-variables-or-indeed-any-declaration-can-be-quite-painful-without-it-or-if-you-cant-comment-are-there-any-good-workarounds-for-this-currently","text":"These are some awesome feature ideas! While we can't comment on plans for the future, we highly encourage you to submit an enhancement request through the Feedback Assistant: https://developer.apple.com/bug-reporting/","title":"Might the upcoming Playgrounds release include support for find &amp; replace, or refactoring? Renaming variables, or indeed any declaration, can be quite painful without it. Or if you can't comment - are there any good workarounds for this currently?"},{"location":"wwdc21/DevTools.html#will-swift-playgrounds-support-all-the-latest-new-technologies-swift-55-is-introducing-asyncaway-actorsetc","text":"Yes! Swift Playgrounds 4 will ship with the Swift 5.5 compiler, including support for structured concurrency.","title":"Will Swift Playgrounds support all the latest new technologies Swift 5.5 is introducing ? (Async/Away, Actors...etc)"},{"location":"wwdc21/DevTools.html#does-swift-playgrounds-support-using-packages-with-swift-package-manager-for-example-using-a-networking-library-when-building-a-swiftui-app-on-the-ipad","text":"Yes! The apps you build can depend on any publicly-available Swift Package. You are specifically saying \u201cpublicly available\u201d. So I assume there will be no support for signing into a GitHub account for example, or to provide SSH credentials for a non-public repository at the beginning, right? That's correct\u2014at this time, we do not plan to support packages that require authentication.","title":"Does Swift Playgrounds support using packages with Swift Package Manager? For example, using a networking library when building a SwiftUI app on the iPad."},{"location":"wwdc21/DevTools.html#what-kind-of-apps-can-be-created-and-submitted-to-the-app-store-ipad-and-iphone-only-or-macos-apps-as-well-even-if-only-through-catalyst","text":"Apps built using Swift Playgrounds are built universal against the iOS SDK, so when they\u2019re distributed using TestFlight or the App Store, they\u2019ll run on iPhone, iPad, and Apple Silicon Macs.","title":"What kind of apps can be created and submitted to the App Store? iPad and iPhone only? Or macOS apps as well, even if only through Catalyst?"},{"location":"wwdc21/DevTools.html#thursday","text":"What are best practices for including and managing a lot of Swift Package dependencies in a project? Adding them to the project file doesn't scale well after more than a handful. I believe this WWDC talk is a great resource for Swift Package Management Best Practices: https://developer.apple.com/videos/play/wwdc2019/408/","title":"\ud83d\uddd3 Thursday"},{"location":"wwdc21/DevTools.html#so-excited-about-the-vim-keybindings-i-have-waited-more-than-a-decade-for-it","text":"Does the team have a plan for supporting a richer set (repeat command (.), replace command \u00ae, macros, \u2026) Good morning, and thanks for taking the time to ask this question! If there are suggestions or features your would like please submit them using the feedback assistant on https://developer.apple.com/bug-reporting/","title":"So excited about the Vim keybindings (I have waited more than a decade for it!)."},{"location":"wwdc21/DevTools.html#i-am-seeing-that-sometimes-xcode-13-cant-optclick-on-symbols-cant-find-quick-help-it-works-on-some-frameworks-methods-but-not-on-class-properties-and-methods-and-frameworks-from-cocoapods-most-of-the-time-the-same-click-worked-in-xcode-12-i-cant-attach-a-screenshot-to-the-question-haha","text":"That sounds like an issue we'd like to investigate further. If you can reproduce it, would you mind filing a bug report using feedback assistant? If you can run Xcode with some logging enabled, it would help us understand it better. Try running Xcode from Terminal like this, env SOURCEKIT_LOGGING=3 /path/to/Xcode.app/Contents/MacOS/Xcode , and then reproduce the problem by opt+clicking the symbol. If you can also provide a screen recording demonstrating the problem, it would help.","title":"I am seeing that sometimes Xcode 13 cant opt+Click on symbols can't find quick help. It works on some frameworks methods, but not on class properties and methods, and frameworks from cocoapods most of the time. The same click worked in Xcode 12. I can't attach a screenshot to the question haha."},{"location":"wwdc21/DevTools.html#are-you-taking-xcodebuild-questions","text":"If I try to create an xcframework ( xcodebuild -create-xcframework ... ) that contains both Intel and ARM simulator frameworks I get the error Both ios-arm64-simulator and ios-x86_64-simulator represent two equivalent library definitions. Has this been fixed in Xcode 13? It sounds like you have two separate thin (single-architecture) binaries, one for each arch, but both for the simulator. XCFrameworks don't support that; there needs to be only a single multi-arch binary. You can use the lipo tool (which is what Xcode uses) to combine your binaries if you're not using Xcode to build the product already.","title":"Are you taking xcodebuild questions? :)"},{"location":"wwdc21/DevTools.html#does-the-new-xcode-pr-review-feature-supports-a-strategy-with-forking-so-far-i-can-only-create-a-pr-that-points-my-forks-main-branch-not-the-original-repo-of-my-fork","text":"Xcode\u2019s pull request feature doesn\u2019t work with forked repositories. It doesn't support submitting pull requests to an upstream repository.","title":"Does the new Xcode PR review feature supports a strategy with forking? So far I can only create a PR that points my fork's main branch, not the original repo of my fork."},{"location":"wwdc21/DevTools.html#when-you-create-a-new-project-using-the-ios-app-template-and-select-swiftui-for-the-interface-there-is-no-more-infoplist-file-instead-i-can-find-it-inside-project-targets-info-however-how-can-i-access-the-files-xml-source-code","text":"You should still be able to find the Info.plist file in the project directory under: <Project Root Directory>/<Project Name>/Info.plist . From there, it can be edited with your favorite editor, including Xcode. It can also be opened quickly in Xcode by using Cmd + Shift + O and searching for Info.plist.","title":"When you create a new project using the iOS App template, and select SwiftUI for the interface, there is no more Info.plist file. Instead, I can find it inside Project -&gt; Targets -&gt; Info. However, how can I access the file's XML source code?"},{"location":"wwdc21/DevTools.html#our-repo-doesnt-commit-in-the-xcodeproj-but-generates-it-locally-on-the-fly-does-xcode-cloud-support-a-case-when-a-project-is-generated-or-modified-in-the-post-clone-script","text":"Yes. Bear in mind that creating your first workflow with Xcode Cloud happens from Xcode. It analyzes your project locally, suggests a configuration to quickly create you first workflow. In the scenario where you do not check in your Xcode project file, then these are the adjusted setup steps: You need to generate the Xcode project file locally. To exclude it from your repo, you could add a .gitignore rule. Add the add Xcode project generation to post-clone script. Create first workflow from Xcode. Create first workflow from Xcode: https://developer.apple.com/documentation/xcode/configuring-your-first-xcode-cloud-workflow Post clone script: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts","title":"Our repo doesn't commit in the .xcodeproj but generates it locally on the fly. Does Xcode Cloud support a case when a project is generated or modified in the post-clone script?"},{"location":"wwdc21/DevTools.html#is-there-a-way-to-quickly-mark-a-swift-spm-package-for-editing-in-xcode-13-similar-to-how-you-can-swift-package-edit-at-the-command-line-or-is-it-still-required-that-i-separately-clone-the-package-and-drag-it-into-my-project","text":"The workflow for this remains unchanged from previous versions of Xcode.","title":"Is there a way to quickly mark a Swift (SPM) Package for editing in Xcode 13? (Similar to how you can swift package edit at the command-line), or is it still required that I separately clone the package, and drag it into my project?"},{"location":"wwdc21/DevTools.html#how-did-the-team-prioritize-which-new-features-to-build-vs-which-features-to-work-on-later","text":"One of the many considerations we take into account would be what we receive from Developers via the Feedback application. Please send us your suggestions \u2014you can send those with the Feedback Assistant. https://developer.apple.com/bug-reporting/","title":"How did the team prioritize which new features to build vs which features to work on later?"},{"location":"wwdc21/DevTools.html#creating-a-new-project-with-xcode-13-doesnt-show-up-the-products-directory-in-the-project-navigator-is-it-possible-to-add-it-back-per-project-or-globally","text":"The Products directory will no longer be shown by default in newly created projects, as you noticed. You can still get to the directory with your build products by using the new Product -> Show Build Folder in Finder menu item. If there is a reason why you would want to always show the products directory in the project navigator, we'd love for you to file a Feedback via Feedback Assistant to explain your use case. https://developer.apple.com/bug-reporting/","title":"Creating a new project with Xcode 13 doesn't show up the Products directory in the Project Navigator, is it possible to add it back per project or globally?"},{"location":"wwdc21/DevTools.html#what-powers-xcodes-new-swift-syntax-highlighting-it-looks-very-similar-to-the-semantic-highlighting-recently-merged-into-sourcekit-lsp","text":"If sourcekit-lsp is being used, will future improvements be available in Xcode too, and will Xcode's features (like live issues) be available in sourcekit-lsp at some point? Xcode 13 has a brand new syntax highlighting implementation for Swift, which is optimized for performance. This is unrelated to sourcekit-lsp. If you see any issues, please report feedback: https://developer.apple.com/bug-reporting/","title":"What powers Xcode's new Swift syntax highlighting? It looks very similar to the semantic highlighting recently merged into sourcekit-lsp!"},{"location":"wwdc21/DevTools.html#what-is-recommended-option-of-adding-spm-dependency-from-private-github-repo-am-i-right-that-for-xcode-it-is-https-url-accompanied-by-xcode-github-account-question-if-i-need-to-connect-two-private-repos-from-github-and-one-uses-account1-another-uses-account2-how-to-specify-what-account-to-use-with-what-repository","text":"You're correct that when using privately hosted packages in Xcode you'll want to be signed into your source control account(s) (in Preferences \u2192 Accounts). Authentication should be handled automatically and should choose the correct account for each package. If you run in to any issues, we'd really love to hear about it via Feedback Assistant. :bow:","title":"What is recommended option of adding SPM dependency from private GitHub repo? Am I right, that for Xcode it is https:// url accompanied by Xcode GitHub account? Question: if I need to connect two private repos from GitHub, and one uses account1, another uses account2. How to specify, what account to use with what repository?"},{"location":"wwdc21/DevTools.html#what-is-the-simplest-way-to-view-a-diff-of-changes-to-the-documentation-sometimes-ill-find-on-twitter-from-an-apple-engineer-or-writer-that-particular-docs-have-been-updated-but-im-curious-if-theres-a-more-consolidated-and-comprehensive-way-to-find-docs-updates","text":"I checked in with the documentation team on this \u2013 while it's possible to view a diff to see what changed for individual APIs, there is not currently an overarching view that would show all the diffs between two versions. If that's something you'd find useful, please do submit that feedback via feedbackassistant.apple.com for the documentation team to look into!","title":"What is the simplest way to view a diff of changes to the documentation? Sometimes I'll find on Twitter from an Apple engineer or writer that particular docs have been updated, but I'm curious if there's a more consolidated and comprehensive way to find docs updates."},{"location":"wwdc21/DevTools.html#given-that-the-legacy-build-system-is-deprecated-were-still-noticing-that-the-new-build-system-is-slower-to-run-a-build-than-the-old-one-were-very-sensitive-to-build-performance-currently-a-roughly-no-op-build-with-the-legacy-build-system-is-roughly-60s-on-a-recent-mac-pro-with-our-codebase-and-considerably-less-than-that-on-legacy-any-optimization-tips-would-be-greatly-appreciated","text":"Also - a lot of this is spent in the build planning step -- are there plans to cache or reduce build graph evaluation in the future? Let me know if filing a radar/feedback is useful. Task planning should only re-run if something in the project structure has changed between builds; when doing (for example) an immediate no-op rebuild, this shouldn't happen. Please file a bug report. If you can attach a sample project which reproduces the problem, that would help, as I suspect there's something in the project structure which is triggering this.","title":"Given that the legacy build system is deprecated, we're still noticing that the new build system is slower to run a build than the old one. We're very sensitive to build performance -- currently a roughly no-op build with the legacy build system is roughly 60s on a recent Mac Pro with our codebase and considerably less than that on legacy. Any optimization tips would be greatly appreciated!"},{"location":"wwdc21/DevTools.html#do-you-have-any-general-guidance-for-writing-useful-feedback-assistant-reports-for-xcode-for-example-is-it-a-good-idea-to-make-bug-reports-isolated-and-minimal-in-an-xcode-playground","text":"We have some great tips for filing bug reports using Feedback Assistant here: https://developer.apple.com/news/?id=vvrgkboh Yes, it is an excellent idea to create a minimal scenario that shows your issue. Screenshots and a sysdiagnose also help us investigate the issue. We really do read every one of your bug reports too so as one of the folks that reads those, I just want to say thanks for asking this question! We really appreciate those reproducible, actionable bug reports!","title":"Do you have any general guidance for writing useful Feedback Assistant reports for Xcode? For example, is it a good idea to make bug reports isolated and minimal in an Xcode playground?"},{"location":"wwdc21/DevTools.html#how-is-your-experience-using-xcode-to-work-on-xcode","text":"It\u2019s definitely a very \u201cmeta\u201d experience to write code for new functionality in the Source Editor \u2026 in the Source Editor. Or to debug Xcode using Xcode. Honestly, you sometimes loose track of which one you are working in and which one your are debugging. We can also profile Instruments with Instruments by the way. So it\u2019s definitely fun to build the tools you are working with every day. It\u2019s not as fun to break the tools you are working with every day. Luckily, we\u2019ve been using Xcode Cloud with the Xcode project to make this less likely!","title":"How is your experience using Xcode to work on Xcode?"},{"location":"wwdc21/DevTools.html#why-doesnt-the-new-http-traffic-instrument-work-with-the-simulator-is-this-a-limitation-that-might-be-fixed-in-a-future-beta","text":"Thank you for your feedback; it's a known issue in Beta-1 that we are investigating, as the feature is is intended to work in simulator.","title":"Why doesn't the new HTTP Traffic Instrument work with the simulator. Is this a limitation that might be fixed in a future beta?"},{"location":"wwdc21/DevTools.html#how-does-the-team-decide-if-a-particular-subsystem-of-xcode-should-be-open-source-for-community-contributions-also-do-you-have-any-tips-for-getting-started-contributing-to-aspects-to-tooling-on-apples-github-perhaps-low-hanging-fruit","text":"For the Swift project, we have some bugs tagged as \"starter bugs\". Suitable for new contributors, some even if you have no experience with compilers! https://bugs.swift.org/browse/TF-67?jql=status%20%3D%20Open%20AND%20labels%20%3D%20StarterBug%20AND%20assignee%20in%20(EMPTY)","title":"How does the team decide if a particular subsystem of Xcode should be open source for community contributions? Also, do you have any tips for getting started contributing to aspects to tooling on Apple's GitHub (perhaps low-hanging fruit)?"},{"location":"wwdc21/DevTools.html#is-there-any-way-to-cache-swift-package-manager-dependency-builds-this-is-one-of-the-longest-steps-in-our-current-build-especially-on-ci-we-cache-the-checkouts-for-them-but-being-able-to-cache-spm-builds-would-save-a-lot-of-time","text":"Adding clonedSourcePackagesDirPath to your xcodebuild flags should help you point CI systems to already downloaded packages. There are Swift Package Manager Office Hours starting today at 3pm and it might be a better forum for this question.","title":"Is there any way to cache Swift Package Manager dependency builds? This is one of the longest steps in our current build, especially on CI. We cache the checkouts for them, but being able to cache SPM builds would save a lot of time!"},{"location":"wwdc21/DevTools.html#what-does-allow_target_platform_specialization-supported_platforms-actually-do-can-i-build-xcframework-fro-macosiostvos-with-one-xcodebuild-call","text":"SUPPORTED_PLATFORMS defines the platforms a target supports building for. For example a framework target which can be built for either iOS or watchOS. ALLOW_TARGET_PLATFORM_SPECIALIZATION allows a target to build for multiple platforms in a single build. For example, building an iOS app with an included watchOS app, both of which depend on that framework, so the framework can be built for both platforms. But XCFrameworks still need to be built with a separate xcodebuild invocation.","title":"What does ALLOW_TARGET_PLATFORM_SPECIALIZATION + SUPPORTED_PLATFORMS actually do? Can I build XCFramework fro macOS/iOS/tvOS with one xcodebuild call?"},{"location":"wwdc21/DevTools.html#is-there-a-way-to-create-our-own-file-templates-for-adding-a-new-file-to-a-swift-package-manager-module-the-existing-templates-dont-include-substitution-templates-so-they-require-modification-ie-replacing-swiftuiview-in-both-the-view-and-the-previewprovider-before-theyre-as-useful-as-their-standard-xcode-new-file-equivalent-templates","text":"There's currently no way to customize new file templates (but, as ever, if you'd like to see this happen please file Feedback!). Generally, creating a new file in a package from within Xcode (e.g., \u2318n while your package is selected in the Project Navigator) should bring up the same New File template pane as you would see for a project. If you're seeing something else or if this doesn't meet your expectations, we'd love to hear about it.","title":"Is there a way to create our own file templates for adding a new file to a Swift Package Manager module? The existing templates don't include substitution templates, so they require modification (i.e. replacing SwiftUIView in both the View and the PreviewProvider) before they're as useful as their standard Xcode New File equivalent templates."},{"location":"wwdc21/DevTools.html#is-there-some-way-to-download-xcode-using-the-app-store-connect-api-and-the-keys-that-it-generates-this-would-be-super-useful-in-automated-workflows-to-setup-ci-machines-with-xcode","text":"While there are solutions like Configurator and Remote Desktop that allow you to install apps from the Mac App Store, including Xcode, there is not a way to install Xcode through the App Store Connect API. It sounds like the best solution for your situation would likely be to utilize Xcode available as a download in XIP format from developer.apple.com/download","title":"Is there some way to download Xcode using the App Store Connect API and the keys that it generates ? This would be super useful in automated workflows to setup CI machines with Xcode."},{"location":"wwdc21/DevTools.html#how-does-xcode-cloud-work-for-projects-that-make-use-of-multiple-developer-accounts-for-example-we-have-one-enterprise-account-for-development-and-beta-builds-and-then-a-separate-developer-account-for-our-app-store-builds-would-we-be-able-to-share-configuration-between-these-two-accounts-or-will-they-have-to-be-separate","text":"You cannot share data between different Developer Program accounts, all accounts are private and secure as you would expect. Also Apple Developer Enterprise Program accounts are not supported in Xcode Cloud at this point in time.","title":"How does Xcode Cloud work for projects that make use of multiple developer accounts? For example, we have one enterprise account for development and beta builds, and then a separate developer account for our App Store builds. Would we be able to share configuration between these two accounts or will they have to be separate?"},{"location":"wwdc21/DevTools.html#in-objc-projects-where-headers-havent-changed-how-can-we-debug-unexpected-fullnear-full-rebuilds-for-debug","text":"If you think this could be a bug and you can attach a reproducible case, please file a bug report. That said, you can set a user default: defaults write com.apple.dt.XCBuild EnableBuildDebugging -bool YES which will dump Intermediates.noindex/XCBuildData/BuildDebugging- directories containing trace files (which are text files despite the file extension) which have the low-level dependency analysis that the build performs. There is not tooling for processing these files, but they might provide some insight into your situation. Important: You should delete this user default when you've collected the data you want, as enabling build debugging can affect build times. It's not intended to be left enabled all the time.","title":"In ObjC projects where headers haven't changed, how can we debug unexpected full/near-full rebuilds for Debug?"},{"location":"wwdc21/DevTools.html#what-was-the-reasoning-behind-having-the-http-request-monitoring-in-instruments-vs-the-activity-monitor-in-xcode-it-wouldve-been-cool-to-not-have-to-intentionally-run-and-setup-instruments-to-have-that-working","text":"I assume you are referring to the debug gauges in the Debug navigator. We do already show the \u201cNetwork\u201d activity. However, the HTTP Traffic instrument shows very detailed information that wouldn\u2019t all fit into the the existing debug gauges. There are many different ways to visualizes network activity and even the HTTP Traffic instruments provides several ways to visualize your activity. However, if a specific summary view about HTTP traffic would be really helpful to you in the Debug Navigator within Xcode, please file a Feedback to describe what you would like to see there. https://developer.apple.com/bug-reporting/","title":"What was the reasoning behind having the HTTP Request monitoring in Instruments vs the Activity Monitor in Xcode? It would've been cool to not have to intentionally run and setup Instruments to have that working."},{"location":"wwdc21/DevTools.html#has-the-team-ever-considered-allowing-xcode-to-send-macos-notifications-for-example-when-tests-pass-fail","text":"After tests finish, if Xcode is not the frontmost application, it posts a macOS notification describing whether testing succeeded or failed. Beyond testing, Xcode includes a flexible feature called Behaviors which allows you to opt-in to receiving notifications when various events occur during your development workflow. For more details, see https://developer.apple.com/library/archive/documentation/ToolsLanguages/Conceptual/Xcode_Overview/CustomizingYourWorkflow.html","title":"Has the team ever considered allowing Xcode to send macOS notifications - for example when tests pass / fail ?"},{"location":"wwdc21/DevTools.html#macos-build-worked-on-xcode-cloud-are-these-apple-silicon-devices-or-intel-based-or-can-we-select","text":"Xcode Cloud builds universal binaries, just as Xcode does on your local Mac, that are compatible with all Macs. For more information about Universal Binaries, please check the developer documentation here: https://developer.apple.com/documentation/apple-silicon/building-a-universal-macos-binary","title":"MacOS build worked on xcode cloud, are these Apple silicon devices or Intel based? or can we select?"},{"location":"wwdc21/DevTools.html#my-team-produces-a-number-of-software-components-and-weve-recently-switched-from-packaging-them-as-static-frameworks-to-xcframeworks-xcode-doesnt-have-a-target-type-for-producing-xcframeworks-please-tell-me-that-im-wrong-so-that-packaging-happens-in-a-post-archive-script-when-we-were-just-producing-static-frameworks-we-could-drop-several-of-our-projects-into-a-common-workspace-and-it-was-easy-to-debug-right-into-each-of-those-frameworks-xcode-figures-out-that-since-the-foo-project-builds-the-foo-framework-other-projects-in-the-workspace-should-link-against-that-wed-really-like-to-do-the-same-thing-with-our-xcframeworks-but-they-dont-seem-to-be-supported-as-well-is-there-something-we-can-do-to-debug-into-our-xcframeworks-when-their-respective-projects-are-all-in-a-workspace-together","text":"Thanks for reaching out! XCFrameworks are not intended to be used during development if you're a team that produces them. From your perspective, they should be used only for distribution. For development, you should use dynamic framework targets and then link them into your executable that you'd like to debug. For more information on multi-platform frameworks, we suggest to watch \"Explore advanced project configuration in Xcode\" WWDC Session, which is set to be released on Friday: https://developer.apple.com/wwdc21/10210 Please let us know if you have any follow up questions or thoughts!","title":"My team produces a number of software components, and we've recently switched from packaging them as static frameworks to XCFrameworks. Xcode doesn't have a target type for producing XCFrameworks (please tell me that I'm wrong!), so that packaging happens in a post-archive script. When we were just producing static frameworks, we could drop several of our projects into a common workspace and it was easy to debug right into each of those frameworks -- Xcode figures out that since the Foo project builds the Foo framework, other projects in the workspace should link against that. We'd really like to do the same thing with our XCFrameworks, but they don't seem to be supported as well. Is there something we can do to debug into our XCFrameworks when their respective projects are all in a workspace together?"},{"location":"wwdc21/DevTools.html#with-vim-mode-can-you-support-v-to-visual-select-lines-fb9145839","text":"It looks like you already submitted the request via feedback assistant. Thanks for doing that, it's the best way to let us know of any enhancements you'd like to see.","title":"With Vim mode, can you support \u201cV\u201d to visual select lines? FB9145839"},{"location":"wwdc21/DevTools.html#when-are-we-going-to-see-testflight-beta-user-crash-report-with-the-user-information-it-helps-a-lot-when-we-receive-a-bug-report-from-the-user-and-find-the-stacktrace-of-the-crash","text":"In Xcode 13, the Crashes Organizer will show you feedback from TestFlight users of your app, including user information like name and email address if the user chose to provide it. In addition, you can view feedback and download the crash log directly from App Store Connect.","title":"When are we going to see TestFlight beta user crash report with the user information? It helps a lot when we receive a bug report from the user and find the stacktrace of the crash."},{"location":"wwdc21/DevTools.html#xcode-cloud-looks-great-has-the-team-or-others-in-channel-tested-it-with-fastlane-based-ci-workflows-to-date","text":"Xcode Cloud is designed to automatically build, test, and prepare your app for TestFlight and the App Store. And should you need to install additional third-party tools to accomplish other tasks, then this can be done using Custom Build Scripts. You can learn more about extending Xcode Cloud here: https://developer.apple.com/documentation/xcode/writing-custom-build-scripts","title":"Xcode Cloud looks great. Has the team (or others in-channel) tested it with Fastlane-based CI workflows to date ?"},{"location":"wwdc21/DevTools.html#we-built-sdk-for-our-clients-not-an-app-and-have-several-semi-independent-components","text":"I want to understand what would be ideal way to distribute our SDK as XCFramework where it depends on our two other internal static libraries. But out of one we need to distribute one dependency as dynamic framework mainly as XCFramework moving forward. SDK is mix of Objc and Swift. Existing Structure SDK - Distribution as Dynamic FAT Framework. SDK has dependency on StaticLibrary1 and resources which are brought using cocoapods - whose public API needs to be available as part of SDK public interface. SDK contains StaticLibrary2 - whose public API needs to be available as part of SDK public interface. End goal is to distribute SDK as XCFramework. Should we distribute other dependency as XCFramework as well and integrator needs to bring all those three in order to utilize our SDK An XCFramework can be used to distribute a framework for multiple platforms in a single bundle, but XCFrameworks don\u2019t provide any support for distributing an SDK; you still need to distribute a single XCFramework for each framework you\u2019re distributing. Xcode doesn\u2019t provide support for constructing SDKs like you\u2019re distributing; you would need to provide guidance to your users to use the frameworks you're providing to them.","title":"We built SDK for our clients, not an app and have several semi-independent components."},{"location":"wwdc21/DevTools.html#xcode-cloud-can-it-produce-signed-developer-id-mac-apps-two-answers-in-this-channel-conflict-one-said-only-app-store-the-other-said-yes-developer-id","text":"Great question! Xcode Cloud does not support Developer ID-signed Mac apps. It supports Development and Mac App Store signing. I encourage you to submit a feedback request for any enhancements you'd like to see in the future!","title":"Xcode Cloud: can it produce signed Developer ID Mac apps? Two answers in this channel conflict (one said only App Store, the other said yes Developer ID)"},{"location":"wwdc21/DevTools.html#will-it-be-possible-to-be-notified-by-email-when-a-user-submits-a-feedbackwith-a-crash","text":"That is great feedback! Please submit an enhancement request with details about what you'd like to see via Feedback Assistant.","title":"Will it be possible to be notified by email when a user submits a feedback(with a crash)?"},{"location":"wwdc21/DevTools.html#in-xcode-organizer-is-there-anyway-to","text":"See watchdog crashes Search by the stack trace content of a crash See a version's all crashes and not only the top ones There is not a way to see individual watchdog crashes in the Xcode Organizer. However the new Terminations metric in Xcode 13 will show you aggregated data about all types of terminations that customers are experiencing in your app. Searching by the content of a stack trace is not available in the Organizer, but this would be a great enhancement to request via Feedback Assistant! In Xcode 13, you can see up to 1000 of your app's crashes for the past year, so you can view and investigate far more than the top 25 crashes.","title":"In Xcode Organizer, is there anyway to:"},{"location":"wwdc21/DevTools.html#do-you-think-xcode-cloud-has-value-for-individual-developers-or-just-teams","text":"Xcode Cloud can greatly add value, even for individual developers. It can easily help you validate the code you write by starting integration jobs that will also run your tests on multiple run destinations (i.e. iOS/iPadOS simulator, Mac Catalyst) in addition to building. This way, you will feel more confident about your work when shipping updates of your App (it also handles code-signing for you). With Xcode Cloud, your project is ready to accept outside contributions without needing to maintain a CI infrastructure.","title":"Do you think Xcode Cloud has value for individual developers, or just teams?"},{"location":"wwdc21/DevTools.html#what-are-some-of-your-favorite-tips-and-tricks-for-triaging-crashes","text":"When I am looking at a new crash in an area I am not familiar with, one of the first things I do is try to understand why it is crashing. Asking yourself questions can also help you get a quick understanding of what kind of crash it is. \"Is it crashing because I made an assumption I shouldn't be?\" \"Is this framework I am using expecting something different than what I am giving it?\" \"Does this only happen sometimes?\" Answering those questions can get you started on understand the crash and then working towards a fix! Using the Organizer's inspector allows me to see how big of a problem it is. It can tell me all about how many devices this crash is happening on, what versions of the OS, and new this year,: where is it crashing (App Store or TestFlight) and which app versions?","title":"What are some of your favorite tips and tricks for triaging crashes?"},{"location":"wwdc21/DevTools.html#our-project-has-multiple-build-configurations-not-just-release-and-debug-currently-with-cocoapods-we-can-have-different-dependencies-for-each-build-configuration-last-time-we-checked-this-was-not-possible-with-spm-has-this-changed-if-not-do-you-expect-this-to-change-in-the-future","text":"There is a proposal for conditional target dependencies (see https://github.com/apple/swift-evolution/blob/main/proposals/0273-swiftpm-conditional-target-dependencies.md) but this has not yet been fully implemented.","title":"Our project has multiple build configurations, not just release and debug. Currently with Cocoapods we can have different dependencies for each build configuration. Last time we checked this was not possible with SPM. Has this changed? If not, do you expect this to change in the future?"},{"location":"wwdc21/DevTools.html#is-there-a-way-to-only-update-a-single-package-if-i-have-multiple-packages-specifying-up-to-next-major-xcode-will-try-to-update-all-of-them-when-i-run-update-to-latest-package-versions-but-sometimes-i-might-only-want-to-update-a-specific-one","text":"Not at the moment, but it is something we are looking to make available on both the CLI and Xcode.","title":"Is there a way to only update a single package? If I have multiple packages specifying \"Up to next major\", Xcode will try to update all of them when I run \"Update to Latest Package Versions\" but sometimes I might only want to update a specific one."},{"location":"wwdc21/DevTools.html#hi-spm-is-incredible-does-the-presence-of-playgrounds-for-ipad-mean-that-we-can-replace-xcodeproj-based-projects-with-spm-based-projects-even-for-non-console-apps-thanks","text":"We're glad you like it! Because projects in Swift Playgrounds are built on top of the Swift Package format, they'll also work great in Xcode when making apps for iPadOS.","title":"Hi, SPM is incredible! Does the presence of Playgrounds for iPad mean that we can replace xcodeproj-based projects with SPM-based projects even for non-console apps? Thanks!"},{"location":"wwdc21/DevTools.html#whats-the-best-method-for-simultaneously-develop-an-app-and-its-dependent-swift-packages-local-development-with-submodule","text":"Check out https://developer.apple.com/documentation/swift_packages/editing_a_package_dependency_as_a_local_package","title":"What's the best method for simultaneously develop an app and its dependent Swift packages? (local development with submodule?)"},{"location":"wwdc21/DevTools.html#if-ive-got-private-dependencies-from-lets-say-github-via-ssh-if-they-require-different-credentials-i-can-use-sshconfig-to-tie-ssh-keys-to-specific-configurations-spm-uses-that-perfectly-but-if-i-put-same-packageswift-file-into-xcode-it-does-not-is-it-possible-to-make-xcode-behave-like-swift-package-commands-in-this-case-thanks","text":"You can configure Xcode to use the system git instead of the builtin one, which is what SwiftPM uses on the command line. See \"Use Your System\u2019s Git Tooling\" in https://developer.apple.com/documentation/swift_packages/building_swift_packages_or_apps_that_use_them_in_continuous_integration_workflows","title":"If I've got private dependencies from, let's say GitHub via ssh. If they require different credentials, I can use ~/.ssh/config to tie ssh keys to specific configurations. SPM uses that perfectly. But if I put same Package.swift file into Xcode, it does not. Is it possible to make Xcode behave like swift package commands in this case? Thanks!"},{"location":"wwdc21/DevTools.html#have-there-been-improvements-to-checkout-times-in-xcode-13-weve-had-to-fork-many-of-our-dependencies-and-remove-the-git-history-to-allow-for-reasonable-checkout-times","text":"Starting Swift 5.4, Swift Package Manager caches package dependencies on a per-user basis, which reduces the amount of network traffic and increases performance of dependency resolution for subsequent uses of the same package. This means that while the initial clone may still be slow depending on the repository size, subsequent clones are much faster. This can also be used in CI by configuring the cache location. See https://swift.org/blog/swift-5-4-released/","title":"Have there been improvements to checkout times in Xcode 13? We've had to fork many of our dependencies and remove the git history to allow for reasonable checkout times."},{"location":"wwdc21/DevTools.html#is-there-a-way-to-check-to-see-if-there-are-updates-available-without-installing-them-equivalent-to-pod-outdated","text":"There's not a convenient command to do this yet. As a workaround, you could perform an update, then revert your Package.resolved file to its prior state with your source control system to go back to your old dependency versions.","title":"Is there a way to check to see if there are updates available without installing them, equivalent to pod outdated?"},{"location":"wwdc21/DevTools.html#friday","text":"What is the recommended practice to run a test case only on the specific OS version? Adding @available(iOS 14, *) directive on the top of class declarations seems not working somehow. The recommended way for this is to use one of the XCTest APIs to skip the test if the required APIs are not available. There is more information at https://developer.apple.com/documentation/xctest/methods_for_skipping_tests","title":"\ud83d\uddd3 Friday"},{"location":"wwdc21/DevTools.html#maybe-this-has-been-asked-but-with-the-new-testflight-crash-logging-in-xcode-organizer-will-there-be-a-way-to-integrate-this-with-issue-trackers-like-jira","text":"Sorry, we don't have the answer to your question in this lab, but, if you haven't already seen it, I would recommend checking out the Triage TestFlight crashes in Xcode Organizer video at https://developer.apple.com/wwdc21/10203","title":"Maybe this has been asked, but with the new TestFlight crash logging in Xcode Organizer, will there be a way to integrate this with issue trackers like Jira?"},{"location":"wwdc21/DevTools.html#are-there-any-recommended-workflows-for-using-run-scripts-inside-of-a-packageswift-file-there-have-definitely-been-a-few-times-where-ive-wanted-to-have-my-packageswift-file-generate-things-like-compiled-protobuf-models-thanks","text":"SE-0303 defines new functionality in SwiftPM enabling the creation of structured plugins for code generation.","title":"Are there any recommended workflows for using run scripts inside of a Package.swift file? There have definitely been a few times where I've wanted to have my Package.swift file generate things like compiled protobuf models. Thanks!"},{"location":"wwdc21/DevTools.html#can-static-analyzer-in-xcode-13-find-dead-code-like-unused-variables-and-methods","text":"Xcode provides compiler warnings for unused local variables and unused static functions and unused C++ private methods. There are no warnings for unused Objective-C methods because in Objective-C it's a fairly hard technical problem as it's a very dynamic language. The static analyzer provides additional warnings for problems that require deeper analysis such as unused initializations or assignments that get immediately overwritten (\"dead stores\") and redundant if-clauses and operands in expressions (new in this release! - off by default, see the \"Unused Code\" section in build settings). Do Static Analyzer works with Swift? The static analyzer can detect bugs in C/C++ and ObjC code. It doesn\u2019t detect bugs in Swift code. However, you can use the analyzer on mixed Swift-ObjC projects. We recommend running the analyzer on your project if it has C/C++ or ObjC code as some issues the analyzer can detect on ObjC code can manifest as hard-to-find bugs when used from Swift.","title":"Can Static Analyzer in Xcode 13 find dead code like unused variables and methods?"},{"location":"wwdc21/DevTools.html#how-big-would-be-the-time-impact-on-building-with-xcode-if-i-turn-on-the-analyze-during-build-flag","text":"Usually we expect 2-5x slowdown. You can set mode for analysis to \"Shallow\", which is generally faster (2-3x slowdown), but finds fewer bugs.","title":"How big would be the time impact on building with Xcode if I turn ON the \u201cAnalyze During build flag?\u201d"},{"location":"wwdc21/DevTools.html#does-the-static-analyzer-analyzes-the-full-source-code-of-my-project-including-third-party-libraries-and-frameworks-if-so-can-i-configure-which-frameworks-to-exclude","text":"This is a great question! The static analyzer will analyze all third party code as long as it's getting compiled when you're building your app (i.e., binary frameworks aren't analyzed) and it's written in a language supported by the static analyzer (i.e. C/C++/Objective-C but not Swift). There is no way to exclude individual frameworks but given that the source code is available, you may have a chance to edit it to suppress unwanted warnings.","title":"Does the Static Analyzer, analyzes the full source code of my project including third party libraries and frameworks? If so, can I configure which frameworks to exclude?"},{"location":"wwdc21/DevTools.html#is-there-any-point-in-running-on-the-static-analyzer-on-pure-swift-code","text":"If your app has only Swift code, the analyzer wouldn\u2019t find any issues. However, if your project has ObjC code as well, we recommend running the analyzer.","title":"Is there any point in running on the static analyzer on pure Swift code?"},{"location":"wwdc21/SwiftUI.html","text":"SwiftUI Lounge QAs by emin \ud83d\uddd3 Tuesday What's your recommended approach to displaying a new macOS window? Not a new document or a confirmation dialog, just \"here is some information that makes sense to show right now\", such as unlocking an IAP. I've seen hacks with handlesExternalEvents() but it's not very pleasant. We don\u2019t have much API in this area. A NavigationLink when used within a commands context will open a new window, and as you noted, handlesExternalEvents can be used to, though this is primarily due to the default behavior of that modifier, which is to create a new window for the given scene, if no windows prefer to handle the event. We\u2019d love a feedback for this functionality, and if you can share any specifics of your use case in it, that is even better. Thanks! For SwiftUI macOS document apps, is there a recommended way to save and restore window size and position? Hi, when state restoration is not enabled on macOS, this is expected behavior at the moment. We\u2019d welcome a feedback for this, though. If you could include any information about your use case as well, that\u2019d be very helpful. Thanks! Suggestion to the designers amongst the SwiftUI folks present: it would be great if there were more examples of unique user experience interactions, especially navigation, instead of assuming that everyone wants to always use NavigationView as the central UX construct. UIKit was only slightly better with sparse examples to customize Navigation View transitions, but it too, lacked complete flexibility. Would love to hear your feedback on this! Unfortunately customizing navigation transitions isn\u2019t supported today \u2014 meaning if you want to invent awesome transitions you\u2019ll need to create your own containers and the state that manages them\u2026 but you\u2019ll need to ensure your custom goodies properly handle accessibility, common system interactions, and more which can be tricky. Ideally, please file feedback with some use cases or examples for what you have in mind, I would love to see the team take a look at enabling awesome experiences that work well with the system! Is there ever a case in which Spacer() is superior to .frame(maxWidth: .infinity)/.frame(maxHeight: .infinity)? Howdy! There was a similar question yesterday about Spacer \u2018versus\u2019 frame, check out the answer for how they compare: While Spacer is a view, it doesn't end up displaying anything of its own so it is plenty lightweight. Using .frame can have other behavior introduced to the way the view gets laid out beyond just changing its size. They both have their uses, so use them each where appropriate. To add a little more onto this, even in cases where you will get almost entirely the same behavior between the two, the performance difference will be so minimal that I would strongly suggest prioritizing code readability over performance / memory use to make this decision. If Spacer makes it more clear what layout you\u2019re trying to specify, use that, and vice versa. But the gist is: they both have their purposes and are extremely efficient from a performance perspective. So, you should focus on what your intention is and which reads better or offers the functionality you\u2019re trying to utilize as you construct your hierarchies Hey, Is it a way to have the coordinate of the cursor inside an image in SwiftUI. I would like to display a crosshair by drawing two lines from the cursor to the edges of the picture. I tried with NSEvents but it gave me the cursor location relative to my screen. thanks, Nicolas Howdy! This sounds like excellent topic to file feedback to help the team understand your use case, as I don\u2019t believe this is supported today. How do we set preferredScreenEdgesDeferringSystemGestures in a swiftUI app? Currently you need to use a representable to access that property. Are you able to give some indication of why the semantic background colors from UIKit are missing? We only have Color.background as opposed to the main varying alternatives, and it's annoying to have to wrap them repeatedly. A goal for us is that the Color and other API we provide works great on all platforms. So there are often other APIs to help get those effects: for instance GroupBox on iOS will produce the stacking (secondary and so on) system background colors, and on macOS gives you a similar effect but using the more platform appropriate group box (9-part) artwork instead of colors at all. The new .primary, .secondary, .tertiary, .quaternary shape styles are similar in that they also work across platforms and now can correctly give the correct vibrant rendering effect on top of materials So while we might not add a Color for everything that UIKit has, we do want to make sure we have coverage for those design concepts. (so specific feedbacks on things that are missing are always appreciated!) Things like label , secondaryLabel, etc are all achievable using those shape styles above and are even smarter in those material contexts, i.e. .foregroundStyle(.secondary) If I pass around a single @StateObject through injection from View A to Subview B to Subview C, when the data updates, will all 3 views get invalidated and redrawn? What if only View A and C need to use the data, and view B is only a middleman? Only views that read properties of the object should update, but do note that if a view reads any property it will be invalidated on objectWillChange, even if the particular property begin read did not change. I'm trying to update a Text view based on some state (eg. from a text field). The text is inside a scroll view, and I am using ScrollViewReader to scroll to the end whenever the text changes. For some reason, scrollTo doesn't work until I manually start scrolling \u2014 after that it works fine struct Foo: View { @State private var text = \"\" private let letters = Array(\"abcdefghijklmnopqrstuvqxyz\") private let timer = Timer.publish(every: 0.1, on: .main, in: .common).autoconnect() var body: some View { ScrollView(.horizontal, showsIndicators: false) { ScrollViewReader { scrollView in HStack { Text(self.text) .fixedSize(horizontal: true, vertical: false) .onAppear { scrollView.scrollTo(0) } .onChange(of: self.text) { _ in scrollView.scrollTo(0) } Color.clear .id(0) } } } .onReceive(self.timer) { _ in self.text.append(letters.randomElement()!) } } } Howdy! Please file feedback with this sample code, this looks like an issue the team should take a look at. Additionally, you should take a look at the new TimelineView and replace your use of Timer here. It\u2019s not recommended to declare new objects in your view\u2019s initializer without a @StateObject or @ObservedObject property wrapper, if you must use a Timer, store it inside an ObservableObject that you can refer to as a StateObject, or another similar technique. Is it possible to enable \u201csloppy swiping\u201d for navigation links in SwiftUI (swiping to go back from anywhere on the screen)? Also, is it possible to remove the 8pt buffer before a swipe to go back is recognized (so swipe to go back is recognized instantly rather than having the slight delay)? Unfortunately, this isn\u2019t currently supported, but feel free to file feedback requesting these features! Can a SwiftUI View or Canvas be rendered into a PDF like a UIView can using draw ? Is there another method that does this? Unfortunately, this isn\u2019t currently supported. Please do file a feedback report if that\u2019s a feature you\u2019re interested in. We really do appreciate them! Is there anyway to dismiss a keyboard presented from a TextField while scrolling a List? There is a behavior provided in UIKit by keyboardDismissMode UIScrollView but it's disabled in SwiftUI List. Also, I tried to embed a UIScrollView using UIViewRepresentable and enable keyboardDismissMode but the behavior is buggy Unfortunately, that\u2019s not currently supported. Thank you for the feedback you already filed! We really do appreciate it. If you want to file an additional feedback report requesting API for solving the problem in a SwiftUI-esque way as well, that would be appreciated as well! Is there any way to convert from the old AppDelegate/SceneDelegate lifecycle to the new \"SwiftUI 2\" lifecycle? I'd rather not re-write my entire application, so while it's still in the early-ish phases I wanted to know if this was possible Yes! You can use the UIApplicationDelegateAdaptor property wrapper in your App. Something like this: UIApplicationDelegateAdaptor var myDelegate: MyAppDelegate SwiftUI will instantiate an instance of your UIApplicationDelegate and call it in the normal fashion. Furthermore, if you return a custom scene delegate class from configurationForConnectingSceneSession, SwiftUI will instantiate it and call it as well. Is there a way to make two views from different hierarchies, have the same hight/width/x/y position, that is easier to understand than using Preference Keys? Yes! Look into matchedGeometryEffect. Is it possible to display a SwiftUI view on an external display, on iPad/iPhone, using the SwiftUI App lifecycle? Thanks for the question. I\u2019m afraid that\u2019s not currently supported. this is a strange request, but is it possible to \"intercept\" the actions of a button or gesture with a modifier? wanting to devise a way \"wrap\" the invocation (for example with analytics calls) Unfortunately, that\u2019s not currently possible today. If that\u2019s something you\u2019d find useful, please feel free to file a Feedback requesting it! Are there any places where you think \"yep, this is the place to use onAppear() rather than task() ? onAppear() is still fine to use. There\u2019s no need to update existing code that uses it. Going forward, I think task() provides a more general solution, even for short synchronous work, since it sets you up to evolve towards asynchronous work if necessary in the future. So\u2026 you would always use task() for new code, or are there still places where onAppear() it the right solution? I was maybe expecting to see onAppear() being gently deprecated or similar. I\u2019d always use task() personally, but there\u2019s also a nice symmetry between onAppear() and onDisappear() that some folks will want to maintain. Our apps stores the position of (our custom) split view controls in its documents. It would be nice to use HSplitView, VSplitView and give them as a parameter a binding to set their position to simplify storing and restoring their state. For now it seems I'd need to use a GeometryReader to get the state and a .frame on one of the child views to set the state. Can you recommend a cleaner approach? Howdy! Unfortunately there is no API to do this in SwiftUI today, would you mind filing feedback on this? It would be great to understand your use case here! Is there a way to measure the rendering time or memory footprint for a SwiftUI's body method being called in XCTest? I'd like to setup some baseline testing so that I can be aware of any regressions in performance, but I haven't been able to work out how to do it. I\u2019m afraid not. I\u2019d love to get an enhancement request Feedback for that use case. We have support for Instruments, but not XCTest. In general, to pass data around, would it better to have an EnvironmentObject that could be called within a view, or an ObservedObject that gets passed down (and/or injected) through child views? Both have their uses, and it depends on the architecture you\u2019re building. If you have one (or a few) large ObservableObjects that large parts of the view hierarchy need to see, I would generally recommend EnvironmentObject as SwiftUI can look at which of your views depend on the EnvironmentObject and only invalidate those when your ObservableObject changes (you can get this behavior with ObservedObject too, but it\u2019s more cumbersome). Plus, views that don\u2019t actually use the ObservableObject don\u2019t get cluttered with code relating to it. That said, if your model is, for example, an object graph that is largely not structured based on your view hierarchy, it may make more sense to use ObservedObject to grab pieces of that model out to use in your view. on macOS, you can prevent a window from resizing by setting a specific frame size, however this doesn't prevent fullscreen/zoom. this results in a broken animation and a tiny view in the middle of the screen. it is possible to tell swiftui the window does not support fullscreen? Hi - sorry you are hitting this bug. Using a fixed size frame is the correct expression here. Also, thank you for the feedback! Can you provide guidance for when the following error message is printed in the console: Bound preference CollectionViewSizeKey<UUID> tried to update multiple times per frame ( CollectionViewSizeKey<UUID> is the name of the preference.) The preference is updated from inside a GeometryReader. I think the error message happens if the preference modifier is inside a ScrollView. That indicates a cyclic update. The size of your GeometryReader is changing based on the size of your GeometryReader. Try lifting your GeometryReader higher in the view hierarchy if possible, or look for other ways to avoid the cycle. I am using @ObservedObject now for my model, since I still need to support iOS 13. However I know that @StateObject provides the correct behaviors to me. Is there a suggested way to use them at the same time for back compatibility? I first thought if #available might work, but it does not work for a property. For supporting iOS 13, you\u2019ll need to use @ObservedObject and keep your object alive through some other means, like using a static property or keeping a reference in your application delegate. I don\u2019t think trying to switch between observed object and state object buys you much here, since changing the owner of the object with availability checks would be awkward. I've had several intermittent crashes from environment objects being nil when I pass them to a sheet or NavigationLink. It's tricky to replicate due to being intermittent and I usually work around it by architecting my code differently to avoid passing environment objects. Do you know of reasons this might happen? All I can think of is that the views that originate the environmentObject further up the view hierarchy are being taken out of memory. Thanks for any help you can provide! NavigationLink by design doesn\u2019t flow EnvironmentObjects through to its destination as it\u2019s unclear where the environmentObject should be inherited from. I suspect this might what\u2019s causing your issue. In order to get the behavior you expect, you\u2019ll have to explicitly pass the environmentObject through at that point. You can also apply the environmentObject to the NavigationView itself, which will make it available to all pushed content. Will the project in its entirety from Build apps that share data through CloudKit and Core Data (wwdc21-10015) be available for download? Are GeometryReaders really bad in terms of performance? I'm getting the feeling that they should be avoided, but I don't know if it's because they're inefficient, because they're \"breaking\" the layout, or because other solutions may exist (like anchored preferences) and be better suited for the purpose? Just like any tool, GeometryReader has a time and place where it is correct to use. There aren\u2019t any particular performance pitfalls I\u2019d call out with them, but they shouldn\u2019t be used as a hammer. I noticed that while using Combine and lists (say local search combined with remote results). Lists have a really, REALLY hard time keeping up with animated updates. I found the only reliable way to force correct data representation is to use .id(UUID()) and turn off animations. Is this somewhat expected? We worked really hard on improving List performance in iOS 15, macOS Monterey, and aligned releases. Please try there and let us know if you\u2019re still seeing issues. If you aren\u2019t doing so already, it also may be good to debounce the queries. when creating a UIViewRepresentable , it is dangerous for the Coordinator to hold an instance of the UIView passed in updateUIView() or should it be strictly treated as ephemeral? That is OK! Your coordinator will have been created before any views are \u2014 so in makeUIView you can give the coordinator a reference to that view Hi guys, you all did a great job again congrats. My question is related to CanvasView, that view is pretty awesome and opens lots of possibilities for creative and generative art for example. I would like to know how behaves a Canvas encapsulated in a Timeline receiving updates of Bindings Combine's streams. During the render, does it lost the current time information, does it resets to re-render the new state or there's no re-rende at all and the Timeline still running wherever the body changes? Thank you! As long as the identity of the timeline is the same, and the value of the schedule didn\u2019t change based on the body update, it shouldn\u2019t trigger a new update of the contained Canvas. If the content of the canvas itself changed, you will likely see another update from the TimelineView with the date of the current entry of the schedule. What is the best practice for bypassing the system styling for a component? For example, in a Section header in a grouped list, SwiftUI automatically dims the content and capitalizes all text (as you would expect in a text-only section header for UIKit). Is there a built-in way to bypass this dimming and capitalization? Many of the customizations you see are part of the default styling that lets you have the most natural feeling UI by default. However, most of these default stylings should be overridable by using the same modifiers you would to get that style. For example, if you want to remove the default capitalization of the section header, you can use the text case modifier: Section { // ... } header: { Text(\"My Header\") .textCase(nil) } The same goes for other customization points like foreground style, font, etc. Canvas seems to lose its intrinsic content size and appear as 10x10 when nested in a ScrollView, is there a as to work around this behaviour? Thanks for the question! You might try putting a flexible frame on the Canvas, something like .frame(maxWidth: .infinity, maxHeight: .infinity). (Apologies if I got that a little wrong. \\me files Feedback asking for auto-complete in Slack.) How do I profile SwiftUI code, to know how to optimize my views? Instruments is almost only showing SwiftUI library code, so it's hard to see what is expensive to render... Using the SwiftUI instrument will help call out expensive body methods. In addition it\u2019s important to limit the number of times each views body will get reevaluated. Highly recommend watching the Demystifying talk for some in depth looks at how this works. Pro tip: call the new debug helper Self._printChanges() inside body to log which property caused the view to be reevaluated. Hi folks! Thank you for SwiftUI -- I can't believe how straightforward it makes so many things. I'm wondering, is there any way to bind to the state of expansion in a hierarchical List (i.e. one created with a children ) parameter, so that we can expand and collapse items programmatically? Thanks for the question! I\u2019m afraid there\u2019s not currently a way to bind the expansion state of an entire hierarchy. You can use DisclosureGroup inside a List to bind a single level. I\u2019d love an enhancement request Feedback with your particular use case. Managing the expansion state of an entire hierarchy is a difficult API design problem, so more info on your use case would be super helpful! How can I pass an @EnvironmentObject to a ViewModel? I'm trying to change a tab inside the ViewModel using the UserSettings. @EnvironmentObject and all the other SwiftUI\u2019s property wrappers are only valid and functional when used inside a View. In general I would discourage you from using views to pass data between different model objects. what is the best way to deal with atomic changes in state when changing several state variables in sequence? I can manage this generally with onChange but it doesn't trigger the action when the state variable was set to the same value it had before (thus no change). I'd like to make sure that I trigger my update code even if the value didn't change (equivalent to didSet in Swift, which is triggered regardless of the old value) Additional info pending SE-309 allows using protocols with associated types as existentials, assumedly including View . At that point, will there be a difference between a View existential and AnyView in terms of view identity? I \u2665\ufe0f that proposal! I can\u2019t comment on implementation details, but generally AnyView erases more information than an existential, so the existential would still have the edge. I noticed @FocusState doesn't support an initial value \u2014 instead, you can set the focus in onAppear . Is there a reason for this? One way to think of focus (not to be confused with the new user-facing feature we just launched) is that it is global state managed by the framework. A lot of the times, the user will be the one in control of this state, by selecting a text field, etc. The new @FocusState and focused(_:) API allows for some influence over that state as well, but ultimately, the source of this state is still internal to SwiftUI. Does that help to answer your question? I wasn't able to secure a lab spot (\"applied\" for all of the SwiftUI open labs, but didn't get selected for any). Does anyone know why a disabled button being re-enabled would have the text rendered under the re-enabled color? Thanks for the question. We might need sample code to understand this one. Can you reproduce in a sample view? If so, you could add that sample to a Developer Forums post or ask Developer Technical Support. I'm curious, is ShapeRole working at all? I made my custom shape and overwrite it with .stroke but there was no change in rendering. It always fills the shape. I know the role variable is being accessed (I put a print statement)... but maybe I'm missing something. And also, what is the purpose of the separator role? It is not very clear from the documentation how that would affect output. The role doesn\u2019t strictly dictate exactly how the shape will be treated \u2014 it can still be explicitly fill or stroke\u2019d by its use site. This more determines the expected semantics of how its intended to be used, in a way that affects its default foreground styling. For instance, you could have a stroke role Rectangle that is fill\u2019ed and used as a divider-like element, and so it will receive divider-like default foreground treatment. Is there a way to animate hiding/showing the Navigation Bar in SwiftUI? If you change the state that drives navigationBarHidden from inside a withAnimation block, it should animate. If it doesn\u2019t, that\u2019s a bug in SwiftUI. What's the best replacement for a tableHeaderView for a grouped list? I want to avoid having a cell with a different visual style containing the content to be used as the header view, but I want to avoid sticky headers that are typically found in plain table views. What is the state of the art here? Howdy! Great question, it\u2019s not clear to me if using a custom row instead of a section header would get what you want here, but I think it\u2019s a totally viable reason for feedback explaining your use case to make usually-sticky headers no longer sticky! Is there a way to get the current font size and weight? I want to draw in a canvas in a way that matches SF Symbols as well as possible. Is there a way to resolve the current Font like i can resolve Text and get these attributes? Howdy! Great question, I don\u2019t believe there\u2019s a way to get the resolved font size, weight, and design today. This is an excellent case where a feedback from you could help the team understand your needs! Are there any methods that allow restriction of view orientation to just landscape or just portrait on a per view basis without defaulting to UIHostingviewcontroller I\u2019m afraid not currently. That would be a great enhancement request Feedback! When I put Markdown-formatted text as a String literal in a Text, it formats with nice attribution, but when I pass it in from a variable, it doesn't. Is this a bug, or have I misunderstood something about how Texts take Markdown-formatted Strings? struct ContentView: View { var text: AttributedString = \" Hello , world ! Visit our website .\" var body: some View { VStack { Text(\" Hello , world ! Visit our website .\") Text(text) } } } The first Text renders nicely with bold, code and a link, while the second doesn't - it just shows the Markdown annotations in the string directly. I think that\u2019s a bug in beta 1. You might try Text(\"(text)\") to trigger the use of a different initializer. Hello, Is it possible to have a simultaneous DragGesture in an inner view of a ScrollView which doesn't make the ScrollView drag gesture fail? The DragGesture has a minimumDistance of 0. It seems it's impossible to scroll the scrollView if the DragGesture is recognized (it's installed as simultaneous and the GestureMask is .all ). Is there a workaround? Thanks! Howdy! Great question, this is a great case where we could use feedback from you to better understand how folks use-cases vary with gestures and ScrollView. In the meantime, a possible workaround would be to use a UIViewRepresentable where you can wrap a UIScrollView and get access to all its goodies and expose any functionality you need to your SwiftUI hierarchy. Is it intended that setting a value to a @FocusState property inside a Form does not work? The property stays nil and thus the focus isn't moved e.g. to a TextField inside the form. Using a VStack instead of the form works like a charm. Yes, please file feedbacks (if you already haven\u2019t) for any context where @FocusState doesn\u2019t work the way you\u2019d expect. Thanks! Is there any way to get GeometryReader size from another view? I want to replace \"???\" with the height of \"Hello world!\". struct ContentView: View { var body: some View { VStack { Text(\"Hello world!\") .background( GeometryReader { proxy in Color.clear /// placeholder let _ = print(proxy.size.height) /// 20.333333333333332 } ) Text(\"Height of first text is ???\") } } } Howdy! Using a GeometryReader in a background of a view ensures the GeometryReader doesn\u2019t grow to be larger than that containing view, but it makes it tricky to bubble its size out. You could do something like this: struct ContentView: View { @State private var height = 100.0 var body: some View { MyView().background { GeometryReader { proxy in Color.clear .onAppear { height = proxy.size.height } .onChange(of: proxy.height) { height = $0 } } } } } Then you can use your height State property like usual. \u26a0\ufe0f Beware: You must ensure you will not cause a continuous layout loop here, if your layout responds to height changing in a way that causes the GeometryReader to lay out again and cause height to get updated, you can get into a loop! Hi, I'm trying to implement an interactive media playback timeline (i.e., slider) for my audio player app in SwiftUI. However, when I use a high frequency timer to update the playback timeline, the app and other views become unresponsive :(. How can I implement high frequency SwiftUI view updates, and still maintain a responsive and interactive app? Thanks for the question! It\u2019s hard to give specific advice without seeing code, so this might be a good question for the Developer Forums. A couple of other useful things to look at: There\u2019s an Instruments tool for diagnosing why an update is slow. Check out the new TimelineView for creating views that update at the same frequency as the display. It doesn\u2019t do any good to try to update faster than that, because the display can show intermediate frames anyway. By default the title of .newItem is \"New Window\". How I can customize it? For example \"New Window\" We will use the title of the WindowGroup in the menu item, if you provide one. For example: WindowGroup(\"Viewer\") { ... } Will give you a menu item titled \u201cNew Viewer Window\u201d. This title will also be used for the default window title, which can be changed with navigationTitle(_:). \ud83d\uddd3 Friday Hello, When using TCA, a new instance of ObservableObject (the ViewStore ) is frequently installed as @ObservedObject when the view tree invalidates. What are the consequences of vending a new instance (possibly with the same properties' values) in terms of performances? Would it be absolutely better to reuse the same instance if possible, or is it acceptable as @ObservedObject installation is quite efficient? Thanks! I can\u2019t speak to a third party framework, but in general if the observable object is replaced, every view that has a dependency on it must be re-rendered. Check out the talk Demystify SwiftUI for more details on dependencies. The Canvas looks like a great new addition. I'm wondering about using it to render a single drawn surface that can be nested in a ScrollView which is then panned, zoomed into etc. Is this a reasonable solution and in terms of performance, will there be any guidelines on how far to push it? I intend to put a whole bunch of images as well as other shape data in there. This sounds like a good application for Canvas. Like anything, performance will depend on how far you push it and the hardware it\u2019s running on. I\u2019m afraid that\u2019s not a very actionable answer, but every app is different. The API documentation is here: https://developer.apple.com/documentation/swiftui/canvas/ The talk \u201cAdd rich graphics to your SwiftUI app\u201d supports the copy code feature too, so you can grab some fun snippets to play with. https://developer.apple.com/wwdc21/10021 If we break our Views into separate some View properties to help readability, is there much cost to marking those other properties as ViewBuilders to get the nicer syntax? Is that something we need to worry about? Nope, in fact, we encourage you to do so! Using the @ViewBuilder syntax helps nudge you towards structuring your code in a way that SwiftUI can make use of intelligently, so using it in more places is never a problem. Check out the talk, Demystify SwiftUI for more on this! https://developer.apple.com/videos/play/wwdc2021/10022/ Is it safe to employ multiple TimelineView with the animation schedule or is it equivalent to instantiating a number of CADisplayLink s? I was thinking of the adage around \u201creusing\u201d CADisplayLink s as much as possible. Yes! You should be able to use as many TimelineView as appropriate to get your interface behaving how you want. SwiftUI will take care of scheduling things so they update like you want. The thing to be careful of is to not have too much \"different\" between each update of the timeline content. Dumb question here... but I was just playing around with the code in the Advanced Graphics section of Whats new in SwiftUI. Taylor showed a symbols browser.... I copied the code and when I run the performance is really bad... he showed the smooth gesture with the fisheye and the timeline view animation... on my side it is super jerky with seconds delay to render... is that right? Sorry about that! That\u2019s a known issue in beta 1. As a quick workaround to see the same smooth effect today, you can manually cache the resolved images. One thing you could try is moving that resolution to be outside of the inner for loop, so it only happens once. (cheating since for the code snippet it only uses the swift bird rather than every symbol like my demo had). What's the recommended way to share SwiftUI code between multiple platform targets? Recent Xcode multiplatform templates simply make source files members of multiple targets, but a few years ago the recommendation for sharing code between e.g. an app target and an app extension was to create a shared dynamic framework (e.g. MyApp, MyAppExtension and MyAppKit). What would be the pros and cons of either approach? It really depends on the complexity of your particular app. The templates are the fastest way to get started and aim to minimize complexity. A shared dynamic framework is a great approach as your app grows. How about dynamic frameworks vs. SPM packages? I believe an SPM package can produce a dynamic framework now, but that\u2019s outside my expertise. It might be a good question for the DevTools Lounge or a Swift lab though! When an observedObject is passed into a view, does SwiftUI differentiate between views that actually use it (the object is used in the body) and 'intermediate' views (which just pass that object to a child? )? Or are all views just invalidated? Yes, there is a difference. If you don\u2019t use any of the ObservableObject property wrappers (@StateObject, @ObservedObject ) the view would not observe and update the instance. So you you just need to pass an ObservableObject through some intermediary view just make it a regular property on the view but make sure to use the property wrapper if you ever read any of the value in the view, otherwise your view will no be consistent with your data. Also, @EnvironmetObject is a great tool when you have an ObservableObject that you want to pass down multiple levels of your view hierarchy without having to manually do it every step of the way. What is the recommended architecture when using SwiftUI? In a lot of the demos (and this is probably my inexperience showing) it seems that the business logic leaks into the view. Would you recommend using MVVM for example and how would you set that up? Thanks for the question! Josh S. gave a great answer to this in the Q&A yesterday: For me the most important thing is that you have a software architecture in mind that works well for you, and helps you craft maintainable apps with a good separation ways to achieve that, but they\u2019re by no means the only one. If you\u2019ve got a well-considered architecture that works well for you, I think that\u2019s great and helps you craft maintainable apps with a good separation ways to achieve that,\u201d Could we use .prominent Window Scene Presentation Style in SwiftUI? Hi, we do not have support for this, but would love a feedback with any specifics you could provide about your use case. If I have to embed a SwiftUI View into a UIViewController to do something only available for UIViewController (like customize appearance that SwiftUI not support), is there any problem about the following two way? Create a UIHostingController A, add A's view as a subview to my own UIViewController B's view. (of course do some layout and addChild didMove stuff) Inherit UIHostingController directly to override some VC methods Either approach is fine! Please do file feedback though about what you're trying to achieve that requires wrapping the SwiftUI content like this. Why does a UIViewRepresentable update once after makeUIView and once before dismantleUIView ? The update function can be called for a number of reasons. It will be called at least once after make as the UIView becomes extant. It may be called multiple times before the UIView is eventually dismantled. You should not rely on any frequency (or lack thereof) of update calls. How do we avoid incurring in Bound preference SizePreferenceKey tried to update multiple times per frame ? It sounds like you have a cycle in your updates. For example, a GeometryReader that writes a preference, that causes the containing view to resize, which causes the GeometryReader to write the preference again. It\u2019s important to avoid creating such cycles. Often that can be done by moving the GeometryReader higher in the view hierarchy so that it\u2019s size doesn\u2019t change and it can communicate size to it\u2019s subviews instead of using a preference. I\u2019m afraid I can\u2019t give any more specific guidance than that without seeing your code, but hopefully that helps you track down the issue! When using View.frame(minWidth:, idealWidth, maxWidth:\u2026) are there any combinations of parameters that are more performant that others? Any combination to avoid? There isn't really any performance difference between any combination of parameters. You should just use the appropriate values to get the layout you're looking for. what's the best way to enforce a specific size? min + max? or ideal? or all? You would want to use the other version of .frame for that which just takes a width and a height. View.frame(width:,height:) Is there any way to set/control backButtonDisplayMode from SwiftUI? By the way, thanks for all the great new features Thanks for the question. I\u2019m afraid there\u2019s no SwiftUI API for this currently. If you could file an enhancement request Feedback, that would really help me prioritize that work though! Can you explain the AttributeGraph that comes up while debugging sometimes? I\u2019m afraid we can\u2019t discuss implementation details, but be sure to check out the talk \u201cDemystify SwiftUI\u201d for the details of the dependency graph. https://developer.apple.com/wwdc21/10022 Is there a way to disable pixel rounding in SwiftUI? The hover effect from my OS X Dock yesterday is jittery because padding is being applied to many views in a row, but rounded to the nearest pixel for each view. This inaccuracy in padding adds up, making the entire view jitter by a few pixels whenever I move the mouse. SwiftUI layout is always rounded to the nearest pixel. But using any GeometryEffect won't take on the snapping behavior. Things like .offset and .scaleEffect are existing ways to achieve this, but you can also implement your own GeometryEffect if you need something custom. In most cases, the layout behavior with Spacer can be replaced with .frame(maxWidth:,alignment:) (or height) seamlessly. Since Spacer is an actual View that is arranged within the view hierarchy, using Spacer will consume more memory and cpu resources. And Demystify SwiftUI also says \"modifier is cheap\". So should I use .frame instead of Spacer as much as possible? While Spacer is a view, it doesn't end up displaying anything of its own so it is plenty lightweight. Using .frame can have other behavior introduced to the way the view gets laid out beyond just changing its size. They both have their uses, so use them each where appropriate. To add a little more onto this, even in cases where you will get almost entirely the same behavior between the two, the performance difference will be so minimal that I would strongly suggest prioritizing code readability over performance / memory use to make this decision. If Spacer makes it more clear what layout you\u2019re trying to specify, use that, and vice versa. Does SwiftUI expose enough API that would allow us to build our own Lazy{H,V}{Stack,Grid} with a fully custom layout, or are there still a lot of \"magical bits\" under the hood that prevent us from doing so? Unfortunately, we don\u2019t offer support for building custom layouts today. What is the handlesExternalEvents modifier? I\u2019ve seen it used, but don\u2019t know what it is. This modifier allows you to specify a Scene to be used when external data is sent to the app - a URL or an NSUserActivity . For example, on macOS, when a url comes in that matches the pattern, we will either create a new window for that Scene, or use an existing one, if a view has been modified with handlesExternalEvents to prefer that incoming value. Is there a way to customize NavigationView's navbar other than UINavigationBar.appearance ? Unfortunately, there isn\u2019t any SwiftUI native way to do this right now. If this is something you want, please file a feedback report requesting the feature! The State documentation states \u201cIt is safe to mutate state properties from any thread.\u201d What does it refer to considering that SwiftUI complains with runtime warnings when a PassthroughSubject published from a non-main thread? State is thread safe and can be mutated from any thread. When you mention PassthroughSubject I would imagine you are using that in the context of an ObservableObject with either one of @StateObject, @ObserverdObject, or @EnvironmentObject . ObservableObject does require that you mutate all the properties observed by the view, and publish objectWillChange on the Main Thread. I would recommend you check out Discover Concurrency in SwiftUI where my colleague Curt and Jessica talk all about Swift and concurrency https://developer.apple.com/videos/play/wwdc2021/10019/ Is there a way for a view to know when the window it is hosted in is key or not on macOS? Could you elaborate some on what you\u2019re trying to achieve? There is the controlActiveState property on the Environment, but there is also focusedValue(: ,: ) and @FocusedValue which are used in the context of the key window on macOS. Is there a way to disable the scrolling on a List so I can use it within a ScrollView? I have a ScrollView, inside which I'm placing an Image, some Text, and then I would like to be able to place a List. However, currently, the List doesn't appear unless I give it a fixed frame size, and the List is also separately scrollable. Is there a way around this? I\u2019m sorry, but that\u2019s not currently supported. If you would be able to file an enhancement request Feedback with your use case, I\u2019d really appreciate it! I know Feedback can seem like a black box (sorry about that), but they really do help us, even if we can\u2019t respond directly to every request. Can items in a list accept drop items? It seems to immediately crash my app. Could you please file a feedback for this crash? If you can also include a sample that reproduces the issue, that would be especially helpful. If we create custom property wrappers that embed existing ones (like State, ObservedObject, etc) does SwiftUI still \"see\" those embedded wrappers and do the right thing? For example, can I create an @Whatever wrapper as a convenience for @Environment(\\.whatever) and still expect that to work the same way? Yes, you can make do that. As long as you add conform to the DynamicProperty to your wrapper this will work. without having to jump to ios15 for focusedSceneValue, is there a recommended way for non-document windows in a window group to surface a kind of \"current object\" that can be used to activate/deactive menus & send commands to, even if no editable control as focus? (in swiftui) Good question! focusedSceneValue provides a way for the current focused / key window to surface its values to @FocusedValues in menus and other places; this is especially important on macOS since an app can have many windows, but only one of those is key. As for other approaches, it somewhat depends on your use case: if you do need your current object to be tied to the key window specifically, there isn\u2019t a substitute. But if you more want to aggregate information across all of your windows, you could use things like preferences and the environment to manually pass that data around I have a framework that vends a SwiftUI View to the main app via a HostingController. The view handles everything it needs internally, so all the types are internal. The only public method is the one that vends the HostingController. In order to maintain the isolation I do it this way: return UIHostingController(rootView: AnyView(SchedulesView(store: store))) Is this a correct way to use AnyView Yes, that\u2019s an OK usage, particularly because it\u2019s at the base of the hierarchy view and not used to add dynamism. But there are other ways to encapsulate the exact type of the hosting controller, for instance returning an upcast or custom protocol type Returning it typed as UIViewController instead its actual UIHostingController<..> type Creating a protocol with the extra API that clients might expect from the view controller, and returning it typed as that Or by using a container UIViewController that has a child of your hosting controller When I've needed to inject data into a detail view, but still let the view have ownership of its StateObject, I've used the StateObject(wrappedValue:) initializer directly in my view initializer, for example: public struct PlanDetailsView: View { @StateObject var model: PlanDetailsModel public init(plan: Plan) { self._model = StateObject(wrappedValue: PlanDetailsModel(plan: plan)) } ... } Is this an acceptable use of the initializer? I know StateObject is only supposed to initialize at the start of the View's lifetime, and not on subsequent instantiations of the View value, so I want to make sure I'm not forcing it to re-allocate new storage each time the View is re-instantiated. Yes, this is an acceptable use of the initializer and your understanding is correct: that object will be create only at the beginning of the view lifetime and kept alive. The StateObject \u2019s wrapped value is an autoclosure that is invoke only once at the beginning of the view lifetime. That also means that SwiftUI will capture the value of plan when is firstly created; something to keep in mind is that if you view identity doesn\u2019t change but you pass a different plan SwiftUI will not notice that. Hello! I have tried to implement with SwiftUI a common feature in iOS apps that is when tapping again a tab item of a TabView with a nested NavigationView, it pops to the root view of the navigation view. Using presentationMode.wrappedValue.dismiss() it just chains the view dismiss, any idea of how to implement this in a more clean way ? (mine feels \"hacky\") Thanks for the question! I\u2019m afraid I don\u2019t have a great answer here, but there are a couple of options you can try. One option is to use a representable to embed a UITabBarController, so you can hook the delegate methods. Another option, if you can detect when the user taps the same SwiftUI tab again, is to decorate your NavigationView with .id(counter), where counter is @State private var counter = 0. Then when the user taps the same SwiftUI tab again, you can increment counter, which changes the identity of the navigation view, causing SwiftUI to replace it. I\u2019d love to get an enhancement request Feedback for your particular use case so we can prioritize making this easier! How can I control the ideal size of a UIViewRepresentable View? I have a lot of trouble getting correct automatic sizing of wrapped views, especially if they wrap UIStackView. Any recommendations for getting proper automatic sizing so I don't need to use fixedSize so much? Try implementing intrinsicContentSize on your view. Is there any way to open additional window from SwiftUI Life cycle? I mean call additional WindowGroup. For example Inspector panel. Hi - we currently do not have any API for this, though we\u2019d love if you could file a feedback with any specifics you have for your use case. One option for something like an inspector panel is that you could use NSApplicationDelegateAdaptor, and open your window by communicating with that. A secondary WindowGroup might not be the best match for something like an inspector panel, since WindowGroup supports multiple windows, and will should up in the File -> New menu. Oh, I should mention that the window created via the delegate adaptor would be an NSWindow instance, with an NSHostingView as the contentView. For when using @EnvironmentObjects, it seems like I need to pass the .environmentObject modifier for every layer of subview. If I don't do that, it will give error \"view environmentObject may be missing as an ancestor of this view\" But I thought the point of Environment Objects is to make data available to all view without writing a lot of code. Is that a design choice or am I doing something wrong? Thanks for the question. .environmentObject(\u2026) mostly flows down the view hierarchy, however there are some places where we intentionally do not forward it. For example, the destination of a navigation link does not get the environment object, because there is a conflict as to whether the destination should get the object from the originating link or the placeholder of the destination, or even the previous view on the stack. There was also a bug where sheets and popovers didn\u2019t get the environment object, but that was fixed. If you find other places where environment object does not propagate as expected, please file a Feedback. It\u2019s important to me that we get this right! Is there a way to control the opacity/overlay effect applied to a Menu's label in iOS? Menu currently applies a high-opacity white overlay (I assume!) on its label when tapped, and I'd like to make it less opaque/override the effect entirely. Not today, but if you haven\u2019t already please file a feedback requesting the ability to do so. Oh, and adding context for why you want to customize that effect is always really useful too! Can the new AtrributedString be used to provide accessibility notations in SwiftUI like NSAttributedString s can in UIKit? Should we report any attributes from NSAttributedString we need on AttributedString ? AttributedString should have the full suite of accessibility attributes and they should now be standardized across platforms! But if you see any we missed, please do file Feedback so we can fix any oversights. Thanks for the question. We\u2019re super excited about the AttributedString support! Localization, accessibility, formatting, and morphology agreement in one tidy package. I\u2019ve had a lot of inconsistent behaviour with EnvironmentObject where it isn\u2019t passed in, nil when referenced, etc. Am I holding it wrong, or what could some common pitfalls be with using it? I've reverted to a singleton for global access but I'd rather get EnvironmentObject right. It\u2019s a little hard to tell exactly what issue you\u2019re running into just from this description, but sorry to hear it\u2019s been causing you trouble! How have you been \u201cholding it\u201d, so to speak? Hi! Thanks for answering questions here! I have an app that needs to hide the home indicator, but that does not seem to be possible in SwiftUI. I know I can do it with exchanging the root ViewController but that has a lot of side effects in a pure SwiftUI app. Is there a better workaround? Thanks for the question. I\u2019m afraid we don\u2019t have SwiftUI API for this currently. Using UIKit is the best option for now. If you could submit an enhancement request Feedback, I\u2019d really appreciate it. Is it okay to use child CoreData context as @StateObject vars in SwiftUI views, or would you recommend passing those in the environment and holding on to them outside of SwiftUI? There is not need to use a @StateObject since the context is not an ObservableObject. Passing it down using the environment is a good solution. What\u2019s the best practices for making a SwiftUI view accessible? You should get a lot of Accessibility automatically. The best way to determine whether you need to make tweaks, using our Accessibility APIs, is to try your view with some of our features, like Large Text, VoiceOver, etc. You can also take a look at the new Accessibility Inspector in SwiftUI and see what it shows for your Preview view. In general, system standard controls tend to be accessibly by default, but when you use a lot of Shape or other manual drawing, you may need to do more work. Hi Arie. A great place to start is our introduction video from 2019. https://developer.apple.com/videos/play/wwdc2019/238/ This will introduce you to accessibility in SwiftUI. You could follow that up with this years presentation where we go beyond the basics. https://developer.apple.com/videos/play/wwdc2021/10119/ We also have many WWDC videos on best practices when making a view accessible. Such as writing great accessibility labels. https://developer.apple.com/videos/play/wwdc2019/254/ When using SwiftUI with something like Core Data and UIHostingController, how can we avoid using AnyView if the environment modifier changes the type of the view. See example below. `import UIKit import SwiftUI import CoreData struct MyView: View { var body: some View { Text(\"!\") } } class MyHostingController: UIHostingController<MyView> { override func viewDidAppear(_ animated: Bool) { super.viewDidAppear(animated) // custom stuff here } } class TestViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() let persistentContainer = NSPersistentContainer(name: \"MyStore\") let rootView = MyView().environment(\\.managedObjectContext, persistentContainer.viewContext) let hostingController = MyHostingController(rootView: rootView) // this will not work anymore because type has changed with the environment modifier // more stuff } } Great question! Here\u2019s a technique, though it runs the risk of angle-bracket-blindness \u2026 In class MyHostingController: UIHostingController MyView isn\u2019t really the right type. You want the type of MyView().environment(.managedObjectContext, persistentContainer.viewContext) On the let hostingController = \u2026 line, you\u2019re probably getting an error message about the types not matching. That error message will include the full type of the right-hand side of the assignment. Something like ModifiedContent with lots of stuff inside the \u2026 there. What I like to do is (1) copy that type, then (2) add a top-level type alias: typealias MyModifiedView = ModifiedContent where the right-hand side is the value I copied from the error message. Then you can write class MyHostingController: UIHostingController \ud83d\uddd3 Thursday When using the new SwiftUI Table View, can you Group to have more than 10 TableColumns? Yes, you can; just like Views! Do I understand correctly that there are no more limitation on the number of objects in @ViewBuilder? There is no change to @ViewBuilder this year, so it is still limited in the number of elements it can build into a block. But Group as well as nested builders are great tools to allow for as many views to be combined together as you want What happens when you have three views in a hierarchy, where the root and leaf views share dependency, but neither share dependencies with the middle view. Would the middle view get redrawn? If the middle view isn\u2019t dependent on the others, it isn\u2019t redrawn. Note that if the root view passes parameters to the middle view, then it has a dependency. Is there a good way to switch between HStacks and VStacks while allowing SwiftUI to understand that the contents of those stacks are the same? Lots of questions similar to this, so repeating an answer from earlier: Yes! You should check out the matchedGeometryEffect() API, which was introduced last year: https://developer.apple.com/documentation/swiftui/view/matchedgeometryeffect(id:in:properties:anchor:issource:) Is the @ViewBuilder to remove AnyView in \"Demystify SwiftUI\" only available with iOS 15 ? No, in fact it can back-deploy to any previous version! Are there performance considerations when generating UUIDs? (I.e. generating a lot of UUIDs for elements in an array to display in a list). It depends. You'll need to measure. However, consider generating IDs lazily (make sure you store them though!) Performance wise, is it preferred to pass ObservableObject s down to subviews explicitly, or use EnvironmentObject s? Using one or the other shouldn\u2019t make much difference in any given view, but if you don\u2019t need to use the object in some views then EnvironmentObject is a great way to avoid the boilerplate of pass it down through intermediary layers, and can avoid creating unintentional dependencies. Is there a good way to apply a modifier to a SwiftUI view conditionally? I'm using a custom .if modifier, and it refreshes the whole view on state change :( Consider making an inert version of the modifier as discussed in the session. If there is a modifier that lacks an inert version that you'd like to see, please file a feedback. How can we conditionally set different modifier, for example list styles? Styles in SwiftUI are static, and are not permitted to change at runtime. This is a case where a branch is more appropriate, but consider whether you really want to change the style\u2014 a single style is almost always the correct choice. If there is a certain dynamism you're looking for here, please file a feedback. Is there any situation in which Hashable may be preferred to Identifiable? If you just need to be able to identify a value, that\u2019s what Identifiable is for, which means that only the id needs to be Hashable, not the whole type. If we apply same id in the condition, will SwiftUI sees this as a same view var body: some View { if isTrue { Text(\"Hello\") .id(viewID) } else { Text(\"World\") .id(viewID) } } No, these will be two different views. Raj (Apple) 3 days ago This is because body is implicitly a ViewBuilder. If you don't use a ViewBuilder, such as in another property, it would be the same view. Are there any difference between passing a State as a binding through each view in the hierarchy vs passing it as an environment object and only access it in the subviews that use it, say in a Text view would the first one have worse dependency graph that needs more updates? environment object is a slightly different tool than State since it requires an ObservableObject. Environment object is optimized to invalidate only the views that read its value. As for State, when you pass the binding down to subviews, changing the binding will mutate the state that in turn will invalidate the view holding the state and its children. What is a good way to have a custom order of elements in a list so the user can change the order (using a stable ID)? A List or ForEach will preserve the same order as used in the collection passed into them. The identity of each element should be independent of that order \u2014 even if a collection is reordered, each element should maintain the same ID. If you maintain a stable identity like that, then you should be able to reorder the collection no problem in response to user actions Is there a way to animate a View moving from one place in hierarchy to another? Like when a View changes parents? Yes! You should check out the matchedGeometryEffect() API, which was introduced last year: https://developer.apple.com/documentation/swiftui/view/matchedgeometryeffect(id:in:properties:anchor:issource:) Fun question on conditions: We need to put them at least in Group { if whatever { ... } else { ... } }. So ... is it preferable to use ViewBuilder commands to create it, like ViewBuilder.buildBlock(pageInfo == nil ? ViewBuilder.buildEither(first: EmptyView()) : ViewBuilder.buildEither(second: renderPage) ) Awful to read, but is it better? Using a Group is preferred in this case. The Dependency Graph \u2014 can it have loops, or is it acyclic? Graph cycles are not allowed and will trap at runtime. I have a class Store : ObservableObject that holds the whole app state. It lives as @StateObject in the root of the App lifecycle and passed via environment to all views. View send actions to the store and update as soon as store's state updated. It allows me to keep the app consistent. Could you please mention any downsides of this approach from your prospective? That approach makes every view in your app dependent on a single Observable object. Any change to a Published property forces every view that references the environment object to be updated. I think this might cover your question too. -by \"forces every view that references the environment object to be updated\" you mean that view produces new body to diff with old body? That view produces a new body. SwiftUI doesn\u2019t diff views. It regenerates views when dependencies change. is there any difference between the persistence of @State vs @StateObject vs @ObservedObject when identity changes? Yes! ObservedObject does not have any lifetime implications\u2014 when you use that property wrapper, you are responsible for managing the lifetime yourself. The object can live anywhere\u2014 either in StateObject or possibly even in another data structure in your app. Should @State and @StateObject be usually defined as private ? Yes! It's often helpful to make them private to indicate that the state is for this view and its descendants. Thank you! It was a great talk! If the only reasonable property for establishing stable identity in a model from, say, an existing JSON API, is a string that might be quite long (maybe it's more of a description than a name), is that large ID enough of a potential performance problem that we should consider modifying the model/API to add a more compact ID? Thanks much! Great question! As with any performance problem, it's best to measure first before over-optimizing. Long strings can often be expensive, so it might make sense to optimize the identifier, but I'd recommend measuring first. I have created a ViewModifier that adds a custom modal \u201coverlay\u201d on top of a view, somewhat similar to a sheet. Is there a way to pass a View as parameter of this ViewModifier initializer, without resorting to AnyView? I want to be able to pass the view that will be the actual \u201ccontent\u201d of the overlay. You can do this by making your custom ViewModifiergeneric on the content, something like: struct MyModifier : ViewModifier { ... } , then using a property with the generic like so: var content: C From the dependency graph part of \"Demystifying SwiftUI\": The video mentioned that both views, two views that are dependent on the same dependency, would need to generate a new body. Which view would need to generate a new body first if one view is the child of the other? The parent view will generate its body first, and recursively travers all the child views. Is the computed UUID not stable because we cannot predict the position of the value, therefore where it will be placed in the view? Yes Do situations exist when the developers really need to use AnyView and there are totally no alternatives using Swift constructs? Lot\u2019s of questions about when it is okay or not to use AnyView! If you can avoid AnyView, we recommend that. For example, use @ViewBuilder or pass view values around using generics instead of using AnyView. However, we offer AnyView as API because we understand that there are some situations where no other solution is possible, or comes with other important tradeoffs. One rule of thumb: AnyView can be okay if used in situations where the wrapped view will rarely or never change. But using AnyView to switch back and forth between different values can cause performance issues, since SwiftUI has to do extra work to manage that. Do you have any suggestions for why Image(\"AppIcon\") would fail to work, when Image(uiImage: UIImage(named: \"AppIcon\")!) works just fine? Is that intentional behavior? Is the image located in an asset catalog? Yes. It\u2019s the AppIcon in the asset catalog. (As in, the one used for the app icon on the home screen.) Please file a report through feedback assistant if you haven\u2019t already. If you have, can you post the number here? I\u2019d wonder whether the asset compiler is treating the app icon specially. We should fix it, but perhaps the app icon is being copied elsewhere in the bundle so the system can reach it. Is there a way to debug AttributeGraph crashes? I\u2019m getting AttributeGraph precondition failure: \"setting value during update\": 696. , probably due to a hosting controller somewhere, but don\u2019t know how to track it down. Thanks for the question! That error usual means that some code evaluated in body or updateUIViewController (or NS\u2026) is mutating state. We have a new debugging tool in the new SDK that might help narrow it down. Inside body if you write Self._printChanges() SwiftUI will log the name of the property that changed to cause the view to be redrawn. (Note the underscore. This isn\u2019t API, but is for exposed for debugging.) Still hoping to get any info on if you can hide the tab bar when using TabView and pushing items onto a NavigationView like you can in UIKit. I do not believe there is a TabViewStyle that will allow this. However, please file a request using feedback assistant describing your use case. Thanks! I've started an app where I've got 3 screens - selected by tab views. I'd like to create a setting to default to one of the screens or the other. What's an efficient way to do that? Maybe a lab type question... You would use the selection Binding passed into the TabView initializer to decide which view will be selected by default. struct ContentView: View { @State var selectedTab = 1 var body: some View { TabView(selection: $selectedTab) { Text(\"First\") .tag(0) Text(\"Second\") .tag(1) Text(\"Third\") .tag(2) } } } That will have the view start with the 2nd tab selected Any way to control when the @SceneStorage gets saved? Many are in my root app view, and they only seem to be written at app background, not before. I\u2019m sorry, but we don\u2019t provide API to control scene storage. The intent is that it\u2019s saved on backgrounding and restored on foreground/scene recreation. As a college student, working as an intern at Apple is really important to me. What is your best recommendation for individuals considering applying for an internship at Apple, whether it be related to a resume, project types, or otherwise? It\u2019s great to hear you\u2019re interested in working at Apple! I joined as an intern so I can definitely relate to the craziness of applying, but it is absolutely worth doing. Interning at Apple is a really wonderful experience, and you really do some incredible work over the course of an internship! Getting hired at Apple means being very intentional about what you\u2019re looking for, and the best way to do that is to explore the jobs site for roles you think you can do, and applying specifically for that role. We have thousands of hiring managers and I don\u2019t know most of them, so my recommendation won\u2019t carry much weight, so I can\u2019t be of much help there. My best advice is to create a compelling cover letter and r\u00e9sum\u00e9 that expresses why you are right for this specific role, rather than applying for dozens of jobs at once. There\u2019s lots more information for you here as well: https://www.apple.com/careers/us/students.htm Is there an efficient way to set the values of individual pixels in a view in SwiftUI? Sorry I haven't fully examined the new Canvas API yet. For the record, I don't really know how to do this in UIKit either, other than writing a custom fragment shader for a Metal view. Thank you! Canvas is really the best bet here. You could fill a 1px x 1px rectangle of the color you want, and that would be the most efficient expression of that within SwiftUI itself. And to get the size of a pixel you would request the pixelLength from the environment of the GraphicsContent of the Canvas When to iterate a collection directly from List(items) { items in Code here vs. List { ForEach(items) { items in Code here... Fundamentally these should provide the same behavior, in almost all use cases this would be a stylistic choice over which to use. The former works great if you know you'll only ever have that single array of items in your list. The latter is where I generally start because its less things to change if I want to provide content from multiple sources into my list. Will TextEditor have any level of support for AttributedStrings? While we can\u2019t comment on any future plans, please do file a request through feedback assistant for any improvements to TextEditor that you would like to see! Is it possible to specify paragraph styles AttributedStrings used by SwiftUI Text views? I don't see any mention of paragraph style in the SwiftUI AttributeScope (but it's there for the UIKit scope) SwiftUI currently supports all of the attributes in the SwiftUI scope and some in the nested Foundation scope, the supported Foundation attributes are links and inlinePresentationIntent. So it\u2019s not possible to specify paragraph styles Is it possible to place a CALayer inside of a SwiftUI view without using a UIViewRepresentable? Not currently. UIViewRepresentable is the way to go if you want to expose custom layers inside of your SwiftUI content. If you have a specific use-case where this can't work, please do file a feedback request! How do I get a SwiftUI time picker to change to 5min instead of 1min? I tried to find that but so far I haven't found how to do that... Thanks for the question. Making sure I understand\u2026 Are you using a DatePicker and want the stepper increments to be 5 minutes instead of 1 minute? no actually a time picker, .datePickerStyle(WheelDatePickerStyle()) Ah, right. And that\u2019s applied to a view hierarchy containing a DatePicker then? Set to just show time components? Yes: aDatePicker(\u201cPlease enter a time\u201d, selection: $startTime, displayedComponents: .hourAndMinute) .labelsHidden() .datePickerStyle(WheelDatePickerStyle()) And after all that clarification, I\u2019m afraid the answer is that we don\u2019t provide customization for that currently. In the past, I have created Button views that will have a local state and present a sheet. For instance, I might have a Button whose purpose is to add a new item to my app's data. This was super convenient because I could add such a button in any context without needing the view adding the button to know about that functionality. Running my App Store version of my app on iOS 15, those buttons no longer are working. (I haven't yet debugged the app; I'm simply noticing it across my iOS 15 devices.) Is this a sign that this is a pattern I shouldn't be using? Or should SwiftUI continue to support such patterns? Please file a report using feedback assistant and a sample project. Thanks! There are instances when Buttons in SwiftUI stop responding to the iPadOS pointer, but work when tapped normally. Is this an expected behavior? Please file a report using feedback assistant and a sample project that reproduces this issue! I would not consider this expected Is it possible to change the spacing between views in a List (or stack view) as the list scrolls up and down in a springlike manner? (i.e. is there a SwiftUI equivalent to UIKitDynamics?) Hi Andrew, we don\u2019t have any support for this in SwiftUI. Could you please file a feedback with the use case you are interested in? Thanks! What\u2019s the recommended practice for custom sheet presentations? SwiftUI\u2019s .sheet only supports the standard card-style modal presentation. It doesn\u2019t have the flexibility of e.g. UIKit\u2019s transitioning delegates and presentation controllers to let us size and position modally presented controllers. Thanks for the question. For custom presentations, I\u2019ve had good luck putting the presentation content in an overlay, then using an offset modifier to shift it off screen. To bring it on screen, I use State to zero out the offset. If the state is updated inside withAnimation to transition is animated. That content won\u2019t be lazy then, right? It\u2019ll be computed in advance and off-screen. You could conditionalize the content too. An offscreen empty view would be essentially free. As would an offscreen view with opacity of 0. You might need to play with the transitions to get the appearance just right. And it\u2019s essentially free because the framework is smart enough to not bother rendering content that\u2019s invisible? Yep! This isn't a technical question, but what is your favorite part about working on the SwiftUI Team? The people, both inside and outside of Apple, by a million miles! Everyone I\u2019ve worked with here at Apple is kind, empathetic, thoughtful, and incredibly passionate about the work we do. It requires lots of collaboration to pull all the work we do into one cohesive framework, so working with a great team is super important. Re. people outside of Apple, you all are the best community I could imagine. It\u2019s really wonderful getting your feedback, hearing from you at WWDC, seeing you at community events, etc. As Josh said, empowering you all to make amazing apps is really special. Is it possible to prevent a Button inside a List item from being activated when the list row is selected? Right now, if you have multiple Buttons they will all be triggered when the row is selected, so can't really have a secondary button in a list row. Explicitly setting the buttonStyle of the nested buttons will stop the List from capturing the event. What is the recommended way to open a View in a new window in SwiftUI for macOS? We don\u2019t have much API in this area at the moment - using a NavigationLink in a commands context will open a window with the destination view. Could you elaborate on your use case here? If not, that is ok, but I would definitely suggest filing a feedback with any info you can provide us. How would someone go about turning a List cell into a TextField on tap, such as to edit the name (similar to the Reminders app)? I\u2019d try putting a TextField in an overlay of your cell content with an opacity of 0 and disabled. Then use a tap gesture to switch the opacity and You could use the new FocusState or onSubmit when the user is done editing to switch back. What about the new selectable text modifier?? Would that work here? The trickiest bit will be getting your TextField and the regular Text to line up. You might need some custom padding for that. Take a look at the ScaledMetric property wrapper to make padding that adapts to dynamic text size. That\u2019s a great idea. It would work for selecting and copying the text, but not for editing it. There's a lot of sample code out there where people use GeometryReader inside of clear overlays and the like to not impact layout... is that \"ok\"? Seems not like the way it was designed to work. In general we'd consider that a bit of a misuse of the API. If you have specific use-cases where you're needing to do this we'd love to hear about it in a feedback report! This is kind of vague, but I know HID declares that a second button below a Sign in With Apple Button can't look too similar as to distinguish them. How lenient is this? Like I tried to \"replicate\" the sign in with apple button for a Sign in With Email Button (like a logo next to it with an envelope instead of the apple logo, used a similar but not SF font, etc), but I'm afraid this would constitute as being \"too similar to the Sign in With Apple Button\". Sorry for the vague question, and let me know if this is more suited towards a lab! We cannot provide that kind of guidance here; you are probably better off reaching out to App Store developer review labs for confirmation. I would also review the guidance provided here re: custom buttons: https://developer.apple.com/design/human-interface-guidelines/sign-in-with-apple/overview/buttons/ What is the layout and sizing behavior of Canvas? A Canvas will consume the space offered to it. You can put it in a .frame to control the size if needed. Is there a new SwiftUI way to convert a view into an image or pdf? Currently, this can be done, but it is one of those round about, hacks that no one likes using. Unfortunately, there\u2019s no native SwiftUI api for this right now. As was mentioned elsewhere in the thread though and in your question though, it can be done by wrapping in a UIHostingController. Feel free to file a feedback requesting the feature though! What is the best way to implement keyboard bindings on swiftUI, for macOS? I'm building a video player where I would like to forward and rewind with the left and right buttons. I've tried attaching keyboardShortcut(.leftArrow) on my button but it doesn't work? Is it the view I'm attaching it wrong (shall I attach it to my window root view maybe?) or is there some known bug around it? Thanks Attaching it to the button is the correct expression here - it sounds like you may be encountering a bug with this. Could you file a feedback with this issue? With Sign in With Apple, is it a requirement to use a nonce? I've seen conflicting information, some stating that if it's done externally (through something like firebase), then a nonce is required, but if it's done using SignInWithAppleButton then no nonce is required. What's the verdict on this? As this does not appear to be related to SwiftUI, I recommend scheduling an appointment at one of the Apple Pay labs Is there a modifier to control how links in AttributedString are handled? Specifically, I'd like to be able to handle some links in-app, rather than booting the user out to Safari Currently the links will be handled by using the default OpenURLAction from the environment, it\u2019s not possible to override it at the moment. You could however use a custom url scheme that will open the url in your app, for example myapp:// Let\u2019s say I have a purely SwiftUI flow. I have a ListView with a @StateObject var listFetcher , that makes requests for a list of items. Tapping on an item in the list navigates to a DetailView, which has an ObservableObject property detailFetcher , that makes requests for details on the item. What\u2019s the best way to structure DetailView and which property wrapper would we use for detailFetcher in DetailView? 1. Have an initializer like init(itemID: Int) , and use @StateObject ? This would require us to eventually update the detailFetcher property with something like detailFetcher.itemID = itemID in the body\u2019s onAppear 2. Pass in the detailFetcher into the initializer like init(detailFetcher: ObservableObject) and make the property @ObservableObject? If this is preferred, where would this detailFetcher live if not in SwiftUI? In general, use @StateObject when the view in question owns the associated object, i.e. the object will be created when the view is created, and should be destroyed when the view is removed. In contrast, use @ObservedObject when the view needs to reference an object that is owned by another view or something else. So the view is dependent on the object, but their lifetimes are not tied together. For example, you could have a main screen that uses @StateObject to initialize your app\u2019s model, and then pass that object off to detail screens using @ObservedObject. Also check out the \u201cDemystify SwiftUI\u201d talk tomorrow to learn more about this! With the new Markdown support for Text, are you able to control what specific Markdown elements look like? For example, can we specify that bold is supposed to display as Demi-bold rather than bold? Or what the font, font size, or font weight of an H1 header should be? In general, how much flexibility do we have in styling the markdown? At the moment, it\u2019s not possible to control the default styling in Markdown for SwiftUI. It would be great if you could file a feedback as a feature request/suggestion. Another thing to note, SwiftUI only supports inline styles, paragraph styling such as headers (H1 for example) is not supported. Does SwiftUI support device rotation animations? When I rotate from portrait to landscape, my app just changes without an animation. Please file a report using feedback assistant and a sample project! Is there a suggested way or best practice to show a UIDocumentPickerViewController in a SwiftUI sheet without triggering an \"Attempt to present on which is already presenting ? You should check out the .fileImporter modifier if you haven\u2019t already: https://developer.apple.com/documentation/swiftui/form/fileimporter(ispresented:allowedcontenttypes:allowsmultipleselection:oncompletion:) If you are encountering a specific issue that you think may be a bug, please file a Feedback report! I saw in the \"What's new with SwiftUI\" that TextFields will now have .onSubmit for when the return key is pressed for software and hardware keyboards. Does this mean that .onCommit will be depreciated or does .onSubmit serve a different function? The initializers with .onCommit are soft deprecated. So no warnings in Xcode, but we\u2019re encouraging new code to use .onSubmit. Are current changes \u2018set in stone\u2019 and the feature release plan for SwiftUI has been wrapped up for iOS15 or we could expect more functionalities along the way until the official release? We cannot comment on future plans. However, please file feedback for any issues you find and they will be triaged appropriately! Note that \u201cissues\u201d here refers to both what is traditionally considered \u201cbugs\u201d as well as an feature requests or enhancements both for existing API as well as API that is newly introduced this year If we're using Sign in With Apple and have some text that says \"Sign in with Apple [apple logo here from SF Symbols]\" on a screen that comes after sign in with apple where we ask for some additional information (specifically just the birthday of the user), is that an HID violation? It's only used to reference Sign in With Apple, so I'm not sure if that would violate the rules or not Just to clarify, your view has static \u201cSign in with Apple\u201d text after the button has been tapped/clicked on another screen? Well so this is how it is - There\u2019s a login screen - it either lets you sign in with apple, sign in with email/password, or create an account. If they pick sign in with apple, and this is the first time they go through the process, then it prompts them with the hide my email/name sheet. Then after that, again if it\u2019s their first time, there\u2019s a screen they go to that asks for their birthday, and it says : Sign in with Apple [apple logo] with a subtitle stating that some additional info is required (paraphrased since it\u2019s a little bit of lengthy text), and then a date picker for their birthday. That Sign in With Apple [apple logo] is sort of replicating what\u2019s on the Sign in with Email screen which similarly says Sign in with Email [envelope icon]. My guess is that it would not be allowed since it is giving users the impression that the birthday is part of the Sign In With Apple process, but that is not the case. I think it\u2019s okay to show the birthday step following the sign in process, but as for showing \u201cSign In with Apple \uf8ff\u201d after that, I do not think it is allowed. My recommendation would be to remove this text entirely and instead have something like \u201cFinish setting up your account\u201d As indeed this does not have anything to do with signing in with Apple Are NavigationLinks \"lazy\" in iOS 15? NavigationLinks do not fully resolve their destinations until they are triggered, though the value of the destination view is created when the NavigationLink is created. In general, we recommend avoiding as much work as possible when a view is initialized, which would avoid potential issues here. This is important for performance. Instead, have that work be triggered within the view\u2019s body, such as using onAppear or the new task() modifier. SwiftUI may reinitialize views for any number of reasons, and this is a normal part of the update process. Is there a way to show a watchOS app view using sample data in an iOS app, such as for an app that showcases various custom complication styles? I understand that the alternative would be a screenshot, but I thought it would be a much simpler workflow to have the watchOS views dynamically generated since both platforms use SwiftUI. There is not, but please file feedback for the idea! Using screenshots is your best option as a workaround. On watchOS, I'm using .onContinueUserActivity(NSUserActivityTypeBrowsingWeb) and .onOpenURL , however these only receive callbacks once the app is already open. Is it possible to use these in a way that they receive the callback from a cold launch? Those should function appropriately on cold launch. Please file a feedback with a sample showing where it isn't working if possible. What\u2019s the easiest way to setup the site association file for the example? That is an excellent question. Just the general shape of the application and where it's wired in will go a long way, even if the entire association isn't hooked up. What is the recommended way to account for a NavigationView title that may take up multiple lines? I tried setting .lineLimit to nil, but I am not getting the results that I want. For instance, I have \"XYZ Club Executive Meeting\" for the navigationBarTitle text, but it only shows \"XYZ Club Execut...\" Take a look at the toolbar modifier and the .principal placement. If that doesn\u2019t meet your needs, please file a Feedback so we can look into your use case. The \"What's new in Foundation\" session demos localizing content in a SwiftUI app, \"Caffe\". Is this demo app available as sample code? It doesn't appear to be listed on the sample code page. It is not available. But thanks for the suggestion, we\u2019ll consider that! Is there a way to hide a default green scroll indicator on watchOS when using digitalCrownRotation modifier? There is not, but please file feedback! I'm also curious why you would want hide the indicator? I believe it does hide if you use make it \"continuous\" what would be the recommended way to handle complex navigations in a 100% SwiftUI project, in all my past UIKit project the coordinator pattern was really powerful for that but it's quite tricky to have it ported to swiftui. thanks It\u2019s difficult to make a specific recommendation for a complex app. Every app is so different. This is definitely a great question to request a lab appointment for. Then we can understand your use case and make some suggestions, and maybe even look at some code together. Is there a limit to where SFSymbols change size? I put them as items in a TabView but it doesn\u2019t seem like they are scaling with dynamic text \u2026 TabViews do have a standard symbol size across all apps and how far they scale Does that mean they don\u2019t scale as much as other places? Right, other elements could end up scaling more, which is expected. Ah thanks - As a UI element I sort of wanted it to scale more. Is there a better way to do that. I\u2019d rather not leave SF Symbols If you have a more custom UI element where you want that scaling, you could build that! Especially with the new .bar material, you could still get a similar material background behind that custom bottom bar Ooo - I\u2019ll have to read up about that. Is there a talk with more in depth discussion of material? Today\u2019s rich graphics session is the one you\u2019re looking for https://developer.apple.com/videos/play/wwdc2021/10021/ Not entirely SwiftUI exclusive question, but worth a try: Xcode 13 release notes list as \"known issue\": \"Swift Concurrency requires a deployment target of macOS 12, iOS 15, tvOS 15, and watchOS 8 or newer. (70738378) This suggests that concurrency might be backwards deployed? This seems too good to be true because I'd would need runtime support, but perhaps you guys can shed some light on this? For now you should assume that Swift concurrency cannot be back-deployed, however the team understands that this is a popular request and is investigating. The fun part is that Swift is open source, and so you can visit https://forums.swift.org to peek in on the active development process for Swift concurrency, and they will likely discuss this topic more there as well. Is there a way to change the background colour of a list? Last year this wasn\u2019t possible Currently it\u2019s only possible to change background color of list rows, not the List itself. Thanks @Natalia (Apple) , is this intended, or something that might change in the future? If so, what is the best way to change the background colour currently? I\u2019d like to use system materials specifically Unfortunately, it\u2019s not possible to use system materials with List at the moment, please file a feedback and we will take a look at it. In the mean time, you could try using a ScrollView instead of a List. Refreshable doesn\u2019t work with scroll view. Is it a bug or desired behaviour? Currently refreshable only supports List. Please file a feedback request if you have other use-cases you\u2019d like to see supported. How would one go about adding the .searchable modifier to a PDFView? On iOS, the searchable modifier works with a navigation view to render its search field. On macOS, it works with a navigation view or the root view of your scene. If you combine a PDFView with a navigation view, things should hopefully work as you expect. If not, please do file a feedback with some more details on what you would like to achieve. Is .searchable for local data only, or can it be used to query services? Searchable specifies that the view supports search. You can implement that search however works best for your app. For example, you could kick off a query using the bound search term, then update the results when your query completes. Are you able to programmatically change focus of text fields natively in SwiftUI this year? For example, having the a button at the bottom of the keyboard say \u201cnext\u201d and focus into the next text field? Posting this back here in case anyone present does have a moment to answer, but if not, there\u2019s a focus and accessibility specific office hours later this week. If we\u2019re not able to get to this during this block, definitely check back then! Indeed, I would take a look at the new FocusState APIs! https://developer.apple.com/documentation/swiftui/focusstate/ Is there an equivalent API similar to UISheetPresentationController for SwiftUI? Sorry. There is not a SwiftUI equivalent at this time. Is that something that would be useful to file feedback for? Certainly! Especially if you have a specific use case around when you need it or whether you need variable height. Thanks! Hi! intro? I\u2019m a hobbyist who is getting better an programming mostly because SwiftUI is so productive! Can you talk about ways to have existing SwiftUI code inherit the newer features so I could support both iOS 14 and 15 with one codebase? Most new features do not back-deploy to earlier OS versions. You can use e.g. if #available(iOS 15, *) { to check for whether a feature can be used. That said, some functionality is able to back-deploy. For example, the new ability to pass bindings to collections into List and ForEach, and get back a binding to each element, e.g. ForEach($elements) { $element in ... is able to back-deploy to any earlier release supported by SwiftUI. I thought Matt said that - So the new feature can actually work in older code? Correct Can .importsFromDevices (i.e. continuity camera) work as a source for AsyncImage? No, continuity camera will callback your importsItemProviders with an item provider that will give the full image data and not a URL. But if you have a use case where there could be some better integration between the two, please file a feedback Is there a way to animate text font size changes? Currently in iOS14 a view can smoothly animate between two sizes or colored backgrounds, but fonts will just jump from the before animation size to the after size with no interpolation in between. This would be a great feedback to file, so that we can investigate! If you\u2019re interested in a workaround, the Fruta sample code project has an AnimatableFontModifier that uses an explicit font size as its animatable data. This is used for the ingredient cards in the main smoothie detail view. This works for Fruta because the use-case is limited and the text using this modifier is for primarily for graphical purposes. https://developer.apple.com/documentation/swiftui/fruta_building_a_feature-rich_app_with_swiftui Will Xcode 13 run on Big Sur 11.3 Xcode 13 requires macOS 11.3 or later. You can find the full release notes here: https://developer.apple.com/documentation/xcode-release-notes/xcode-13-beta-release-notes Does AsyncImage support caching, and will there be an API to customize that cache? AsyncImage uses the shared URLSession , and so uses the shared URLCache. There\u2019s currently no support for customizing the cache. If that\u2019s something you\u2019d like support for though, feel free to file feedback with that request Is there a way to make refreshable work with LazyVGrid or do we have to adapt List as a Grid layout? Refreshable is currently only supported by List. With a document-based app using a DocumentGroup scene, is there any way to present a choice of templates when opening the app without an open document? I\u2019m assuming you mean something similar to the theme chooser you get in Keynote or Pages? This is not something that we have an API for at the moment, but we\u2019d love a feedback for this functionality. Any prospects for AttributedString support in TextEditor? While we can\u2019t comment on any future plans, please do log a request through the feedback assistant that this is a desired feature so that we can gauge developer interest! What is the proper way to dismiss a .fullscreenCover when it is being presented based off an item: setting the item to nil or dismissing using the presentationMode? That really depends on what is doing the dismissing. They both have the same effect in setting the item binding back to nil. It is more about where you are driving the dismissal state from. If you're dismissing from within the sheet's content, then using the presentation mode will be more flexible because that will work no matter what is controlling the presentation. There is also a new environment property introduced this year that can accomplish this as well, which is dismiss. https://developer.apple.com/documentation/swiftui/environmentvalues/dismiss/ is the refreshable property the only SwiftUI property that supports async code? The task modifier also provides support for async code out of the box! Generally, we only make user provided closures async if we can provide some additional utility to you by doing so, such as canceling a task when the attached view\u2019s lifetime ends, or finishing the refresh animation. If you want to dispatch to async code in other places, you can use an async block! \ud83d\uddd3 Wednesday Using @Environment(\\.isSearching) , how would multiple navigation bar in an app work? The isSearching environment property is tied to the searchable modifier that sets it up. If you have the following: NavigationView { ContentView() DetailView() } .searchable(text: $text) Then you could query the isSearching property inside of ContentView or DetailView. If you have the following: NavigationView { ContentView() DetailView() .searchable(text: $text) } Then you could only query it inside of DetailView. Similarly, if you have the following: NavigationView { ContentView() MiddleView() .searchable(text: $text) DetailView() .searchable(text: $text) } Then you could query isSearching in either MiddleView or DetailView and the property would relate to the enclosing searchable modifier. With the goal to bring deeper adoption of SwiftUI this year, should we implement more SwiftUI code instead of improve catalyst apps ? This really depends on your particular app and use case. It would be a great question to take to the labs! Does using more SF Symbols have an impact on total app size? Or they all are stored in the OS? These are part of the OS, so you can feel free to go wild with all the symbols you want with no impact to app size When using the WatchKit way to get text input, do we still get the text input improvements from SwiftUI? E.g. remembering the user choice of scribble vs. voice input If don't pass any suggestions to the WatchKit API, then yes, you get all of the new behavior. does the modifier of importItemProviders work with other commands, or other stuff to import things in the app ? It works with any macOS system service that vends data. So if you had a service or shortcut that didn\u2019t take input, but produced output \u2014 your app\u2019s importItemProviders could consume that data when the user invokes that service The new detents for sheets in UIKit, to allow them to sit at different heights, looks amazing! Unfortunately there doesn't seem to be an equivalent in SwiftUI \u2013 is there any plan to bring it across? This is something we know folks would appreciate. Enhancement request Feedback with particularly use case would really help us. We\u2019re particularly interested in use cases around variable heights in SwiftUI sheets. Can the new .refreshable API be used with custom refresh controls / spinners? Take a look at the EnvironmentValues.refresh property. The refreshable modifier sets this property up with closure provided to the modifier. You can query this property ( @Environment(.refresh) private var refresh ) to hook up to your own UI Can we use the new buttons with non-SF single-color graphics? Yep, in fact the new buttons can have a label of any view, including shapes and more! The one thing to be careful of with custom images is that they use template rendering if you want the standard foreground styling within the button. (otherwise they\u2019ll be the exact color of the image\u2019s pixels) Can we customize and put \"searchable\" search box anywhere? (Without going the old way, adding text views) The searchable modifier supports a few customization points with the SearchFieldPlacement type. You can pass a placement into the modifier: .searchable(text: $text, placement: .navigationBarDrawer(displayMode: .always)). How do we assign a TextField in SwiftUI to gain focus as soon as a View opens? You can use the new @FocusState property wrapper and focused() modifier to programmatically control focus in many different ways, including when your UI first appears. For example, you could trigger a specific view to be focused using onAppear or any other event callback. How to force SF Symbols not to be automatically filled/unfilled based on the context (as shown in tab bars)? Is there any way to force them to be non-filled? You can use .symbolVariant(.fill) to opt in a view or view hierarchy to fill variants, and see the SymbolVariants type for other variants you can use with it. Is there a SwiftUI equivalent for preferredStatusBarStyle in iOS 15? I\u2019m afraid there is no new API for that at this time. That would be a great enhancement Feedback to help us gauge interest. If you\u2019ve already filed it, thank you! We know it\u2019s something folks want.","title":"SwiftUI"},{"location":"wwdc21/SwiftUI.html#swiftui-lounge-qas","text":"","title":"SwiftUI Lounge QAs"},{"location":"wwdc21/SwiftUI.html#by-emin","text":"","title":"by emin"},{"location":"wwdc21/SwiftUI.html#tuesday","text":"","title":"\ud83d\uddd3 Tuesday"},{"location":"wwdc21/SwiftUI.html#whats-your-recommended-approach-to-displaying-a-new-macos-window-not-a-new-document-or-a-confirmation-dialog-just-here-is-some-information-that-makes-sense-to-show-right-now-such-as-unlocking-an-iap-ive-seen-hacks-with-handlesexternalevents-but-its-not-very-pleasant","text":"We don\u2019t have much API in this area. A NavigationLink when used within a commands context will open a new window, and as you noted, handlesExternalEvents can be used to, though this is primarily due to the default behavior of that modifier, which is to create a new window for the given scene, if no windows prefer to handle the event. We\u2019d love a feedback for this functionality, and if you can share any specifics of your use case in it, that is even better. Thanks!","title":"What's your recommended approach to displaying a new macOS window? Not a new document or a confirmation dialog, just \"here is some information that makes sense to show right now\", such as unlocking an IAP. I've seen hacks with handlesExternalEvents() but it's not very pleasant."},{"location":"wwdc21/SwiftUI.html#for-swiftui-macos-document-apps-is-there-a-recommended-way-to-save-and-restore-window-size-and-position","text":"Hi, when state restoration is not enabled on macOS, this is expected behavior at the moment. We\u2019d welcome a feedback for this, though. If you could include any information about your use case as well, that\u2019d be very helpful. Thanks!","title":"For SwiftUI macOS document apps, is there a recommended way to save and restore window size and position?"},{"location":"wwdc21/SwiftUI.html#suggestion-to-the-designers-amongst-the-swiftui-folks-present-it-would-be-great-if-there-were-more-examples-of-unique-user-experience-interactions-especially-navigation-instead-of-assuming-that-everyone-wants-to-always-use-navigationview-as-the-central-ux-construct-uikit-was-only-slightly-better-with-sparse-examples-to-customize-navigation-view-transitions-but-it-too-lacked-complete-flexibility-would-love-to-hear-your-feedback-on-this","text":"Unfortunately customizing navigation transitions isn\u2019t supported today \u2014 meaning if you want to invent awesome transitions you\u2019ll need to create your own containers and the state that manages them\u2026 but you\u2019ll need to ensure your custom goodies properly handle accessibility, common system interactions, and more which can be tricky. Ideally, please file feedback with some use cases or examples for what you have in mind, I would love to see the team take a look at enabling awesome experiences that work well with the system!","title":"Suggestion to the designers amongst the SwiftUI folks present:  it would be great if there were more examples of unique user experience interactions, especially navigation, instead of assuming that everyone wants to always use NavigationView as the central UX construct.  UIKit was only slightly better with sparse examples to customize Navigation View transitions, but it too, lacked complete flexibility.  Would love to hear your feedback on this!"},{"location":"wwdc21/SwiftUI.html#is-there-ever-a-case-in-which-spacer-is-superior-to-framemaxwidth-infinityframemaxheight-infinity","text":"Howdy! There was a similar question yesterday about Spacer \u2018versus\u2019 frame, check out the answer for how they compare: While Spacer is a view, it doesn't end up displaying anything of its own so it is plenty lightweight. Using .frame can have other behavior introduced to the way the view gets laid out beyond just changing its size. They both have their uses, so use them each where appropriate. To add a little more onto this, even in cases where you will get almost entirely the same behavior between the two, the performance difference will be so minimal that I would strongly suggest prioritizing code readability over performance / memory use to make this decision. If Spacer makes it more clear what layout you\u2019re trying to specify, use that, and vice versa. But the gist is: they both have their purposes and are extremely efficient from a performance perspective. So, you should focus on what your intention is and which reads better or offers the functionality you\u2019re trying to utilize as you construct your hierarchies","title":"Is there ever a case in which Spacer() is superior to .frame(maxWidth: .infinity)/.frame(maxHeight: .infinity)?"},{"location":"wwdc21/SwiftUI.html#hey-is-it-a-way-to-have-the-coordinate-of-the-cursor-inside-an-image-in-swiftui","text":"I would like to display a crosshair by drawing two lines from the cursor to the edges of the picture. I tried with NSEvents but it gave me the cursor location relative to my screen. thanks, Nicolas Howdy! This sounds like excellent topic to file feedback to help the team understand your use case, as I don\u2019t believe this is supported today.","title":"Hey, Is it a way to have the coordinate of the cursor inside an image in SwiftUI."},{"location":"wwdc21/SwiftUI.html#how-do-we-set-preferredscreenedgesdeferringsystemgestures-in-a-swiftui-app","text":"Currently you need to use a representable to access that property.","title":"How do we set preferredScreenEdgesDeferringSystemGestures in a swiftUI app?"},{"location":"wwdc21/SwiftUI.html#are-you-able-to-give-some-indication-of-why-the-semantic-background-colors-from-uikit-are-missing-we-only-have-colorbackground-as-opposed-to-the-main-varying-alternatives-and-its-annoying-to-have-to-wrap-them-repeatedly","text":"A goal for us is that the Color and other API we provide works great on all platforms. So there are often other APIs to help get those effects: for instance GroupBox on iOS will produce the stacking (secondary and so on) system background colors, and on macOS gives you a similar effect but using the more platform appropriate group box (9-part) artwork instead of colors at all. The new .primary, .secondary, .tertiary, .quaternary shape styles are similar in that they also work across platforms and now can correctly give the correct vibrant rendering effect on top of materials So while we might not add a Color for everything that UIKit has, we do want to make sure we have coverage for those design concepts. (so specific feedbacks on things that are missing are always appreciated!) Things like label , secondaryLabel, etc are all achievable using those shape styles above and are even smarter in those material contexts, i.e. .foregroundStyle(.secondary)","title":"Are you able to give some indication of why the semantic background colors from UIKit are missing? We only have Color.background as opposed to the main varying alternatives, and it's annoying to have to wrap them repeatedly."},{"location":"wwdc21/SwiftUI.html#if-i-pass-around-a-single-stateobject-through-injection-from-view-a-to-subview-b-to-subview-c-when-the-data-updates-will-all-3-views-get-invalidated-and-redrawn-what-if-only-view-a-and-c-need-to-use-the-data-and-view-b-is-only-a-middleman","text":"Only views that read properties of the object should update, but do note that if a view reads any property it will be invalidated on objectWillChange, even if the particular property begin read did not change.","title":"If I pass around a single @StateObject through injection from View A to Subview B to Subview C, when the data updates, will all 3 views get invalidated and redrawn? What if only View A and C need to use the data, and view B is only a middleman?"},{"location":"wwdc21/SwiftUI.html#im-trying-to-update-a-text-view-based-on-some-state-eg-from-a-text-field-the-text-is-inside-a-scroll-view-and-i-am-using-scrollviewreader-to-scroll-to-the-end-whenever-the-text-changes-for-some-reason-scrollto-doesnt-work-until-i-manually-start-scrolling-after-that-it-works-fine","text":"struct Foo: View { @State private var text = \"\" private let letters = Array(\"abcdefghijklmnopqrstuvqxyz\") private let timer = Timer.publish(every: 0.1, on: .main, in: .common).autoconnect() var body: some View { ScrollView(.horizontal, showsIndicators: false) { ScrollViewReader { scrollView in HStack { Text(self.text) .fixedSize(horizontal: true, vertical: false) .onAppear { scrollView.scrollTo(0) } .onChange(of: self.text) { _ in scrollView.scrollTo(0) } Color.clear .id(0) } } } .onReceive(self.timer) { _ in self.text.append(letters.randomElement()!) } } } Howdy! Please file feedback with this sample code, this looks like an issue the team should take a look at. Additionally, you should take a look at the new TimelineView and replace your use of Timer here. It\u2019s not recommended to declare new objects in your view\u2019s initializer without a @StateObject or @ObservedObject property wrapper, if you must use a Timer, store it inside an ObservableObject that you can refer to as a StateObject, or another similar technique.","title":"I'm trying to update a Text view based on some state (eg. from a text field). The text is inside a scroll view, and I am using ScrollViewReader to scroll to the end whenever the text changes. For some reason, scrollTo doesn't work until I manually start scrolling \u2014 after that it works fine"},{"location":"wwdc21/SwiftUI.html#is-it-possible-to-enable-sloppy-swiping-for-navigation-links-in-swiftui-swiping-to-go-back-from-anywhere-on-the-screen-also-is-it-possible-to-remove-the-8pt-buffer-before-a-swipe-to-go-back-is-recognized-so-swipe-to-go-back-is-recognized-instantly-rather-than-having-the-slight-delay","text":"Unfortunately, this isn\u2019t currently supported, but feel free to file feedback requesting these features!","title":"Is it possible to enable \u201csloppy swiping\u201d for navigation links in SwiftUI (swiping to go back from anywhere on the screen)? Also, is it possible to remove the 8pt buffer before a swipe to go back is recognized (so swipe to go back is recognized instantly rather than having the slight delay)?"},{"location":"wwdc21/SwiftUI.html#can-a-swiftui-view-or-canvas-be-rendered-into-a-pdf-like-a-uiview-can-using-draw-is-there-another-method-that-does-this","text":"Unfortunately, this isn\u2019t currently supported. Please do file a feedback report if that\u2019s a feature you\u2019re interested in. We really do appreciate them!","title":"Can a SwiftUI View or Canvas be rendered into a PDF like a UIView can using draw ?  Is there another method that does this?"},{"location":"wwdc21/SwiftUI.html#is-there-anyway-to-dismiss-a-keyboard-presented-from-a-textfield-while-scrolling-a-list-there-is-a-behavior-provided-in-uikit-by-keyboarddismissmode-uiscrollview-but-its-disabled-in-swiftui-list-also-i-tried-to-embed-a-uiscrollview-using-uiviewrepresentable-and-enable-keyboarddismissmode-but-the-behavior-is-buggy","text":"Unfortunately, that\u2019s not currently supported. Thank you for the feedback you already filed! We really do appreciate it. If you want to file an additional feedback report requesting API for solving the problem in a SwiftUI-esque way as well, that would be appreciated as well!","title":"Is there anyway to dismiss a keyboard presented from a TextField while scrolling a List? There is a behavior provided in UIKit by keyboardDismissMode UIScrollView but it's disabled in SwiftUI List. Also, I tried to embed a UIScrollView using UIViewRepresentable and enable keyboardDismissMode but the behavior is buggy"},{"location":"wwdc21/SwiftUI.html#is-there-any-way-to-convert-from-the-old-appdelegatescenedelegate-lifecycle-to-the-new-swiftui-2-lifecycle-id-rather-not-re-write-my-entire-application-so-while-its-still-in-the-early-ish-phases-i-wanted-to-know-if-this-was-possible","text":"Yes! You can use the UIApplicationDelegateAdaptor property wrapper in your App. Something like this: UIApplicationDelegateAdaptor var myDelegate: MyAppDelegate SwiftUI will instantiate an instance of your UIApplicationDelegate and call it in the normal fashion. Furthermore, if you return a custom scene delegate class from configurationForConnectingSceneSession, SwiftUI will instantiate it and call it as well.","title":"Is there any way to convert from the old AppDelegate/SceneDelegate lifecycle to the new \"SwiftUI 2\" lifecycle? I'd rather not re-write my entire application, so while it's still in the early-ish phases I wanted to know if this was possible"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-make-two-views-from-different-hierarchies-have-the-same-hightwidthxy-position-that-is-easier-to-understand-than-using-preference-keys","text":"Yes! Look into matchedGeometryEffect.","title":"Is there a way to make two views from different hierarchies, have the same hight/width/x/y position, that is easier to understand than using Preference Keys?"},{"location":"wwdc21/SwiftUI.html#is-it-possible-to-display-a-swiftui-view-on-an-external-display-on-ipadiphone-using-the-swiftui-app-lifecycle","text":"Thanks for the question. I\u2019m afraid that\u2019s not currently supported.","title":"Is it possible to display a SwiftUI view on an external display, on iPad/iPhone, using the SwiftUI App lifecycle?"},{"location":"wwdc21/SwiftUI.html#this-is-a-strange-request-but-is-it-possible-to-intercept-the-actions-of-a-button-or-gesture-with-a-modifier-wanting-to-devise-a-way-wrap-the-invocation-for-example-with-analytics-calls","text":"Unfortunately, that\u2019s not currently possible today. If that\u2019s something you\u2019d find useful, please feel free to file a Feedback requesting it!","title":"this is a strange request, but is it possible to \"intercept\" the actions of a button or gesture with a modifier? wanting to devise a way \"wrap\" the invocation (for example with analytics calls)"},{"location":"wwdc21/SwiftUI.html#are-there-any-places-where-you-think-yep-this-is-the-place-to-use-onappear-rather-than-task","text":"onAppear() is still fine to use. There\u2019s no need to update existing code that uses it. Going forward, I think task() provides a more general solution, even for short synchronous work, since it sets you up to evolve towards asynchronous work if necessary in the future.","title":"Are there any places where you think \"yep, this is the place to use onAppear() rather than task()?"},{"location":"wwdc21/SwiftUI.html#so-you-would-always-use-task-for-new-code-or-are-there-still-places-where-onappear-it-the-right-solution-i-was-maybe-expecting-to-see-onappear-being-gently-deprecated-or-similar","text":"I\u2019d always use task() personally, but there\u2019s also a nice symmetry between onAppear() and onDisappear() that some folks will want to maintain.","title":"So\u2026 you would always use task() for new code, or are there still places where onAppear() it the right solution? I was maybe expecting to see onAppear() being gently deprecated or similar."},{"location":"wwdc21/SwiftUI.html#our-apps-stores-the-position-of-our-custom-split-view-controls-in-its-documents-it-would-be-nice-to-use-hsplitview-vsplitview-and-give-them-as-a-parameter-a-binding-to-set-their-position-to-simplify-storing-and-restoring-their-state-for-now-it-seems-id-need-to-use-a-geometryreader-to-get-the-state-and-a-frame-on-one-of-the-child-views-to-set-the-state-can-you-recommend-a-cleaner-approach","text":"Howdy! Unfortunately there is no API to do this in SwiftUI today, would you mind filing feedback on this? It would be great to understand your use case here!","title":"Our apps stores the position of (our custom) split view controls in its documents. It would be nice to use HSplitView, VSplitView and give them as a parameter a binding to set their position to simplify storing and restoring their state. For now it seems I'd need to use a GeometryReader to get the state and a .frame on one of the child views to set the state. Can you recommend a cleaner approach?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-measure-the-rendering-time-or-memory-footprint-for-a-swiftuis-body-method-being-called-in-xctest-id-like-to-setup-some-baseline-testing-so-that-i-can-be-aware-of-any-regressions-in-performance-but-i-havent-been-able-to-work-out-how-to-do-it","text":"I\u2019m afraid not. I\u2019d love to get an enhancement request Feedback for that use case. We have support for Instruments, but not XCTest.","title":"Is there a way to measure the rendering time or memory footprint for a SwiftUI's body method being called in XCTest?  I'd like to setup some baseline testing so that I can be aware of any regressions in performance, but I haven't been able to work out how to do it."},{"location":"wwdc21/SwiftUI.html#in-general-to-pass-data-around-would-it-better-to-have-an-environmentobject-that-could-be-called-within-a-view-or-an-observedobject-that-gets-passed-down-andor-injected-through-child-views","text":"Both have their uses, and it depends on the architecture you\u2019re building. If you have one (or a few) large ObservableObjects that large parts of the view hierarchy need to see, I would generally recommend EnvironmentObject as SwiftUI can look at which of your views depend on the EnvironmentObject and only invalidate those when your ObservableObject changes (you can get this behavior with ObservedObject too, but it\u2019s more cumbersome). Plus, views that don\u2019t actually use the ObservableObject don\u2019t get cluttered with code relating to it. That said, if your model is, for example, an object graph that is largely not structured based on your view hierarchy, it may make more sense to use ObservedObject to grab pieces of that model out to use in your view.","title":"In general, to pass data around, would it better to have an EnvironmentObject that could be called within a view, or an ObservedObject that gets passed down (and/or injected) through child views?"},{"location":"wwdc21/SwiftUI.html#on-macos-you-can-prevent-a-window-from-resizing-by-setting-a-specific-frame-size-however-this-doesnt-prevent-fullscreenzoom-this-results-in-a-broken-animation-and-a-tiny-view-in-the-middle-of-the-screen","text":"it is possible to tell swiftui the window does not support fullscreen? Hi - sorry you are hitting this bug. Using a fixed size frame is the correct expression here. Also, thank you for the feedback!","title":"on macOS, you can prevent a window from resizing by setting a specific frame size, however this doesn't prevent fullscreen/zoom. this results in a broken animation and a tiny view in the middle of the screen."},{"location":"wwdc21/SwiftUI.html#can-you-provide-guidance-for-when-the-following-error-message-is-printed-in-the-console","text":"Bound preference CollectionViewSizeKey<UUID> tried to update multiple times per frame ( CollectionViewSizeKey<UUID> is the name of the preference.) The preference is updated from inside a GeometryReader. I think the error message happens if the preference modifier is inside a ScrollView. That indicates a cyclic update. The size of your GeometryReader is changing based on the size of your GeometryReader. Try lifting your GeometryReader higher in the view hierarchy if possible, or look for other ways to avoid the cycle.","title":"Can you provide guidance for when the following error message is printed in the console:"},{"location":"wwdc21/SwiftUI.html#i-am-using-observedobject-now-for-my-model-since-i-still-need-to-support-ios-13-however-i-know-that-stateobject-provides-the-correct-behaviors-to-me-is-there-a-suggested-way-to-use-them-at-the-same-time-for-back-compatibility-i-first-thought-if-available-might-work-but-it-does-not-work-for-a-property","text":"For supporting iOS 13, you\u2019ll need to use @ObservedObject and keep your object alive through some other means, like using a static property or keeping a reference in your application delegate. I don\u2019t think trying to switch between observed object and state object buys you much here, since changing the owner of the object with availability checks would be awkward.","title":"I am using @ObservedObject now for my model, since I still need to support iOS 13. However I know that @StateObject provides the correct behaviors to me. Is there a suggested way to use them at the same time for back compatibility? I first thought if #available might work, but it does not work for a property."},{"location":"wwdc21/SwiftUI.html#ive-had-several-intermittent-crashes-from-environment-objects-being-nil-when-i-pass-them-to-a-sheet-or-navigationlink-its-tricky-to-replicate-due-to-being-intermittent-and-i-usually-work-around-it-by-architecting-my-code-differently-to-avoid-passing-environment-objects","text":"Do you know of reasons this might happen? All I can think of is that the views that originate the environmentObject further up the view hierarchy are being taken out of memory. Thanks for any help you can provide! NavigationLink by design doesn\u2019t flow EnvironmentObjects through to its destination as it\u2019s unclear where the environmentObject should be inherited from. I suspect this might what\u2019s causing your issue. In order to get the behavior you expect, you\u2019ll have to explicitly pass the environmentObject through at that point. You can also apply the environmentObject to the NavigationView itself, which will make it available to all pushed content.","title":"I've had several intermittent crashes from environment objects being nil when I pass them to a sheet or NavigationLink. It's tricky to replicate due to being intermittent and I usually work around it by architecting my code differently to avoid passing environment objects."},{"location":"wwdc21/SwiftUI.html#will-the-project-in-its-entirety-from-build-apps-that-share-data-through-cloudkit-and-core-data-wwdc21-10015-be-available-for-download","text":"","title":"Will the project in its entirety from Build apps that share data through CloudKit and Core Data (wwdc21-10015) be available for download?"},{"location":"wwdc21/SwiftUI.html#are-geometryreaders-really-bad-in-terms-of-performance-im-getting-the-feeling-that-they-should-be-avoided-but-i-dont-know-if-its-because-theyre-inefficient-because-theyre-breaking-the-layout-or-because-other-solutions-may-exist-like-anchored-preferences-and-be-better-suited-for-the-purpose","text":"Just like any tool, GeometryReader has a time and place where it is correct to use. There aren\u2019t any particular performance pitfalls I\u2019d call out with them, but they shouldn\u2019t be used as a hammer.","title":"Are GeometryReaders really bad in terms of performance? I'm getting the feeling that they should be avoided, but I don't know if it's because they're inefficient, because they're \"breaking\" the layout, or because other solutions may exist (like anchored preferences) and be better suited for the purpose?"},{"location":"wwdc21/SwiftUI.html#i-noticed-that-while-using-combine-and-lists-say-local-search-combined-with-remote-results-lists-have-a-really-really-hard-time-keeping-up-with-animated-updates-i-found-the-only-reliable-way-to-force-correct-data-representation-is-to-use-iduuid-and-turn-off-animations-is-this-somewhat-expected","text":"We worked really hard on improving List performance in iOS 15, macOS Monterey, and aligned releases. Please try there and let us know if you\u2019re still seeing issues. If you aren\u2019t doing so already, it also may be good to debounce the queries.","title":"I noticed that while using Combine and lists (say local search combined with remote results). Lists have a really, REALLY hard time keeping up with animated updates. I found the only reliable way to force correct data representation is to use .id(UUID()) and turn off animations. Is this somewhat expected?"},{"location":"wwdc21/SwiftUI.html#when-creating-a-uiviewrepresentable-it-is-dangerous-for-the-coordinator-to-hold-an-instance-of-the-uiview-passed-in-updateuiview-or-should-it-be-strictly-treated-as-ephemeral","text":"That is OK! Your coordinator will have been created before any views are \u2014 so in makeUIView you can give the coordinator a reference to that view","title":"when creating a UIViewRepresentable, it is dangerous for the Coordinator to hold an instance of the UIView passed in updateUIView() or should it be strictly treated as ephemeral?"},{"location":"wwdc21/SwiftUI.html#hi-guys-you-all-did-a-great-job-again-congrats-my-question-is-related-to-canvasview-that-view-is-pretty-awesome-and-opens-lots-of-possibilities-for-creative-and-generative-art-for-example-i-would-like-to-know-how-behaves-a-canvas-encapsulated-in-a-timeline-receiving-updates-of-bindings-combines-streams-during-the-render-does-it-lost-the-current-time-information-does-it-resets-to-re-render-the-new-state-or-theres-no-re-rende-at-all-and-the-timeline-still-running-wherever-the-body-changes","text":"Thank you! As long as the identity of the timeline is the same, and the value of the schedule didn\u2019t change based on the body update, it shouldn\u2019t trigger a new update of the contained Canvas. If the content of the canvas itself changed, you will likely see another update from the TimelineView with the date of the current entry of the schedule.","title":"Hi guys, you all did a great job again congrats. My question is related to CanvasView, that view is pretty awesome and opens lots of possibilities for creative and generative art for example. I would like to know how behaves a Canvas encapsulated in a Timeline receiving updates of Bindings Combine's streams. During the render, does it lost the current time information, does it resets to re-render the new state or there's no re-rende at all and the Timeline still running wherever the body changes?"},{"location":"wwdc21/SwiftUI.html#what-is-the-best-practice-for-bypassing-the-system-styling-for-a-component","text":"For example, in a Section header in a grouped list, SwiftUI automatically dims the content and capitalizes all text (as you would expect in a text-only section header for UIKit). Is there a built-in way to bypass this dimming and capitalization? Many of the customizations you see are part of the default styling that lets you have the most natural feeling UI by default. However, most of these default stylings should be overridable by using the same modifiers you would to get that style. For example, if you want to remove the default capitalization of the section header, you can use the text case modifier: Section { // ... } header: { Text(\"My Header\") .textCase(nil) } The same goes for other customization points like foreground style, font, etc.","title":"What is the best practice for bypassing the system styling for a component?"},{"location":"wwdc21/SwiftUI.html#canvas-seems-to-lose-its-intrinsic-content-size-and-appear-as-10x10-when-nested-in-a-scrollview-is-there-a-as-to-work-around-this-behaviour","text":"Thanks for the question! You might try putting a flexible frame on the Canvas, something like .frame(maxWidth: .infinity, maxHeight: .infinity). (Apologies if I got that a little wrong. \\me files Feedback asking for auto-complete in Slack.)","title":"Canvas seems to lose its intrinsic content size and appear as 10x10 when nested in a ScrollView, is there a as to work around this behaviour?"},{"location":"wwdc21/SwiftUI.html#how-do-i-profile-swiftui-code-to-know-how-to-optimize-my-views-instruments-is-almost-only-showing-swiftui-library-code-so-its-hard-to-see-what-is-expensive-to-render","text":"Using the SwiftUI instrument will help call out expensive body methods. In addition it\u2019s important to limit the number of times each views body will get reevaluated. Highly recommend watching the Demystifying talk for some in depth looks at how this works. Pro tip: call the new debug helper Self._printChanges() inside body to log which property caused the view to be reevaluated.","title":"How do I profile SwiftUI code, to know how to optimize my views? Instruments is almost only showing SwiftUI library code, so it's hard to see what is expensive to render..."},{"location":"wwdc21/SwiftUI.html#hi-folks-thank-you-for-swiftui-i-cant-believe-how-straightforward-it-makes-so-many-things","text":"I'm wondering, is there any way to bind to the state of expansion in a hierarchical List (i.e. one created with a children ) parameter, so that we can expand and collapse items programmatically? Thanks for the question! I\u2019m afraid there\u2019s not currently a way to bind the expansion state of an entire hierarchy. You can use DisclosureGroup inside a List to bind a single level. I\u2019d love an enhancement request Feedback with your particular use case. Managing the expansion state of an entire hierarchy is a difficult API design problem, so more info on your use case would be super helpful!","title":"Hi folks! Thank you for SwiftUI -- I can't believe how straightforward it makes so many things."},{"location":"wwdc21/SwiftUI.html#how-can-i-pass-an-environmentobject-to-a-viewmodel-im-trying-to-change-a-tab-inside-the-viewmodel-using-the-usersettings","text":"@EnvironmentObject and all the other SwiftUI\u2019s property wrappers are only valid and functional when used inside a View. In general I would discourage you from using views to pass data between different model objects.","title":"How can I pass an @EnvironmentObject to a ViewModel? I'm trying to change a tab inside the ViewModel using the UserSettings."},{"location":"wwdc21/SwiftUI.html#what-is-the-best-way-to-deal-with-atomic-changes-in-state-when-changing-several-state-variables-in-sequence-i-can-manage-this-generally-with-onchange-but-it-doesnt-trigger-the-action-when-the-state-variable-was-set-to-the-same-value-it-had-before-thus-no-change-id-like-to-make-sure-that-i-trigger-my-update-code-even-if-the-value-didnt-change-equivalent-to-didset-in-swift-which-is-triggered-regardless-of-the-old-value","text":"Additional info pending","title":"what is the best way to deal with atomic changes in state when changing several state variables in sequence? I can manage this generally with onChange but it doesn't trigger the action when the state variable was set to the same value it had before (thus no change). I'd like to make sure that I trigger my update code even if the value didn't change (equivalent to didSet in Swift, which is triggered regardless of the old value)"},{"location":"wwdc21/SwiftUI.html#se-309-allows-using-protocols-with-associated-types-as-existentials-assumedly-including-view-at-that-point-will-there-be-a-difference-between-a-view-existential-and-anyview-in-terms-of-view-identity","text":"I \u2665\ufe0f that proposal! I can\u2019t comment on implementation details, but generally AnyView erases more information than an existential, so the existential would still have the edge.","title":"SE-309 allows using protocols with associated types as existentials, assumedly including View. At that point, will there be a difference between a View existential and AnyView in terms of view identity?"},{"location":"wwdc21/SwiftUI.html#i-noticed-focusstate-doesnt-support-an-initial-value-instead-you-can-set-the-focus-in-onappear-is-there-a-reason-for-this","text":"One way to think of focus (not to be confused with the new user-facing feature we just launched) is that it is global state managed by the framework. A lot of the times, the user will be the one in control of this state, by selecting a text field, etc. The new @FocusState and focused(_:) API allows for some influence over that state as well, but ultimately, the source of this state is still internal to SwiftUI. Does that help to answer your question?","title":"I noticed @FocusState doesn't support an initial value \u2014 instead, you can set the focus in onAppear. Is there a reason for this?"},{"location":"wwdc21/SwiftUI.html#i-wasnt-able-to-secure-a-lab-spot-applied-for-all-of-the-swiftui-open-labs-but-didnt-get-selected-for-any-does-anyone-know-why-a-disabled-button-being-re-enabled-would-have-the-text-rendered-under-the-re-enabled-color","text":"Thanks for the question. We might need sample code to understand this one. Can you reproduce in a sample view? If so, you could add that sample to a Developer Forums post or ask Developer Technical Support.","title":"I wasn't able to secure a lab spot (\"applied\" for all of the SwiftUI open labs, but didn't get selected for any). Does anyone know why a disabled button being re-enabled would have the text rendered under the re-enabled color?"},{"location":"wwdc21/SwiftUI.html#im-curious-is-shaperole-working-at-all-i-made-my-custom-shape-and-overwrite-it-with-stroke-but-there-was-no-change-in-rendering-it-always-fills-the-shape-i-know-the-role-variable-is-being-accessed-i-put-a-print-statement-but-maybe-im-missing-something-and-also-what-is-the-purpose-of-the-separator-role-it-is-not-very-clear-from-the-documentation-how-that-would-affect-output","text":"The role doesn\u2019t strictly dictate exactly how the shape will be treated \u2014 it can still be explicitly fill or stroke\u2019d by its use site. This more determines the expected semantics of how its intended to be used, in a way that affects its default foreground styling. For instance, you could have a stroke role Rectangle that is fill\u2019ed and used as a divider-like element, and so it will receive divider-like default foreground treatment.","title":"I'm curious, is ShapeRole  working at all? I made my custom shape and overwrite it with .stroke but there was no change in rendering. It always fills the shape. I know the role variable is being accessed (I put a print statement)... but maybe I'm missing something. And also, what is the purpose of the separator role? It is not very clear from the documentation how that would affect output."},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-animate-hidingshowing-the-navigation-bar-in-swiftui","text":"If you change the state that drives navigationBarHidden from inside a withAnimation block, it should animate. If it doesn\u2019t, that\u2019s a bug in SwiftUI.","title":"Is there a way to animate hiding/showing the Navigation Bar in SwiftUI?"},{"location":"wwdc21/SwiftUI.html#whats-the-best-replacement-for-a-tableheaderview-for-a-grouped-list","text":"I want to avoid having a cell with a different visual style containing the content to be used as the header view, but I want to avoid sticky headers that are typically found in plain table views. What is the state of the art here? Howdy! Great question, it\u2019s not clear to me if using a custom row instead of a section header would get what you want here, but I think it\u2019s a totally viable reason for feedback explaining your use case to make usually-sticky headers no longer sticky!","title":"What's the best replacement for a tableHeaderView for a grouped list?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-get-the-current-font-size-and-weight-i-want-to-draw-in-a-canvas-in-a-way-that-matches-sf-symbols-as-well-as-possible-is-there-a-way-to-resolve-the-current-font-like-i-can-resolve-text-and-get-these-attributes","text":"Howdy! Great question, I don\u2019t believe there\u2019s a way to get the resolved font size, weight, and design today. This is an excellent case where a feedback from you could help the team understand your needs!","title":"Is there a way to get the current font size and weight? I want to draw in a canvas in a way that matches SF Symbols as well as possible. Is there a way to resolve the current Font like i can resolve Text and get these attributes?"},{"location":"wwdc21/SwiftUI.html#are-there-any-methods-that-allow-restriction-of-view-orientation-to-just-landscape-or-just-portrait-on-a-per-view-basis-without-defaulting-to-uihostingviewcontroller","text":"I\u2019m afraid not currently. That would be a great enhancement request Feedback!","title":"Are there any methods that allow restriction of view orientation to just landscape or just portrait on a per view basis without defaulting  to UIHostingviewcontroller"},{"location":"wwdc21/SwiftUI.html#when-i-put-markdown-formatted-text-as-a-string-literal-in-a-text-it-formats-with-nice-attribution-but-when-i-pass-it-in-from-a-variable-it-doesnt-is-this-a-bug-or-have-i-misunderstood-something-about-how-texts-take-markdown-formatted-strings","text":"struct ContentView: View { var text: AttributedString = \" Hello , world ! Visit our website .\" var body: some View { VStack { Text(\" Hello , world ! Visit our website .\") Text(text) } } } The first Text renders nicely with bold, code and a link, while the second doesn't - it just shows the Markdown annotations in the string directly. I think that\u2019s a bug in beta 1. You might try Text(\"(text)\") to trigger the use of a different initializer.","title":"When I put Markdown-formatted text as a String literal in a Text, it formats with nice attribution, but when I pass it in from a variable, it doesn't. Is this a bug, or have I misunderstood something about how Texts take Markdown-formatted Strings?"},{"location":"wwdc21/SwiftUI.html#hello","text":"Is it possible to have a simultaneous DragGesture in an inner view of a ScrollView which doesn't make the ScrollView drag gesture fail? The DragGesture has a minimumDistance of 0. It seems it's impossible to scroll the scrollView if the DragGesture is recognized (it's installed as simultaneous and the GestureMask is .all ). Is there a workaround? Thanks! Howdy! Great question, this is a great case where we could use feedback from you to better understand how folks use-cases vary with gestures and ScrollView. In the meantime, a possible workaround would be to use a UIViewRepresentable where you can wrap a UIScrollView and get access to all its goodies and expose any functionality you need to your SwiftUI hierarchy.","title":"Hello,"},{"location":"wwdc21/SwiftUI.html#is-it-intended-that-setting-a-value-to-a-focusstate-property-inside-a-form-does-not-work-the-property-stays-nil-and-thus-the-focus-isnt-moved-eg-to-a-textfield-inside-the-form-using-a-vstack-instead-of-the-form-works-like-a-charm","text":"Yes, please file feedbacks (if you already haven\u2019t) for any context where @FocusState doesn\u2019t work the way you\u2019d expect. Thanks!","title":"Is it intended that setting a value to a @FocusState property inside a Form does not work? The property stays nil and thus the focus isn't moved e.g. to a TextField inside the form. Using a VStack instead of the form works like a charm."},{"location":"wwdc21/SwiftUI.html#is-there-any-way-to-get-geometryreader-size-from-another-view-i-want-to-replace-with-the-height-of-hello-world","text":"struct ContentView: View { var body: some View { VStack { Text(\"Hello world!\") .background( GeometryReader { proxy in Color.clear /// placeholder let _ = print(proxy.size.height) /// 20.333333333333332 } ) Text(\"Height of first text is ???\") } } } Howdy! Using a GeometryReader in a background of a view ensures the GeometryReader doesn\u2019t grow to be larger than that containing view, but it makes it tricky to bubble its size out. You could do something like this: struct ContentView: View { @State private var height = 100.0 var body: some View { MyView().background { GeometryReader { proxy in Color.clear .onAppear { height = proxy.size.height } .onChange(of: proxy.height) { height = $0 } } } } } Then you can use your height State property like usual. \u26a0\ufe0f Beware: You must ensure you will not cause a continuous layout loop here, if your layout responds to height changing in a way that causes the GeometryReader to lay out again and cause height to get updated, you can get into a loop!","title":"Is there any way to get GeometryReader size from another view? I want to replace \"???\" with the height of \"Hello world!\"."},{"location":"wwdc21/SwiftUI.html#hi-im-trying-to-implement-an-interactive-media-playback-timeline-ie-slider-for-my-audio-player-app-in-swiftui-however-when-i-use-a-high-frequency-timer-to-update-the-playback-timeline-the-app-and-other-views-become-unresponsive-how-can-i-implement-high-frequency-swiftui-view-updates-and-still-maintain-a-responsive-and-interactive-app","text":"Thanks for the question! It\u2019s hard to give specific advice without seeing code, so this might be a good question for the Developer Forums. A couple of other useful things to look at: There\u2019s an Instruments tool for diagnosing why an update is slow. Check out the new TimelineView for creating views that update at the same frequency as the display. It doesn\u2019t do any good to try to update faster than that, because the display can show intermediate frames anyway.","title":"Hi, I'm trying to implement an interactive media playback timeline (i.e., slider) for my audio player app in SwiftUI. However, when I use a high frequency timer to update the playback timeline, the app and other views become unresponsive :(. How can I implement high frequency SwiftUI view updates, and still maintain a responsive and interactive app?"},{"location":"wwdc21/SwiftUI.html#by-default-the-title-of-newitem-is-new-window-how-i-can-customize-it-for-example-new-window","text":"We will use the title of the WindowGroup in the menu item, if you provide one. For example: WindowGroup(\"Viewer\") { ... } Will give you a menu item titled \u201cNew Viewer Window\u201d. This title will also be used for the default window title, which can be changed with navigationTitle(_:).","title":"By default the title of .newItem is \"New Window\". How I can customize it? For example \"New  Window\""},{"location":"wwdc21/SwiftUI.html#friday","text":"","title":"\ud83d\uddd3 Friday"},{"location":"wwdc21/SwiftUI.html#hello_1","text":"When using TCA, a new instance of ObservableObject (the ViewStore ) is frequently installed as @ObservedObject when the view tree invalidates. What are the consequences of vending a new instance (possibly with the same properties' values) in terms of performances? Would it be absolutely better to reuse the same instance if possible, or is it acceptable as @ObservedObject installation is quite efficient? Thanks! I can\u2019t speak to a third party framework, but in general if the observable object is replaced, every view that has a dependency on it must be re-rendered. Check out the talk Demystify SwiftUI for more details on dependencies.","title":"Hello,"},{"location":"wwdc21/SwiftUI.html#the-canvas-looks-like-a-great-new-addition","text":"I'm wondering about using it to render a single drawn surface that can be nested in a ScrollView which is then panned, zoomed into etc. Is this a reasonable solution and in terms of performance, will there be any guidelines on how far to push it? I intend to put a whole bunch of images as well as other shape data in there. This sounds like a good application for Canvas. Like anything, performance will depend on how far you push it and the hardware it\u2019s running on. I\u2019m afraid that\u2019s not a very actionable answer, but every app is different. The API documentation is here: https://developer.apple.com/documentation/swiftui/canvas/ The talk \u201cAdd rich graphics to your SwiftUI app\u201d supports the copy code feature too, so you can grab some fun snippets to play with. https://developer.apple.com/wwdc21/10021","title":"The Canvas looks like a great new addition."},{"location":"wwdc21/SwiftUI.html#if-we-break-our-views-into-separate-some-view-properties-to-help-readability-is-there-much-cost-to-marking-those-other-properties-as-viewbuilders-to-get-the-nicer-syntax-is-that-something-we-need-to-worry-about","text":"Nope, in fact, we encourage you to do so! Using the @ViewBuilder syntax helps nudge you towards structuring your code in a way that SwiftUI can make use of intelligently, so using it in more places is never a problem. Check out the talk, Demystify SwiftUI for more on this! https://developer.apple.com/videos/play/wwdc2021/10022/","title":"If we break our Views into separate some View properties to help readability, is there much cost to marking those other properties as ViewBuilders to get the nicer syntax? Is that something we need to worry about?"},{"location":"wwdc21/SwiftUI.html#is-it-safe-to-employ-multiple-timelineview-with-the-animation-schedule-or-is-it-equivalent-to-instantiating-a-number-of-cadisplaylinks-i-was-thinking-of-the-adage-around-reusing-cadisplaylinks-as-much-as-possible","text":"Yes! You should be able to use as many TimelineView as appropriate to get your interface behaving how you want. SwiftUI will take care of scheduling things so they update like you want. The thing to be careful of is to not have too much \"different\" between each update of the timeline content.","title":"Is it safe to employ multiple TimelineView with the animation schedule or is it equivalent to instantiating a number of CADisplayLinks? I was thinking of the adage around \u201creusing\u201d CADisplayLinks as much as possible."},{"location":"wwdc21/SwiftUI.html#dumb-question-here-but-i-was-just-playing-around-with-the-code-in-the-advanced-graphics-section-of-whats-new-in-swiftui-taylor-showed-a-symbols-browser-i-copied-the-code-and-when-i-run-the-performance-is-really-bad-he-showed-the-smooth-gesture-with-the-fisheye-and-the-timeline-view-animation-on-my-side-it-is-super-jerky-with-seconds-delay-to-render-is-that-right","text":"Sorry about that! That\u2019s a known issue in beta 1. As a quick workaround to see the same smooth effect today, you can manually cache the resolved images. One thing you could try is moving that resolution to be outside of the inner for loop, so it only happens once. (cheating since for the code snippet it only uses the swift bird rather than every symbol like my demo had).","title":"Dumb question here... but I was just playing around with the code in the Advanced Graphics section of Whats new in SwiftUI. Taylor showed a symbols browser.... I copied the code and when I run the performance is really bad... he showed the smooth gesture with the fisheye and the timeline view animation... on my side it is super jerky with seconds delay to render... is that right?"},{"location":"wwdc21/SwiftUI.html#whats-the-recommended-way-to-share-swiftui-code-between-multiple-platform-targets-recent-xcode-multiplatform-templates-simply-make-source-files-members-of-multiple-targets-but-a-few-years-ago-the-recommendation-for-sharing-code-between-eg-an-app-target-and-an-app-extension-was-to-create-a-shared-dynamic-framework-eg-myapp-myappextension-and-myappkit-what-would-be-the-pros-and-cons-of-either-approach","text":"It really depends on the complexity of your particular app. The templates are the fastest way to get started and aim to minimize complexity. A shared dynamic framework is a great approach as your app grows. How about dynamic frameworks vs. SPM packages? I believe an SPM package can produce a dynamic framework now, but that\u2019s outside my expertise. It might be a good question for the DevTools Lounge or a Swift lab though!","title":"What's the recommended way to share SwiftUI code between multiple platform targets? Recent Xcode multiplatform templates simply make source files members of multiple targets, but a few years ago the recommendation for sharing code between e.g. an app target and an app extension was to create a shared dynamic framework (e.g. MyApp, MyAppExtension and MyAppKit). What would be the pros and cons of either approach?"},{"location":"wwdc21/SwiftUI.html#when-an-observedobject-is-passed-into-a-view-does-swiftui-differentiate-between-views-that-actually-use-it-the-object-is-used-in-the-body-and-intermediate-views-which-just-pass-that-object-to-a-child-or-are-all-views-just-invalidated","text":"Yes, there is a difference. If you don\u2019t use any of the ObservableObject property wrappers (@StateObject, @ObservedObject ) the view would not observe and update the instance. So you you just need to pass an ObservableObject through some intermediary view just make it a regular property on the view but make sure to use the property wrapper if you ever read any of the value in the view, otherwise your view will no be consistent with your data. Also, @EnvironmetObject is a great tool when you have an ObservableObject that you want to pass down multiple levels of your view hierarchy without having to manually do it every step of the way.","title":"When an observedObject is passed into a view, does SwiftUI differentiate between views that actually use it (the object is used in the body) and 'intermediate' views (which just pass that object to a child? )? Or are all views just invalidated?"},{"location":"wwdc21/SwiftUI.html#what-is-the-recommended-architecture-when-using-swiftui","text":"In a lot of the demos (and this is probably my inexperience showing) it seems that the business logic leaks into the view. Would you recommend using MVVM for example and how would you set that up? Thanks for the question! Josh S. gave a great answer to this in the Q&A yesterday: For me the most important thing is that you have a software architecture in mind that works well for you, and helps you craft maintainable apps with a good separation ways to achieve that, but they\u2019re by no means the only one. If you\u2019ve got a well-considered architecture that works well for you, I think that\u2019s great and helps you craft maintainable apps with a good separation ways to achieve that,\u201d","title":"What is the recommended architecture when using SwiftUI?"},{"location":"wwdc21/SwiftUI.html#could-we-use-prominent-window-scene-presentation-style-in-swiftui","text":"Hi, we do not have support for this, but would love a feedback with any specifics you could provide about your use case.","title":"Could we use .prominent Window Scene Presentation Style in SwiftUI?"},{"location":"wwdc21/SwiftUI.html#if-i-have-to-embed-a-swiftui-view-into-a-uiviewcontroller-to-do-something-only-available-for-uiviewcontroller-like-customize-appearance-that-swiftui-not-support-is-there-any-problem-about-the-following-two-way","text":"Create a UIHostingController A, add A's view as a subview to my own UIViewController B's view. (of course do some layout and addChild didMove stuff) Inherit UIHostingController directly to override some VC methods Either approach is fine! Please do file feedback though about what you're trying to achieve that requires wrapping the SwiftUI content like this.","title":"If I have to embed a SwiftUI View into a UIViewController to do something only available for UIViewController (like customize appearance that SwiftUI not support), is there any problem about the following two way?"},{"location":"wwdc21/SwiftUI.html#why-does-a-uiviewrepresentable-update-once-after-makeuiview-and-once-before-dismantleuiview","text":"The update function can be called for a number of reasons. It will be called at least once after make as the UIView becomes extant. It may be called multiple times before the UIView is eventually dismantled. You should not rely on any frequency (or lack thereof) of update calls.","title":"Why does a UIViewRepresentable update once after makeUIView and once before dismantleUIView?"},{"location":"wwdc21/SwiftUI.html#how-do-we-avoid-incurring-in-bound-preference-sizepreferencekey-tried-to-update-multiple-times-per-frame","text":"It sounds like you have a cycle in your updates. For example, a GeometryReader that writes a preference, that causes the containing view to resize, which causes the GeometryReader to write the preference again. It\u2019s important to avoid creating such cycles. Often that can be done by moving the GeometryReader higher in the view hierarchy so that it\u2019s size doesn\u2019t change and it can communicate size to it\u2019s subviews instead of using a preference. I\u2019m afraid I can\u2019t give any more specific guidance than that without seeing your code, but hopefully that helps you track down the issue!","title":"How do we avoid incurring in Bound preference SizePreferenceKey tried to update multiple times per frame?"},{"location":"wwdc21/SwiftUI.html#when-using-viewframeminwidth-idealwidth-maxwidth-are-there-any-combinations-of-parameters-that-are-more-performant-that-others-any-combination-to-avoid","text":"There isn't really any performance difference between any combination of parameters. You should just use the appropriate values to get the layout you're looking for. what's the best way to enforce a specific size? min + max? or ideal? or all? You would want to use the other version of .frame for that which just takes a width and a height. View.frame(width:,height:)","title":"When using View.frame(minWidth:, idealWidth, maxWidth:\u2026) are there any combinations of parameters that are more performant that others? Any combination to avoid?"},{"location":"wwdc21/SwiftUI.html#is-there-any-way-to-setcontrol-backbuttondisplaymode-from-swiftui-by-the-way-thanks-for-all-the-great-new-features","text":"Thanks for the question. I\u2019m afraid there\u2019s no SwiftUI API for this currently. If you could file an enhancement request Feedback, that would really help me prioritize that work though!","title":"Is there any way to set/control backButtonDisplayMode from SwiftUI? By the way, thanks for all the great new features"},{"location":"wwdc21/SwiftUI.html#can-you-explain-the-attributegraph-that-comes-up-while-debugging-sometimes","text":"I\u2019m afraid we can\u2019t discuss implementation details, but be sure to check out the talk \u201cDemystify SwiftUI\u201d for the details of the dependency graph. https://developer.apple.com/wwdc21/10022","title":"Can you explain the AttributeGraph that comes up while debugging sometimes?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-disable-pixel-rounding-in-swiftui-the-hover-effect-from-my-os-x-dock-yesterday-is-jittery-because-padding-is-being-applied-to-many-views-in-a-row-but-rounded-to-the-nearest-pixel-for-each-view-this-inaccuracy-in-padding-adds-up-making-the-entire-view-jitter-by-a-few-pixels-whenever-i-move-the-mouse","text":"SwiftUI layout is always rounded to the nearest pixel. But using any GeometryEffect won't take on the snapping behavior. Things like .offset and .scaleEffect are existing ways to achieve this, but you can also implement your own GeometryEffect if you need something custom.","title":"Is there a way to disable pixel rounding in SwiftUI? The hover effect from my OS X Dock yesterday is jittery because padding is being applied to many views in a row, but rounded to the nearest pixel for each view. This inaccuracy in padding adds up, making the entire view jitter by a few pixels whenever I move the mouse."},{"location":"wwdc21/SwiftUI.html#in-most-cases-the-layout-behavior-with-spacer-can-be-replaced-with-framemaxwidthalignment-or-height-seamlessly-since-spacer-is-an-actual-view-that-is-arranged-within-the-view-hierarchy-using-spacer-will-consume-more-memory-and-cpu-resources-and-demystify-swiftui-also-says-modifier-is-cheap","text":"So should I use .frame instead of Spacer as much as possible? While Spacer is a view, it doesn't end up displaying anything of its own so it is plenty lightweight. Using .frame can have other behavior introduced to the way the view gets laid out beyond just changing its size. They both have their uses, so use them each where appropriate. To add a little more onto this, even in cases where you will get almost entirely the same behavior between the two, the performance difference will be so minimal that I would strongly suggest prioritizing code readability over performance / memory use to make this decision. If Spacer makes it more clear what layout you\u2019re trying to specify, use that, and vice versa.","title":"In most cases, the layout behavior with Spacer can be replaced with .frame(maxWidth:,alignment:) (or height)  seamlessly. Since Spacer is an actual View that is arranged within the view hierarchy, using Spacer will consume more memory and cpu resources. And Demystify SwiftUI also says \"modifier is cheap\"."},{"location":"wwdc21/SwiftUI.html#does-swiftui-expose-enough-api-that-would-allow-us-to-build-our-own-lazyhvstackgrid-with-a-fully-custom-layout-or-are-there-still-a-lot-of-magical-bits-under-the-hood-that-prevent-us-from-doing-so","text":"Unfortunately, we don\u2019t offer support for building custom layouts today.","title":"Does SwiftUI expose enough API that would allow us to build our own Lazy{H,V}{Stack,Grid} with a fully custom layout, or are there still a lot of \"magical bits\" under the hood that prevent us from doing so?"},{"location":"wwdc21/SwiftUI.html#what-is-the-handlesexternalevents-modifier-ive-seen-it-used-but-dont-know-what-it-is","text":"This modifier allows you to specify a Scene to be used when external data is sent to the app - a URL or an NSUserActivity . For example, on macOS, when a url comes in that matches the pattern, we will either create a new window for that Scene, or use an existing one, if a view has been modified with handlesExternalEvents to prefer that incoming value.","title":"What is the handlesExternalEvents modifier? I\u2019ve seen it used, but don\u2019t know what it is."},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-customize-navigationviews-navbar-other-than-uinavigationbarappearance","text":"Unfortunately, there isn\u2019t any SwiftUI native way to do this right now. If this is something you want, please file a feedback report requesting the feature!","title":"Is there a way to customize NavigationView's navbar other than UINavigationBar.appearance?"},{"location":"wwdc21/SwiftUI.html#the-state-documentation-states-it-is-safe-to-mutate-state-properties-from-any-thread-what-does-it-refer-to-considering-that-swiftui-complains-with-runtime-warnings-when-a-passthroughsubject-published-from-a-non-main-thread","text":"State is thread safe and can be mutated from any thread. When you mention PassthroughSubject I would imagine you are using that in the context of an ObservableObject with either one of @StateObject, @ObserverdObject, or @EnvironmentObject . ObservableObject does require that you mutate all the properties observed by the view, and publish objectWillChange on the Main Thread. I would recommend you check out Discover Concurrency in SwiftUI where my colleague Curt and Jessica talk all about Swift and concurrency https://developer.apple.com/videos/play/wwdc2021/10019/","title":"The State documentation states \u201cIt is safe to mutate state properties from any thread.\u201d What does it refer to considering that SwiftUI complains with runtime warnings when a PassthroughSubject published from a non-main thread?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-for-a-view-to-know-when-the-window-it-is-hosted-in-is-key-or-not-on-macos","text":"Could you elaborate some on what you\u2019re trying to achieve? There is the controlActiveState property on the Environment, but there is also focusedValue(: ,: ) and @FocusedValue which are used in the context of the key window on macOS.","title":"Is there a way for a view to know when the window it is hosted in is key or not on macOS?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-disable-the-scrolling-on-a-list-so-i-can-use-it-within-a-scrollview-i-have-a-scrollview-inside-which-im-placing-an-image-some-text-and-then-i-would-like-to-be-able-to-place-a-list-however-currently-the-list-doesnt-appear-unless-i-give-it-a-fixed-frame-size-and-the-list-is-also-separately-scrollable-is-there-a-way-around-this","text":"I\u2019m sorry, but that\u2019s not currently supported. If you would be able to file an enhancement request Feedback with your use case, I\u2019d really appreciate it! I know Feedback can seem like a black box (sorry about that), but they really do help us, even if we can\u2019t respond directly to every request.","title":"Is there a way to disable the scrolling on a List so I can use it within a ScrollView? I have a ScrollView, inside which I'm placing an Image, some Text, and then I would like to be able to place a List. However, currently, the List doesn't appear unless I give it a fixed frame size, and the List is also separately scrollable. Is there a way around this?"},{"location":"wwdc21/SwiftUI.html#can-items-in-a-list-accept-drop-items-it-seems-to-immediately-crash-my-app","text":"Could you please file a feedback for this crash? If you can also include a sample that reproduces the issue, that would be especially helpful.","title":"Can items in a list accept drop items? It seems to immediately crash my app."},{"location":"wwdc21/SwiftUI.html#if-we-create-custom-property-wrappers-that-embed-existing-ones-like-state-observedobject-etc-does-swiftui-still-see-those-embedded-wrappers-and-do-the-right-thing","text":"For example, can I create an @Whatever wrapper as a convenience for @Environment(\\.whatever) and still expect that to work the same way? Yes, you can make do that. As long as you add conform to the DynamicProperty to your wrapper this will work.","title":"If we create custom property wrappers that embed existing ones (like State, ObservedObject, etc) does SwiftUI still \"see\" those embedded wrappers and do the right thing?"},{"location":"wwdc21/SwiftUI.html#without-having-to-jump-to-ios15-for-focusedscenevalue-is-there-a-recommended-way-for-non-document-windows-in-a-window-group-to-surface-a-kind-of-current-object-that-can-be-used-to-activatedeactive-menus-send-commands-to-even-if-no-editable-control-as-focus-in-swiftui","text":"Good question! focusedSceneValue provides a way for the current focused / key window to surface its values to @FocusedValues in menus and other places; this is especially important on macOS since an app can have many windows, but only one of those is key. As for other approaches, it somewhat depends on your use case: if you do need your current object to be tied to the key window specifically, there isn\u2019t a substitute. But if you more want to aggregate information across all of your windows, you could use things like preferences and the environment to manually pass that data around","title":"without having to jump to ios15 for focusedSceneValue, is there a recommended way for non-document windows in a window group to surface a kind of \"current object\" that can be used to activate/deactive menus &amp; send commands to, even if no editable control as focus? (in swiftui)"},{"location":"wwdc21/SwiftUI.html#i-have-a-framework-that-vends-a-swiftui-view-to-the-main-app-via-a-hostingcontroller-the-view-handles-everything-it-needs-internally-so-all-the-types-are-internal-the-only-public-method-is-the-one-that-vends-the-hostingcontroller-in-order-to-maintain-the-isolation-i-do-it-this-way","text":"return UIHostingController(rootView: AnyView(SchedulesView(store: store))) Is this a correct way to use AnyView Yes, that\u2019s an OK usage, particularly because it\u2019s at the base of the hierarchy view and not used to add dynamism. But there are other ways to encapsulate the exact type of the hosting controller, for instance returning an upcast or custom protocol type Returning it typed as UIViewController instead its actual UIHostingController<..> type Creating a protocol with the extra API that clients might expect from the view controller, and returning it typed as that Or by using a container UIViewController that has a child of your hosting controller","title":"I have a framework that vends a SwiftUI View to the main app via a HostingController. The view handles everything it needs internally, so all the types are internal. The only public method is the one that vends the HostingController. In order to maintain the isolation I do it this way:"},{"location":"wwdc21/SwiftUI.html#when-ive-needed-to-inject-data-into-a-detail-view-but-still-let-the-view-have-ownership-of-its-stateobject-ive-used-the-stateobjectwrappedvalue-initializer-directly-in-my-view-initializer-for-example","text":"public struct PlanDetailsView: View { @StateObject var model: PlanDetailsModel public init(plan: Plan) { self._model = StateObject(wrappedValue: PlanDetailsModel(plan: plan)) } ... } Is this an acceptable use of the initializer? I know StateObject is only supposed to initialize at the start of the View's lifetime, and not on subsequent instantiations of the View value, so I want to make sure I'm not forcing it to re-allocate new storage each time the View is re-instantiated. Yes, this is an acceptable use of the initializer and your understanding is correct: that object will be create only at the beginning of the view lifetime and kept alive. The StateObject \u2019s wrapped value is an autoclosure that is invoke only once at the beginning of the view lifetime. That also means that SwiftUI will capture the value of plan when is firstly created; something to keep in mind is that if you view identity doesn\u2019t change but you pass a different plan SwiftUI will not notice that.","title":"When I've needed to inject data into a detail view, but still let the view have ownership of its StateObject, I've used the StateObject(wrappedValue:) initializer directly in my view initializer, for example:"},{"location":"wwdc21/SwiftUI.html#hello-i-have-tried-to-implement-with-swiftui-a-common-feature-in-ios-apps-that-is-when-tapping-again-a-tab-item-of-a-tabview-with-a-nested-navigationview-it-pops-to-the-root-view-of-the-navigation-view-using-presentationmodewrappedvaluedismiss-it-just-chains-the-view-dismiss-any-idea-of-how-to-implement-this-in-a-more-clean-way-mine-feels-hacky","text":"Thanks for the question! I\u2019m afraid I don\u2019t have a great answer here, but there are a couple of options you can try. One option is to use a representable to embed a UITabBarController, so you can hook the delegate methods. Another option, if you can detect when the user taps the same SwiftUI tab again, is to decorate your NavigationView with .id(counter), where counter is @State private var counter = 0. Then when the user taps the same SwiftUI tab again, you can increment counter, which changes the identity of the navigation view, causing SwiftUI to replace it. I\u2019d love to get an enhancement request Feedback for your particular use case so we can prioritize making this easier!","title":"Hello! I have tried to implement with SwiftUI a common feature in iOS apps that is when tapping again a tab item of a TabView with a nested NavigationView, it pops to the root view of the navigation view. Using presentationMode.wrappedValue.dismiss() it just chains the view dismiss, any idea of how to implement this in a more clean way ? (mine feels \"hacky\")"},{"location":"wwdc21/SwiftUI.html#how-can-i-control-the-ideal-size-of-a-uiviewrepresentable-view-i-have-a-lot-of-trouble-getting-correct-automatic-sizing-of-wrapped-views-especially-if-they-wrap-uistackview-any-recommendations-for-getting-proper-automatic-sizing-so-i-dont-need-to-use-fixedsize-so-much","text":"Try implementing intrinsicContentSize on your view.","title":"How can I control the ideal size of a UIViewRepresentable View? I have a lot of trouble getting correct automatic sizing of wrapped views, especially if they wrap UIStackView. Any recommendations for getting proper automatic sizing so I don't need to use fixedSize so much?"},{"location":"wwdc21/SwiftUI.html#is-there-any-way-to-open-additional-window-from-swiftui-life-cycle-i-mean-call-additional-windowgroup-for-example-inspector-panel","text":"Hi - we currently do not have any API for this, though we\u2019d love if you could file a feedback with any specifics you have for your use case. One option for something like an inspector panel is that you could use NSApplicationDelegateAdaptor, and open your window by communicating with that. A secondary WindowGroup might not be the best match for something like an inspector panel, since WindowGroup supports multiple windows, and will should up in the File -> New menu. Oh, I should mention that the window created via the delegate adaptor would be an NSWindow instance, with an NSHostingView as the contentView.","title":"Is there any way to open additional window from SwiftUI Life cycle? I mean call additional WindowGroup. For example Inspector panel."},{"location":"wwdc21/SwiftUI.html#for-when-using-environmentobjects-it-seems-like-i-need-to-pass-the-environmentobject-modifier-for-every-layer-of-subview-if-i-dont-do-that-it-will-give-error-view-environmentobject-may-be-missing-as-an-ancestor-of-this-view","text":"But I thought the point of Environment Objects is to make data available to all view without writing a lot of code. Is that a design choice or am I doing something wrong? Thanks for the question. .environmentObject(\u2026) mostly flows down the view hierarchy, however there are some places where we intentionally do not forward it. For example, the destination of a navigation link does not get the environment object, because there is a conflict as to whether the destination should get the object from the originating link or the placeholder of the destination, or even the previous view on the stack. There was also a bug where sheets and popovers didn\u2019t get the environment object, but that was fixed. If you find other places where environment object does not propagate as expected, please file a Feedback. It\u2019s important to me that we get this right!","title":"For when using @EnvironmentObjects, it seems like I need to pass the .environmentObject modifier for every layer of subview. If I don't do that, it will give error \"view environmentObject may be missing as an ancestor of this view\""},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-control-the-opacityoverlay-effect-applied-to-a-menus-label-in-ios-menu-currently-applies-a-high-opacity-white-overlay-i-assume-on-its-label-when-tapped-and-id-like-to-make-it-less-opaqueoverride-the-effect-entirely","text":"Not today, but if you haven\u2019t already please file a feedback requesting the ability to do so. Oh, and adding context for why you want to customize that effect is always really useful too!","title":"Is there a way to control the opacity/overlay effect applied to a Menu's label in iOS? Menu currently applies a high-opacity white overlay (I assume!) on its label when tapped, and I'd like to make it less opaque/override the effect entirely."},{"location":"wwdc21/SwiftUI.html#can-the-new-atrributedstring-be-used-to-provide-accessibility-notations-in-swiftui-like-nsattributedstrings-can-in-uikit-should-we-report-any-attributes-from-nsattributedstring-we-need-on-attributedstring","text":"AttributedString should have the full suite of accessibility attributes and they should now be standardized across platforms! But if you see any we missed, please do file Feedback so we can fix any oversights. Thanks for the question. We\u2019re super excited about the AttributedString support! Localization, accessibility, formatting, and morphology agreement in one tidy package.","title":"Can the new AtrributedString be used to provide accessibility notations in SwiftUI like NSAttributedStrings can in UIKit? Should we report any attributes from NSAttributedString we need on AttributedString?"},{"location":"wwdc21/SwiftUI.html#ive-had-a-lot-of-inconsistent-behaviour-with-environmentobject-where-it-isnt-passed-in-nil-when-referenced-etc-am-i-holding-it-wrong-or-what-could-some-common-pitfalls-be-with-using-it-ive-reverted-to-a-singleton-for-global-access-but-id-rather-get-environmentobject-right","text":"It\u2019s a little hard to tell exactly what issue you\u2019re running into just from this description, but sorry to hear it\u2019s been causing you trouble! How have you been \u201cholding it\u201d, so to speak?","title":"I\u2019ve had a lot of inconsistent behaviour with EnvironmentObject where it isn\u2019t passed in, nil when referenced, etc. Am I holding it wrong, or what could some common pitfalls be with using it? I've reverted to a singleton for global access but I'd rather get EnvironmentObject right."},{"location":"wwdc21/SwiftUI.html#hi-thanks-for-answering-questions-here-i-have-an-app-that-needs-to-hide-the-home-indicator-but-that-does-not-seem-to-be-possible-in-swiftui-i-know-i-can-do-it-with-exchanging-the-root-viewcontroller-but-that-has-a-lot-of-side-effects-in-a-pure-swiftui-app-is-there-a-better-workaround","text":"Thanks for the question. I\u2019m afraid we don\u2019t have SwiftUI API for this currently. Using UIKit is the best option for now. If you could submit an enhancement request Feedback, I\u2019d really appreciate it.","title":"Hi! Thanks for answering questions here! I have an app that needs to hide the home indicator, but that does not seem to be possible in SwiftUI. I know I can do it with exchanging the root ViewController but that has a lot of side effects in a pure SwiftUI app. Is there a better workaround?"},{"location":"wwdc21/SwiftUI.html#is-it-okay-to-use-child-coredata-context-as-stateobject-vars-in-swiftui-views-or-would-you-recommend-passing-those-in-the-environment-and-holding-on-to-them-outside-of-swiftui","text":"There is not need to use a @StateObject since the context is not an ObservableObject. Passing it down using the environment is a good solution.","title":"Is it okay to use child CoreData context as @StateObject vars in SwiftUI views, or would you recommend passing those in the environment and holding on to them outside of SwiftUI?"},{"location":"wwdc21/SwiftUI.html#whats-the-best-practices-for-making-a-swiftui-view-accessible","text":"You should get a lot of Accessibility automatically. The best way to determine whether you need to make tweaks, using our Accessibility APIs, is to try your view with some of our features, like Large Text, VoiceOver, etc. You can also take a look at the new Accessibility Inspector in SwiftUI and see what it shows for your Preview view. In general, system standard controls tend to be accessibly by default, but when you use a lot of Shape or other manual drawing, you may need to do more work. Hi Arie. A great place to start is our introduction video from 2019. https://developer.apple.com/videos/play/wwdc2019/238/ This will introduce you to accessibility in SwiftUI. You could follow that up with this years presentation where we go beyond the basics. https://developer.apple.com/videos/play/wwdc2021/10119/ We also have many WWDC videos on best practices when making a view accessible. Such as writing great accessibility labels. https://developer.apple.com/videos/play/wwdc2019/254/","title":"What\u2019s the best practices for making a SwiftUI view accessible?"},{"location":"wwdc21/SwiftUI.html#when-using-swiftui-with-something-like-core-data-and-uihostingcontroller-how-can-we-avoid-using-anyview-if-the-environment-modifier-changes-the-type-of-the-view-see-example-below","text":"`import UIKit import SwiftUI import CoreData struct MyView: View { var body: some View { Text(\"!\") } } class MyHostingController: UIHostingController<MyView> { override func viewDidAppear(_ animated: Bool) { super.viewDidAppear(animated) // custom stuff here } } class TestViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() let persistentContainer = NSPersistentContainer(name: \"MyStore\") let rootView = MyView().environment(\\.managedObjectContext, persistentContainer.viewContext) let hostingController = MyHostingController(rootView: rootView) // this will not work anymore because type has changed with the environment modifier // more stuff } } Great question! Here\u2019s a technique, though it runs the risk of angle-bracket-blindness \u2026 In class MyHostingController: UIHostingController MyView isn\u2019t really the right type. You want the type of MyView().environment(.managedObjectContext, persistentContainer.viewContext) On the let hostingController = \u2026 line, you\u2019re probably getting an error message about the types not matching. That error message will include the full type of the right-hand side of the assignment. Something like ModifiedContent with lots of stuff inside the \u2026 there. What I like to do is (1) copy that type, then (2) add a top-level type alias: typealias MyModifiedView = ModifiedContent where the right-hand side is the value I copied from the error message. Then you can write class MyHostingController: UIHostingController","title":"When using SwiftUI with something like Core Data and UIHostingController, how can we avoid using AnyView if the environment modifier changes the type of the view. See example below."},{"location":"wwdc21/SwiftUI.html#thursday","text":"","title":"\ud83d\uddd3 Thursday"},{"location":"wwdc21/SwiftUI.html#when-using-the-new-swiftui-table-view-can-you-group-to-have-more-than-10-tablecolumns","text":"Yes, you can; just like Views! Do I understand correctly that there are no more limitation on the number of objects in @ViewBuilder? There is no change to @ViewBuilder this year, so it is still limited in the number of elements it can build into a block. But Group as well as nested builders are great tools to allow for as many views to be combined together as you want","title":"When using the new SwiftUI Table View, can you Group to have more than 10 TableColumns?"},{"location":"wwdc21/SwiftUI.html#what-happens-when-you-have-three-views-in-a-hierarchy-where-the-root-and-leaf-views-share-dependency-but-neither-share-dependencies-with-the-middle-view-would-the-middle-view-get-redrawn","text":"If the middle view isn\u2019t dependent on the others, it isn\u2019t redrawn. Note that if the root view passes parameters to the middle view, then it has a dependency.","title":"What happens when you have three views in a hierarchy, where the root and leaf views share dependency, but neither share dependencies with the middle view. Would the middle view get redrawn?"},{"location":"wwdc21/SwiftUI.html#is-there-a-good-way-to-switch-between-hstacks-and-vstacks-while-allowing-swiftui-to-understand-that-the-contents-of-those-stacks-are-the-same","text":"Lots of questions similar to this, so repeating an answer from earlier: Yes! You should check out the matchedGeometryEffect() API, which was introduced last year: https://developer.apple.com/documentation/swiftui/view/matchedgeometryeffect(id:in:properties:anchor:issource:)","title":"Is there a good way to switch between HStacks and VStacks while allowing SwiftUI to understand that the contents of those stacks are the same?"},{"location":"wwdc21/SwiftUI.html#is-the-viewbuilder-to-remove-anyview-in-demystify-swiftui-only-available-with-ios-15","text":"No, in fact it can back-deploy to any previous version!","title":"Is the @ViewBuilder to remove AnyView in \"Demystify SwiftUI\" only available with iOS 15 ?"},{"location":"wwdc21/SwiftUI.html#are-there-performance-considerations-when-generating-uuids-ie-generating-a-lot-of-uuids-for-elements-in-an-array-to-display-in-a-list","text":"It depends. You'll need to measure. However, consider generating IDs lazily (make sure you store them though!)","title":"Are there performance considerations when generating UUIDs? (I.e. generating a lot of UUIDs for elements in an array to display in a list)."},{"location":"wwdc21/SwiftUI.html#performance-wise-is-it-preferred-to-pass-observableobjects-down-to-subviews-explicitly-or-use-environmentobjects","text":"Using one or the other shouldn\u2019t make much difference in any given view, but if you don\u2019t need to use the object in some views then EnvironmentObject is a great way to avoid the boilerplate of pass it down through intermediary layers, and can avoid creating unintentional dependencies.","title":"Performance wise, is it preferred to pass ObservableObjects down to subviews explicitly, or use EnvironmentObjects?"},{"location":"wwdc21/SwiftUI.html#is-there-a-good-way-to-apply-a-modifier-to-a-swiftui-view-conditionally-im-using-a-custom-if-modifier-and-it-refreshes-the-whole-view-on-state-change","text":"Consider making an inert version of the modifier as discussed in the session. If there is a modifier that lacks an inert version that you'd like to see, please file a feedback.","title":"Is there a good way to apply a modifier to a SwiftUI view conditionally? I'm using a custom .if modifier, and it refreshes the whole view on state change :("},{"location":"wwdc21/SwiftUI.html#how-can-we-conditionally-set-different-modifier-for-example-list-styles","text":"Styles in SwiftUI are static, and are not permitted to change at runtime. This is a case where a branch is more appropriate, but consider whether you really want to change the style\u2014 a single style is almost always the correct choice. If there is a certain dynamism you're looking for here, please file a feedback.","title":"How can we conditionally set different modifier, for example list styles?"},{"location":"wwdc21/SwiftUI.html#is-there-any-situation-in-which-hashable-may-be-preferred-to-identifiable","text":"If you just need to be able to identify a value, that\u2019s what Identifiable is for, which means that only the id needs to be Hashable, not the whole type.","title":"Is there any situation in which Hashable may be preferred to Identifiable?"},{"location":"wwdc21/SwiftUI.html#if-we-apply-same-id-in-the-condition-will-swiftui-sees-this-as-a-same-view","text":"var body: some View { if isTrue { Text(\"Hello\") .id(viewID) } else { Text(\"World\") .id(viewID) } } No, these will be two different views. Raj (Apple) 3 days ago This is because body is implicitly a ViewBuilder. If you don't use a ViewBuilder, such as in another property, it would be the same view.","title":"If we apply same id in the condition, will SwiftUI sees this as a same view"},{"location":"wwdc21/SwiftUI.html#are-there-any-difference-between","text":"passing a State as a binding through each view in the hierarchy vs passing it as an environment object and only access it in the subviews that use it, say in a Text view would the first one have worse dependency graph that needs more updates? environment object is a slightly different tool than State since it requires an ObservableObject. Environment object is optimized to invalidate only the views that read its value. As for State, when you pass the binding down to subviews, changing the binding will mutate the state that in turn will invalidate the view holding the state and its children.","title":"Are there any difference between"},{"location":"wwdc21/SwiftUI.html#what-is-a-good-way-to-have-a-custom-order-of-elements-in-a-list-so-the-user-can-change-the-order-using-a-stable-id","text":"A List or ForEach will preserve the same order as used in the collection passed into them. The identity of each element should be independent of that order \u2014 even if a collection is reordered, each element should maintain the same ID. If you maintain a stable identity like that, then you should be able to reorder the collection no problem in response to user actions","title":"What is a good way to have a custom order of elements in a list so the user can change the order (using a stable ID)?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-animate-a-view-moving-from-one-place-in-hierarchy-to-another-like-when-a-view-changes-parents","text":"Yes! You should check out the matchedGeometryEffect() API, which was introduced last year: https://developer.apple.com/documentation/swiftui/view/matchedgeometryeffect(id:in:properties:anchor:issource:)","title":"Is there a way to animate a View moving from one place in hierarchy to another? Like when a View changes parents?"},{"location":"wwdc21/SwiftUI.html#fun-question-on-conditions-we-need-to-put-them-at-least-in-group-if-whatever-else-so-is-it-preferable-to-use-viewbuilder-commands-to-create-it-like-viewbuilderbuildblockpageinfo-nil-viewbuilderbuildeitherfirst-emptyview-viewbuilderbuildeithersecond-renderpage","text":"Awful to read, but is it better? Using a Group is preferred in this case.","title":"Fun question on conditions: We need to put them at least in Group { if whatever { ... } else { ... } }. So ... is it preferable to use ViewBuilder commands to create it, like             ViewBuilder.buildBlock(pageInfo == nil ? ViewBuilder.buildEither(first: EmptyView()) : ViewBuilder.buildEither(second: renderPage) )"},{"location":"wwdc21/SwiftUI.html#the-dependency-graph-can-it-have-loops-or-is-it-acyclic","text":"Graph cycles are not allowed and will trap at runtime.","title":"The Dependency Graph \u2014 can it have loops, or is it acyclic?"},{"location":"wwdc21/SwiftUI.html#i-have-a-class-store-observableobject-that-holds-the-whole-app-state-it-lives-as-stateobject-in-the-root-of-the-app-lifecycle-and-passed-via-environment-to-all-views-view-send-actions-to-the-store-and-update-as-soon-as-stores-state-updated-it-allows-me-to-keep-the-app-consistent-could-you-please-mention-any-downsides-of-this-approach-from-your-prospective","text":"That approach makes every view in your app dependent on a single Observable object. Any change to a Published property forces every view that references the environment object to be updated. I think this might cover your question too. -by \"forces every view that references the environment object to be updated\" you mean that view produces new body to diff with old body? That view produces a new body. SwiftUI doesn\u2019t diff views. It regenerates views when dependencies change.","title":"I have a class Store: ObservableObject that holds the whole app state. It lives as @StateObject in the root of the App lifecycle and passed via environment to all views. View send actions to the store and update as soon as store's state updated. It allows me to keep the app consistent. Could you please mention any downsides of this approach from your prospective?"},{"location":"wwdc21/SwiftUI.html#is-there-any-difference-between-the-persistence-of-state-vs-stateobject-vs-observedobject-when-identity-changes","text":"Yes! ObservedObject does not have any lifetime implications\u2014 when you use that property wrapper, you are responsible for managing the lifetime yourself. The object can live anywhere\u2014 either in StateObject or possibly even in another data structure in your app.","title":"is there any difference between the persistence of @State vs @StateObject vs @ObservedObject when identity changes?"},{"location":"wwdc21/SwiftUI.html#should-state-and-stateobject-be-usually-defined-as-private","text":"Yes! It's often helpful to make them private to indicate that the state is for this view and its descendants. Thank you! It was a great talk!","title":"Should @State and @StateObject be usually defined as private?"},{"location":"wwdc21/SwiftUI.html#if-the-only-reasonable-property-for-establishing-stable-identity-in-a-model-from-say-an-existing-json-api-is-a-string-that-might-be-quite-long-maybe-its-more-of-a-description-than-a-name-is-that-large-id-enough-of-a-potential-performance-problem-that-we-should-consider-modifying-the-modelapi-to-add-a-more-compact-id-thanks-much","text":"Great question! As with any performance problem, it's best to measure first before over-optimizing. Long strings can often be expensive, so it might make sense to optimize the identifier, but I'd recommend measuring first.","title":"If the only reasonable property for establishing stable identity in a model from, say, an existing JSON API, is a string that might be quite long (maybe it's more of a description than a name), is that large ID enough of a potential performance problem that we should consider modifying the model/API to add a more compact ID? Thanks much!"},{"location":"wwdc21/SwiftUI.html#i-have-created-a-viewmodifier-that-adds-a-custom-modal-overlay-on-top-of-a-view-somewhat-similar-to-a-sheet-is-there-a-way-to-pass-a-view-as-parameter-of-this-viewmodifier-initializer-without-resorting-to-anyview-i-want-to-be-able-to-pass-the-view-that-will-be-the-actual-content-of-the-overlay","text":"You can do this by making your custom ViewModifiergeneric on the content, something like: struct MyModifier : ViewModifier { ... } , then using a property with the generic like so: var content: C","title":"I have created a ViewModifier that adds a custom modal \u201coverlay\u201d on top of a view, somewhat similar to a sheet. Is there a way to pass a View as parameter of this ViewModifier initializer, without resorting to AnyView? I want to be able to pass the view that will be the actual \u201ccontent\u201d of the overlay."},{"location":"wwdc21/SwiftUI.html#from-the-dependency-graph-part-of-demystifying-swiftui","text":"The video mentioned that both views, two views that are dependent on the same dependency, would need to generate a new body. Which view would need to generate a new body first if one view is the child of the other? The parent view will generate its body first, and recursively travers all the child views.","title":"From the dependency graph part of \"Demystifying SwiftUI\":"},{"location":"wwdc21/SwiftUI.html#is-the-computed-uuid-not-stable-because-we-cannot-predict-the-position-of-the-value-therefore-where-it-will-be-placed-in-the-view","text":"Yes","title":"Is the computed UUID not stable because we cannot predict the position of the value, therefore where it will be placed in the view?"},{"location":"wwdc21/SwiftUI.html#do-situations-exist-when-the-developers-really-need-to-use-anyview-and-there-are-totally-no-alternatives-using-swift-constructs","text":"Lot\u2019s of questions about when it is okay or not to use AnyView! If you can avoid AnyView, we recommend that. For example, use @ViewBuilder or pass view values around using generics instead of using AnyView. However, we offer AnyView as API because we understand that there are some situations where no other solution is possible, or comes with other important tradeoffs. One rule of thumb: AnyView can be okay if used in situations where the wrapped view will rarely or never change. But using AnyView to switch back and forth between different values can cause performance issues, since SwiftUI has to do extra work to manage that.","title":"Do situations exist when the developers really need to use AnyView and there are totally no alternatives using Swift constructs?"},{"location":"wwdc21/SwiftUI.html#do-you-have-any-suggestions-for-why-imageappicon-would-fail-to-work-when-imageuiimage-uiimagenamed-appicon-works-just-fine-is-that-intentional-behavior","text":"Is the image located in an asset catalog? Yes. It\u2019s the AppIcon in the asset catalog. (As in, the one used for the app icon on the home screen.) Please file a report through feedback assistant if you haven\u2019t already. If you have, can you post the number here? I\u2019d wonder whether the asset compiler is treating the app icon specially. We should fix it, but perhaps the app icon is being copied elsewhere in the bundle so the system can reach it.","title":"Do you have any suggestions for why Image(\"AppIcon\") would fail to work, when Image(uiImage: UIImage(named: \"AppIcon\")!) works just fine? Is that intentional behavior?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-debug-attributegraph-crashes-im-getting-attributegraph-precondition-failure-setting-value-during-update-696-probably-due-to-a-hosting-controller-somewhere-but-dont-know-how-to-track-it-down","text":"Thanks for the question! That error usual means that some code evaluated in body or updateUIViewController (or NS\u2026) is mutating state. We have a new debugging tool in the new SDK that might help narrow it down. Inside body if you write Self._printChanges() SwiftUI will log the name of the property that changed to cause the view to be redrawn. (Note the underscore. This isn\u2019t API, but is for exposed for debugging.)","title":"Is there a way to debug AttributeGraph crashes? I\u2019m getting AttributeGraph precondition failure: \"setting value during update\": 696., probably due to a hosting controller somewhere, but don\u2019t know how to track it down."},{"location":"wwdc21/SwiftUI.html#still-hoping-to-get-any-info-on-if-you-can-hide-the-tab-bar-when-using-tabview-and-pushing-items-onto-a-navigationview-like-you-can-in-uikit","text":"I do not believe there is a TabViewStyle that will allow this. However, please file a request using feedback assistant describing your use case. Thanks!","title":"Still hoping to get any info on if you can hide the tab bar when using TabView and pushing items onto a NavigationView like you can in UIKit."},{"location":"wwdc21/SwiftUI.html#ive-started-an-app-where-ive-got-3-screens-selected-by-tab-views-id-like-to-create-a-setting-to-default-to-one-of-the-screens-or-the-other-whats-an-efficient-way-to-do-that-maybe-a-lab-type-question","text":"You would use the selection Binding passed into the TabView initializer to decide which view will be selected by default. struct ContentView: View { @State var selectedTab = 1 var body: some View { TabView(selection: $selectedTab) { Text(\"First\") .tag(0) Text(\"Second\") .tag(1) Text(\"Third\") .tag(2) } } } That will have the view start with the 2nd tab selected","title":"I've started an app where I've got 3 screens - selected by tab views. I'd like to create a setting to default to one of the screens or the other. What's an efficient way to do that? Maybe a lab type question..."},{"location":"wwdc21/SwiftUI.html#any-way-to-control-when-the-scenestorage-gets-saved-many-are-in-my-root-app-view-and-they-only-seem-to-be-written-at-app-background-not-before","text":"I\u2019m sorry, but we don\u2019t provide API to control scene storage. The intent is that it\u2019s saved on backgrounding and restored on foreground/scene recreation.","title":"Any way to control when the @SceneStorage gets saved? Many are in my root app view, and they only seem to be written at app background, not before."},{"location":"wwdc21/SwiftUI.html#as-a-college-student-working-as-an-intern-at-apple-is-really-important-to-me-what-is-your-best-recommendation-for-individuals-considering-applying-for-an-internship-at-apple-whether-it-be-related-to-a-resume-project-types-or-otherwise","text":"It\u2019s great to hear you\u2019re interested in working at Apple! I joined as an intern so I can definitely relate to the craziness of applying, but it is absolutely worth doing. Interning at Apple is a really wonderful experience, and you really do some incredible work over the course of an internship! Getting hired at Apple means being very intentional about what you\u2019re looking for, and the best way to do that is to explore the jobs site for roles you think you can do, and applying specifically for that role. We have thousands of hiring managers and I don\u2019t know most of them, so my recommendation won\u2019t carry much weight, so I can\u2019t be of much help there. My best advice is to create a compelling cover letter and r\u00e9sum\u00e9 that expresses why you are right for this specific role, rather than applying for dozens of jobs at once. There\u2019s lots more information for you here as well: https://www.apple.com/careers/us/students.htm","title":"As a college student, working as an intern at Apple is really important to me. What is your best recommendation for individuals considering applying for an internship at Apple, whether it be related to a resume, project types, or otherwise?"},{"location":"wwdc21/SwiftUI.html#is-there-an-efficient-way-to-set-the-values-of-individual-pixels-in-a-view-in-swiftui-sorry-i-havent-fully-examined-the-new-canvas-api-yet-for-the-record-i-dont-really-know-how-to-do-this-in-uikit-either-other-than-writing-a-custom-fragment-shader-for-a-metal-view-thank-you","text":"Canvas is really the best bet here. You could fill a 1px x 1px rectangle of the color you want, and that would be the most efficient expression of that within SwiftUI itself. And to get the size of a pixel you would request the pixelLength from the environment of the GraphicsContent of the Canvas","title":"Is there an efficient way to set the values of individual pixels in a view in SwiftUI? Sorry I haven't fully examined the new Canvas API yet. For the record, I don't really know how to do this in UIKit either, other than writing a custom fragment shader for a Metal view. Thank you!"},{"location":"wwdc21/SwiftUI.html#when-to-iterate-a-collection-directly-from-listitems-items-in","text":"Code here vs. List { ForEach(items) { items in Code here... Fundamentally these should provide the same behavior, in almost all use cases this would be a stylistic choice over which to use. The former works great if you know you'll only ever have that single array of items in your list. The latter is where I generally start because its less things to change if I want to provide content from multiple sources into my list.","title":"When to iterate a collection directly from List(items) { items in"},{"location":"wwdc21/SwiftUI.html#will-texteditor-have-any-level-of-support-for-attributedstrings","text":"While we can\u2019t comment on any future plans, please do file a request through feedback assistant for any improvements to TextEditor that you would like to see!","title":"Will TextEditor have any level of support for AttributedStrings?"},{"location":"wwdc21/SwiftUI.html#is-it-possible-to-specify-paragraph-styles-attributedstrings-used-by-swiftui-text-views-i-dont-see-any-mention-of-paragraph-style-in-the-swiftui-attributescope-but-its-there-for-the-uikit-scope","text":"SwiftUI currently supports all of the attributes in the SwiftUI scope and some in the nested Foundation scope, the supported Foundation attributes are links and inlinePresentationIntent. So it\u2019s not possible to specify paragraph styles","title":"Is it possible to specify paragraph styles AttributedStrings used by SwiftUI Text views? I don't see any mention of paragraph style in the SwiftUI AttributeScope (but it's there for the UIKit scope)"},{"location":"wwdc21/SwiftUI.html#is-it-possible-to-place-a-calayer-inside-of-a-swiftui-view-without-using-a-uiviewrepresentable","text":"Not currently. UIViewRepresentable is the way to go if you want to expose custom layers inside of your SwiftUI content. If you have a specific use-case where this can't work, please do file a feedback request!","title":"Is it possible to place a CALayer inside of a SwiftUI view without using a UIViewRepresentable?"},{"location":"wwdc21/SwiftUI.html#how-do-i-get-a-swiftui-time-picker-to-change-to-5min-instead-of-1min-i-tried-to-find-that-but-so-far-i-havent-found-how-to-do-that","text":"Thanks for the question. Making sure I understand\u2026 Are you using a DatePicker and want the stepper increments to be 5 minutes instead of 1 minute? no actually a time picker, .datePickerStyle(WheelDatePickerStyle()) Ah, right. And that\u2019s applied to a view hierarchy containing a DatePicker then? Set to just show time components? Yes: aDatePicker(\u201cPlease enter a time\u201d, selection: $startTime, displayedComponents: .hourAndMinute) .labelsHidden() .datePickerStyle(WheelDatePickerStyle()) And after all that clarification, I\u2019m afraid the answer is that we don\u2019t provide customization for that currently.","title":"How do I get a SwiftUI time picker to change to 5min instead of 1min? I tried to find that but so far I haven't found how to do that..."},{"location":"wwdc21/SwiftUI.html#in-the-past-i-have-created-button-views-that-will-have-a-local-state-and-present-a-sheet-for-instance-i-might-have-a-button-whose-purpose-is-to-add-a-new-item-to-my-apps-data-this-was-super-convenient-because-i-could-add-such-a-button-in-any-context-without-needing-the-view-adding-the-button-to-know-about-that-functionality","text":"Running my App Store version of my app on iOS 15, those buttons no longer are working. (I haven't yet debugged the app; I'm simply noticing it across my iOS 15 devices.) Is this a sign that this is a pattern I shouldn't be using? Or should SwiftUI continue to support such patterns? Please file a report using feedback assistant and a sample project. Thanks!","title":"In the past, I have created Button views that will have a local state and present a sheet. For instance, I might have a Button whose purpose is to add a new item to my app's data. This was super convenient because I could add such a button in any context without needing the view adding the button to know about that functionality."},{"location":"wwdc21/SwiftUI.html#there-are-instances-when-buttons-in-swiftui-stop-responding-to-the-ipados-pointer-but-work-when-tapped-normally-is-this-an-expected-behavior","text":"Please file a report using feedback assistant and a sample project that reproduces this issue! I would not consider this expected","title":"There are instances when Buttons in SwiftUI stop responding to the iPadOS pointer, but work when tapped normally. Is this an expected behavior?"},{"location":"wwdc21/SwiftUI.html#is-it-possible-to-change-the-spacing-between-views-in-a-list-or-stack-view-as-the-list-scrolls-up-and-down-in-a-springlike-manner-ie-is-there-a-swiftui-equivalent-to-uikitdynamics","text":"Hi Andrew, we don\u2019t have any support for this in SwiftUI. Could you please file a feedback with the use case you are interested in? Thanks!","title":"Is it possible to change the spacing between views in a List (or stack view) as the list scrolls up and down in a springlike manner? (i.e. is there a SwiftUI equivalent to UIKitDynamics?)"},{"location":"wwdc21/SwiftUI.html#whats-the-recommended-practice-for-custom-sheet-presentations-swiftuis-sheet-only-supports-the-standard-card-style-modal-presentation-it-doesnt-have-the-flexibility-of-eg-uikits-transitioning-delegates-and-presentation-controllers-to-let-us-size-and-position-modally-presented-controllers","text":"Thanks for the question. For custom presentations, I\u2019ve had good luck putting the presentation content in an overlay, then using an offset modifier to shift it off screen. To bring it on screen, I use State to zero out the offset. If the state is updated inside withAnimation to transition is animated. That content won\u2019t be lazy then, right? It\u2019ll be computed in advance and off-screen. You could conditionalize the content too. An offscreen empty view would be essentially free. As would an offscreen view with opacity of 0. You might need to play with the transitions to get the appearance just right. And it\u2019s essentially free because the framework is smart enough to not bother rendering content that\u2019s invisible? Yep!","title":"What\u2019s the recommended practice for custom sheet presentations? SwiftUI\u2019s .sheet only supports the standard card-style modal presentation. It doesn\u2019t have the flexibility of e.g. UIKit\u2019s transitioning delegates and presentation controllers to let us size and position modally presented controllers."},{"location":"wwdc21/SwiftUI.html#this-isnt-a-technical-question-but-what-is-your-favorite-part-about-working-on-the-swiftui-team","text":"The people, both inside and outside of Apple, by a million miles! Everyone I\u2019ve worked with here at Apple is kind, empathetic, thoughtful, and incredibly passionate about the work we do. It requires lots of collaboration to pull all the work we do into one cohesive framework, so working with a great team is super important. Re. people outside of Apple, you all are the best community I could imagine. It\u2019s really wonderful getting your feedback, hearing from you at WWDC, seeing you at community events, etc. As Josh said, empowering you all to make amazing apps is really special.","title":"This isn't a technical question, but what is your favorite part about working on the SwiftUI Team?"},{"location":"wwdc21/SwiftUI.html#is-it-possible-to-prevent-a-button-inside-a-list-item-from-being-activated-when-the-list-row-is-selected-right-now-if-you-have-multiple-buttons-they-will-all-be-triggered-when-the-row-is-selected-so-cant-really-have-a-secondary-button-in-a-list-row","text":"Explicitly setting the buttonStyle of the nested buttons will stop the List from capturing the event.","title":"Is it possible to prevent a Button inside a List item from being activated when the list row is selected? Right now, if you have multiple Buttons they will all be triggered when the row is selected, so can't really have a secondary button in a list row."},{"location":"wwdc21/SwiftUI.html#what-is-the-recommended-way-to-open-a-view-in-a-new-window-in-swiftui-for-macos","text":"We don\u2019t have much API in this area at the moment - using a NavigationLink in a commands context will open a window with the destination view. Could you elaborate on your use case here? If not, that is ok, but I would definitely suggest filing a feedback with any info you can provide us.","title":"What is the recommended way to open a View in a new window in SwiftUI for macOS?"},{"location":"wwdc21/SwiftUI.html#how-would-someone-go-about-turning-a-list-cell-into-a-textfield-on-tap-such-as-to-edit-the-name-similar-to-the-reminders-app","text":"I\u2019d try putting a TextField in an overlay of your cell content with an opacity of 0 and disabled. Then use a tap gesture to switch the opacity and You could use the new FocusState or onSubmit when the user is done editing to switch back. What about the new selectable text modifier?? Would that work here? The trickiest bit will be getting your TextField and the regular Text to line up. You might need some custom padding for that. Take a look at the ScaledMetric property wrapper to make padding that adapts to dynamic text size. That\u2019s a great idea. It would work for selecting and copying the text, but not for editing it.","title":"How would someone go about turning a List cell into a TextField on tap, such as to edit the name (similar to the Reminders app)?"},{"location":"wwdc21/SwiftUI.html#theres-a-lot-of-sample-code-out-there-where-people-use-geometryreader-inside-of-clear-overlays-and-the-like-to-not-impact-layout-is-that-ok-seems-not-like-the-way-it-was-designed-to-work","text":"In general we'd consider that a bit of a misuse of the API. If you have specific use-cases where you're needing to do this we'd love to hear about it in a feedback report!","title":"There's a lot of sample code out there where people use GeometryReader inside of clear overlays and the like to not impact layout... is that \"ok\"? Seems not like the way it was designed to work."},{"location":"wwdc21/SwiftUI.html#this-is-kind-of-vague-but-i-know-hid-declares-that-a-second-button-below-a-sign-in-with-apple-button-cant-look-too-similar-as-to-distinguish-them-how-lenient-is-this-like-i-tried-to-replicate-the-sign-in-with-apple-button-for-a-sign-in-with-email-button-like-a-logo-next-to-it-with-an-envelope-instead-of-the-apple-logo-used-a-similar-but-not-sf-font-etc-but-im-afraid-this-would-constitute-as-being-too-similar-to-the-sign-in-with-apple-button-sorry-for-the-vague-question-and-let-me-know-if-this-is-more-suited-towards-a-lab","text":"We cannot provide that kind of guidance here; you are probably better off reaching out to App Store developer review labs for confirmation. I would also review the guidance provided here re: custom buttons: https://developer.apple.com/design/human-interface-guidelines/sign-in-with-apple/overview/buttons/","title":"This is kind of vague, but I know HID declares that a second button below a Sign in With Apple Button can't look too similar as to distinguish them. How lenient is this? Like I tried to \"replicate\" the sign in with apple button for a Sign in With Email Button (like a logo next to it with an envelope instead of the apple logo, used a similar but not SF font, etc), but I'm afraid this would constitute as being \"too similar to the Sign in With Apple Button\". Sorry for the vague question, and let me know if this is more suited towards a lab!"},{"location":"wwdc21/SwiftUI.html#what-is-the-layout-and-sizing-behavior-of-canvas","text":"A Canvas will consume the space offered to it. You can put it in a .frame to control the size if needed.","title":"What is the layout and sizing behavior of Canvas?"},{"location":"wwdc21/SwiftUI.html#is-there-a-new-swiftui-way-to-convert-a-view-into-an-image-or-pdf-currently-this-can-be-done-but-it-is-one-of-those-round-about-hacks-that-no-one-likes-using","text":"Unfortunately, there\u2019s no native SwiftUI api for this right now. As was mentioned elsewhere in the thread though and in your question though, it can be done by wrapping in a UIHostingController. Feel free to file a feedback requesting the feature though!","title":"Is there a new SwiftUI way to convert a view into an image or pdf?  Currently, this can be done, but it is one of those round about, hacks that no one likes using."},{"location":"wwdc21/SwiftUI.html#what-is-the-best-way-to-implement-keyboard-bindings-on-swiftui-for-macos","text":"I'm building a video player where I would like to forward and rewind with the left and right buttons. I've tried attaching keyboardShortcut(.leftArrow) on my button but it doesn't work? Is it the view I'm attaching it wrong (shall I attach it to my window root view maybe?) or is there some known bug around it? Thanks Attaching it to the button is the correct expression here - it sounds like you may be encountering a bug with this. Could you file a feedback with this issue?","title":"What is the best way to implement keyboard bindings on swiftUI, for macOS?"},{"location":"wwdc21/SwiftUI.html#with-sign-in-with-apple-is-it-a-requirement-to-use-a-nonce-ive-seen-conflicting-information-some-stating-that-if-its-done-externally-through-something-like-firebase-then-a-nonce-is-required-but-if-its-done-using-signinwithapplebutton-then-no-nonce-is-required-whats-the-verdict-on-this","text":"As this does not appear to be related to SwiftUI, I recommend scheduling an appointment at one of the Apple Pay labs","title":"With Sign in With Apple, is it a requirement to use a nonce? I've seen conflicting information, some stating that if it's done externally (through something like firebase), then a nonce is required, but if it's done using SignInWithAppleButton then no nonce is required. What's the verdict on this?"},{"location":"wwdc21/SwiftUI.html#is-there-a-modifier-to-control-how-links-in-attributedstring-are-handled","text":"Specifically, I'd like to be able to handle some links in-app, rather than booting the user out to Safari Currently the links will be handled by using the default OpenURLAction from the environment, it\u2019s not possible to override it at the moment. You could however use a custom url scheme that will open the url in your app, for example myapp://","title":"Is there a modifier to control how links in AttributedString are handled?"},{"location":"wwdc21/SwiftUI.html#lets-say-i-have-a-purely-swiftui-flow-i-have-a-listview-with-a-stateobject-var-listfetcher-that-makes-requests-for-a-list-of-items-tapping-on-an-item-in-the-list-navigates-to-a-detailview-which-has-an-observableobject-property-detailfetcher-that-makes-requests-for-details-on-the-item","text":"What\u2019s the best way to structure DetailView and which property wrapper would we use for detailFetcher in DetailView? 1. Have an initializer like init(itemID: Int) , and use @StateObject ? This would require us to eventually update the detailFetcher property with something like detailFetcher.itemID = itemID in the body\u2019s onAppear 2. Pass in the detailFetcher into the initializer like init(detailFetcher: ObservableObject) and make the property @ObservableObject? If this is preferred, where would this detailFetcher live if not in SwiftUI? In general, use @StateObject when the view in question owns the associated object, i.e. the object will be created when the view is created, and should be destroyed when the view is removed. In contrast, use @ObservedObject when the view needs to reference an object that is owned by another view or something else. So the view is dependent on the object, but their lifetimes are not tied together. For example, you could have a main screen that uses @StateObject to initialize your app\u2019s model, and then pass that object off to detail screens using @ObservedObject. Also check out the \u201cDemystify SwiftUI\u201d talk tomorrow to learn more about this!","title":"Let\u2019s say I have a purely SwiftUI flow. I have a ListView with a @StateObject var listFetcher, that makes requests for a list of items. Tapping on an item in the list navigates to a DetailView, which has an ObservableObject property detailFetcher, that makes requests for details on the item."},{"location":"wwdc21/SwiftUI.html#with-the-new-markdown-support-for-text-are-you-able-to-control-what-specific-markdown-elements-look-like-for-example-can-we-specify-that-bold-is-supposed-to-display-as-demi-bold-rather-than-bold-or-what-the-font-font-size-or-font-weight-of-an-h1-header-should-be-in-general-how-much-flexibility-do-we-have-in-styling-the-markdown","text":"At the moment, it\u2019s not possible to control the default styling in Markdown for SwiftUI. It would be great if you could file a feedback as a feature request/suggestion. Another thing to note, SwiftUI only supports inline styles, paragraph styling such as headers (H1 for example) is not supported.","title":"With the new Markdown support for Text, are you able to control what specific Markdown elements look like?  For example, can we specify that bold is supposed to display as Demi-bold rather than bold?  Or what the font, font size, or font weight of an H1 header should be?  In general, how much flexibility do we have in styling the markdown?"},{"location":"wwdc21/SwiftUI.html#does-swiftui-support-device-rotation-animations-when-i-rotate-from-portrait-to-landscape-my-app-just-changes-without-an-animation","text":"Please file a report using feedback assistant and a sample project!","title":"Does SwiftUI support device rotation animations? When I rotate from portrait to landscape, my app just changes without an animation."},{"location":"wwdc21/SwiftUI.html#is-there-a-suggested-way-or-best-practice-to-show-a-uidocumentpickerviewcontroller-in-a-swiftui-sheet-without-triggering-an-attempt-to-present-on-which-is-already-presenting","text":"You should check out the .fileImporter modifier if you haven\u2019t already: https://developer.apple.com/documentation/swiftui/form/fileimporter(ispresented:allowedcontenttypes:allowsmultipleselection:oncompletion:) If you are encountering a specific issue that you think may be a bug, please file a Feedback report!","title":"Is there a suggested way or best practice to show a UIDocumentPickerViewController in a SwiftUI sheet without triggering an \"Attempt to present  on  which is already presenting ?"},{"location":"wwdc21/SwiftUI.html#i-saw-in-the-whats-new-with-swiftui-that-textfields-will-now-have-onsubmit-for-when-the-return-key-is-pressed-for-software-and-hardware-keyboards-does-this-mean-that-oncommit-will-be-depreciated-or-does-onsubmit-serve-a-different-function","text":"The initializers with .onCommit are soft deprecated. So no warnings in Xcode, but we\u2019re encouraging new code to use .onSubmit.","title":"I saw in the \"What's new with SwiftUI\" that TextFields will now have .onSubmit for when the return key is pressed for software and hardware keyboards. Does this mean that .onCommit will be depreciated or does .onSubmit serve a different function?"},{"location":"wwdc21/SwiftUI.html#are-current-changes-set-in-stone-and-the-feature-release-plan-for-swiftui-has-been-wrapped-up-for-ios15-or-we-could-expect-more-functionalities-along-the-way-until-the-official-release","text":"We cannot comment on future plans. However, please file feedback for any issues you find and they will be triaged appropriately! Note that \u201cissues\u201d here refers to both what is traditionally considered \u201cbugs\u201d as well as an feature requests or enhancements both for existing API as well as API that is newly introduced this year","title":"Are current changes \u2018set in stone\u2019 and the feature release plan for SwiftUI has been wrapped up for iOS15 or we could expect more functionalities along the way until the official release?"},{"location":"wwdc21/SwiftUI.html#if-were-using-sign-in-with-apple-and-have-some-text-that-says-sign-in-with-apple-apple-logo-here-from-sf-symbols-on-a-screen-that-comes-after-sign-in-with-apple-where-we-ask-for-some-additional-information-specifically-just-the-birthday-of-the-user-is-that-an-hid-violation-its-only-used-to-reference-sign-in-with-apple-so-im-not-sure-if-that-would-violate-the-rules-or-not","text":"Just to clarify, your view has static \u201cSign in with Apple\u201d text after the button has been tapped/clicked on another screen? Well so this is how it is - There\u2019s a login screen - it either lets you sign in with apple, sign in with email/password, or create an account. If they pick sign in with apple, and this is the first time they go through the process, then it prompts them with the hide my email/name sheet. Then after that, again if it\u2019s their first time, there\u2019s a screen they go to that asks for their birthday, and it says : Sign in with Apple [apple logo] with a subtitle stating that some additional info is required (paraphrased since it\u2019s a little bit of lengthy text), and then a date picker for their birthday. That Sign in With Apple [apple logo] is sort of replicating what\u2019s on the Sign in with Email screen which similarly says Sign in with Email [envelope icon]. My guess is that it would not be allowed since it is giving users the impression that the birthday is part of the Sign In With Apple process, but that is not the case. I think it\u2019s okay to show the birthday step following the sign in process, but as for showing \u201cSign In with Apple \uf8ff\u201d after that, I do not think it is allowed. My recommendation would be to remove this text entirely and instead have something like \u201cFinish setting up your account\u201d As indeed this does not have anything to do with signing in with Apple","title":"If we're using Sign in With Apple and have some text that says \"Sign in with Apple [apple logo here from SF Symbols]\" on a screen that comes after sign in with apple where we ask for some additional information (specifically just the birthday of the user), is that an HID violation? It's only used to reference Sign in With Apple, so I'm not sure if that would violate the rules or not"},{"location":"wwdc21/SwiftUI.html#are-navigationlinks-lazy-in-ios-15","text":"NavigationLinks do not fully resolve their destinations until they are triggered, though the value of the destination view is created when the NavigationLink is created. In general, we recommend avoiding as much work as possible when a view is initialized, which would avoid potential issues here. This is important for performance. Instead, have that work be triggered within the view\u2019s body, such as using onAppear or the new task() modifier. SwiftUI may reinitialize views for any number of reasons, and this is a normal part of the update process.","title":"Are NavigationLinks \"lazy\" in iOS 15?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-show-a-watchos-app-view-using-sample-data-in-an-ios-app-such-as-for-an-app-that-showcases-various-custom-complication-styles-i-understand-that-the-alternative-would-be-a-screenshot-but-i-thought-it-would-be-a-much-simpler-workflow-to-have-the-watchos-views-dynamically-generated-since-both-platforms-use-swiftui","text":"There is not, but please file feedback for the idea! Using screenshots is your best option as a workaround.","title":"Is there a way to show a watchOS app view using sample data in an iOS app, such as for an app that showcases various custom complication styles? I understand that the alternative would be a screenshot, but I thought it would be a much simpler workflow to have the watchOS views dynamically generated since both platforms use SwiftUI."},{"location":"wwdc21/SwiftUI.html#on-watchos-im-using-oncontinueuseractivitynsuseractivitytypebrowsingweb-and-onopenurl-however-these-only-receive-callbacks-once-the-app-is-already-open-is-it-possible-to-use-these-in-a-way-that-they-receive-the-callback-from-a-cold-launch","text":"Those should function appropriately on cold launch. Please file a feedback with a sample showing where it isn't working if possible. What\u2019s the easiest way to setup the site association file for the example? That is an excellent question. Just the general shape of the application and where it's wired in will go a long way, even if the entire association isn't hooked up.","title":"On watchOS, I'm using .onContinueUserActivity(NSUserActivityTypeBrowsingWeb) and .onOpenURL, however these only receive callbacks once the app is already open. Is it possible to use these in a way that they receive the callback from a cold launch?"},{"location":"wwdc21/SwiftUI.html#what-is-the-recommended-way-to-account-for-a-navigationview-title-that-may-take-up-multiple-lines-i-tried-setting-linelimit-to-nil-but-i-am-not-getting-the-results-that-i-want-for-instance-i-have-xyz-club-executive-meeting-for-the-navigationbartitle-text-but-it-only-shows-xyz-club-execut","text":"Take a look at the toolbar modifier and the .principal placement. If that doesn\u2019t meet your needs, please file a Feedback so we can look into your use case.","title":"What is the recommended way to account for a NavigationView title that may take up multiple lines? I tried setting .lineLimit to nil, but I am not getting the results that I want. For instance, I have \"XYZ Club Executive Meeting\" for the navigationBarTitle text, but it only shows \"XYZ Club Execut...\""},{"location":"wwdc21/SwiftUI.html#the-whats-new-in-foundation-session-demos-localizing-content-in-a-swiftui-app-caffe-is-this-demo-app-available-as-sample-code-it-doesnt-appear-to-be-listed-on-the-sample-code-page","text":"It is not available. But thanks for the suggestion, we\u2019ll consider that!","title":"The \"What's new in Foundation\" session demos localizing content in a SwiftUI app, \"Caffe\". Is this demo app available as sample code? It doesn't appear to be listed on the sample code page."},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-hide-a-default-green-scroll-indicator-on-watchos-when-using-digitalcrownrotation-modifier","text":"There is not, but please file feedback! I'm also curious why you would want hide the indicator? I believe it does hide if you use make it \"continuous\"","title":"Is there a way to hide a default green scroll indicator on watchOS when using digitalCrownRotation modifier?"},{"location":"wwdc21/SwiftUI.html#what-would-be-the-recommended-way-to-handle-complex-navigations-in-a-100-swiftui-project-in-all-my-past-uikit-project-the-coordinator-pattern-was-really-powerful-for-that-but-its-quite-tricky-to-have-it-ported-to-swiftui-thanks","text":"It\u2019s difficult to make a specific recommendation for a complex app. Every app is so different. This is definitely a great question to request a lab appointment for. Then we can understand your use case and make some suggestions, and maybe even look at some code together.","title":"what would be the recommended way to handle complex navigations in a 100% SwiftUI project, in all my past UIKit project the coordinator pattern was really powerful for that but it's quite tricky to have it ported to swiftui. thanks"},{"location":"wwdc21/SwiftUI.html#is-there-a-limit-to-where-sfsymbols-change-size-i-put-them-as-items-in-a-tabview-but-it-doesnt-seem-like-they-are-scaling-with-dynamic-text","text":"TabViews do have a standard symbol size across all apps and how far they scale Does that mean they don\u2019t scale as much as other places? Right, other elements could end up scaling more, which is expected. Ah thanks - As a UI element I sort of wanted it to scale more. Is there a better way to do that. I\u2019d rather not leave SF Symbols If you have a more custom UI element where you want that scaling, you could build that! Especially with the new .bar material, you could still get a similar material background behind that custom bottom bar Ooo - I\u2019ll have to read up about that. Is there a talk with more in depth discussion of material? Today\u2019s rich graphics session is the one you\u2019re looking for https://developer.apple.com/videos/play/wwdc2021/10021/","title":"Is there a limit to where SFSymbols change size? I put them as items in a TabView but it doesn\u2019t seem like they are scaling with dynamic text \u2026"},{"location":"wwdc21/SwiftUI.html#not-entirely-swiftui-exclusive-question-but-worth-a-try","text":"Xcode 13 release notes list as \"known issue\": \"Swift Concurrency requires a deployment target of macOS 12, iOS 15, tvOS 15, and watchOS 8 or newer. (70738378) This suggests that concurrency might be backwards deployed? This seems too good to be true because I'd would need runtime support, but perhaps you guys can shed some light on this? For now you should assume that Swift concurrency cannot be back-deployed, however the team understands that this is a popular request and is investigating. The fun part is that Swift is open source, and so you can visit https://forums.swift.org to peek in on the active development process for Swift concurrency, and they will likely discuss this topic more there as well.","title":"Not entirely SwiftUI exclusive question, but worth a try:"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-change-the-background-colour-of-a-list-last-year-this-wasnt-possible","text":"Currently it\u2019s only possible to change background color of list rows, not the List itself. Thanks @Natalia (Apple) , is this intended, or something that might change in the future? If so, what is the best way to change the background colour currently? I\u2019d like to use system materials specifically Unfortunately, it\u2019s not possible to use system materials with List at the moment, please file a feedback and we will take a look at it. In the mean time, you could try using a ScrollView instead of a List.","title":"Is there a way to change the background colour of a list? Last year this wasn\u2019t possible"},{"location":"wwdc21/SwiftUI.html#refreshable-doesnt-work-with-scroll-view-is-it-a-bug-or-desired-behaviour","text":"Currently refreshable only supports List. Please file a feedback request if you have other use-cases you\u2019d like to see supported.","title":"Refreshable doesn\u2019t work with scroll view. Is it a bug or desired behaviour?"},{"location":"wwdc21/SwiftUI.html#how-would-one-go-about-adding-the-searchable-modifier-to-a-pdfview","text":"On iOS, the searchable modifier works with a navigation view to render its search field. On macOS, it works with a navigation view or the root view of your scene. If you combine a PDFView with a navigation view, things should hopefully work as you expect. If not, please do file a feedback with some more details on what you would like to achieve.","title":"How would one go about adding the .searchable modifier to a PDFView?"},{"location":"wwdc21/SwiftUI.html#is-searchable-for-local-data-only-or-can-it-be-used-to-query-services","text":"Searchable specifies that the view supports search. You can implement that search however works best for your app. For example, you could kick off a query using the bound search term, then update the results when your query completes.","title":"Is .searchable for local data only, or can it be used to query services?"},{"location":"wwdc21/SwiftUI.html#are-you-able-to-programmatically-change-focus-of-text-fields-natively-in-swiftui-this-year-for-example-having-the-a-button-at-the-bottom-of-the-keyboard-say-next-and-focus-into-the-next-text-field","text":"Posting this back here in case anyone present does have a moment to answer, but if not, there\u2019s a focus and accessibility specific office hours later this week. If we\u2019re not able to get to this during this block, definitely check back then! Indeed, I would take a look at the new FocusState APIs! https://developer.apple.com/documentation/swiftui/focusstate/","title":"Are you able to programmatically change focus of text fields natively in SwiftUI this year? For example, having the a button at the bottom of the keyboard say \u201cnext\u201d and focus into the next text field?"},{"location":"wwdc21/SwiftUI.html#is-there-an-equivalent-api-similar-to-uisheetpresentationcontroller-for-swiftui","text":"Sorry. There is not a SwiftUI equivalent at this time. Is that something that would be useful to file feedback for? Certainly! Especially if you have a specific use case around when you need it or whether you need variable height. Thanks!","title":"Is there an equivalent API similar to UISheetPresentationController for SwiftUI?"},{"location":"wwdc21/SwiftUI.html#hi-intro-im-a-hobbyist-who-is-getting-better-an-programming-mostly-because-swiftui-is-so-productive","text":"Can you talk about ways to have existing SwiftUI code inherit the newer features so I could support both iOS 14 and 15 with one codebase? Most new features do not back-deploy to earlier OS versions. You can use e.g. if #available(iOS 15, *) { to check for whether a feature can be used. That said, some functionality is able to back-deploy. For example, the new ability to pass bindings to collections into List and ForEach, and get back a binding to each element, e.g. ForEach($elements) { $element in ... is able to back-deploy to any earlier release supported by SwiftUI. I thought Matt said that - So the new feature can actually work in older code? Correct","title":"Hi!  intro? I\u2019m a hobbyist who is getting better an programming mostly because SwiftUI is so productive!"},{"location":"wwdc21/SwiftUI.html#can-importsfromdevices-ie-continuity-camera-work-as-a-source-for-asyncimage","text":"No, continuity camera will callback your importsItemProviders with an item provider that will give the full image data and not a URL. But if you have a use case where there could be some better integration between the two, please file a feedback","title":"Can .importsFromDevices (i.e. continuity camera) work as a source for AsyncImage?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-animate-text-font-size-changes-currently-in-ios14-a-view-can-smoothly-animate-between-two-sizes-or-colored-backgrounds-but-fonts-will-just-jump-from-the-before-animation-size-to-the-after-size-with-no-interpolation-in-between","text":"This would be a great feedback to file, so that we can investigate! If you\u2019re interested in a workaround, the Fruta sample code project has an AnimatableFontModifier that uses an explicit font size as its animatable data. This is used for the ingredient cards in the main smoothie detail view. This works for Fruta because the use-case is limited and the text using this modifier is for primarily for graphical purposes. https://developer.apple.com/documentation/swiftui/fruta_building_a_feature-rich_app_with_swiftui","title":"Is there a way to animate text font size changes? Currently in iOS14 a view can smoothly animate between two sizes or colored backgrounds, but fonts will just jump from the before animation size to the after size with no interpolation in between."},{"location":"wwdc21/SwiftUI.html#will-xcode-13-run-on-big-sur-113","text":"Xcode 13 requires macOS 11.3 or later. You can find the full release notes here: https://developer.apple.com/documentation/xcode-release-notes/xcode-13-beta-release-notes","title":"Will Xcode 13 run on Big Sur 11.3"},{"location":"wwdc21/SwiftUI.html#does-asyncimage-support-caching-and-will-there-be-an-api-to-customize-that-cache","text":"AsyncImage uses the shared URLSession , and so uses the shared URLCache. There\u2019s currently no support for customizing the cache. If that\u2019s something you\u2019d like support for though, feel free to file feedback with that request","title":"Does AsyncImage support caching, and will there be an API to customize that cache?"},{"location":"wwdc21/SwiftUI.html#is-there-a-way-to-make-refreshable-work-with-lazyvgrid-or-do-we-have-to-adapt-list-as-a-grid-layout","text":"Refreshable is currently only supported by List.","title":"Is there a way to make refreshable work with LazyVGrid or do we have to adapt List as a Grid layout?"},{"location":"wwdc21/SwiftUI.html#with-a-document-based-app-using-a-documentgroup-scene-is-there-any-way-to-present-a-choice-of-templates-when-opening-the-app-without-an-open-document","text":"I\u2019m assuming you mean something similar to the theme chooser you get in Keynote or Pages? This is not something that we have an API for at the moment, but we\u2019d love a feedback for this functionality.","title":"With a document-based app using a DocumentGroup scene, is there any way to present a choice of templates when opening the app without an open document?"},{"location":"wwdc21/SwiftUI.html#any-prospects-for-attributedstring-support-in-texteditor","text":"While we can\u2019t comment on any future plans, please do log a request through the feedback assistant that this is a desired feature so that we can gauge developer interest!","title":"Any prospects for AttributedString support in TextEditor?"},{"location":"wwdc21/SwiftUI.html#what-is-the-proper-way-to-dismiss-a-fullscreencover-when-it-is-being-presented-based-off-an-item-setting-the-item-to-nil-or-dismissing-using-the-presentationmode","text":"That really depends on what is doing the dismissing. They both have the same effect in setting the item binding back to nil. It is more about where you are driving the dismissal state from. If you're dismissing from within the sheet's content, then using the presentation mode will be more flexible because that will work no matter what is controlling the presentation. There is also a new environment property introduced this year that can accomplish this as well, which is dismiss. https://developer.apple.com/documentation/swiftui/environmentvalues/dismiss/","title":"What is the proper way to dismiss a .fullscreenCover when it is being presented based off an item: setting the item to nil or dismissing using the presentationMode?"},{"location":"wwdc21/SwiftUI.html#is-the-refreshable-property-the-only-swiftui-property-that-supports-async-code","text":"The task modifier also provides support for async code out of the box! Generally, we only make user provided closures async if we can provide some additional utility to you by doing so, such as canceling a task when the attached view\u2019s lifetime ends, or finishing the refresh animation. If you want to dispatch to async code in other places, you can use an async block!","title":"is the refreshable property the only SwiftUI property that supports async code?"},{"location":"wwdc21/SwiftUI.html#wednesday","text":"","title":"\ud83d\uddd3 Wednesday"},{"location":"wwdc21/SwiftUI.html#using-environmentissearching-how-would-multiple-navigation-bar-in-an-app-work","text":"The isSearching environment property is tied to the searchable modifier that sets it up. If you have the following: NavigationView { ContentView() DetailView() } .searchable(text: $text) Then you could query the isSearching property inside of ContentView or DetailView. If you have the following: NavigationView { ContentView() DetailView() .searchable(text: $text) } Then you could only query it inside of DetailView. Similarly, if you have the following: NavigationView { ContentView() MiddleView() .searchable(text: $text) DetailView() .searchable(text: $text) } Then you could query isSearching in either MiddleView or DetailView and the property would relate to the enclosing searchable modifier.","title":"Using @Environment(\\.isSearching), how would multiple navigation bar in an app work?"},{"location":"wwdc21/SwiftUI.html#with-the-goal-to-bring-deeper-adoption-of-swiftui-this-year-should-we-implement-more-swiftui-code-instead-of-improve-catalyst-apps","text":"This really depends on your particular app and use case. It would be a great question to take to the labs!","title":"With the goal to bring deeper adoption of SwiftUI this year, should we implement more SwiftUI code instead of improve catalyst apps ?"},{"location":"wwdc21/SwiftUI.html#does-using-more-sf-symbols-have-an-impact-on-total-app-size-or-they-all-are-stored-in-the-os","text":"These are part of the OS, so you can feel free to go wild with all the symbols you want with no impact to app size","title":"Does using more SF Symbols have an impact on total app size? Or they all are stored in the OS?"},{"location":"wwdc21/SwiftUI.html#when-using-the-watchkit-way-to-get-text-input-do-we-still-get-the-text-input-improvements-from-swiftui-eg-remembering-the-user-choice-of-scribble-vs-voice-input","text":"If don't pass any suggestions to the WatchKit API, then yes, you get all of the new behavior.","title":"When using the WatchKit way to get text input, do we still get the text input improvements from SwiftUI? E.g. remembering the user choice of scribble vs. voice input"},{"location":"wwdc21/SwiftUI.html#does-the-modifier-of-importitemproviders-work-with-other-commands-or-other-stuff-to-import-things-in-the-app","text":"It works with any macOS system service that vends data. So if you had a service or shortcut that didn\u2019t take input, but produced output \u2014 your app\u2019s importItemProviders could consume that data when the user invokes that service","title":"does the modifier of importItemProviders work with other commands, or other stuff to import things in the app ?"},{"location":"wwdc21/SwiftUI.html#the-new-detents-for-sheets-in-uikit-to-allow-them-to-sit-at-different-heights-looks-amazing-unfortunately-there-doesnt-seem-to-be-an-equivalent-in-swiftui-is-there-any-plan-to-bring-it-across","text":"This is something we know folks would appreciate. Enhancement request Feedback with particularly use case would really help us. We\u2019re particularly interested in use cases around variable heights in SwiftUI sheets.","title":"The new detents for sheets in UIKit, to allow them to sit at different heights, looks amazing! Unfortunately there doesn't seem to be an equivalent in SwiftUI \u2013 is there any plan to bring it across?"},{"location":"wwdc21/SwiftUI.html#can-the-new-refreshable-api-be-used-with-custom-refresh-controls-spinners","text":"Take a look at the EnvironmentValues.refresh property. The refreshable modifier sets this property up with closure provided to the modifier. You can query this property ( @Environment(.refresh) private var refresh ) to hook up to your own UI","title":"Can the new .refreshable API be used with custom refresh controls / spinners?"},{"location":"wwdc21/SwiftUI.html#can-we-use-the-new-buttons-with-non-sf-single-color-graphics","text":"Yep, in fact the new buttons can have a label of any view, including shapes and more! The one thing to be careful of with custom images is that they use template rendering if you want the standard foreground styling within the button. (otherwise they\u2019ll be the exact color of the image\u2019s pixels)","title":"Can we use the new buttons with non-SF single-color graphics?"},{"location":"wwdc21/SwiftUI.html#can-we-customize-and-put-searchable-search-box-anywhere-without-going-the-old-way-adding-text-views","text":"The searchable modifier supports a few customization points with the SearchFieldPlacement type. You can pass a placement into the modifier: .searchable(text: $text, placement: .navigationBarDrawer(displayMode: .always)).","title":"Can we customize and put \"searchable\" search box anywhere? (Without going the old way, adding text views)"},{"location":"wwdc21/SwiftUI.html#how-do-we-assign-a-textfield-in-swiftui-to-gain-focus-as-soon-as-a-view-opens","text":"You can use the new @FocusState property wrapper and focused() modifier to programmatically control focus in many different ways, including when your UI first appears. For example, you could trigger a specific view to be focused using onAppear or any other event callback.","title":"How do we assign a TextField in SwiftUI to gain focus as soon as a View opens?"},{"location":"wwdc21/SwiftUI.html#how-to-force-sf-symbols-not-to-be-automatically-filledunfilled-based-on-the-context-as-shown-in-tab-bars-is-there-any-way-to-force-them-to-be-non-filled","text":"You can use .symbolVariant(.fill) to opt in a view or view hierarchy to fill variants, and see the SymbolVariants type for other variants you can use with it.","title":"How to force SF Symbols not to be automatically filled/unfilled based on the context (as shown in tab bars)? Is there any way to force them to be non-filled?"},{"location":"wwdc21/SwiftUI.html#is-there-a-swiftui-equivalent-for-preferredstatusbarstyle-in-ios-15","text":"I\u2019m afraid there is no new API for that at this time. That would be a great enhancement Feedback to help us gauge interest. If you\u2019ve already filed it, thank you! We know it\u2019s something folks want.","title":"Is there a SwiftUI equivalent for preferredStatusBarStyle in iOS 15?"},{"location":"wwdc21/a11y.html","text":"Accessibility Lounge QAs by paul \ud83d\uddd3 Tuesday As a relatively new iOS developer, what should I keep in mind for developing more accessible apps? This is a great question! For starters, I\u2019d familiarize myself with Voice Over. Spend some time with it and get an idea of how to use it. Once you do, you\u2019ll start to get a feel for what would work well with it. If you\u2019re into SwiftUI, you\u2019ll find that the accessibility modifiers are simple tools that can do a ton of work. Likewise, UIKit offers the same functionality with UIAccessibility. I encourage you to dig into some of the documentation to see what you can find. My recommendation for new iOS devs is to stick as much as possible to native components as they're accessible out of the box. You may start running into problems when developing custom components and I would do that only if necessary, and as Adrian says, if you are familiar with UIAccessibility and the SwiftUI accessibility modifiers available. What are some of the best practices of testing for accessibility? That depends on what part of accessibility you want to test, I guess. Personally I (and my team) test Voice Over and swipe navigation on a physical device with VO turned on, the screen turned black (triple tap with three fingers) and head phones on (so my peers won\u2019t beat me up) Dynamic font sizes and responsive layout I test inside the simulator changing accessibility settings in Xcode. But there is so much more to accessibility than this. Having a clean, straightforward design and using default views, controls and so on will do the heavy lifting for you in most situations. A crucial part is to find people that rely on accessibility features and get them to test your app for you. Creating a test group that is diverse is key here. It can be hard to get those especially for independent developers, I guess. So the least you can do is to give them the ability to send you feedback on your app\u2019s accessibility and make this way of giving feedback as accessible as possible. Hi, I have some questions about voiceover: Voiceover read all of the contents in table view cell. I found the only way to turn it off is to set the accessibility label to empty string for the cell. Is there any other way? How to prevent voiceover keep reading the contents in fast updating view? Are you trying to exclude only specific contents from your table cell? One option might be to make the cell itself an accessibility element so that it doesn\u2019t expose its children, and then override accessibilityLabel on the cell. Then any controls within the cell you can expose via accessibilityCustomActions . https://developer.apple.com/documentation/objectivec/nsobject/1615150-accessibilitycustomactions You can probably use the updatesFrequently trait for this: https://developer.apple.com/documentation/uikit/uiaccessibility/uiaccessibilitytraits/1620187-updatesfrequently We also have a WWDC talk on \u201cTailor the VoiceOver experience in your data-rich apps\u201d this Friday! https://developer.apple.com/videos/play/wwdc2021/10121/ Hey guys! This is my first WWDC! I am a developer with a disability called Cerebral Palsy. I use a joystick and the on screen keyboard on my iMac to code! I also have a iPad Pro that I work with my nose. Who should I connect with at Apple about making the iPad experience even better for people like me. :heart: Adrian (Apple) 4 days ago Hey Josh! While people often use the Feedback Assistant for bugs, it\u2019s also a great tool for suggesting features and improvements. If you\u2019ve got something in mind, we\u2019d love to hear about it. Simply file feedback :slightly_smiling_face: Hello! Some slightly in the weeds questions, but things I've never quite figured out :) Why do all NSObjects implement accessibility features such as accessibilityLabel etc, rather than just starting with UIView ? Why is accessibilityIdentifier part of UIAccessibility ? I understand its purpose is for identifying the view for UI Testing, but I'm unclear on the accessibility connection. Is this used on Mac for tools like automator or for 3rd party assistive tech? Is there any use for this on iOS other than UI testing? Why is there no textField accessibility trait? I suspect the answer to this is obvious :) axLabel and friends can be used on many kinds of objects. UIView , CALayers , NSStrings (in the case of UISegments , SpriteKit nodes and many more) Almost all automation relies on the accessibility runtime system to provide its information. When no axIdentifier is present, it falls back to axLabel. So that is the connection at least. Whats the best practice for using UIAccessibility.post(notification: .announcement, argument: \"Announcement\") for informing VoiceOver users of status updates. eg form validation errors, toasts etc? Is there a way we can guarantee the utterance will be announced in full without VO interrupting it when changing focus? Someone \ud83d\ude00 wrote a library that kinda does what I want here, but I wondered if there was something that was preferred? I don't find UIAccessibilitySpeechAttributeQueueAnnouncement to be that reliable There is also this: https://developer.apple.com/documentation/uikit/uiaccessibilityspeechattributequeueannouncement/ So I believe that is all there currently is, i.e. listening for UIAccessibilityAnnouncementDidFinishNotification or trying UIAccessibilitySpeechAttributeQueueAnnouncement . Is there a way to make status update announcements with VoiceOver in SwiftUI, similar to UIAccessibility.post(notification: .announcement, argument: \"Announcement\") in UIKit? Hi there :wave: I\u2019m an indie dev (not from Apple). But I recently added an extension to my Views in SwiftUI that receives a message and post it with UIAccessibility.post(notification: .announcement, argument: message) if UIAccessibility.isVoiceOverRunning , and it works like a charm. I hope this helps! Aside from avoiding the work, doing the announcement notification is essentially a no-op is VoiceOver isn't running When using accessibilityFrame I would expect the frame to be discoverable by touch if the frame is larger than the view, but it appears that the view is only discoverable by touch when the visual frame of the view is explored, is this expected? If so is there a way to get a view to absorb explore by touch from an area larger than its bounds? You would probably need to override the axFrame for the upper views. Otherwise when the touch event goes down the view chain, it will get clipped out by a parent that is \"too small\". I'm currently working on some new interactions for my iOS app, and some of the solutions I'm exploring rely in swipe interactions. From an accessibility point of view, I wondered if that's ok or it's better to explore other alternatives. And if I need that kind of interaction, what's the best way to handle them with VoiceOver in SwiftUI. Thanks! Voice Over includes a ton of support for swiping, but if you want to make your own actions, I\u2019d suggest taking a look at https://developer.apple.com/documentation/uikit/uiaccessibilitycustomaction Swipe actions are great candidates to get folded into Accessibility Custom Actions \u2014 custom actions for VoiceOver, Switch Control and more! Here\u2019s a great talk on them: https://developer.apple.com/videos/play/wwdc2019/250/ In our app running iOS 14.x, we used UISegmentedControl . I've noticed that when scaling for bigger dynamic font the UISegmentedControl stays the same size and doesn't reflect the accessibility font trait. The same happens in iOS 14.x app \"Phone\" - \"Last calls\" tab. Because of that I had to build own UIControl that would mimic the UISegmentedControl . Here's the question, since the minimal tappable size according to Human Interface Guidelines should be 44x44 pt and the mentioned UISegmentControl doesn't scale more than 32pt on height, isn't this an accessibility issue? Is there a solution that I maybe missing? :) One of the ideas was to keep the mimic size if our custom like UISegmentControl UIControl and the provide some hacking with larger tappable areas, but I don't like it. If you long-press the UISegmentedControl, a larger version of the segments will appear under the segments that also indicates which segment is selected and you can use that to move between segments. You can see more info at https://developer.apple.com/videos/play/wwdc2019/261/ and https://developer.apple.com/videos/play/wwdc2017/245/ What animation options can we have if user has enabled reduced motion? Fade in and fade out work well for this. This video has a great section on reduce motion: https://developer.apple.com/videos/play/wwdc2018/230/ In yesterday's keynote during the segment on new APIs, I noticed an API titled \"Virtual Game Controller\". Is this related to the new feature that allows game developers to put up onscreen controls in their games if no physical controller is paired? Please check out this awesome session by the Game Technologies team if you haven't already. The resources here will be super helpful, too! https://developer.apple.com/videos/play/wwdc2021/10081/ \ud83d\uddd3 Wednesday When coming to Haptics does iOS provide a way to check if users want haptics enabled? Thanks! Great question! When your app uses the Core Haptics Framework, iOS will handle the delivery of haptics based on the user\u2019s setting for System Haptics. You don\u2019t need to worry about accidentally breaking this setting. For devices that don\u2019t support Core Haptics, like iPads and iPods, you can check the device\u2019s capability and use that to make decisions about what kind of feedback to give to the user. var supportsHaptics: Bool = false ... // Check if the device supports haptics. let hapticCapability = CHHapticEngine.capabilitiesForHardware() supportsHaptics = hapticCapability.supportsHaptics Are there any any accessibility considerations to using haptics aside from the general usability guidelines set out in the HIG? eg, are there circumstances where we shouldn't be using them, should be using lighter haptics etc? Or perhaps are there places where you would recommend haptics are essential as part of an accessible app? VoiceOver actually implements an entire set of its own haptics that work system-wide so it may be the case that an app\u2019s haptics could conflict with VoiceOver and cause confusion for a user. One idea might be to offer the ability to turn your app haptics off if VoiceOver users find the combined experience to be overwhelming The new material backgrounds look stunning, but I'm concerned about the accessibility of the thinner options. Does the system do anything to remedy this when 'reduce transparency' or 'increase contrast' is enabled, or should we be proceeding with caution? In general this is a case where reduce transparency should replace the material with a solid color. Using system components ensures that these settings will get applied at the right time. What behavioural changes happen in VoiceOver or other assistive tech with the .tabBar accessibility trait? This lets VoiceOver notify users they are currently navigating a tab bar. for example it might say \u201cMovies, tab 1 of 5\u201d What behavioural changes happen in VoiceOver or other assistive tech with the .keyboardKey accessibility trait? This may control what input/typing rotors are presented by VoiceOver. Also, it may affect what sound effects VoiceOver plays when the key is activated. Normally VoiceOver plays its activate sound when an element is activated but keys often play their own sound the the activate sound is not necessary. What are the behavioural differences between the .playsSound and .startsMediaSession traits? Should they be used together? Or perhaps never together? PlaysSound will ensure VoiceOver doesn't play its own sounds when an item is \u201cactivated.\u201d Example is a keyboard key that plays its own click sound when pressed StartsMediaSession will make VoiceOver not output any speech or sound after an item is activated. Good for a play button in an audio app where you don't want VO to interrupt the content What approaches do you think that could be used to improve the accessibility for apps with highly customised UI? And I mean specifically apps that have highly variable and even parametric UI, such as TouchOSC (not the developer). I understand that in general the most efficient approach is to extract meaning from the views hierarchy and find an alternative way to translate it for the user to understand. For example rendering it using speech in VoiceOver instead of using pixels in the screen. But these kind of apps essentially are interactive dashboards with controls that may not hold a hierarchical relationship. And I'm wondering if it would make sense to think of the whole app UI as a canvas. And try to provide haptic feedback based on the borders and shapes of the controls, so that the users that are not able to see the screen can still explore the canvas and learn the position of the controls using their fingertips. Do you know of any apps that use a similar approach? How else do you think we could make these apps accessible? One thing to consider is that directly touching-to-explore is not the only way to use VoiceOver. Some users perform the discrete gestures like swipe to navigate by element. Other users pair a Braille display and do not touch the screen at all. Seems like with accessibilityRepresentation we have to rebuild our UI again with views that can handle default accessibility. Is this correct? Ex: Custom List, I have to rebuild with a regular list? We have made LazyVStack accessible, and you should not use accessibilityRepresentation { List ... } . Its ideal for custom controls, like your custom button if you do not use Button Is there a list available of UI elements that are best served with accessibilityRepresentation because they don\u2019t translate well accessibility-wise? The ideal use case is controls made with shapes and other views that are not accessible by default. Gotcha, so as I understand it, it's mainly for shapes and custom views? Essentially an easy way to add accessibility for when it doesn't come \u201cfree\u201d? Yes. You wouldn't need to use it on something like Button, because we make it accessible by default. But for custom controls, one would often need the \"right\" value, label, traits and actions. So accessibilityRepresentation can give users the same experience as standard controls, regardless of how you choose to design the control Or in cases like the SymbolButtonStyle I demoed. Checkout the sample project for more interesting use cases Will accessiblityRepresentation override any accessibility I had on the view it's being placed on? Or are those accessibiltyLabel s getting merged somehow? It will override. It will replace the accessibility of all views at the point it is called. accessibilityChildren { ... }.accessibilityElement(children: .combine) can be used to add a child and then merge its properties Some of my more complex windows cannot have a preview. Are these values accessible if I dump the graph to Xcode in debug sessions? Or what would be a rule of thumb to actually make sure everything is properly accessible by looking at code only? If I look at a piece of code, without having any other means to know whether this is accessibility enabled or not, what few modifiers would you add up to any given element to make sure the application is properly accessible, at least for the minimum (other than testing in Voiceover for example) There is no substitute to testing in VoiceOver. That's what we do, for example, in our Accessibility Design labs with third party developers. The AX Inspector is a powerful tool for rapidly iterating, but for determining \"is my app really accessible\", the best way to do that is to use VoiceOver, especially with a screen curtain on (so you can't see what's on screen). If you can use your app that way, you have done a great job. My biggest hurdle right now is striking the balance between too much and two little Voice Overs and descriptive elements. I feel like the rotor will be a game changer for discoverability. Have you considered accessibilityCustomContent? its ideal for when you want to add many descriptions to the same element, but do not want each spoken together I want to get the VO on what you select, but once you selected it, I need to add up \u201cselected the upper right text zone\u201d and probably \u201ccontaining your text \\whattheuserwrote)\u201d If you are trying to model a selected state, dont include selected in the label. Instead, use accessibilityAddTraits(.isSelected) . For example: accessibilityAddTraits(isSelected ? .isSelected : []) In this year of automatic UX testing, have automation started being shown up in accessibility? I feel like multiple representations, children and alternate ways be a nightmare of maintainability under the premises of great features. We have made quite a few changes to improve the information returned for SwiftUI accessibility elements for automation. Is that what you are asking? Automated testing in particular. You should see much more show up in automation now, including elements that were combined to make a new element and hidden elements. Elements should also have the right automation type. Hello! What are best practices around an accessibility label being different to the content on screen. For example, an equation such as 'xyz / x', would 'x times y times z, divide by x' be a good accessibility label? Or would 'xyz divide by x' better? My concern is the accessibility label being too different to the original string, especially when the equations get a little more complicated and I would need to add many commas to an accessibility label to make the operations clear. It\u2019s fine to have Accessibility labels that differ a bit from what is on-screen. The best way to determine what is a good AX label is by trying the app with something like VoiceOver, and getting a sense of what it is like to use it that way. For example, a label might be too long, which might make it difficult to navigate the app. Or it might be not specific enough or clear, where it doesn\u2019t convey enough information to the user. (edited) We have a WWDC Session from an earlier year that talks about accessibility labels specifically and how to write them. https://developer.apple.com/videos/play/wwdc2019/254/ It is not a question! But I want to say thank you to the whole SwiftUI Accessibility team for the great work you have done. These new APIs so easy to use to achieve great UX. Thank you! We are hoping that as some of these things like Rotors, synthetic elements, and AX Focus are brought into SwiftUI, and are easier to achieve, that will lead to more apps with exceptional and high quality VoiceOver experiences! I forgot to mention Accessibility Preview! It is so easy to monitor AX during development without need to run accessibility inspector. What are the best practices around accessibility and displaying progress or loading states to users? For example if there is a HUD indicate a loading state, should developer implement special handling for voiceover users by announcing the loading state is still occurring? The best practice here would be to make sure your indicator has an appropriate accessibility label, like \"Loading\" or \"Downloading\". If you know the progress of the loading state, you could add an accessibility value like \"45%\" and include the .updatesFrequently trait. Is this a blocking loading state? Can the user interact with your app while this is happening meaningfully? Originally, I was thinking of a blocking loading state. But I guess it's applicable to also non-blocking state. So if a state is non-blocking, and the loading indicator is not interactive, it might make sense just to use an announcement. I believe that is what pull to refresh does. You let the user know they are refreshing, but you don\u2019t need to interrupt their VoiceOver focus, etc. However, if the loading state is blocking, and certainly if it might be interactive such as allow a cancel action, it would probably make sense to have the VoiceOver user land on the indicator. Terence, you can use accessibilityRepresentation { ProgressView(...) } as a starting point to make a custom progress view accessible. If it works in your app, you can also make a custom ProgressViewStyle and use that for your ProgressView. Then you dont need accessibilityRepresentation since you are using ProgressView directly. Is there a string for accessibilityValue or accessibilityLabel that is interrupted as Silence? Context: I use accessibilityIncrement() and accessibilityDecrement() to control media. But these function always end up calling accessibilityValue, interrupting media playback. I can't use \" \" (space), or punctuation. I used \" \" 2 spaces which is voiced as drum banging. Add the startsMediaSession trait to the element with accessibilityAddTraits so that VoiceOver knows not to interrupt the audio. Tried that. It doesn\u2019t work: it appears that startsMediaSession does not apply to accessibilityIncrement() and accessibilityDecrement(). startsMediaSession only applies when you activate an object (not necessarily for incrementing) if you just return nil for axValue is that not what you want? If I return nil, it\u2019ll read accessibilityLabel and try to speak that. Is there a string that is interpreted as silence? when you adjust (swipe up or down) it reads the label again? \" \" should be intrepreted as silence Is there a UIKit equivalent to accessibilityRepresentation or is it SwiftUI only? Either way this is an awesome idea thank you for building it! Sorry there isn't an equivalent in UIKit. We welcome any feedback if thats something you would be interested in! Well you can achieve this similarly by overriding accessibilityElements What are you trying to do with it? There is usually a way to achieve the same with UIKit, but those ways might differ depending on what you would be using accessibilityRepresentation for. Didnt have a specific case in mind but we pretty notoriously re-write native UI components for various designs. In the event we can subclass it all works great, but if theyre really custom it might be nice to get the native UI accessibility behavior \"for free\" Ah I see. Yes in that case pretty much anything should be achieve-able with UIKit AX API, but it might require a couple more lines of code and might be a little differently abstracted, i.e. you might want to add an action + a value + a label, for example. But yes, simplifying that is a part of why accessibilityRepresentation was created. If I have a custom, complex view that doesn't look like a button but I want to add functionality when tapped, I may just add a tap gesture recognizer to it. Is there an accessible alternative so a VoiceOver user can know the view is tappable? Usually VoiceOver users expect that buttons are tappable. So add the isButton trait. Is this complex view a singular control? Yep it is! So I could just add the isButton trait to the base view then? Yes. You may want to consider creating a custom ButtonStyle. This has a few benefits, 1) you get all the AX stuff we do for Button for free, and 2) all the other SwiftUI defaults for Button work, like keyboard shortcuts Is there a native-SwiftUI way to add an attributed label or hint? I asked a question on SO here: https://stackoverflow.com/q/66863764/14351818, but ended up using UIViewRepresentable. SwiftUI Text can contain attributes. Although there is no way to add those specific attributes, yet. There are also new swiftUI modifiers for accessibility related text stuff, like zspeechAdjustedPitch`. SwiftUI now supports attributed values and labels based on how the Text is marked up automatically, like font, color, etc Should we expect to change our limiting .sizeCategory based on the watch screen size (38mm vs 44mm)? It seems like on the 38mm it would need a much lower limit before for going vertical. That's definitely one approach! If possible stick with a smaller number of cases here though, and if your UI allows, do try to wrap the content in a way that allows the content size to grow without needing to adjust the layout at all Just a quick question. I have a free watchOS complication called Roughly; it's on the App Store now. It tells the approximate time in words, in many languages, but I'm not a good programmer! It would be great to see this rolled into watchOS properly by Apple; some people who suffer from dyscalculia use this to tell time without reading numbers. Supporting many more languages is a tricky problem too, as different countries tell time in very different ways. Also, I've got a minor issue with VoiceOver. Although the one-line complications are read out correctly when tapped, the three-line complication is not \u2014 only the first line (e.g. \"It's about\" instead of \"It's about ten to three\") is read out. Any ideas why that might be? Thanks in advance, and please keep up the great work on Accessibility. Thanks for the idea! This sounds like a bug. Do you have a code snippet of the complication type you're using so we can confirm we can check the same thing? The one that has issues is .modularLarge swift case .modularLarge: > let template = CLKComplicationTemplateModularLargeStandardBody() I\u2019m just using the standard template and populating all three lines: swift template.headerTextProvider = CLKSimpleTextProvider(text: theItsAbout, shortText: theItsAbout) > template.body1TextProvider = CLKSimpleTextProvider(text: thisFirstLine, shortText: thisFirstLine) > template.body2TextProvider = CLKSimpleTextProvider(text: thisSecondLine, shortText: thisSecondLine) thanks! And just to be clear, only the headerTextProvider is being read out. k, writing up a radar now I don't remember the name of it but there is a new accessibilityModifier in SwiftUI that allows you to add a hierarchy of SwiftUI views and have VoiceOver look at those instead of the non accessibility Views. Are there any issues of syncing the non-accessibility views with the accessibility views during dynamic changes? I'm just curious about best practices. I think you are referring to accessibilityElement(children: ..) Im not sure I understand your question though. Are you referring to how it works with dynamic number of elements? Yes that\u2019s the one I\u2019m referring to. if I understand correctly accessibilityElement(children: ..) is replacing the view hierarchy in VoiceOver with whatever you input as the children, correct? If so, could there be issues if the view hierarchy inside of accessibilityElement(children: ..) doesn\u2019t match the hierarchy of SwiftUI views for your sighted users? zaccessibilityElement(children: )z creates or updates an accessibility elements visibility. Such as, accessibilityElement(children: .contain) would wrap children in an new accessibility container element. Are you perhaps mixing up accessibilityChildren , which allows you to pass in views as children? No I guess I was misunderstanding the purpose of .accessibilityElement(children: ..) I guess the purpose of it is to add accessibilityChildren if the view doesn\u2019t already have children (like in the example of that custom Canvas View)? Yes accessibilityChildren is used to add \"synthetic\" children, which are not directly visible (though could be positioned to match whats drawn in Canvas). accessibilityElement(children: ) transforms the exiting accessibility tree, to make a new container or combine child properties into a new element Is it possible to use accessibilityChildren in the use case of replacing the children in the accessibility tree (if the children in the tree are problematic)? Yes it can be used anywhere. I go more into details in the Beyond the Basics talk: https://developer.apple.com/videos/play/wwdc2021/10119/ Attached to that talk is a sample project where we have some examples of accessibilityChildren How do you handle potential \u201cover verbosity\u201d when using VoiceOver. Sometimes if a table cell for instance has a great deal of content I can sometimes be unsure if I should err on the side of reading most of it (and the user potentially have a bunch of stuff read at them), or read less so they can quickly scan. Do you have any advice on how you manage this? Hey Christian! We have a session coming up this Friday called \u201cTailor the VoiceOver experience in your data-rich apps\u201d, and it will cover precisely this, so stay tuned! I\u2019d ask the presenter any questions you have afterward during that watch party. \ud83d\uddd3 Thursday When designing Accessibility Label/Value/Hint content, what considerations do we need to think about for refreshable braille displays? VoiceOver already abbreviates common UI words, like button to shorter forms. There's not too much API level control over what gets sent to the Braille display at this time I think in general, the same advice applies for spoken axLabels - which is try to be concise, but accurate. Don't be overly verbose, and front-load the important information I learned that apple created a people locator that helped blind people during the pandemic! https://support.apple.com/guide/iphone/people-detection-iph41bdfe6c7/ios Do you know of any good guides for learning how to use VoiceOver on macOS? Try this! It should get you started. https://support.apple.com/guide/iphone/turn-on-and-practice-voiceover-iph3e2e415f/ios When should we be using the link trait rather than the button trait? On web the general rule is that a link goes somewhere and a button does something, but this doesn't always match up in a native app. Should a text button be presented as a link? Should links only be links that are external to your app - say opening web pages in safari or making phone calls? Is it ok to have a button that opens a web link, or should this always be a link? If a control takes you out to a webpage usually we use link. If the control operates some function within the app we call it a button. I think we have usually avoided marking something as a button AND a link What's the recommendation when working with a long list of items in a table view and voice control. should we put the same label in the accessibilityUserInputLabels so the user will say it's name then automatically after the OS will shows the indices or is it better to expose instead the specific cell title for example ? thanks It would depend on your label and what\u2019s appropriate for it. Generally you should put the name the cell or the relevant content. But if the cell has a super long label, you might want to put something shorter in userInputLabels . Like if it\u2019s a travel app and the cell label is like \u201cNew York, a bustling city with tons of nightlife and famous attractions\u201d a good userInputLabel might be just \u201cNew York\u201d \u2014 this makes it easier to speak. Today\u2019s Voice Control Challenge will actually have some good tips on this topic. But if you have a screenshot I can take a more detailed look. Our app has over 20 different Font text styles. If we want to support dynamic type through UIFontMetrics, is there any way we can extend the 11 we get with UIFont.TextStyle ? Gold to hear your interest in fonts! We have a talk from WWDC17 that I think might help. Take a look: https://developer.apple.com/wwdc17/245 Check out Building Apps with Dynamic Type UIFontMetrics can also give you a scalar to scale a custom font appropriately based on the user\u2019s text size. How can I incorporate accessibility inspector (command line interface) into my CI pipeline to provide me insight into my current accessibility audit level? We are aware of this limitation, and unfortunately we do not have this functionality at the present time. Thank you for your feedback! I'm working on a UI for a tooltip on iOS 13+. It has some text and a close button. It's not modal, so you can still read content without dismissing it. Do you have any tips or examples for the best accessibility (specifically VoiceOver) experience for a component like this? I think the main question to answer is where in the list of elements should this be placed. Either at the beginning of everything, end of everything or inter-mixed. If this is for a specific component in the UI, i would try to put it in the UI element list after the component it references. This could be done by overriding accessibilityElements for a containerView that contains both of those views. What is Apple's recommended methods to using or creating numeric telephone specific keyboard? If you use our standard keyboard, and set the type to phonePad, then this should already be accessible. https://developer.apple.com/documentation/uikit/uikeyboardtype/phonepad If you\u2019re rolling your own, I\u2019d recommend keeping it as similar to the the stock on for VO, etc. as possible. You\u2019ll also want to use (on phone number text) UITextContentType of .TelephoneNumber And for SwiftUI: https://developer.apple.com/documentation/swiftui/menu/keyboardtype(_:) Also some related talks just for good measure! https://developer.apple.com/wwdc20/10115 https://developer.apple.com/wwdc17/242 Not totally related to the talk - but I tried using VoiceControl on my app that has DatePickers. I must be missing something because I find moving the picker to be impossible. It says \"swipe up or down with 1 finger to adjust the value\" but that doesn't work You can say \u201cIncrement \u201d or \u201cDecrement \u201d to increment and decrement date pickers The \u201cswipe up or down with 1 finger to adjust the value\u201d comes from VoiceOver. Are you using VoiceOver at the same time? Yes - I have voiceover turned on and I selected the element of the picker Ok got it. So a UIDatePicker should be accessible by default via and up/down swipe when the cursor is on it. (This is b/c it uses the .adjustable trait). Are you doing anything particularly custom with this item? Don\u2019t think so - I was trying to find another stock example - but when I\u2019m in VoiceOver I don\u2019t know how to get the control center. And it seems like the Things app removes the \u201c+ Add Reminder\u201d control when VoiceOver is on VO gestures (for Control Center, etc) are explained here: https://support.apple.com/guide/iphone/operate-iphone-using-voiceover-gestures-iph3e2e2329/ios OK - we can\u2019t do VoiceOver in the simulator can we? VoiceOver is only supported on device. I never knew about Tab-Z! During the talk I hit it and Slack gave me a summary of how to arrow around. And now it's gone - is it a one time thing, or am I doing something wrong? Tab + Z while Full Keyboard Access is running should bring up the Custom Actions for the currently selected element. Tab + H should bring up the help menu which shows you all your FKA commands! Tab + Z will not show anything if the currently selected element has no custom actions associated with it. Something like a \u201csend\u201d button may only have 1 operation and thus would likely not have custom actions, for example. Thank you for the fantastic talk! I was wondering if there are any special considerations we need to make with regards to Full Keyboard Access when the user has mirrored their iPad display to an external display for a larger view of their screen? Full Keyboard Access should work exactly the same when the screen is mirrored to another device! If you\u2019re doing a presentation and want the cursor to be larger or brighter, there are color and thickness settings in the Full Keyboard Access settings area! Hi! Can I ask for any advice on how to make sure a ML model is accessible for any user? You might say they inherently are, especially if they're performing sound recognition. However, it would be great to know which accessibility features can make sure they are as straightforward as possible to use, for everyone. Thank you! This will be a great question for the Accessibility + ML event happening in at 4PM! Both these videos talk about this topic, in terms of inclusion, https://developer.apple.com/wwdc21/10275 https://developer.apple.com/wwdc21/10304 It's all about making sure your data is collected and annotated in a way that honors the differences in us all Is Full Keyboard Access a good way for developers to test our apps' experience with Switch Control? Or is there a better way to do that without specialized hardware? You can test SC w/o specialized hardware by adding a Screen switch in the Switch Control menu - this allows you to tap anywhere on the screen to activate the switch. You can also use (new in iOS 15) Sound Actions to test w/o specialized hardware - just my making sounds like \u201cshh\u201d or \u201cpop\u201d \ud83d\uddd3 Friday One of my favorite features on Apple TV is the High-Contrast Focus Style. This feature adds an additional white border around focused cells. I've never been able to find an API to implement this for a custom collection view cell. A similar question was asked in the forums a few years ago and has more details: https://developer.apple.com/forums/thread/99313 Good point Christopher. We dont currently have public API for this setting but we should add it. Is the VoiceOver Image Description available on macOS or only iOS/iPadOS? It is available on both. It is on macOS as well! Once enabled in VoiceOver Utility, you can get a description by focusing on an element and pressing VO+Shift+L Do you know of any good examples of apps that support drawing via VoiceOver/Voice Control or Switch Control? Annotating using Switch Control/Voice Over would be an awesome feature to provide. Switch Control itself provide freehand drawing tools in the Switch Control menu. These allow SC-users to draw beautiful art, etc using Switch Control. I\u2019m trying to find the name of an App that\u2019s particularly popular with some of our SC users. Procreate is the app - you can see a video it in use here: https://www.youtube.com/watch?v=N034rhJJe8g What would be ideal experience from auto capture perspective and Voice Over? We have SDK where one of the entry point allows user to change between Auto Capture and Manual Capture. If person who needs accessibility starts screen with auto capture enabled, our logic announces detecting object and captures it. Sometimes its too fast and doesn't allow breathing room to let user understand what is happening. Any Suggestion? That's a great question. One idea might be to give more feedback about the capture before it happens. For example you could use the UIAcccessibilityPostNotification() function to make announcements about the orientation of what's going to be captured, or other feedback that might be useful if someone could not see the screen. Additionally there could be a small timer announcement of a second or two where if the user holds the phone steady for that time, only then does the capture occur. I believe Vision framework has API called image registration that would be useful in determining if the camera frames appear constant (i.e. little movement) What considerations should we give to accessibility when using a UIPageControl ? You should include the label: \"Page\", value: \"1 of 5\", traits: adjustable, button then implement accessibilityIncrement/Decrement and accessibilityActivate (which moves forward one page at a time) I had a question related to languages. I have a Dutch soccer app (with a lot of Dutch names and terms). We know that we have some users that use Voice over and have their system language set to English. Other then forcing the accessibility language to dutch, is there a better way to let the system and user know the app is in Dutch? Translating the entire app to English wouldn't help much because of all the dutch names and their pronunciations. I think in this case I would recommend using accessibilityLanguage property or the UIAccessibility or UIAccessibilitySpeechAttributeLanguage in the attributed string to mark up certain words in the right language Is AXCustomContent used in any Apple apps? Photos and Calendar (in the detail view) Does anyone have any advice or inspiring examples of a great accessibility experience for color swatches? Specifically I'm experimenting with the differentiate without color feature when choosing the color of shoes, clothing, and jewelry in our app. There\u2019s a fantastic talk from last year\u2019s WWDC on combining colors, symbols, and text together to make a really solid experience. Might not be 100% what you\u2019re going for, but it\u2019s definitely worth a look! I\u2019ll attach the link Can SwiftUI views support AXCustomContent? yes! hang on :slightly_smiling_face: details at the end of the video! This is excellent. It is absolutely relevant to my app, it's just the dogs are people. :) This gets at a challenge I have as a sighted developer. I use VoiceOver myself for some functional tasks, such as reading out content I'd rather hear than read myself. But it is hard to get to know the deeper layers of VoiceOver. One key third party app for teaching even the main VoiceOver gestures and interactions has been discontinued. What resources do you recommend for people to learn good interaction patterns for VoiceOver beyond the surface level? Check this out.. https://support.apple.com/guide/iphone/learn-voiceover-gestures-iph3e2e2281/ios This has been really helpful to understand how to move content rich material to the \"More Content\" section of the Rotor. Are there other features of the Rotor that likewise require explicit developer implementation like this? Checkout Accessibility Custom Rotors session from WWDC 2020 https://developer.apple.com/videos/play/wwdc2020/10116/ (edited) What has been the most challenging Apple app to work on for accessibility? Personally, I would say, non UIKit based apps, like PDFs and Photos app!","title":"A11y"},{"location":"wwdc21/a11y.html#accessibility-lounge-qas","text":"","title":"Accessibility Lounge QAs"},{"location":"wwdc21/a11y.html#by-paul","text":"","title":"by paul"},{"location":"wwdc21/a11y.html#tuesday","text":"","title":"\ud83d\uddd3 Tuesday"},{"location":"wwdc21/a11y.html#as-a-relatively-new-ios-developer-what-should-i-keep-in-mind-for-developing-more-accessible-apps","text":"This is a great question! For starters, I\u2019d familiarize myself with Voice Over. Spend some time with it and get an idea of how to use it. Once you do, you\u2019ll start to get a feel for what would work well with it. If you\u2019re into SwiftUI, you\u2019ll find that the accessibility modifiers are simple tools that can do a ton of work. Likewise, UIKit offers the same functionality with UIAccessibility. I encourage you to dig into some of the documentation to see what you can find. My recommendation for new iOS devs is to stick as much as possible to native components as they're accessible out of the box. You may start running into problems when developing custom components and I would do that only if necessary, and as Adrian says, if you are familiar with UIAccessibility and the SwiftUI accessibility modifiers available.","title":"As a relatively new iOS developer, what should I keep in mind for developing more accessible apps?"},{"location":"wwdc21/a11y.html#what-are-some-of-the-best-practices-of-testing-for-accessibility","text":"That depends on what part of accessibility you want to test, I guess. Personally I (and my team) test Voice Over and swipe navigation on a physical device with VO turned on, the screen turned black (triple tap with three fingers) and head phones on (so my peers won\u2019t beat me up) Dynamic font sizes and responsive layout I test inside the simulator changing accessibility settings in Xcode. But there is so much more to accessibility than this. Having a clean, straightforward design and using default views, controls and so on will do the heavy lifting for you in most situations. A crucial part is to find people that rely on accessibility features and get them to test your app for you. Creating a test group that is diverse is key here. It can be hard to get those especially for independent developers, I guess. So the least you can do is to give them the ability to send you feedback on your app\u2019s accessibility and make this way of giving feedback as accessible as possible.","title":"What are some of the best practices of testing for accessibility?"},{"location":"wwdc21/a11y.html#hi-i-have-some-questions-about-voiceover","text":"Voiceover read all of the contents in table view cell. I found the only way to turn it off is to set the accessibility label to empty string for the cell. Is there any other way? How to prevent voiceover keep reading the contents in fast updating view? Are you trying to exclude only specific contents from your table cell? One option might be to make the cell itself an accessibility element so that it doesn\u2019t expose its children, and then override accessibilityLabel on the cell. Then any controls within the cell you can expose via accessibilityCustomActions . https://developer.apple.com/documentation/objectivec/nsobject/1615150-accessibilitycustomactions You can probably use the updatesFrequently trait for this: https://developer.apple.com/documentation/uikit/uiaccessibility/uiaccessibilitytraits/1620187-updatesfrequently We also have a WWDC talk on \u201cTailor the VoiceOver experience in your data-rich apps\u201d this Friday! https://developer.apple.com/videos/play/wwdc2021/10121/","title":"Hi, I have some questions about voiceover:"},{"location":"wwdc21/a11y.html#hey-guys-this-is-my-first-wwdc-i-am-a-developer-with-a-disability-called-cerebral-palsy-i-use-a-joystick-and-the-on-screen-keyboard-on-my-imac-to-code-i-also-have-a-ipad-pro-that-i-work-with-my-nose-who-should-i-connect-with-at-apple-about-making-the-ipad-experience-even-better-for-people-like-me","text":":heart: Adrian (Apple) 4 days ago Hey Josh! While people often use the Feedback Assistant for bugs, it\u2019s also a great tool for suggesting features and improvements. If you\u2019ve got something in mind, we\u2019d love to hear about it. Simply file feedback :slightly_smiling_face:","title":"Hey guys! This is my first WWDC! I am a developer with a disability called Cerebral Palsy. I use a joystick and the on screen keyboard on my iMac to code! I also have a iPad Pro that I work with my nose. Who should I connect with at Apple about making the iPad experience even better for people like me."},{"location":"wwdc21/a11y.html#hello-some-slightly-in-the-weeds-questions-but-things-ive-never-quite-figured-out","text":"Why do all NSObjects implement accessibility features such as accessibilityLabel etc, rather than just starting with UIView ? Why is accessibilityIdentifier part of UIAccessibility ? I understand its purpose is for identifying the view for UI Testing, but I'm unclear on the accessibility connection. Is this used on Mac for tools like automator or for 3rd party assistive tech? Is there any use for this on iOS other than UI testing? Why is there no textField accessibility trait? I suspect the answer to this is obvious :) axLabel and friends can be used on many kinds of objects. UIView , CALayers , NSStrings (in the case of UISegments , SpriteKit nodes and many more) Almost all automation relies on the accessibility runtime system to provide its information. When no axIdentifier is present, it falls back to axLabel. So that is the connection at least.","title":"Hello! Some slightly in the weeds questions, but things I've never quite figured out :)"},{"location":"wwdc21/a11y.html#whats-the-best-practice-for-using-uiaccessibilitypostnotification-announcement-argument-announcement-for-informing-voiceover-users-of-status-updates-eg-form-validation-errors-toasts-etc-is-there-a-way-we-can-guarantee-the-utterance-will-be-announced-in-full-without-vo-interrupting-it-when-changing-focus","text":"Someone \ud83d\ude00 wrote a library that kinda does what I want here, but I wondered if there was something that was preferred? I don't find UIAccessibilitySpeechAttributeQueueAnnouncement to be that reliable There is also this: https://developer.apple.com/documentation/uikit/uiaccessibilityspeechattributequeueannouncement/ So I believe that is all there currently is, i.e. listening for UIAccessibilityAnnouncementDidFinishNotification or trying UIAccessibilitySpeechAttributeQueueAnnouncement .","title":"Whats the best practice for using UIAccessibility.post(notification: .announcement, argument: \"Announcement\") for informing VoiceOver users of status updates. eg form validation errors, toasts etc? Is there a way we can guarantee the utterance will be announced in full without VO interrupting it when changing focus?"},{"location":"wwdc21/a11y.html#is-there-a-way-to-make-status-update-announcements-with-voiceover-in-swiftui-similar-to-uiaccessibilitypostnotification-announcement-argument-announcement-in-uikit","text":"Hi there :wave: I\u2019m an indie dev (not from Apple). But I recently added an extension to my Views in SwiftUI that receives a message and post it with UIAccessibility.post(notification: .announcement, argument: message) if UIAccessibility.isVoiceOverRunning , and it works like a charm. I hope this helps! Aside from avoiding the work, doing the announcement notification is essentially a no-op is VoiceOver isn't running","title":"Is there a way to make status update announcements with VoiceOver in SwiftUI, similar to UIAccessibility.post(notification: .announcement, argument: \"Announcement\") in UIKit?"},{"location":"wwdc21/a11y.html#when-using-accessibilityframe-i-would-expect-the-frame-to-be-discoverable-by-touch-if-the-frame-is-larger-than-the-view-but-it-appears-that-the-view-is-only-discoverable-by-touch-when-the-visual-frame-of-the-view-is-explored-is-this-expected-if-so-is-there-a-way-to-get-a-view-to-absorb-explore-by-touch-from-an-area-larger-than-its-bounds","text":"You would probably need to override the axFrame for the upper views. Otherwise when the touch event goes down the view chain, it will get clipped out by a parent that is \"too small\".","title":"When using accessibilityFrame I would expect the frame to be discoverable by touch if the frame is larger than the view, but it appears that the view is only discoverable by touch when the visual frame of the view is explored, is this expected? If so is there a way to get a view to absorb explore by touch from an area larger than its bounds?"},{"location":"wwdc21/a11y.html#im-currently-working-on-some-new-interactions-for-my-ios-app-and-some-of-the-solutions-im-exploring-rely-in-swipe-interactions-from-an-accessibility-point-of-view-i-wondered-if-thats-ok-or-its-better-to-explore-other-alternatives-and-if-i-need-that-kind-of-interaction-whats-the-best-way-to-handle-them-with-voiceover-in-swiftui-thanks","text":"Voice Over includes a ton of support for swiping, but if you want to make your own actions, I\u2019d suggest taking a look at https://developer.apple.com/documentation/uikit/uiaccessibilitycustomaction Swipe actions are great candidates to get folded into Accessibility Custom Actions \u2014 custom actions for VoiceOver, Switch Control and more! Here\u2019s a great talk on them: https://developer.apple.com/videos/play/wwdc2019/250/","title":"I'm currently working on some new interactions for my iOS app, and some of the solutions I'm exploring rely in swipe interactions. From an accessibility point of view, I wondered if that's ok or it's better to explore other alternatives. And if I need that kind of interaction, what's the best way to handle them with VoiceOver in SwiftUI. Thanks!"},{"location":"wwdc21/a11y.html#in-our-app-running-ios-14x-we-used-uisegmentedcontrol-ive-noticed-that-when-scaling-for-bigger-dynamic-font-the-uisegmentedcontrol-stays-the-same-size-and-doesnt-reflect-the-accessibility-font-trait-the-same-happens-in-ios-14x-app-phone-last-calls-tab-because-of-that-i-had-to-build-own-uicontrol-that-would-mimic-the-uisegmentedcontrol-heres-the-question-since-the-minimal-tappable-size-according-to-human-interface-guidelines-should-be-44x44-pt-and-the-mentioned-uisegmentcontrol-doesnt-scale-more-than-32pt-on-height-isnt-this-an-accessibility-issue-is-there-a-solution-that-i-maybe-missing","text":"One of the ideas was to keep the mimic size if our custom like UISegmentControl UIControl and the provide some hacking with larger tappable areas, but I don't like it. If you long-press the UISegmentedControl, a larger version of the segments will appear under the segments that also indicates which segment is selected and you can use that to move between segments. You can see more info at https://developer.apple.com/videos/play/wwdc2019/261/ and https://developer.apple.com/videos/play/wwdc2017/245/","title":"In our app running iOS 14.x, we used UISegmentedControl. I've noticed that when scaling for bigger dynamic font the UISegmentedControl stays the same size and doesn't reflect the accessibility font trait. The same happens in iOS 14.x app \"Phone\" - \"Last calls\" tab. Because of that I had to build own UIControl that would mimic the UISegmentedControl. Here's the question, since the minimal tappable size according to Human Interface Guidelines should be 44x44 pt and the mentioned UISegmentControl doesn't scale more than 32pt on height, isn't this an accessibility issue? Is there a solution that I maybe missing? :)"},{"location":"wwdc21/a11y.html#what-animation-options-can-we-have-if-user-has-enabled-reduced-motion","text":"Fade in and fade out work well for this. This video has a great section on reduce motion: https://developer.apple.com/videos/play/wwdc2018/230/","title":"What animation options can we have if user has enabled reduced motion?"},{"location":"wwdc21/a11y.html#in-yesterdays-keynote-during-the-segment-on-new-apis-i-noticed-an-api-titled-virtual-game-controller-is-this-related-to-the-new-feature-that-allows-game-developers-to-put-up-onscreen-controls-in-their-games-if-no-physical-controller-is-paired","text":"Please check out this awesome session by the Game Technologies team if you haven't already. The resources here will be super helpful, too! https://developer.apple.com/videos/play/wwdc2021/10081/","title":"In yesterday's keynote during the segment on new APIs, I noticed an API titled \"Virtual Game Controller\". Is this related to the new feature that allows game developers to put up onscreen controls in their games if no physical controller is paired?"},{"location":"wwdc21/a11y.html#wednesday","text":"","title":"\ud83d\uddd3 Wednesday"},{"location":"wwdc21/a11y.html#when-coming-to-haptics-does-ios-provide-a-way-to-check-if-users-want-haptics-enabled-thanks","text":"Great question! When your app uses the Core Haptics Framework, iOS will handle the delivery of haptics based on the user\u2019s setting for System Haptics. You don\u2019t need to worry about accidentally breaking this setting. For devices that don\u2019t support Core Haptics, like iPads and iPods, you can check the device\u2019s capability and use that to make decisions about what kind of feedback to give to the user. var supportsHaptics: Bool = false ... // Check if the device supports haptics. let hapticCapability = CHHapticEngine.capabilitiesForHardware() supportsHaptics = hapticCapability.supportsHaptics","title":"When coming to Haptics does iOS provide a way to check if users want haptics enabled? Thanks!"},{"location":"wwdc21/a11y.html#are-there-any-any-accessibility-considerations-to-using-haptics-aside-from-the-general-usability-guidelines-set-out-in-the-hig-eg-are-there-circumstances-where-we-shouldnt-be-using-them-should-be-using-lighter-haptics-etc-or-perhaps-are-there-places-where-you-would-recommend-haptics-are-essential-as-part-of-an-accessible-app","text":"VoiceOver actually implements an entire set of its own haptics that work system-wide so it may be the case that an app\u2019s haptics could conflict with VoiceOver and cause confusion for a user. One idea might be to offer the ability to turn your app haptics off if VoiceOver users find the combined experience to be overwhelming","title":"Are there any any accessibility considerations to using haptics aside  from the general usability guidelines set out in the HIG? eg, are there circumstances where we shouldn't be using them, should be using lighter haptics etc? Or perhaps are there places where you would recommend haptics are essential as part of an accessible app?"},{"location":"wwdc21/a11y.html#the-new-material-backgrounds-look-stunning-but-im-concerned-about-the-accessibility-of-the-thinner-options-does-the-system-do-anything-to-remedy-this-when-reduce-transparency-or-increase-contrast-is-enabled-or-should-we-be-proceeding-with-caution","text":"In general this is a case where reduce transparency should replace the material with a solid color. Using system components ensures that these settings will get applied at the right time.","title":"The new material backgrounds look stunning, but I'm concerned about the accessibility of the thinner options. Does the system do anything to remedy this when 'reduce transparency' or 'increase contrast' is enabled, or should we be proceeding with caution?"},{"location":"wwdc21/a11y.html#what-behavioural-changes-happen-in-voiceover-or-other-assistive-tech-with-the-tabbar-accessibility-trait","text":"This lets VoiceOver notify users they are currently navigating a tab bar. for example it might say \u201cMovies, tab 1 of 5\u201d","title":"What behavioural changes happen in VoiceOver or other assistive tech with the .tabBar accessibility trait?"},{"location":"wwdc21/a11y.html#what-behavioural-changes-happen-in-voiceover-or-other-assistive-tech-with-the-keyboardkey-accessibility-trait","text":"This may control what input/typing rotors are presented by VoiceOver. Also, it may affect what sound effects VoiceOver plays when the key is activated. Normally VoiceOver plays its activate sound when an element is activated but keys often play their own sound the the activate sound is not necessary.","title":"What behavioural changes happen in VoiceOver or other assistive tech with the .keyboardKey accessibility trait?"},{"location":"wwdc21/a11y.html#what-are-the-behavioural-differences-between-the-playssound-and-startsmediasession-traits-should-they-be-used-together-or-perhaps-never-together","text":"PlaysSound will ensure VoiceOver doesn't play its own sounds when an item is \u201cactivated.\u201d Example is a keyboard key that plays its own click sound when pressed StartsMediaSession will make VoiceOver not output any speech or sound after an item is activated. Good for a play button in an audio app where you don't want VO to interrupt the content","title":"What are the behavioural differences between the .playsSound and .startsMediaSession traits? Should they be used together? Or perhaps never together?"},{"location":"wwdc21/a11y.html#what-approaches-do-you-think-that-could-be-used-to-improve-the-accessibility-for-apps-with-highly-customised-ui-and-i-mean-specifically-apps-that-have-highly-variable-and-even-parametric-ui-such-as-touchosc-not-the-developer","text":"I understand that in general the most efficient approach is to extract meaning from the views hierarchy and find an alternative way to translate it for the user to understand. For example rendering it using speech in VoiceOver instead of using pixels in the screen. But these kind of apps essentially are interactive dashboards with controls that may not hold a hierarchical relationship. And I'm wondering if it would make sense to think of the whole app UI as a canvas. And try to provide haptic feedback based on the borders and shapes of the controls, so that the users that are not able to see the screen can still explore the canvas and learn the position of the controls using their fingertips. Do you know of any apps that use a similar approach? How else do you think we could make these apps accessible? One thing to consider is that directly touching-to-explore is not the only way to use VoiceOver. Some users perform the discrete gestures like swipe to navigate by element. Other users pair a Braille display and do not touch the screen at all.","title":"What approaches do you think that could be used to improve the accessibility for apps with highly customised UI? And I mean specifically apps that have highly variable and even parametric UI, such as TouchOSC (not the developer)."},{"location":"wwdc21/a11y.html#seems-like-with-accessibilityrepresentation-we-have-to-rebuild-our-ui-again-with-views-that-can-handle-default-accessibility-is-this-correct-ex-custom-list-i-have-to-rebuild-with-a-regular-list","text":"We have made LazyVStack accessible, and you should not use accessibilityRepresentation { List ... } . Its ideal for custom controls, like your custom button if you do not use Button","title":"Seems like with accessibilityRepresentation we have to rebuild our UI again with views that can handle default accessibility. Is this correct? Ex: Custom List, I have to rebuild with a regular list?"},{"location":"wwdc21/a11y.html#is-there-a-list-available-of-ui-elements-that-are-best-served-with-accessibilityrepresentation-because-they-dont-translate-well-accessibility-wise","text":"The ideal use case is controls made with shapes and other views that are not accessible by default. Gotcha, so as I understand it, it's mainly for shapes and custom views? Essentially an easy way to add accessibility for when it doesn't come \u201cfree\u201d? Yes. You wouldn't need to use it on something like Button, because we make it accessible by default. But for custom controls, one would often need the \"right\" value, label, traits and actions. So accessibilityRepresentation can give users the same experience as standard controls, regardless of how you choose to design the control Or in cases like the SymbolButtonStyle I demoed. Checkout the sample project for more interesting use cases","title":"Is there a list available of UI elements that are best served with accessibilityRepresentation because they don\u2019t translate well accessibility-wise?"},{"location":"wwdc21/a11y.html#will-accessiblityrepresentation-override-any-accessibility-i-had-on-the-view-its-being-placed-on-or-are-those-accessibiltylabels-getting-merged-somehow","text":"It will override. It will replace the accessibility of all views at the point it is called. accessibilityChildren { ... }.accessibilityElement(children: .combine) can be used to add a child and then merge its properties","title":"Will accessiblityRepresentation override any accessibility I had on the view it's being placed on? Or are those accessibiltyLabels getting merged somehow?"},{"location":"wwdc21/a11y.html#some-of-my-more-complex-windows-cannot-have-a-preview-are-these-values-accessible-if-i-dump-the-graph-to-xcode-in-debug-sessions-or-what-would-be-a-rule-of-thumb-to-actually-make-sure-everything-is-properly-accessible-by-looking-at-code-only","text":"If I look at a piece of code, without having any other means to know whether this is accessibility enabled or not, what few modifiers would you add up to any given element to make sure the application is properly accessible, at least for the minimum (other than testing in Voiceover for example) There is no substitute to testing in VoiceOver. That's what we do, for example, in our Accessibility Design labs with third party developers. The AX Inspector is a powerful tool for rapidly iterating, but for determining \"is my app really accessible\", the best way to do that is to use VoiceOver, especially with a screen curtain on (so you can't see what's on screen). If you can use your app that way, you have done a great job. My biggest hurdle right now is striking the balance between too much and two little Voice Overs and descriptive elements. I feel like the rotor will be a game changer for discoverability. Have you considered accessibilityCustomContent? its ideal for when you want to add many descriptions to the same element, but do not want each spoken together I want to get the VO on what you select, but once you selected it, I need to add up \u201cselected the upper right text zone\u201d and probably \u201ccontaining your text \\whattheuserwrote)\u201d If you are trying to model a selected state, dont include selected in the label. Instead, use accessibilityAddTraits(.isSelected) . For example: accessibilityAddTraits(isSelected ? .isSelected : [])","title":"Some of my more complex windows cannot have a preview. Are these values accessible if I dump the graph to Xcode in debug sessions? Or what would be a rule of thumb to actually make sure everything is properly accessible by looking at code only?"},{"location":"wwdc21/a11y.html#in-this-year-of-automatic-ux-testing-have-automation-started-being-shown-up-in-accessibility-i-feel-like-multiple-representations-children-and-alternate-ways-be-a-nightmare-of-maintainability-under-the-premises-of-great-features","text":"We have made quite a few changes to improve the information returned for SwiftUI accessibility elements for automation. Is that what you are asking? Automated testing in particular. You should see much more show up in automation now, including elements that were combined to make a new element and hidden elements. Elements should also have the right automation type.","title":"In this year of automatic UX testing, have automation started being shown up in accessibility? I feel like multiple representations, children and alternate ways be a nightmare of maintainability under the premises of great features."},{"location":"wwdc21/a11y.html#hello-what-are-best-practices-around-an-accessibility-label-being-different-to-the-content-on-screen-for-example-an-equation-such-as-xyz-x-would-x-times-y-times-z-divide-by-x-be-a-good-accessibility-label-or-would-xyz-divide-by-x-better-my-concern-is-the-accessibility-label-being-too-different-to-the-original-string-especially-when-the-equations-get-a-little-more-complicated-and-i-would-need-to-add-many-commas-to-an-accessibility-label-to-make-the-operations-clear","text":"It\u2019s fine to have Accessibility labels that differ a bit from what is on-screen. The best way to determine what is a good AX label is by trying the app with something like VoiceOver, and getting a sense of what it is like to use it that way. For example, a label might be too long, which might make it difficult to navigate the app. Or it might be not specific enough or clear, where it doesn\u2019t convey enough information to the user. (edited) We have a WWDC Session from an earlier year that talks about accessibility labels specifically and how to write them. https://developer.apple.com/videos/play/wwdc2019/254/","title":"Hello! What are best practices around an accessibility label being different to the content on screen. For example, an equation such as 'xyz / x', would 'x times y times z, divide by x' be a good accessibility label? Or would 'xyz divide by x' better? My concern is the accessibility label being too different to the original string, especially when the equations get a little more complicated and I would need to add many commas to an accessibility label to make the operations clear."},{"location":"wwdc21/a11y.html#it-is-not-a-question-but-i-want-to-say-thank-you-to-the-whole-swiftui-accessibility-team-for-the-great-work-you-have-done-these-new-apis-so-easy-to-use-to-achieve-great-ux","text":"Thank you! We are hoping that as some of these things like Rotors, synthetic elements, and AX Focus are brought into SwiftUI, and are easier to achieve, that will lead to more apps with exceptional and high quality VoiceOver experiences! I forgot to mention Accessibility Preview! It is so easy to monitor AX during development without need to run accessibility inspector.","title":"It is not a question! But I want to say thank you to the whole SwiftUI Accessibility team for the great work you have done. These new APIs so easy to use to achieve great UX."},{"location":"wwdc21/a11y.html#what-are-the-best-practices-around-accessibility-and-displaying-progress-or-loading-states-to-users","text":"For example if there is a HUD indicate a loading state, should developer implement special handling for voiceover users by announcing the loading state is still occurring? The best practice here would be to make sure your indicator has an appropriate accessibility label, like \"Loading\" or \"Downloading\". If you know the progress of the loading state, you could add an accessibility value like \"45%\" and include the .updatesFrequently trait. Is this a blocking loading state? Can the user interact with your app while this is happening meaningfully? Originally, I was thinking of a blocking loading state. But I guess it's applicable to also non-blocking state. So if a state is non-blocking, and the loading indicator is not interactive, it might make sense just to use an announcement. I believe that is what pull to refresh does. You let the user know they are refreshing, but you don\u2019t need to interrupt their VoiceOver focus, etc. However, if the loading state is blocking, and certainly if it might be interactive such as allow a cancel action, it would probably make sense to have the VoiceOver user land on the indicator. Terence, you can use accessibilityRepresentation { ProgressView(...) } as a starting point to make a custom progress view accessible. If it works in your app, you can also make a custom ProgressViewStyle and use that for your ProgressView. Then you dont need accessibilityRepresentation since you are using ProgressView directly.","title":"What are the best practices around accessibility and displaying progress or loading states to users?"},{"location":"wwdc21/a11y.html#is-there-a-string-for-accessibilityvalue-or-accessibilitylabel-that-is-interrupted-as-silence","text":"Context: I use accessibilityIncrement() and accessibilityDecrement() to control media. But these function always end up calling accessibilityValue, interrupting media playback. I can't use \" \" (space), or punctuation. I used \" \" 2 spaces which is voiced as drum banging. Add the startsMediaSession trait to the element with accessibilityAddTraits so that VoiceOver knows not to interrupt the audio. Tried that. It doesn\u2019t work: it appears that startsMediaSession does not apply to accessibilityIncrement() and accessibilityDecrement(). startsMediaSession only applies when you activate an object (not necessarily for incrementing) if you just return nil for axValue is that not what you want? If I return nil, it\u2019ll read accessibilityLabel and try to speak that. Is there a string that is interpreted as silence? when you adjust (swipe up or down) it reads the label again? \" \" should be intrepreted as silence","title":"Is there a string for accessibilityValue or accessibilityLabel that is interrupted as Silence?"},{"location":"wwdc21/a11y.html#is-there-a-uikit-equivalent-to-accessibilityrepresentation-or-is-it-swiftui-only-either-way-this-is-an-awesome-idea-thank-you-for-building-it","text":"Sorry there isn't an equivalent in UIKit. We welcome any feedback if thats something you would be interested in! Well you can achieve this similarly by overriding accessibilityElements What are you trying to do with it? There is usually a way to achieve the same with UIKit, but those ways might differ depending on what you would be using accessibilityRepresentation for. Didnt have a specific case in mind but we pretty notoriously re-write native UI components for various designs. In the event we can subclass it all works great, but if theyre really custom it might be nice to get the native UI accessibility behavior \"for free\" Ah I see. Yes in that case pretty much anything should be achieve-able with UIKit AX API, but it might require a couple more lines of code and might be a little differently abstracted, i.e. you might want to add an action + a value + a label, for example. But yes, simplifying that is a part of why accessibilityRepresentation was created.","title":"Is there a UIKit equivalent to accessibilityRepresentation or is it SwiftUI only? Either way this is an awesome idea thank you for building it!"},{"location":"wwdc21/a11y.html#if-i-have-a-custom-complex-view-that-doesnt-look-like-a-button-but-i-want-to-add-functionality-when-tapped-i-may-just-add-a-tap-gesture-recognizer-to-it-is-there-an-accessible-alternative-so-a-voiceover-user-can-know-the-view-is-tappable","text":"Usually VoiceOver users expect that buttons are tappable. So add the isButton trait. Is this complex view a singular control? Yep it is! So I could just add the isButton trait to the base view then? Yes. You may want to consider creating a custom ButtonStyle. This has a few benefits, 1) you get all the AX stuff we do for Button for free, and 2) all the other SwiftUI defaults for Button work, like keyboard shortcuts","title":"If I have a custom, complex view that doesn't look like a button but  I want to add functionality when tapped, I may just add a tap gesture recognizer to it. Is there an accessible alternative so a VoiceOver user can know the view is tappable?"},{"location":"wwdc21/a11y.html#is-there-a-native-swiftui-way-to-add-an-attributed-label-or-hint-i-asked-a-question-on-so-here-httpsstackoverflowcomq6686376414351818-but-ended-up-using-uiviewrepresentable","text":"SwiftUI Text can contain attributes. Although there is no way to add those specific attributes, yet. There are also new swiftUI modifiers for accessibility related text stuff, like zspeechAdjustedPitch`. SwiftUI now supports attributed values and labels based on how the Text is marked up automatically, like font, color, etc","title":"Is there a native-SwiftUI way to add an attributed label or hint? I asked a question on SO here: https://stackoverflow.com/q/66863764/14351818, but ended up using UIViewRepresentable."},{"location":"wwdc21/a11y.html#should-we-expect-to-change-our-limiting-sizecategory-based-on-the-watch-screen-size-38mm-vs-44mm-it-seems-like-on-the-38mm-it-would-need-a-much-lower-limit-before-for-going-vertical","text":"That's definitely one approach! If possible stick with a smaller number of cases here though, and if your UI allows, do try to wrap the content in a way that allows the content size to grow without needing to adjust the layout at all","title":"Should we expect to change our limiting .sizeCategory based on the watch screen size (38mm vs 44mm)? It seems like on the 38mm it would need a much lower limit before for going vertical."},{"location":"wwdc21/a11y.html#just-a-quick-question-i-have-a-free-watchos-complication-called-roughly-its-on-the-app-store-now-it-tells-the-approximate-time-in-words-in-many-languages-but-im-not-a-good-programmer-it-would-be-great-to-see-this-rolled-into-watchos-properly-by-apple-some-people-who-suffer-from-dyscalculia-use-this-to-tell-time-without-reading-numbers-supporting-many-more-languages-is-a-tricky-problem-too-as-different-countries-tell-time-in-very-different-ways-also-ive-got-a-minor-issue-with-voiceover-although-the-one-line-complications-are-read-out-correctly-when-tapped-the-three-line-complication-is-not-only-the-first-line-eg-its-about-instead-of-its-about-ten-to-three-is-read-out-any-ideas-why-that-might-be","text":"Thanks in advance, and please keep up the great work on Accessibility. Thanks for the idea! This sounds like a bug. Do you have a code snippet of the complication type you're using so we can confirm we can check the same thing? The one that has issues is .modularLarge swift case .modularLarge: > let template = CLKComplicationTemplateModularLargeStandardBody() I\u2019m just using the standard template and populating all three lines: swift template.headerTextProvider = CLKSimpleTextProvider(text: theItsAbout, shortText: theItsAbout) > template.body1TextProvider = CLKSimpleTextProvider(text: thisFirstLine, shortText: thisFirstLine) > template.body2TextProvider = CLKSimpleTextProvider(text: thisSecondLine, shortText: thisSecondLine) thanks! And just to be clear, only the headerTextProvider is being read out. k, writing up a radar now","title":"Just a quick question. I have a free watchOS complication called Roughly; it's on the App Store now. It tells the approximate time in words, in many languages, but I'm not a good programmer! It would be great to see this rolled into watchOS properly by Apple; some people who suffer from dyscalculia use this to tell time without reading numbers. Supporting many more languages is a tricky problem too, as different countries tell time in very different ways. Also, I've got a minor issue with VoiceOver. Although the one-line complications are read out correctly when tapped, the three-line complication is not \u2014 only the first line (e.g. \"It's about\" instead of \"It's about ten to three\") is read out. Any ideas why that might be?"},{"location":"wwdc21/a11y.html#i-dont-remember-the-name-of-it-but-there-is-a-new-accessibilitymodifier-in-swiftui-that-allows-you-to-add-a-hierarchy-of-swiftui-views-and-have-voiceover-look-at-those-instead-of-the-non-accessibility-views","text":"Are there any issues of syncing the non-accessibility views with the accessibility views during dynamic changes? I'm just curious about best practices. I think you are referring to accessibilityElement(children: ..) Im not sure I understand your question though. Are you referring to how it works with dynamic number of elements? Yes that\u2019s the one I\u2019m referring to. if I understand correctly accessibilityElement(children: ..) is replacing the view hierarchy in VoiceOver with whatever you input as the children, correct? If so, could there be issues if the view hierarchy inside of accessibilityElement(children: ..) doesn\u2019t match the hierarchy of SwiftUI views for your sighted users? zaccessibilityElement(children: )z creates or updates an accessibility elements visibility. Such as, accessibilityElement(children: .contain) would wrap children in an new accessibility container element. Are you perhaps mixing up accessibilityChildren , which allows you to pass in views as children? No I guess I was misunderstanding the purpose of .accessibilityElement(children: ..) I guess the purpose of it is to add accessibilityChildren if the view doesn\u2019t already have children (like in the example of that custom Canvas View)? Yes accessibilityChildren is used to add \"synthetic\" children, which are not directly visible (though could be positioned to match whats drawn in Canvas). accessibilityElement(children: ) transforms the exiting accessibility tree, to make a new container or combine child properties into a new element Is it possible to use accessibilityChildren in the use case of replacing the children in the accessibility tree (if the children in the tree are problematic)? Yes it can be used anywhere. I go more into details in the Beyond the Basics talk: https://developer.apple.com/videos/play/wwdc2021/10119/ Attached to that talk is a sample project where we have some examples of accessibilityChildren","title":"I don't remember the name of it but there is a new accessibilityModifier in SwiftUI that allows you to add a hierarchy of SwiftUI views and have VoiceOver look at those instead of the non accessibility Views."},{"location":"wwdc21/a11y.html#how-do-you-handle-potential-over-verbosity-when-using-voiceover-sometimes-if-a-table-cell-for-instance-has-a-great-deal-of-content-i-can-sometimes-be-unsure-if-i-should-err-on-the-side-of-reading-most-of-it-and-the-user-potentially-have-a-bunch-of-stuff-read-at-them-or-read-less-so-they-can-quickly-scan-do-you-have-any-advice-on-how-you-manage-this","text":"Hey Christian! We have a session coming up this Friday called \u201cTailor the VoiceOver experience in your data-rich apps\u201d, and it will cover precisely this, so stay tuned! I\u2019d ask the presenter any questions you have afterward during that watch party.","title":"How do you handle potential \u201cover verbosity\u201d when using VoiceOver. Sometimes if a table cell for instance has a great deal of content I can sometimes be unsure if I should err on the side of reading most of it (and the user potentially have a bunch of stuff read at them), or read less so they can quickly scan. Do you have any advice on how you manage this?"},{"location":"wwdc21/a11y.html#thursday","text":"","title":"\ud83d\uddd3 Thursday"},{"location":"wwdc21/a11y.html#when-designing-accessibility-labelvaluehint-content-what-considerations-do-we-need-to-think-about-for-refreshable-braille-displays","text":"VoiceOver already abbreviates common UI words, like button to shorter forms. There's not too much API level control over what gets sent to the Braille display at this time I think in general, the same advice applies for spoken axLabels - which is try to be concise, but accurate. Don't be overly verbose, and front-load the important information","title":"When designing Accessibility Label/Value/Hint content, what considerations do we need to think about for refreshable braille displays?"},{"location":"wwdc21/a11y.html#i-learned-that-apple-created-a-people-locator-that-helped-blind-people-during-the-pandemic","text":"https://support.apple.com/guide/iphone/people-detection-iph41bdfe6c7/ios","title":"I learned that apple created a people locator that helped blind people during the pandemic!"},{"location":"wwdc21/a11y.html#do-you-know-of-any-good-guides-for-learning-how-to-use-voiceover-on-macos","text":"Try this! It should get you started. https://support.apple.com/guide/iphone/turn-on-and-practice-voiceover-iph3e2e415f/ios","title":"Do you know of any good guides for learning how to use VoiceOver on macOS?"},{"location":"wwdc21/a11y.html#when-should-we-be-using-the-link-trait-rather-than-the-button-trait-on-web-the-general-rule-is-that-a-link-goes-somewhere-and-a-button-does-something-but-this-doesnt-always-match-up-in-a-native-app","text":"Should a text button be presented as a link? Should links only be links that are external to your app - say opening web pages in safari or making phone calls? Is it ok to have a button that opens a web link, or should this always be a link? If a control takes you out to a webpage usually we use link. If the control operates some function within the app we call it a button. I think we have usually avoided marking something as a button AND a link","title":"When should we be using the link trait rather than the button trait? On web the general rule is that a link goes somewhere and a button does something, but this doesn't always match up in a native app."},{"location":"wwdc21/a11y.html#whats-the-recommendation-when-working-with-a-long-list-of-items-in-a-table-view-and-voice-control","text":"should we put the same label in the accessibilityUserInputLabels so the user will say it's name then automatically after the OS will shows the indices or is it better to expose instead the specific cell title for example ? thanks It would depend on your label and what\u2019s appropriate for it. Generally you should put the name the cell or the relevant content. But if the cell has a super long label, you might want to put something shorter in userInputLabels . Like if it\u2019s a travel app and the cell label is like \u201cNew York, a bustling city with tons of nightlife and famous attractions\u201d a good userInputLabel might be just \u201cNew York\u201d \u2014 this makes it easier to speak. Today\u2019s Voice Control Challenge will actually have some good tips on this topic. But if you have a screenshot I can take a more detailed look.","title":"What's the recommendation when working with a long list of items in a table view and voice control."},{"location":"wwdc21/a11y.html#our-app-has-over-20-different-font-text-styles-if-we-want-to-support-dynamic-type-through-uifontmetrics-is-there-any-way-we-can-extend-the-11-we-get-with-uifonttextstyle","text":"Gold to hear your interest in fonts! We have a talk from WWDC17 that I think might help. Take a look: https://developer.apple.com/wwdc17/245 Check out Building Apps with Dynamic Type UIFontMetrics can also give you a scalar to scale a custom font appropriately based on the user\u2019s text size.","title":"Our app has over 20 different Font text styles. If we want to support dynamic type through UIFontMetrics, is there any way we can extend the 11 we get with UIFont.TextStyle?"},{"location":"wwdc21/a11y.html#how-can-i-incorporate-accessibility-inspector-command-line-interface-into-my-ci-pipeline-to-provide-me-insight-into-my-current-accessibility-audit-level","text":"We are aware of this limitation, and unfortunately we do not have this functionality at the present time. Thank you for your feedback!","title":"How can I incorporate accessibility inspector (command line interface) into my CI pipeline to provide me insight into my current accessibility audit level?"},{"location":"wwdc21/a11y.html#im-working-on-a-ui-for-a-tooltip-on-ios-13-it-has-some-text-and-a-close-button-its-not-modal-so-you-can-still-read-content-without-dismissing-it-do-you-have-any-tips-or-examples-for-the-best-accessibility-specifically-voiceover-experience-for-a-component-like-this","text":"I think the main question to answer is where in the list of elements should this be placed. Either at the beginning of everything, end of everything or inter-mixed. If this is for a specific component in the UI, i would try to put it in the UI element list after the component it references. This could be done by overriding accessibilityElements for a containerView that contains both of those views.","title":"I'm working on a UI for a tooltip on iOS 13+. It has some text and a close button. It's not modal, so you can still read content without dismissing it. Do you have any tips or examples for the best accessibility (specifically VoiceOver) experience for a component like this?"},{"location":"wwdc21/a11y.html#what-is-apples-recommended-methods-to-using-or-creating-numeric-telephone-specific-keyboard","text":"If you use our standard keyboard, and set the type to phonePad, then this should already be accessible. https://developer.apple.com/documentation/uikit/uikeyboardtype/phonepad If you\u2019re rolling your own, I\u2019d recommend keeping it as similar to the the stock on for VO, etc. as possible. You\u2019ll also want to use (on phone number text) UITextContentType of .TelephoneNumber And for SwiftUI: https://developer.apple.com/documentation/swiftui/menu/keyboardtype(_:) Also some related talks just for good measure! https://developer.apple.com/wwdc20/10115 https://developer.apple.com/wwdc17/242","title":"What is Apple's recommended methods to using or creating numeric telephone specific keyboard?"},{"location":"wwdc21/a11y.html#not-totally-related-to-the-talk-but-i-tried-using-voicecontrol-on-my-app-that-has-datepickers-i-must-be-missing-something-because-i-find-moving-the-picker-to-be-impossible-it-says-swipe-up-or-down-with-1-finger-to-adjust-the-value-but-that-doesnt-work","text":"You can say \u201cIncrement \u201d or \u201cDecrement \u201d to increment and decrement date pickers The \u201cswipe up or down with 1 finger to adjust the value\u201d comes from VoiceOver. Are you using VoiceOver at the same time? Yes - I have voiceover turned on and I selected the element of the picker Ok got it. So a UIDatePicker should be accessible by default via and up/down swipe when the cursor is on it. (This is b/c it uses the .adjustable trait). Are you doing anything particularly custom with this item? Don\u2019t think so - I was trying to find another stock example - but when I\u2019m in VoiceOver I don\u2019t know how to get the control center. And it seems like the Things app removes the \u201c+ Add Reminder\u201d control when VoiceOver is on VO gestures (for Control Center, etc) are explained here: https://support.apple.com/guide/iphone/operate-iphone-using-voiceover-gestures-iph3e2e2329/ios OK - we can\u2019t do VoiceOver in the simulator can we? VoiceOver is only supported on device.","title":"Not totally related to the talk - but I tried using VoiceControl on my app that has DatePickers. I must be missing something because I find moving the picker to be impossible. It says \"swipe up or down with 1 finger to adjust the value\" but that doesn't work"},{"location":"wwdc21/a11y.html#i-never-knew-about-tab-z-during-the-talk-i-hit-it-and-slack-gave-me-a-summary-of-how-to-arrow-around-and-now-its-gone-is-it-a-one-time-thing-or-am-i-doing-something-wrong","text":"Tab + Z while Full Keyboard Access is running should bring up the Custom Actions for the currently selected element. Tab + H should bring up the help menu which shows you all your FKA commands! Tab + Z will not show anything if the currently selected element has no custom actions associated with it. Something like a \u201csend\u201d button may only have 1 operation and thus would likely not have custom actions, for example.","title":"I never knew about Tab-Z! During the talk I hit it and Slack gave me a summary of how to arrow around. And now it's gone - is it a one time thing, or am I doing something wrong?"},{"location":"wwdc21/a11y.html#thank-you-for-the-fantastic-talk-i-was-wondering-if-there-are-any-special-considerations-we-need-to-make-with-regards-to-full-keyboard-access-when-the-user-has-mirrored-their-ipad-display-to-an-external-display-for-a-larger-view-of-their-screen","text":"Full Keyboard Access should work exactly the same when the screen is mirrored to another device! If you\u2019re doing a presentation and want the cursor to be larger or brighter, there are color and thickness settings in the Full Keyboard Access settings area!","title":"Thank you for the fantastic talk! I was wondering if there are any special considerations we need to make with regards to Full Keyboard Access when the user has mirrored their iPad display to an external display for a larger view of their screen?"},{"location":"wwdc21/a11y.html#hi-can-i-ask-for-any-advice-on-how-to-make-sure-a-ml-model-is-accessible-for-any-user-you-might-say-they-inherently-are-especially-if-theyre-performing-sound-recognition-however-it-would-be-great-to-know-which-accessibility-features-can-make-sure-they-are-as-straightforward-as-possible-to-use-for-everyone-thank-you","text":"This will be a great question for the Accessibility + ML event happening in at 4PM! Both these videos talk about this topic, in terms of inclusion, https://developer.apple.com/wwdc21/10275 https://developer.apple.com/wwdc21/10304 It's all about making sure your data is collected and annotated in a way that honors the differences in us all","title":"Hi! Can I ask for any advice on how to make sure a ML model is accessible for any user? You might say they inherently are, especially if they're performing sound recognition. However, it would be great to know which accessibility features can make sure they are as straightforward as possible to use, for everyone. Thank you!"},{"location":"wwdc21/a11y.html#is-full-keyboard-access-a-good-way-for-developers-to-test-our-apps-experience-with-switch-control-or-is-there-a-better-way-to-do-that-without-specialized-hardware","text":"You can test SC w/o specialized hardware by adding a Screen switch in the Switch Control menu - this allows you to tap anywhere on the screen to activate the switch. You can also use (new in iOS 15) Sound Actions to test w/o specialized hardware - just my making sounds like \u201cshh\u201d or \u201cpop\u201d","title":"Is Full Keyboard Access a good way for developers to test our apps' experience with Switch Control? Or is there a better way to do that without specialized hardware?"},{"location":"wwdc21/a11y.html#friday","text":"","title":"\ud83d\uddd3 Friday"},{"location":"wwdc21/a11y.html#one-of-my-favorite-features-on-apple-tv-is-the-high-contrast-focus-style-this-feature-adds-an-additional-white-border-around-focused-cells-ive-never-been-able-to-find-an-api-to-implement-this-for-a-custom-collection-view-cell-a-similar-question-was-asked-in-the-forums-a-few-years-ago-and-has-more-details-httpsdeveloperapplecomforumsthread99313","text":"Good point Christopher. We dont currently have public API for this setting but we should add it.","title":"One of my favorite features on Apple TV is the High-Contrast Focus Style. This feature adds an additional white border around focused cells. I've never been able to find an API to implement this for a custom collection view cell. A similar question was asked in the forums a few years ago and has more details: https://developer.apple.com/forums/thread/99313"},{"location":"wwdc21/a11y.html#is-the-voiceover-image-description-available-on-macos-or-only-iosipados","text":"It is available on both. It is on macOS as well! Once enabled in VoiceOver Utility, you can get a description by focusing on an element and pressing VO+Shift+L","title":"Is the VoiceOver Image Description available on macOS or only iOS/iPadOS?"},{"location":"wwdc21/a11y.html#do-you-know-of-any-good-examples-of-apps-that-support-drawing-via-voiceovervoice-control-or-switch-control-annotating-using-switch-controlvoice-over-would-be-an-awesome-feature-to-provide","text":"Switch Control itself provide freehand drawing tools in the Switch Control menu. These allow SC-users to draw beautiful art, etc using Switch Control. I\u2019m trying to find the name of an App that\u2019s particularly popular with some of our SC users. Procreate is the app - you can see a video it in use here: https://www.youtube.com/watch?v=N034rhJJe8g","title":"Do you know of any good examples of apps that support drawing via VoiceOver/Voice Control or Switch Control? Annotating using Switch Control/Voice Over would be an awesome feature to provide."},{"location":"wwdc21/a11y.html#what-would-be-ideal-experience-from-auto-capture-perspective-and-voice-over-we-have-sdk-where-one-of-the-entry-point-allows-user-to-change-between-auto-capture-and-manual-capture-if-person-who-needs-accessibility-starts-screen-with-auto-capture-enabled-our-logic-announces-detecting-object-and-captures-it-sometimes-its-too-fast-and-doesnt-allow-breathing-room-to-let-user-understand-what-is-happening-any-suggestion","text":"That's a great question. One idea might be to give more feedback about the capture before it happens. For example you could use the UIAcccessibilityPostNotification() function to make announcements about the orientation of what's going to be captured, or other feedback that might be useful if someone could not see the screen. Additionally there could be a small timer announcement of a second or two where if the user holds the phone steady for that time, only then does the capture occur. I believe Vision framework has API called image registration that would be useful in determining if the camera frames appear constant (i.e. little movement)","title":"What would be ideal experience from auto capture perspective and Voice Over? We have SDK where one of the entry point allows user to change between Auto Capture and Manual Capture.  If person who needs accessibility starts screen with auto capture enabled, our logic announces detecting object and captures it. Sometimes its too fast and doesn't allow breathing room to let user understand what is happening. Any Suggestion?"},{"location":"wwdc21/a11y.html#what-considerations-should-we-give-to-accessibility-when-using-a-uipagecontrol","text":"You should include the label: \"Page\", value: \"1 of 5\", traits: adjustable, button then implement accessibilityIncrement/Decrement and accessibilityActivate (which moves forward one page at a time)","title":"What considerations should we give to accessibility when using a UIPageControl?"},{"location":"wwdc21/a11y.html#i-had-a-question-related-to-languages-i-have-a-dutch-soccer-app-with-a-lot-of-dutch-names-and-terms-we-know-that-we-have-some-users-that-use-voice-over-and-have-their-system-language-set-to-english-other-then-forcing-the-accessibility-language-to-dutch-is-there-a-better-way-to-let-the-system-and-user-know-the-app-is-in-dutch-translating-the-entire-app-to-english-wouldnt-help-much-because-of-all-the-dutch-names-and-their-pronunciations","text":"I think in this case I would recommend using accessibilityLanguage property or the UIAccessibility or UIAccessibilitySpeechAttributeLanguage in the attributed string to mark up certain words in the right language","title":"I had a question related to languages. I have a Dutch soccer app (with a lot of Dutch names and terms). We know that we have some users that use Voice over and have their system language set to English. Other then forcing the accessibility language to dutch, is there a better way to let the system and user know the app is in Dutch? Translating the entire app to English wouldn't help much because of all the dutch names and their pronunciations."},{"location":"wwdc21/a11y.html#is-axcustomcontent-used-in-any-apple-apps","text":"Photos and Calendar (in the detail view)","title":"Is AXCustomContent used in any Apple apps?"},{"location":"wwdc21/a11y.html#does-anyone-have-any-advice-or-inspiring-examples-of-a-great-accessibility-experience-for-color-swatches-specifically-im-experimenting-with-the-differentiate-without-color-feature-when-choosing-the-color-of-shoes-clothing-and-jewelry-in-our-app","text":"There\u2019s a fantastic talk from last year\u2019s WWDC on combining colors, symbols, and text together to make a really solid experience. Might not be 100% what you\u2019re going for, but it\u2019s definitely worth a look! I\u2019ll attach the link","title":"Does anyone have any advice or inspiring examples of a great accessibility experience for color swatches? Specifically I'm experimenting with the differentiate without color feature when choosing the color of shoes, clothing, and jewelry in our app."},{"location":"wwdc21/a11y.html#can-swiftui-views-support-axcustomcontent","text":"yes! hang on :slightly_smiling_face: details at the end of the video!","title":"Can SwiftUI views support AXCustomContent?"},{"location":"wwdc21/a11y.html#this-is-excellent-it-is-absolutely-relevant-to-my-app-its-just-the-dogs-are-people-this-gets-at-a-challenge-i-have-as-a-sighted-developer-i-use-voiceover-myself-for-some-functional-tasks-such-as-reading-out-content-id-rather-hear-than-read-myself-but-it-is-hard-to-get-to-know-the-deeper-layers-of-voiceover-one-key-third-party-app-for-teaching-even-the-main-voiceover-gestures-and-interactions-has-been-discontinued-what-resources-do-you-recommend-for-people-to-learn-good-interaction-patterns-for-voiceover-beyond-the-surface-level","text":"Check this out.. https://support.apple.com/guide/iphone/learn-voiceover-gestures-iph3e2e2281/ios","title":"This is excellent. It is absolutely relevant to my app, it's just the dogs are people. :) This gets at a challenge I have as a sighted developer. I use VoiceOver myself for some functional tasks, such as reading out content I'd rather hear than read myself. But it is hard to get to know the deeper layers of VoiceOver. One key third party app for teaching even the main VoiceOver gestures and interactions has been discontinued. What resources do you recommend for people to learn good interaction patterns for VoiceOver beyond the surface level?"},{"location":"wwdc21/a11y.html#this-has-been-really-helpful-to-understand-how-to-move-content-rich-material-to-the-more-content-section-of-the-rotor-are-there-other-features-of-the-rotor-that-likewise-require-explicit-developer-implementation-like-this","text":"Checkout Accessibility Custom Rotors session from WWDC 2020 https://developer.apple.com/videos/play/wwdc2020/10116/ (edited)","title":"This has been really helpful to understand how to move content rich material to the \"More Content\" section of the Rotor. Are there other features of the Rotor that likewise require explicit developer implementation like this?"},{"location":"wwdc21/a11y.html#what-has-been-the-most-challenging-apple-app-to-work-on-for-accessibility","text":"Personally, I would say, non UIKit based apps, like PDFs and Photos app!","title":"What has been the most challenging Apple app to work on for accessibility?"},{"location":"wwdc22/accessibility-lounge.html","text":"accessibility-lounge QAs by FeeTiki Will this plug-in work well for fast-paced games (like FPS)? Certain aspects of this plugin are wellsuited to fast paced games like FPS. Forexample, reacting to users settings forlarge text and dynamic text (e.g. in yourmenus or HUD). what would you say is the biggest challenge of adding accessibility to unity games? Depending on your game, it may be hard to find what could be made accessible to your players. I hope this plugin can enable you to do much more than previously available Certainly before this plugin it was making anything accessible to VoiceOver. I think now the challenge will be more in how you design your game to accommodate people of different abilities Awesome work on the Unity plugin - I can't wait to use it! While the session demo'd VoiceOver use with the plugin, is it safe to assume this will work with Voice Control as well? There's that old overlap of accessibility allowing convenience for those that want it and it'd be nice to allow players to do so with additional voice inputs etc. If you expose accessibility elements via this plugin it will be available to all the accessibility technologies, including Voice Control. <@U03HHHDSD2N> Thank you for the answer! That's great to hear - can we add the equivalent of accessibilityUserInputLabels modifier to elements using the plugin? For now we can't. We will incorporate this feedback so we could consider this in the future No worries. This is fantastic and I'll be sure to give it a try soon this week If you'd like to provide more feedback, please use this link on GitHub https://github.com/apple/unityplugins/blob/main/Documentation/Feedback.md Should SwiftUI .sortPriority modifier on Views work also for Views that dont share the same superview/ancestor? Yes it should. accessibilitySortPriority should work for any views within the same accessibility container, defined by accessibilityElement(children: .contain) modifier. What are some of the best tips and resources for designing and coding for large accessibility size fonts? At the largest sizes, this seems to be very difficult to do well. Hi Klayton, a lot of this likely starts at the design stage. The best high-level recommendation we tend to give is to allow for any given label in a design to grow significantly, both in content and in size. In many cases this involves making content scrollable, or adapting horizontal layouts to become vertical at a particular text size. For the latter, I highly recommend checking out ViewThatFits, which is a new view in SwiftUI this year that lets you specify variations of views, allowing the framework to pick the best one depending on the available size. I\u2019ve also got some links that I highly recommend checking out if you haven\u2019t already: an app challenge from last year, a couple WWDC sessions, as well as the \u201cText display\u201d section in our Human Interface Guidelines. If you have specific questions about making sure your app works great with Dynamic Type we\u2019d be happy to answer those too! https://developer.apple.com/news/?id=w6r26g7r https://developer.apple.com/videos/play/wwdc2017/245/ https://developer.apple.com/videos/play/wwdc2020/10020/ https://developer.apple.com/design/human-interface-guidelines/foundations/accessibility#text-display The upcoming talk on SwiftUI's new custom layout system will be helpful too. https://developer.apple.com/videos/play/wwdc2022/10056 Thanks. That's helpful. I really wish some of these new layout tools were available in earlier OS versions or made available to the same set of devices as iOS 15. For a single word that can't fit on one line, such as someone's name, does a streaming scroll (marquee) element seem better than breaking up that word into multiple lines? Should it require an app-setting toggle to avoid motion issues for people sensitive to motion? I would say multiple lines are much more preferable than implementing marquees Can SwiftUI support tab key switching to different UI components and use space key to trigger default actions of the UI compoments? Thanks for your question. Currently SwiftUI controls can be added to tab key loop with the .focusable modifier, thus also being able to be triggered by space key. Hi Eric, do you have example code for this? I have tried .focusable() for my image button. However, when press space key for it, the Button action didn\u2019t get hit? Does it also work for Link. Toggle \u2026 We have a new sample project Food Truck this year https://developer.apple.com/documentation/swiftui/food_truck_building_a_swiftui_multiplatform_app Button with .focusable modifier should be able to be activated with Space key. If not, we'd love a Feedback! .focusable() should also work for Link and Toggle:smiley: What attracted you to working at Apple, and specifically on accessibility? For me personally, 1. I believe Apple is the best company to work for if you want to change the world and make it a better place with technologies 2. Solutions/products that help people with disabilities could be one of the most impactful things you could do with a known set of inputs/resources 3. Problems in accessibility are really really hard problems, and I get personal satisfaction solving them :slightly_smiling_face: I happened to meet someone who worked on this team when I was in school and managed to get an interview for an internship. I didn't get the internship, but I replied back a year later to see if they wanted interns for the next year, because I knew it'd be such a fit for me. The thought that someone was working on accessibility full time was so cool to me, and I knew it was something I wanted to do. I get to make stuff, basically for myself, I don't think I could find a cooler job :sunglasses: I've used Apple products since I was 10 years old and Steve Wozniak taught my 5th grade class how to use a Mac laptop. Having been born legally blind, accessibility has always been a passion of mine as it helps improve my quality of life. Since age 10, I've been all Apple, and I wanted to be a part of, and share with others, the amazing things that our technology has done to improve my life. I joined pretty much right from university. I was interested in accessibility research at the time and Apple just happened to need help making a newish product accessible (the iPhone). Testing and QA is what I already enjoyed doing. I\u2019ve went most of my professional life searching for that missing additional element and I found it at Apple. I realized that found it more meaningful and fulfillment helping others by helping others by ensuring that our Accessibility features are always as expected. In addition, as a disabled person myself, I am able to provide quality feedback on how to improve or suggest new features that will make life easier for other physically challenged people like myself I\u2019ve been using a Mac in one form or another since the 80s, and at every step of my vision loss, the product was there to meet me where I was at. The opportunity to work side by side with the people who created a huge part of my personal independence was a powerful dream. I have had this goal since I was in the 8th grade. You know those assignments \u201cWhat do you want to do when you grow up?\u201d Well, I still have the paper from that 8th grade exit assignment, and it says in my handwriting that I wanted to work for Apple. Decades later I wouldn\u2019t change a thing. What are some example scenarios of where you could tell a developer tried to meet your accessibility needs but missed something? Are there patterns? Yeah, as a low vision user, the most common pattern I've noticed is in Large Text support. It often doesn't exist, or if support does happen, it is often not in a readable way. I think sometimes when developers make assumptions about what I need they can sometimes miss the mark. Just because I have low vision doesn't mean I need a totally separate design. When I see apps present a totally different design for VoiceOver users, I am always a bit disappointed. We have so many tools to make almost any design accessible, and people want to enjoy those experiences similarly to how their able bodied friends and peers are experiencing them. Implement APIs to support built in settings we've provided instead, and you'll be headed in the right direction One thing I notice is when a developer uses custom UI with several elements on screen, they tend to forget about making every single element accessible. For example, most of the important elements would be scannable with item mode and the ones at the bottom such as sorting items in a list buttons won\u2019t be accessible Should Voiceover order be always top-left to bottom-right in LTR language? Yes, unless a particular UIView has its semanticContentAttribute set to .forceRightToLeft or if a SwiftUI View has its layoutDirection environment set to RTL In some specific cases like two columns content, navigating down the first column before jumping to the second column may make more sense than a strict top-left-bottom-right. For third party apps that are clearly at least making an effort to be accessible, what are the most common accessibility omissions or errors do you see developers making? The most common issues I see when we do app audits are bad accessibility labels and improper groupings for VoiceOver users. Labels should be concise, and shouldn't include redundant information like the role (like the word button). And items should be grouped logically. The best advice I give developers is, think about how the information is \"digested\" visually. Is this group of labels/images conceptually one item when you look at it? Then it probably should be for VoiceOver users too. This isn't something we think about a lot, it's all implicit in the apps we use every day, but taking a step back to notice these things help a lot Here's a great talk on writing good accessibility labels: https://developer.apple.com/wwdc19/254 Lack of support for Large Text sizes. I frequently see apps make an effort to be accessible with VoiceOver, but even apps that do a great job with that often do not have support for Large Text. Making a choice to use a cross platform development package without first investigating how accessibility will be implemented with that package frequently results in a really bad situation. Your customer might report an accessibility problem, and you may be unable to fix it if the frameworks don\u2019t support our accessibility features. This has been the case for countless apps I have encountered so far. For Andres: do you know if there is still a lack of assistive technology for reading scientific publications, or have things improved? For everyone: are there any remaining challenges specific to your life that society or technology could do a better job of addressing? (how?) Thanks Keith. There have been lots of improvements in accessibility of scientific materials . MathML for instance allows VoiceOver users to read mathematical expressions on the web and other documents. There is still a lot to do in making math and science more accessible to students and professionals with visual disabilities. In particular conveying graphical information such as diagrams, function graphs, etc. is still an area of research and development. How do header traits and levels work on macOS? I didn\u2019t see a header rotor option available like on iOS. So in VoiceOver on macOS the headings rotor will be found using the VoiceOver modifier keyboard shortcut + the U key. This will pop open the available rotors and if you're on an element which has a headings rotor you can use the left and right arrow keys to find it in the rotor menu. The headings rotor will include the level of the heading when announcing each heading. Is it possible to emulate a switch control in either the Simulator or via an external Bluetooth Keyboard? Great question. Yes, Switch Control can be configured to respond to external Bluetooth keyboard events! Make sure your keyboard is paired to your iOS or iPadOS device, then head to Settings > Accessibility > Switch Control > Switches > Add New Switch. Here you get to choose the switch source, so tap External. Each key on your keyboard could be its own switch, but you have to add them one by one. When you activate a key on your keyboard, Switch Control will ask you to name it and then assign it to an action, this helps you keep track of all the switches you add. If you're able, I recommend setting up at least three switches if you're using a keyboard with Switch Control: Select Item, Move to Next Item, and Move to Previous Item. For these you could use the space bar and arrow keys respectively! Thanks Drew! This will make switch control development much easier Glad to hear it will help! :tada: Can you give any advice to making a text view with a maximum character limit more accessible, especially with VoiceOver? Visually it shows a counter (eg: 90/100) and highlights text over the limit red. You can override/set accessibilityValue on the text view to return the entire text. For the character limit, you could post an announcement notification when the user starts to go over the limit that says something like \"character limit reached\". You can also add a hint to the text field that conveys the max number of characters. Also make sure that your counter of 90/100 is an accessibility element, so that VO users can focus on it manually if they want to know the character count In SwiftUI & macOS 12 How can I move the voice over focus to a presented view? Accessibility Focus State is gonna be the way go here! You can change the state value to true on an element and it will update VoiceOver focus to that element! https://developer.apple.com/documentation/swiftui/accessibilityfocusstate|https://developer.apple.com/documentation/swiftui/accessibilityfocusstate Does it make sense to change the focus when showing a new view in place of the old one on macOS? We have a list layout, clicking one of the items \"pushes\" a new view similar to a navigation controller. Currently VO loses focus and moves elsewhere. So if voiceover is really not going where you want in this instance I think you could do that. I would warn about using this too often though because jumping around the voiceover focus can be very distracting. If its a totally custom navigation system than in that instance you probably want to either do that or trying to post a layout change notification. Usually though we either let the system framework and voiceover to decide where best to go, or if your UI has a popup or an alert thats a great way to use focus state. I guess there\u2019s a similar property as AccessibilityFocusState in NSAccessibility? I am having some issues to keep the element selected with tabulator keyboard navigation and VoiceOver focus in sync because I have to move the tab focus programmatically in some cases (for example when moving out of a WKWebView to some custom window UI elements below). > trying to post a layout change notification. Will posting a layout notification without passing in a view work? Or is there a way of doing this in SwiftUI? So if you change keyboard focus you can send a notification with NSAccessibilityFocusedUIElementChangedNotification to let voiceover know the focus changed. If you want to change voiceover focus from non-keyboard focus changes you can use NSAccessibilityLayoutChangedNotification with a NSAccessibilityUIElementsKey https://developer.apple.com/documentation/appkit/1534572-nsaccessibilitypostnotificationw?language=objc In that instance <@U03JB2UUT46> if you want to pass a specific element you would use focus state because there isn't really a view to pass it in SwiftUI Thanks <@U03HKTXJ4KE>, I\u2018ll try those notifications. I actually might need LayoutChangedNotification because the elements I\u2018m programmatically focusing are in a different window, so maybe the NSAccessibilityUIElementsKey is needed to be passed. I had a hard time to be able to access window elements in the title bar (if no NSWindowToolbar is used), in the toolbar if it is visible, web content in the WKWebView and those additional UI elements in a separate \u201eDock\u201c window below, but in the end it worked with adding views like NSWindow contentView.superview, contentView and the contentView of the separate window to NSWindow accessibilityChildren. Don\u2018t know if there\u2018s a better way, but it seems to work correctly now. Joining a team without any real accessibility support, what would you all consider top priority support? I'd love to do it all but know that proper support takes time and, also, one thing cannot be more \"impactful\" than another, they are all equally important. How would you consider breaking down a project into incremental wins? I usually say 1. Dynamic Type support will reach the largest audience 2. Add App Accessibility support will be most impactful for users who need to use ATs (Assistive technologies) - like VoiceOver,Switch Control, Voice Control \u2014 because often if app accessibility is NOT provided - they will have no way to access your app Other areas like checking for contrast, transparency, motion will have large impacts IF your app has areas that might be problematic (aka - you have light grey text on white backgrounds) and are usually not that much work to implement True true, I want to be sure our app is impactful for everyone so taking the small wins were we can. Thanks for the callouts here! How are swipe buttons on list items regarded from a general discoverability aspect and then more specifically for VoiceOver? I have a list UI that's more of a fake \"tree view\" with different types of content on most rows. There are correspondingly different swipe buttons on leading & trailing. Currently have no affordances hinting at them even for sighted users. The lovely new HIG doesn't even mention them. As they got significant improvements in SwiftUI3 I assumed they are still \"blessed\" as a UI approach. Swipe items are exposed to assistive technologies via 'custom actions'. They typically should be exposed automatically if you are using system UI controls. https://developer.apple.com/videos/play/wwdc2019/250/ https://developer.apple.com/documentation/uikit/uiaccessibilitycustomaction For VoiceOver specifically, these swipable actions are exposed in the 'Actions' rotor. Are there plans to expand the new eye gaze access in iPad OS to allow apps to directly access the high fidelity eye gaze data and opt out of translating eye gaze data into assistive touch events? Anything is possible. Are you able to explain your use cases in more detail? It may be useful to file a feedback as well so we can look into this in the future. When supporting dynamic type/large text sizes with custom fonts, is there a good way to limit the maximum size a font can be? Or is this just a bad idea in general? Use case: While we are working on supporting dynamic type, we've noticed that we can support many of the larger sizes & all the smaller sizes. But the biggest sizes cause some pretty serious layout breaks throughout & it's going to take us a while to chase them all down + QA. I'd love to roll out support for most sizes so my customers can use it earlier & those who need the largest type sizes can at least get partway there. I suspect you\u2019d want to use UIFontMetrics to help here `- (UIFont *)scaledFontForFont:(UIFont *)font maximumPointSize:(CGFloat)maximumPointSize; ` In general, we would like to NOT cap font sizes, however there are cases within the UI that are very hard to grow given constraints. In iOS and iPadOS we will often use UILargeContentViewer (UIKit/UILargeContentViewer.h) to allow a long press to show text that\u2019s can\u2019t grow well In your case, it sounds like it would be reasonable to roll out support for larger sizes and overtime work to adopt the largest sizes. I'd reiterate that it's really not recommended to cap text sizes, unless the UI literally cannot grow without being unreasonable (like nav bars and tab bars, in which case use large content viewer like Chris said). Limiting text sizes will significantly impede low vision users from using your app properly. Definitely support what you can for now, but I strongly encourage you to work towards supporting all the sizes rather than capping them to a specific value! Yes! Supporting all font sizes is absolutely the plan! Just trying to make it more usable more quickly. Thank you for your answers! Hello. There are a few new languages added to VoiceOver. I'm talking specifically about Persian. The system falsely identifies Persian texts as Arabic or Urdu. Do we need to somehow tag the texts to make it work with Persian VoiceOver or is this a bug? Hi Saeed, please file a bug using the https://feedbackassistant.apple.com|Feedback Assistant . Is this on macOS or iOS? I\u2019ll file it. I\u2019m testing on iOS. On iOS you can programmatically set the speech language ( UIAccessibilitySpeechAttributeLanguage for attributed strings, or by setting .environment(\\.locale, \u2026) in SwiftUI Texts) \u2026but I agree that you shouldn\u2019t have to do this. Some more detailed steps will help us get to the bottom of this :slightly_smiling_face: Are there any specific accessibility API properties/methods we should use for \"Full Keyboard\"? I have a view that is fully interacts with voice over but not with an external keyboard. I started using the accessibilityRespondsToUserInteraction boolean and that seems to work for most of the elements, but not all. NOt really sure if this is the correct api property to use in this case. There is not much documentation out there for full keyboard accessibility. thank you! You can also use the API in <UIKit/UIFocus.h>. FullKeyboardAccess uses the focus engine, so these may be used to customize the FKA experience. thanks, John! so, it seems like simply implementing accessibility for voice over does not automatically carry over to full keyboard? is there any specific accessibility property we should use for keyboards? where you thinking I should use canBecomeFocused ? Right it does not, we do try to respect accessibilityRespondsToUserInteraction and other accessibility methods like accessibilityElements but ultimately the focus engine is what determines the behaviour. Yup! canBecomeFocused is ultimately what the focus engine uses to decide what to focus. I was hesitant to use this boolean -- the wwdc '21 \"Support Full Keyboard Access in your iOS app\" session specifically calls out not to use canBecomeFocused as this has implications beyond accessibility in the app There was a WWDC 2021 talk that could be useful https://developer.apple.com/videos/play/wwdc2021/10120 do you have any other documentation or videos? that video does not fully cover what we should do besides adding custom actions. also, please see my comment above. we currently had an a11y audit on our app and this has come up. the team is having a hard time figuring out how to implement this properly UIKit apps depend on Dynamic Type to adjust their text size to be comfortable for different users. When these apps are brought to the Mac via Catalyst, is there a recommended approach to providing scalable UI text size given there isn't a \"system text size\" as with iOS and iPadOS? Short answer: not really. You could make your own text size slider in settings like Books that uses dynamic type under the hood. Users have a few AT options to help with this as well - Zoom, HoverText In the simulator, you can enable Full Keyboard Access in settings. However, I couldn't find a way to actually use it. Is it actually possible to test Full Keyboard Access in the simulator? And if so, how? Thank you! This looks like it works to me in Seed 1 I see! I haven't tried in Xcode 14, but it is something that wasn't working for me in the iOS 15.5 simulator, for example. So you just turn that switch on and it should work right? No other steps needed. Thanks for your help! yep looks like it. and then pressing tab on your mac keyboard moves focus Currently on macOS keyboard accessibility for views like lazy grids is difficult to implement compared to views like NSCollectionView, where you can get it for free. Things like navigation (moving selection) and multi-selection have to be manually implemented, which is a very difficult and heavy task (and easy to deviate from native.) Are there any recommendations for how we as developers can tackle this or plans to make this easier? Thanks for the feedback! We would love to hear difficulties that you encountered when implementing keyboard focus in lazy grids through Feedback Assistant. We also have a new SwiftUI sample app Food Truck that introduces new features and improvements that you could check out https://developer.apple.com/documentation/swiftui/food_truck_building_a_swiftui_multiplatform_app In addition, you could also join the SwiftUI lounge and Q/A sessions to get more of your questions answered for SwiftUI :slightly_smiling_face: Hi Eric! By \u201ckeyboard accessibility\u201d here I\u2019m not just referring to keyboard focus, but what happens inside a lazy grid when you press the arrow keys (or tab, actually.) Or are trying to multi-select within a grid (Shift + Arrow Key, or Shift + Option Key + Arrow Key). NSCollectionView and NSTableView can give you this out of the box (and you can see it in Finder or other native apps.) But this kind of keyboard accessibility \u2014 plus selection state handing \u2014 isn\u2019t available in SwiftUI out of the box as far as I can tell. Hopefully that clarifies my question! I hope we can make this more easier. We\u2019d love a Feedback! My goal is to get visibility into the accessibility elements of a view and it's children. With UIKit, I am able to walk the view hierarchy in code and log all the accessibility properties of a view and it's subviews. What would be the equivalent with SwiftUI? The goal, given a SwiftUI view, log all the accessible elements and their properties. Thanks for your question! In what context are you trying to get visibility into the accessibility elements? Specifically during debug or some of your app's functionalities rely on this? <@U03HEKAJJBF> Good question. I am writing XCTests that snapshot the a11y tree for a few use cases: 1. snapshot the tree to use for regression testing (this is the major use case) 2. identify elements that are missing a11y 3. identify elements that are candidates to combine a11y Thanks for clarification! We currently don't support introspecting view hierarchies in SwiftUI views from within your process. The closest tools we have for now are Accessibility Inspector, Xcode Previews, and View Hierarchy Debug. We'd love a Feedback with your use case if you find you need more than what those tools currently offer :smiley: Ok. Unfortunately, the lack of introspection is preventing me from adopting SwiftUI since there is no way too automate a11y checks via XCTest framework. Were there any updates to the Accessibility Inspector this year? We have some great improvements for accessibility audits on detecting non-accessible elements in Accessibility Inspector! Please try them and we are very happy to any feedback you have :smiley: How do all of the examples highlighted in this talk pertain to macOS Safari? Are the techniques all general enough to apply to web standard AX, or only to VoiceOver iOS, or only to VoiceOver on Apple platforms in general? Everything we went over today is supported on macOS Safari, too (along with other platforms like Watch and TV)! > Are the techniques all general enough to apply to web standard AX, or only to VoiceOver iOS, or only to VoiceOver on Apple platforms in general? These techniques definitely apply to assistive technologies of all types of Apple platforms. I'm not familiar with the level of support for these techniques in other browsers \u2014 https://caniuse.com might be helpful there. Thanks! The increment/decrement equivalents are accessible by touch or keyboard on macOS\u2026. Look up Mac VoiceOver\u2019s \u201cTrackPad Commander\u201d feature for touch gestures. To increment with a slider on macOS, you can use unmodified arrow keys in most scenarios. If the user wants to override the default key handling and force an \u201cincrement\u201d they\u2019d first \u201cinteract with\u201d slider (VO+Shift+Down) and then increment (VO+Up or VO+Right in LTR languages contexts) Are there any specific guidelines for making interfaces accessibility in the context of embedded web views in native apps? Does WebKit/WKWebView offer any mechanisms separate from web standards to interface with VO for example on an iOS or Mac device? There are not \u2014 accessibility in embedded webviews should, in general, work just as if you had the same HTML loaded in Safari. Just remember to use semantic HTML and / or ARIA attributes on the markup in your embedded HTML :slightly_smiling_face: Great, thanks. Good session, sorry you didn't have more Q&A! Hope springs eternal ... We're now working on a cross-platfrom design system (including Web and Apple platforms). We want to use this project to elevate accessibility of our products and to maintain it at a high level in the long run. What accessibility standards/assistive technologies would you recommend to take into account while building a design system? Do you have any tips to make this project a success in terms of accessibility both on Web and on Apple platforms? Big question! If I'm reading this right, then I would start with making sure your design system is able to make all components accessible to the same level as the native platforms Then you need to transmute those paradigms in your system to the native platforms This is very similar to what we did with Unity accessibility plugins https://developer.apple.com/wwdc22/10151|https://developer.apple.com/wwdc22/10151 So there are Unity methods for everything from custom actions to dynamic type. Those get translated to the native iOS Thanks! :slightly_smiling_face: In addition to Chris' comments, I'll add: > What accessibility standards/assistive technologies would you recommend to take into account while building a design system? https://www.w3.org/TR/WCAG21/|WCAG has a lot of really great guidelines. If your design system includes web technologies, there are automated tools to test some aspects of accessibility, like: https://github.com/dequelabs/axe-core These tools can be run in a CI pipeline, allowing you to catch and fix accessibility issues as early as possible. As far as which assistive technologies to test, VoiceOver is a good one, since often (but not always) if a web app is accessible with VoiceOver it is also accessible with other assistive technologies. But you should still test others! Full Keyboard Access (and even general keyboard navigability) is a good one. Another one to test with is Voice Control. Thank you Tyler! I've just submitted another question precisely about automated testing solutions, just a second before seeing your reply! :slightly_smiling_face: Tyler Great minds think alike :smile: Also make sure your design system requirements account for the superset of all available accessibility features. There are API equivalents for many of these, like isReduceMotionEnabled for native Apple platforms and @media (prefers-reduced-motion) in Web. Likewise isDarkerSystemColorsEnabled vs @media (prefers-contrast: more) Not all features exist on all platforms though, so be sure to account for all in the design, and support the implementation where available. Search bars now usually do not require hitting \"Submit\" to search, the results get updated automatically along with the user typing in their request. Would you say it's a good practice to make VoiceOver announce the number of results as the user types in their search request? Like \"5 items found\" or \"No items found\"? Yes it should be announced but not immediately. There should be a period of quiescence so that your announcements are not interrupting typing feedback Thank you! You\u2019ll get that announcement-without-interruption for free if you use an ARIA \u201clive region\u201d for the \u201c5 items found\u201d announcement. You don\u2019t always need to use WebSpeech or SSML for general user interface notifications. We had a WWDC session cover live regions years ago, but it may be difficult to find now. The incomplete gist of a live region is: `&lt;div role=\"status\"&gt;As the contents of this element are changed, screen readers will announce the new text automatically.&lt;/div&gt; ` A little more here: https://www.w3.org/TR/wai-aria/#attrs_liveregions Are there any solutions for automated testing of accessibility? Would you recommend Apple UI testing framework for these purposes? I know Benjamin has read this in the thread above, but for posterity I'll post it here too: https://github.com/dequelabs/axe-core This tool (and others like it) can be used to automatically catch certain types of accessibility errors in your web apps, and can be run as CI pipelines to catch errors as early as possible. These tools don't catch every accessibility issue, so make sure to still test with actual assistive technologies like VoiceOver :slightly_smiling_face: May I humbly suggest my library for XC UI testing native iOS apps :slightly_smiling_face: https://github.com/rwapp/A11yUITests (obvs not endorsed by Apple in any way) Can't miss an opportunity to say thank you to you <@U03J4DPCXGC> for all your amazing work in this area! We refer to https://mobilea11y.com/ a lot when it comes to accessibility on Apple platfroms! :heart: When adding support for dynamic type, are there any guidelines or recommendations on when it\u2019s best to respect the system settings vs offering a screen scoped font size control? For example reader apps such as Safari and Books offer independent font size controls vs Podcasts and Mail do not. The pattern I see is apps or features where reading content is the primary experience (which I personally have used vs changing the system wide setting) - but from my earlier example Mail could also be classed a reader app :thinking_face: Thank you! Good question! I think your intuition is right here. For longer form reading content, some people prefer to adjust font size on the fly, as lighting conditions change or their eyes tire. For apps that are not for long form reading content, these controls are really not recommended in my opinion. As of iOS 15, we have per app settings where users can set font sizes per app, and they can be adjusted quickly with our control center module. So users already have a way to quickly adjust text sizes on the fly should they require it. Users are expecting to open your app and see the text size they have set globally (or per app if they've adjusted it), so adding a second control to your app just introduces unnecessary complexity Is it possible to use attributed accessibility label features - language, spell out, phonetic notation, pitch etc - in SwiftUI? Thank you :) Hi Rob, there are several speech modifiers available for use with Text in SwiftUI, like speechAdjustedPitch , speechAlwaysIncludesPunctuation , speechAnnouncementsQueued , and speechSpellsOutCharacters . For changing language, you can override the locale environment on a Text view and it should do the trick. Sweet, I have never noticed these before. Thank you! Also note that since iOS 15, Text can natively support attributed string via both new markdown syntax \" Text(\"this is **bold **\") and AttributedString . So if you use those it should also work When we started adding accessibility modifiers to our SwiftUI views we noticed that our XCTest UI tests got broken. The tree of elements changed - some UI elements impacted by accessibility modifiers became unaccessible/non-hittable. We didn't find a way to use accessibility modifiers and keep all the elements accessible for the purposes of UI tests. Is it expected? We ended up creating custom modifiers that would not apply accessibility modifiers in the UI test environment. Is it the best solution at the moment? Thanks for your question! Accessibility modifiers do affect XCUITests because it may change accessibility elements tree. Currently your solution would be the best at this moment. We'd really love a Feedback from your specific case! If you would submit a Feedback and attach your example to us, we can see how we could do better! Got it, thank you! is it possible to turn off the reading of MKMapView map POI in Voiceover? Hi Lior, thank you for your question! It is not possible with MKMapView; however if you subclass it, you might be able to by overriding accessibilityElements. Give that a try and let us know! Thank you for your answer, will do! There are several OOTB accessibility elements that I wish to disable (f.e. Legal view) their voiceover, hopefully subclass will work. After an amazing Accessibility Design Review am busy cringe-testing with Voice Over. I think this is an Apple bug, not mine - the permission alert on accessing the camera doesn't read out the text explaining why Should I worry/fix? Hi Andrew! If this alert is coming from the system, there shouldn't be anything you need to do to make it accessible. If you're finding it's not, please file a bug to us via Feedback Assistant. Note that the heading of the alert and the body are likely separate accessibility elements, and a user must move focus to the body in order to have it read, it won't all read automatically. This is by design After a solid half hour of testing, I think the problem may have been that it was iOS14 and/or it was my not understanding how to move through things with VoiceOver. I may have been doing a swipe down rather than left/right, to try to advance item. Anyway, I now am much more practiced at VoiceOver, have an even healthier respect for the people who designed it, and can not reproduce the problem. (It's also really hard to reliably trigger the state of Application has not yet been authorised for camera - if you reboot the device whilst the alert is in front, it defaults to being disabled, not left ambiguous!) Glad you were able to get more familiar with VoiceOver! Hah! I just replicated this and it was indeed my misunderstanding voiceover. I'll record a little video as a reminder but the short version is: 1. I thought that swipe down advanced to the next control 2. Whilst a slow swipe down will take you through the elements of the alert, a fast swipe down will move the focus from the title to the button you end up on, without having time to say the sub-title I managed to trigger the application state on a test device where it would show this alert then another 10 minutes of playing around to realise the above sequence is what I'd been doing Hmmmm I spoke too soon, maybe it's a thing about alerts. The swipe right for Select the next item doesn't work in them. what happens when you swipe right? It should be a quick swipe to the right (a flick) and that'll move to the next item flicks up/down will activate whatever the current rotor item is OK, yeah I was trying to do that. I will have to get back to the state where the \"Blah would like to access the camera\" alert is shown and re-record with touches captured. It seems that accessibility swipe right behaviour indeed doesn't behave normally no that particular alert ahh Rotor was on Characters not Containers I'll dive into this with more testing when have more time and log an issue if I prove out a problem. I'm confused by the Rotor, now I have it appearing. There are a lot more options on https://support.apple.com/en-au/HT204783 than I see. My rotor only has Headings Characters Words Speaking Rate Containers The rotor options are dynamic based on the type of control you're on, but you can also add a bunch more in VoiceOver settings What is the recommended accessible way for infinite scroll in a feed? Hi Shahla! I don't think there's a ton of special considerations you need to make for infinite scroll views. Assistive Technology users can typically discover and interact with these similarly to mainstream users. Just make sure that the content inside your scroll view is accessible! VO users can navigate via swipe and tap exploration as well. If you have specific questions about your scroll view as it pertains to accessibility, feel free to clarify your question here. :slightly_smiling_face: How to fix the issue about SwiftUI Textfield with too small hit area? https://developer.apple.com/forums/thread/703257|https://developer.apple.com/forums/thread/703257 This looks like a bug for Accessibility Inspector, I have reported this feedback to our engineers. Thanks for the question! <@U03HEKAJJBF>, but if we want a more significant hit area, the current SwiftUI didn't support it yet. Is that right? If I have a sectioned grid of items, what's the best way to expose those sections for easy navigation via Voiceover in SwiftUI? Thanks for the question! One thing you could do is to make sure the section headers have \"header\" accessibility traits via accessibilityAddTraits(.isHeader) The second thing you could do is check out the accessibilityRotorEntry API to provide custom rotors for VoiceOver. This can allow VoiceOver users navigate your app more efficiently. Thanks Eric! If this is a good place to ask implementation questions, I have identified what's happening with the screen that was called out in my accessibility review lab. I have a Page Background page with a segmented control at the top and a bunch of stuff that is hidden depending on choosing Photo or Color background. The visibly hidden items are not hidden from VoiceOver and so it's selecting them as you move through page items. How do I hide them from VoiceOver? You can use setAccessibilityElementsHidden in UIKit or .accessibilityHidden() in SwiftUI https://developer.apple.com/documentation/objectivec/nsobject/1615080-accessibilityelementshidden?language=objc Just looked at how my UIKit code works. I have a hierarchy inside my main view (xib) UISegmentedControl UIScrollView UIView UIView backgroundImage UIView backgroundColor UIView backgroundShader I'm just toggling backgroundImage.isHidden (etc) and relying on that to hide all the innermost controls what view contains the controls? you'd want to set accessibilityElementsHidden on the view that contains all the controls you don't want VoiceOver to see yeah I'm just trying that. Sorry for not being more explicit - the three named views in my little diagram contain all the controls (it's a complex screen, as are too many of the detail editors) Flicking between this and the alert thread ... I've actually replicated the lab problem as well and confirm you were right - it's speaking the \"previous screen items\" which is just weird. but have an added nuance from testing further (after adding the accessibilityElementsHidden code and then removing it) If you tap the segmented control to change to the other elements being visible, VoiceOver is now correctly going through the elements of the page. Tapping it again to change back, it is still correct. It seems that the explicit toggling of isHidden on the parent views is enough to fix things for VoiceOver but relying on the initial isHidden from the xib, at view load, is not. I'm loading these detail editor VCs with func hostVC(_ vc:UIViewController) { for child in children { child.willMove(toParent: nil) child.view.removeFromSuperview() child.removeFromParent() } addChild(vc) vc.view.frame = view.bounds vc.view.translatesAutoresizingMaskIntoConstraints = false view.addSubview(vc.view) vc.view.leftAnchor.constraint(equalTo: view.leftAnchor).isActive = true vc.view.rightAnchor.constraint(equalTo: view.rightAnchor).isActive = true vc.view.topAnchor.constraint(equalTo: view.topAnchor).isActive = true vc.view.bottomAnchor.constraint(equalTo: view.bottomAnchor).isActive = true vc.didMove(toParent: self) } Is there a list of the upcoming languages supported by VoiceOver? Also, can we have a way to programmatically ask the accessibility framework what languages are currently supported? We're running into an issue where in visual text we support Vietnamese, but VoiceOver doesn't. We'd prefer VoiceOver to use a fallback like English in that case. So for this year VoiceOver will add support for Arabic (World), Basque, Bengali (India), Bhojpuri (India), Bulgarian, Catalan, Croatian, Farsi, French (Belgium), Galician, Kannada, Malay, Mandarin (Liaoning, Shaanxi, Sichuan), Marathi, Shanghainese (China), Spanish (Chile), Slovenian, Tamil, Telugu, Ukrainian, Valencian, and Vietnamese! To query a list of languages, you can use the speechVoices property on AVSpeechSynthesisVoice https://developer.apple.com/documentation/avfaudio/avspeechsynthesisvoice/1619697-speechvoices?language=objc this will return you a list of all installed voices on disk. Note that a few of the new languages we are supporting do not have built in voices, so they will not show up in this list unless a user downloads them But you can basically iterate over this list and query the language property on the voice to figure out a list of languages that are available to the user :open_mouth: thank you so much! :point_up: <@U03HKTXJ4KE> and <@U03DQ584A76>! What are best practices for creating a VoiceOver experience for audio recording? Especially if that page includes things like countdown timers or textual prompts for the user? I'd recommend you use announcement notifications for your prompts and countdowns. We do this in the \"Hey Siri\" training flow. Basically, whenever there is a prompt, we post an announcement notification, wait for the announcement to finish, and then start the recording session. When the next prompt is being read, we pause the recording. You can find out when an announcement is finished reading via this notification: https://developer.apple.com/documentation/uikit/uiaccessibilityannouncementdidfinishnotification?language=objc Thank you for the suggestions! I still remember the awesome djay Pro demo on iPad, pretty sure it was during the 2016 Design Award ceremony but sadly the video seems to have disappeared. May be worth looking at their workflow to see if they have other ideas? https://www.algoriddim.com/news/267-djay-pro-wins-2016-apple-design-award As audio features are important to us, am also interested in ways to make this accessible (could have spent an entire day in that accessibility design lab!). Would it make sense to use split output, with editing feedback via headphones and performance via speaker (or vice versa?). Thanks <@U03JELM0ZNV> - that's a neat reference I hadn't considered I'll definitely check it out. Designing VO feedback specifically for headphones is definitely an interesting idea :thinking_face::thinking_face::thinking_face: That way we could use announcements to call out \u201c30 seconds remaining\u201d (or whatever) and give VO users some of the same countdowns. I think really short announcements giving feedback might work too. Just have to make sure they're really targeted & not too distracting\u2026 (I work on an app for practicing public speaking, for context) Would the Assessment Mode entitlement be approved for an app that is not strictly administering an assessment, but where a valid use case can be made for programatically enabling Single App Mode for non-MDM devices? e.g., another use intended for teachers in a classroom to use with their students, or people to use in another public environment, that is not assessment-related? Hey Duncan! I encourage you to explore options of supervising the device yourself using Apple Configurator or other device management tools. If that doesn't work, I really like your use case and you should absolutely apply for an assessment entitlement. While I am not 100% positive on all of the requirements the review team goes though, I don't see harm in you applying! Here is the direct link if you need it https://developer.apple.com/contact/request/automatic-assessment-configuration/ Ok, will do. In this very specific case, while I don\u2019t regard it as \u201cassessment\u201d, I could probably argue it is formative assessment\u2026 The basic format is a quiz that lets students see if they know the names of other kids in the class/school\u2026 kinda getting increasingly important as around here (New Zealand) we now have in some schools open-plan learning environments with 90 kids and three teachers. That\u2019s a lot of new names at the start of a school year. The real reason for this is\u2026 gently\u2026 I think turning on Guided Access is a bit confusing for a user who doesn\u2019t ready know what it is. So cutting through to helping the user trigger it as part of a workflow in-app would be amazing. I\u2019ve got to run to a lab appointment. Thanks for the great work your team does. Super appreciate it. Guided Access requires manual intervention so I agree it doesn't sound perfect for your use case. Autonomous Single App Mode seems more appropriate here. Thanks for hanging out in the lounge with us, enjoy your lab! I was playing with guided access today and noticed \"Time limits\" in guided access settings. It only lets me set a sound for a time limit ending. How do I set duration for a time limit? (for autonomous single access mode, where guided access is entered by triple tap) Hello Jaime! :wave::skin-tone-3: When you triple-click to bring up the Guided Access \"workspace\", you can set the time limit in the options menu there. Got it! Thank you We have those options in the Guided Access workspace because you may want a different time limit depending on the app, and we want it to be easily accessible rather than making you head back to Settings. Thanks for your question! :smile: How do I find the \"Work space\" on my iPhone (so I can set a time limit)? Hey Thomas, first turn on Guided Access in Settings > Accessibility > Guided Access. Then open the app which you want to lock on to. Then triple-click to perform the accessibility shortcut. Depending on your device, it may be the home button if it has one, or the sleep/wake button. This is where you configure options for Guided Access before you begin, and Time Limit is an option in this menu. Here is more information about the Accessibility Shortcut in general if you'd like to know more https://support.apple.com/en-us/HT204390 Got it. Big thanks! Time-limit is pretty cool; :slightly_smiling_face: Do you have any favorite apps that use autonomous single app mode? I don\u2019t know if I would call it a favorite, but we do see a organizations making testing apps use ASAM mode effectively. When the test time starts, the device locks down, then releases access at the end of test time. There\u2019s an 8 hour limit on ASAM mode, so it can\u2019t be run forever either I\u2019m thinking more is there an app you think of that does well in guiding someone in using ASAM for the first time, or one that is clever in how it handles restrictions for guided access Ah, sorry. I do not know specific apps Is there a way to test more advanced accessibility features using XC UI tests? eg custom accessibility actions, custom accessibility content, custom rotors? Thanks for the question <@U03J4DPCXGC>! We don't have a great way to test this in XCUITests at the moment, but please file a request for this through Feedback Assistant! Who uses \"Automatic Assessment Configuration\"? Do teachers comfortably use it or is it only for IT? Apps that implement the Automatic Assessment Configuration frameworks can create easy-to-use UI to enter a testing session for folks like teachers to use. We also offer wide-scale solutions for teachers managing a classroom full of devices called Apple Classroom. There is a lot of information about this topic, more can be found at the link below and I recommend watching past WWDC sessions about assessment mode in particular if you are still interested. https://support.apple.com/en-us/HT206151 In an iOS app, I have a status label in the UI but it's too long and changes too quickly to be appropriate for VoiceOver. Instead I want to announce the beginning and end of a long process, e.g \"Preparing ...\", \"Completed ...\" to VoiceOver (triggered by a user action so not out of the blue). What would be the API to trigger such announcements? you can post an announcement notification using <http://UIAccessibility.post|UIAccessibility.post> , and passing in a notification type of announcement, and a string that you want to have spoken https://developer.apple.com/documentation/uikit/1615194-uiaccessibilitypostnotification?language=objc https://developer.apple.com/documentation/uikit/uiaccessibilityannouncementnotification?language=objc Perfect. Thanks! I'm building a drawing app using PencilKit. What kinds of things can I do to make it accessible using technologies like VoiceOver? Is PencilKit going to do a bunch of heavy lifting for me in a situation like this? Any UI that you're getting from PencilKit should get AX for free, like the palette controls etc. As far as accessibility work for the canvas, you'll need to asses what your users are going to use your app for, and build in accommodations around those use cases. Maybe that's announcement notifications if you're doing some sort of guided drawing to tell people if they are on track. It's always good to talk to blind users of your app if you can, and see what feedback they have. If you have more specific questions, feel free to post a question on the developer forums and tag it with accessibility, as the lounge closes soon!","title":"accessibility"},{"location":"wwdc22/accessibility-lounge.html#accessibility-lounge-qas","text":"","title":"accessibility-lounge QAs"},{"location":"wwdc22/accessibility-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/accessibility-lounge.html#will-this-plug-in-work-well-for-fast-paced-games-like-fps","text":"Certain aspects of this plugin are wellsuited to fast paced games like FPS. Forexample, reacting to users settings forlarge text and dynamic text (e.g. in yourmenus or HUD).","title":"Will this plug-in work well for fast-paced games (like FPS)?"},{"location":"wwdc22/accessibility-lounge.html#what-would-you-say-is-the-biggest-challenge-of-adding-accessibility-to-unity-games","text":"Depending on your game, it may be hard to find what could be made accessible to your players. I hope this plugin can enable you to do much more than previously available Certainly before this plugin it was making anything accessible to VoiceOver. I think now the challenge will be more in how you design your game to accommodate people of different abilities","title":"what would you say is the biggest challenge of adding accessibility to unity games?"},{"location":"wwdc22/accessibility-lounge.html#awesome-work-on-the-unity-plugin-i-cant-wait-to-use-it-while-the-session-demod-voiceover-use-with-the-plugin-is-it-safe-to-assume-this-will-work-with-voice-control-as-well-theres-that-old-overlap-of-accessibility-allowing-convenience-for-those-that-want-it-and-itd-be-nice-to-allow-players-to-do-so-with-additional-voice-inputs-etc","text":"If you expose accessibility elements via this plugin it will be available to all the accessibility technologies, including Voice Control. <@U03HHHDSD2N> Thank you for the answer! That's great to hear - can we add the equivalent of accessibilityUserInputLabels modifier to elements using the plugin? For now we can't. We will incorporate this feedback so we could consider this in the future No worries. This is fantastic and I'll be sure to give it a try soon this week If you'd like to provide more feedback, please use this link on GitHub https://github.com/apple/unityplugins/blob/main/Documentation/Feedback.md","title":"Awesome work on the Unity plugin - I can't wait to use it! While the session demo'd VoiceOver use with the plugin, is it safe to assume this will work with Voice Control as well? There's that old overlap of accessibility allowing convenience for those that want it and it'd be nice to allow players to do so with additional voice inputs etc."},{"location":"wwdc22/accessibility-lounge.html#should-swiftui-sortpriority-modifier-on-views-work-also-for-views-that-dont-share-the-same-superviewancestor","text":"Yes it should. accessibilitySortPriority should work for any views within the same accessibility container, defined by accessibilityElement(children: .contain) modifier.","title":"Should SwiftUI .sortPriority modifier on Views work also for Views that dont share the same superview/ancestor?"},{"location":"wwdc22/accessibility-lounge.html#what-are-some-of-the-best-tips-and-resources-for-designing-and-coding-for-large-accessibility-size-fonts-at-the-largest-sizes-this-seems-to-be-very-difficult-to-do-well","text":"Hi Klayton, a lot of this likely starts at the design stage. The best high-level recommendation we tend to give is to allow for any given label in a design to grow significantly, both in content and in size. In many cases this involves making content scrollable, or adapting horizontal layouts to become vertical at a particular text size. For the latter, I highly recommend checking out ViewThatFits, which is a new view in SwiftUI this year that lets you specify variations of views, allowing the framework to pick the best one depending on the available size. I\u2019ve also got some links that I highly recommend checking out if you haven\u2019t already: an app challenge from last year, a couple WWDC sessions, as well as the \u201cText display\u201d section in our Human Interface Guidelines. If you have specific questions about making sure your app works great with Dynamic Type we\u2019d be happy to answer those too! https://developer.apple.com/news/?id=w6r26g7r https://developer.apple.com/videos/play/wwdc2017/245/ https://developer.apple.com/videos/play/wwdc2020/10020/ https://developer.apple.com/design/human-interface-guidelines/foundations/accessibility#text-display The upcoming talk on SwiftUI's new custom layout system will be helpful too. https://developer.apple.com/videos/play/wwdc2022/10056 Thanks. That's helpful. I really wish some of these new layout tools were available in earlier OS versions or made available to the same set of devices as iOS 15. For a single word that can't fit on one line, such as someone's name, does a streaming scroll (marquee) element seem better than breaking up that word into multiple lines? Should it require an app-setting toggle to avoid motion issues for people sensitive to motion? I would say multiple lines are much more preferable than implementing marquees","title":"What are some of the best tips and resources for designing and coding for large accessibility size fonts?  At the largest sizes, this seems to be very difficult to do well."},{"location":"wwdc22/accessibility-lounge.html#can-swiftui-support-tab-key-switching-to-different-ui-components-and-use-space-key-to-trigger-default-actions-of-the-ui-compoments","text":"Thanks for your question. Currently SwiftUI controls can be added to tab key loop with the .focusable modifier, thus also being able to be triggered by space key. Hi Eric, do you have example code for this? I have tried .focusable() for my image button. However, when press space key for it, the Button action didn\u2019t get hit? Does it also work for Link. Toggle \u2026 We have a new sample project Food Truck this year https://developer.apple.com/documentation/swiftui/food_truck_building_a_swiftui_multiplatform_app Button with .focusable modifier should be able to be activated with Space key. If not, we'd love a Feedback! .focusable() should also work for Link and Toggle:smiley:","title":"Can SwiftUI support tab key switching to different UI components and use space key to trigger default actions of the UI compoments?"},{"location":"wwdc22/accessibility-lounge.html#what-attracted-you-to-working-at-apple-and-specifically-on-accessibility","text":"For me personally, 1. I believe Apple is the best company to work for if you want to change the world and make it a better place with technologies 2. Solutions/products that help people with disabilities could be one of the most impactful things you could do with a known set of inputs/resources 3. Problems in accessibility are really really hard problems, and I get personal satisfaction solving them :slightly_smiling_face: I happened to meet someone who worked on this team when I was in school and managed to get an interview for an internship. I didn't get the internship, but I replied back a year later to see if they wanted interns for the next year, because I knew it'd be such a fit for me. The thought that someone was working on accessibility full time was so cool to me, and I knew it was something I wanted to do. I get to make stuff, basically for myself, I don't think I could find a cooler job :sunglasses: I've used Apple products since I was 10 years old and Steve Wozniak taught my 5th grade class how to use a Mac laptop. Having been born legally blind, accessibility has always been a passion of mine as it helps improve my quality of life. Since age 10, I've been all Apple, and I wanted to be a part of, and share with others, the amazing things that our technology has done to improve my life. I joined pretty much right from university. I was interested in accessibility research at the time and Apple just happened to need help making a newish product accessible (the iPhone). Testing and QA is what I already enjoyed doing. I\u2019ve went most of my professional life searching for that missing additional element and I found it at Apple. I realized that found it more meaningful and fulfillment helping others by helping others by ensuring that our Accessibility features are always as expected. In addition, as a disabled person myself, I am able to provide quality feedback on how to improve or suggest new features that will make life easier for other physically challenged people like myself I\u2019ve been using a Mac in one form or another since the 80s, and at every step of my vision loss, the product was there to meet me where I was at. The opportunity to work side by side with the people who created a huge part of my personal independence was a powerful dream. I have had this goal since I was in the 8th grade. You know those assignments \u201cWhat do you want to do when you grow up?\u201d Well, I still have the paper from that 8th grade exit assignment, and it says in my handwriting that I wanted to work for Apple. Decades later I wouldn\u2019t change a thing.","title":"What attracted you to working at Apple, and specifically on accessibility?"},{"location":"wwdc22/accessibility-lounge.html#what-are-some-example-scenarios-of-where-you-could-tell-a-developer-tried-to-meet-your-accessibility-needs-but-missed-something-are-there-patterns","text":"Yeah, as a low vision user, the most common pattern I've noticed is in Large Text support. It often doesn't exist, or if support does happen, it is often not in a readable way. I think sometimes when developers make assumptions about what I need they can sometimes miss the mark. Just because I have low vision doesn't mean I need a totally separate design. When I see apps present a totally different design for VoiceOver users, I am always a bit disappointed. We have so many tools to make almost any design accessible, and people want to enjoy those experiences similarly to how their able bodied friends and peers are experiencing them. Implement APIs to support built in settings we've provided instead, and you'll be headed in the right direction One thing I notice is when a developer uses custom UI with several elements on screen, they tend to forget about making every single element accessible. For example, most of the important elements would be scannable with item mode and the ones at the bottom such as sorting items in a list buttons won\u2019t be accessible","title":"What are some example scenarios of where you could tell a developer tried to meet your accessibility needs but missed something?  Are there patterns?"},{"location":"wwdc22/accessibility-lounge.html#should-voiceover-order-be-always-top-left-to-bottom-right-in-ltr-language","text":"Yes, unless a particular UIView has its semanticContentAttribute set to .forceRightToLeft or if a SwiftUI View has its layoutDirection environment set to RTL In some specific cases like two columns content, navigating down the first column before jumping to the second column may make more sense than a strict top-left-bottom-right.","title":"Should Voiceover order be always top-left to bottom-right in LTR language?"},{"location":"wwdc22/accessibility-lounge.html#for-third-party-apps-that-are-clearly-at-least-making-an-effort-to-be-accessible-what-are-the-most-common-accessibility-omissions-or-errors-do-you-see-developers-making","text":"The most common issues I see when we do app audits are bad accessibility labels and improper groupings for VoiceOver users. Labels should be concise, and shouldn't include redundant information like the role (like the word button). And items should be grouped logically. The best advice I give developers is, think about how the information is \"digested\" visually. Is this group of labels/images conceptually one item when you look at it? Then it probably should be for VoiceOver users too. This isn't something we think about a lot, it's all implicit in the apps we use every day, but taking a step back to notice these things help a lot Here's a great talk on writing good accessibility labels: https://developer.apple.com/wwdc19/254 Lack of support for Large Text sizes. I frequently see apps make an effort to be accessible with VoiceOver, but even apps that do a great job with that often do not have support for Large Text. Making a choice to use a cross platform development package without first investigating how accessibility will be implemented with that package frequently results in a really bad situation. Your customer might report an accessibility problem, and you may be unable to fix it if the frameworks don\u2019t support our accessibility features. This has been the case for countless apps I have encountered so far.","title":"For third party apps that are clearly at least making an effort to be accessible, what are the most common accessibility omissions or errors do you see developers making?"},{"location":"wwdc22/accessibility-lounge.html#for-andres-do-you-know-if-there-is-still-a-lack-of-assistive-technology-for-reading-scientific-publications-or-have-things-improved-for-everyone-are-there-any-remaining-challenges-specific-to-your-life-that-society-or-technology-could-do-a-better-job-of-addressing-how","text":"Thanks Keith. There have been lots of improvements in accessibility of scientific materials . MathML for instance allows VoiceOver users to read mathematical expressions on the web and other documents. There is still a lot to do in making math and science more accessible to students and professionals with visual disabilities. In particular conveying graphical information such as diagrams, function graphs, etc. is still an area of research and development.","title":"For Andres: do you know if there is still a lack of assistive technology for reading scientific publications, or have things improved?  For everyone: are there any remaining challenges specific to your life that society or technology could do a better job of addressing? (how?)"},{"location":"wwdc22/accessibility-lounge.html#how-do-header-traits-and-levels-work-on-macos-i-didnt-see-a-header-rotor-option-available-like-on-ios","text":"So in VoiceOver on macOS the headings rotor will be found using the VoiceOver modifier keyboard shortcut + the U key. This will pop open the available rotors and if you're on an element which has a headings rotor you can use the left and right arrow keys to find it in the rotor menu. The headings rotor will include the level of the heading when announcing each heading.","title":"How do header traits and levels work on macOS? I didn\u2019t see a header rotor option available like on iOS."},{"location":"wwdc22/accessibility-lounge.html#is-it-possible-to-emulate-a-switch-control-in-either-the-simulator-or-via-an-external-bluetooth-keyboard","text":"Great question. Yes, Switch Control can be configured to respond to external Bluetooth keyboard events! Make sure your keyboard is paired to your iOS or iPadOS device, then head to Settings > Accessibility > Switch Control > Switches > Add New Switch. Here you get to choose the switch source, so tap External. Each key on your keyboard could be its own switch, but you have to add them one by one. When you activate a key on your keyboard, Switch Control will ask you to name it and then assign it to an action, this helps you keep track of all the switches you add. If you're able, I recommend setting up at least three switches if you're using a keyboard with Switch Control: Select Item, Move to Next Item, and Move to Previous Item. For these you could use the space bar and arrow keys respectively! Thanks Drew! This will make switch control development much easier Glad to hear it will help! :tada:","title":"Is it possible to emulate a switch control in either the Simulator or via an external Bluetooth Keyboard?"},{"location":"wwdc22/accessibility-lounge.html#can-you-give-any-advice-to-making-a-text-view-with-a-maximum-character-limit-more-accessible-especially-with-voiceover-visually-it-shows-a-counter-eg-90100-and-highlights-text-over-the-limit-red","text":"You can override/set accessibilityValue on the text view to return the entire text. For the character limit, you could post an announcement notification when the user starts to go over the limit that says something like \"character limit reached\". You can also add a hint to the text field that conveys the max number of characters. Also make sure that your counter of 90/100 is an accessibility element, so that VO users can focus on it manually if they want to know the character count","title":"Can you give any advice to making a text view with a maximum character limit more accessible, especially with VoiceOver? Visually it shows a counter (eg: 90/100) and highlights text over the limit red."},{"location":"wwdc22/accessibility-lounge.html#in-swiftui-macos-12-how-can-i-move-the-voice-over-focus-to-a-presented-view","text":"Accessibility Focus State is gonna be the way go here! You can change the state value to true on an element and it will update VoiceOver focus to that element! https://developer.apple.com/documentation/swiftui/accessibilityfocusstate|https://developer.apple.com/documentation/swiftui/accessibilityfocusstate Does it make sense to change the focus when showing a new view in place of the old one on macOS? We have a list layout, clicking one of the items \"pushes\" a new view similar to a navigation controller. Currently VO loses focus and moves elsewhere. So if voiceover is really not going where you want in this instance I think you could do that. I would warn about using this too often though because jumping around the voiceover focus can be very distracting. If its a totally custom navigation system than in that instance you probably want to either do that or trying to post a layout change notification. Usually though we either let the system framework and voiceover to decide where best to go, or if your UI has a popup or an alert thats a great way to use focus state. I guess there\u2019s a similar property as AccessibilityFocusState in NSAccessibility? I am having some issues to keep the element selected with tabulator keyboard navigation and VoiceOver focus in sync because I have to move the tab focus programmatically in some cases (for example when moving out of a WKWebView to some custom window UI elements below). > trying to post a layout change notification. Will posting a layout notification without passing in a view work? Or is there a way of doing this in SwiftUI? So if you change keyboard focus you can send a notification with NSAccessibilityFocusedUIElementChangedNotification to let voiceover know the focus changed. If you want to change voiceover focus from non-keyboard focus changes you can use NSAccessibilityLayoutChangedNotification with a NSAccessibilityUIElementsKey https://developer.apple.com/documentation/appkit/1534572-nsaccessibilitypostnotificationw?language=objc In that instance <@U03JB2UUT46> if you want to pass a specific element you would use focus state because there isn't really a view to pass it in SwiftUI Thanks <@U03HKTXJ4KE>, I\u2018ll try those notifications. I actually might need LayoutChangedNotification because the elements I\u2018m programmatically focusing are in a different window, so maybe the NSAccessibilityUIElementsKey is needed to be passed. I had a hard time to be able to access window elements in the title bar (if no NSWindowToolbar is used), in the toolbar if it is visible, web content in the WKWebView and those additional UI elements in a separate \u201eDock\u201c window below, but in the end it worked with adding views like NSWindow contentView.superview, contentView and the contentView of the separate window to NSWindow accessibilityChildren. Don\u2018t know if there\u2018s a better way, but it seems to work correctly now.","title":"In SwiftUI &amp; macOS 12 How can I move the voice over focus to a presented view?"},{"location":"wwdc22/accessibility-lounge.html#joining-a-team-without-any-real-accessibility-support-what-would-you-all-consider-top-priority-support-id-love-to-do-it-all-but-know-that-proper-support-takes-time-and-also-one-thing-cannot-be-more-impactful-than-another-they-are-all-equally-important-how-would-you-consider-breaking-down-a-project-into-incremental-wins","text":"I usually say 1. Dynamic Type support will reach the largest audience 2. Add App Accessibility support will be most impactful for users who need to use ATs (Assistive technologies) - like VoiceOver,Switch Control, Voice Control \u2014 because often if app accessibility is NOT provided - they will have no way to access your app Other areas like checking for contrast, transparency, motion will have large impacts IF your app has areas that might be problematic (aka - you have light grey text on white backgrounds) and are usually not that much work to implement True true, I want to be sure our app is impactful for everyone so taking the small wins were we can. Thanks for the callouts here!","title":"Joining a team without any real accessibility support, what would you all consider top priority support? I'd love to do it all but know that proper support takes time and, also, one thing cannot be more \"impactful\" than another, they are all equally important. How would you consider breaking down a project into incremental wins?"},{"location":"wwdc22/accessibility-lounge.html#how-are-swipe-buttons-on-list-items-regarded-from-a-general-discoverability-aspect-and-then-more-specifically-for-voiceover-i-have-a-list-ui-thats-more-of-a-fake-tree-view-with-different-types-of-content-on-most-rows-there-are-correspondingly-different-swipe-buttons-on-leading-trailing-currently-have-no-affordances-hinting-at-them-even-for-sighted-users-the-lovely-new-hig-doesnt-even-mention-them-as-they-got-significant-improvements-in-swiftui3-i-assumed-they-are-still-blessed-as-a-ui-approach","text":"Swipe items are exposed to assistive technologies via 'custom actions'. They typically should be exposed automatically if you are using system UI controls. https://developer.apple.com/videos/play/wwdc2019/250/ https://developer.apple.com/documentation/uikit/uiaccessibilitycustomaction For VoiceOver specifically, these swipable actions are exposed in the 'Actions' rotor.","title":"How are swipe buttons on list items regarded from a general discoverability aspect and then more specifically for VoiceOver?  I have a list UI that's more of a fake \"tree view\" with different types of content on most rows. There are correspondingly different swipe buttons on leading &amp; trailing. Currently have no affordances hinting at them even for sighted users.  The lovely new HIG doesn't even mention them. As they got significant improvements in SwiftUI3 I assumed they are still \"blessed\" as a UI approach."},{"location":"wwdc22/accessibility-lounge.html#are-there-plans-to-expand-the-new-eye-gaze-access-in-ipad-os-to-allow-apps-to-directly-access-the-high-fidelity-eye-gaze-data-and-opt-out-of-translating-eye-gaze-data-into-assistive-touch-events","text":"Anything is possible. Are you able to explain your use cases in more detail? It may be useful to file a feedback as well so we can look into this in the future.","title":"Are there plans to expand the new eye gaze access in iPad OS to allow apps to directly access the high fidelity eye gaze data and opt out of translating eye gaze data into assistive touch events?"},{"location":"wwdc22/accessibility-lounge.html#when-supporting-dynamic-typelarge-text-sizes-with-custom-fonts-is-there-a-good-way-to-limit-the-maximum-size-a-font-can-be-or-is-this-just-a-bad-idea-in-general-use-case-while-we-are-working-on-supporting-dynamic-type-weve-noticed-that-we-can-support-many-of-the-larger-sizes-all-the-smaller-sizes-but-the-biggest-sizes-cause-some-pretty-serious-layout-breaks-throughout-its-going-to-take-us-a-while-to-chase-them-all-down-qa-id-love-to-roll-out-support-for-most-sizes-so-my-customers-can-use-it-earlier-those-who-need-the-largest-type-sizes-can-at-least-get-partway-there","text":"I suspect you\u2019d want to use UIFontMetrics to help here `- (UIFont *)scaledFontForFont:(UIFont *)font maximumPointSize:(CGFloat)maximumPointSize; ` In general, we would like to NOT cap font sizes, however there are cases within the UI that are very hard to grow given constraints. In iOS and iPadOS we will often use UILargeContentViewer (UIKit/UILargeContentViewer.h) to allow a long press to show text that\u2019s can\u2019t grow well In your case, it sounds like it would be reasonable to roll out support for larger sizes and overtime work to adopt the largest sizes. I'd reiterate that it's really not recommended to cap text sizes, unless the UI literally cannot grow without being unreasonable (like nav bars and tab bars, in which case use large content viewer like Chris said). Limiting text sizes will significantly impede low vision users from using your app properly. Definitely support what you can for now, but I strongly encourage you to work towards supporting all the sizes rather than capping them to a specific value! Yes! Supporting all font sizes is absolutely the plan! Just trying to make it more usable more quickly. Thank you for your answers!","title":"When supporting dynamic type/large text sizes with custom fonts, is there a good way to limit the maximum size a font can be? Or is this just a bad idea in general?  Use case: While we are working on supporting dynamic type, we've noticed that we can support many of the larger sizes &amp; all the smaller sizes. But the biggest sizes cause some pretty serious layout breaks throughout &amp; it's going to take us a while to chase them all down + QA. I'd love to roll out support for most sizes so my customers can use it earlier &amp; those who need the largest type sizes can at least get partway there."},{"location":"wwdc22/accessibility-lounge.html#hello-there-are-a-few-new-languages-added-to-voiceover-im-talking-specifically-about-persian-the-system-falsely-identifies-persian-texts-as-arabic-or-urdu-do-we-need-to-somehow-tag-the-texts-to-make-it-work-with-persian-voiceover-or-is-this-a-bug","text":"Hi Saeed, please file a bug using the https://feedbackassistant.apple.com|Feedback Assistant . Is this on macOS or iOS? I\u2019ll file it. I\u2019m testing on iOS. On iOS you can programmatically set the speech language ( UIAccessibilitySpeechAttributeLanguage for attributed strings, or by setting .environment(\\.locale, \u2026) in SwiftUI Texts) \u2026but I agree that you shouldn\u2019t have to do this. Some more detailed steps will help us get to the bottom of this :slightly_smiling_face:","title":"Hello. There are a few new languages added to VoiceOver. I'm talking specifically about Persian. The system falsely identifies Persian texts as Arabic or Urdu. Do we need to somehow tag the texts to make it work with Persian VoiceOver or is this a bug?"},{"location":"wwdc22/accessibility-lounge.html#are-there-any-specific-accessibility-api-propertiesmethods-we-should-use-for-full-keyboard-i-have-a-view-that-is-fully-interacts-with-voice-over-but-not-with-an-external-keyboard-i-started-using-the-accessibilityrespondstouserinteraction-boolean-and-that-seems-to-work-for-most-of-the-elements-but-not-all-not-really-sure-if-this-is-the-correct-api-property-to-use-in-this-case-there-is-not-much-documentation-out-there-for-full-keyboard-accessibility-thank-you","text":"You can also use the API in <UIKit/UIFocus.h>. FullKeyboardAccess uses the focus engine, so these may be used to customize the FKA experience. thanks, John! so, it seems like simply implementing accessibility for voice over does not automatically carry over to full keyboard? is there any specific accessibility property we should use for keyboards? where you thinking I should use canBecomeFocused ? Right it does not, we do try to respect accessibilityRespondsToUserInteraction and other accessibility methods like accessibilityElements but ultimately the focus engine is what determines the behaviour. Yup! canBecomeFocused is ultimately what the focus engine uses to decide what to focus. I was hesitant to use this boolean -- the wwdc '21 \"Support Full Keyboard Access in your iOS app\" session specifically calls out not to use canBecomeFocused as this has implications beyond accessibility in the app There was a WWDC 2021 talk that could be useful https://developer.apple.com/videos/play/wwdc2021/10120 do you have any other documentation or videos? that video does not fully cover what we should do besides adding custom actions. also, please see my comment above. we currently had an a11y audit on our app and this has come up. the team is having a hard time figuring out how to implement this properly","title":"Are there any specific accessibility API properties/methods we  should use for \"Full Keyboard\"? I have a view that is fully interacts with voice over but not with an external keyboard. I started using the accessibilityRespondsToUserInteraction boolean and that seems to work for most of the elements, but not all. NOt really sure if this is the correct api property to use in this case. There is not much documentation out there for full keyboard accessibility. thank you!"},{"location":"wwdc22/accessibility-lounge.html#uikit-apps-depend-on-dynamic-type-to-adjust-their-text-size-to-be-comfortable-for-different-users-when-these-apps-are-brought-to-the-mac-via-catalyst-is-there-a-recommended-approach-to-providing-scalable-ui-text-size-given-there-isnt-a-system-text-size-as-with-ios-and-ipados","text":"Short answer: not really. You could make your own text size slider in settings like Books that uses dynamic type under the hood. Users have a few AT options to help with this as well - Zoom, HoverText","title":"UIKit apps depend on Dynamic Type to adjust their text size to be comfortable for different users. When these apps are brought to the Mac via Catalyst, is there a recommended approach to providing scalable UI text size given there isn't a \"system text size\" as with iOS and iPadOS?"},{"location":"wwdc22/accessibility-lounge.html#in-the-simulator-you-can-enable-full-keyboard-access-in-settings-however-i-couldnt-find-a-way-to-actually-use-it-is-it-actually-possible-to-test-full-keyboard-access-in-the-simulator-and-if-so-how-thank-you","text":"This looks like it works to me in Seed 1 I see! I haven't tried in Xcode 14, but it is something that wasn't working for me in the iOS 15.5 simulator, for example. So you just turn that switch on and it should work right? No other steps needed. Thanks for your help! yep looks like it. and then pressing tab on your mac keyboard moves focus","title":"In the simulator, you can enable Full Keyboard Access in settings. However, I couldn't find a way to actually use it. Is it actually possible to test Full Keyboard Access in the simulator? And if so, how? Thank you!"},{"location":"wwdc22/accessibility-lounge.html#currently-on-macos-keyboard-accessibility-for-views-like-lazy-grids-is-difficult-to-implement-compared-to-views-like-nscollectionview-where-you-can-get-it-for-free-things-like-navigation-moving-selection-and-multi-selection-have-to-be-manually-implemented-which-is-a-very-difficult-and-heavy-task-and-easy-to-deviate-from-native-are-there-any-recommendations-for-how-we-as-developers-can-tackle-this-or-plans-to-make-this-easier","text":"Thanks for the feedback! We would love to hear difficulties that you encountered when implementing keyboard focus in lazy grids through Feedback Assistant. We also have a new SwiftUI sample app Food Truck that introduces new features and improvements that you could check out https://developer.apple.com/documentation/swiftui/food_truck_building_a_swiftui_multiplatform_app In addition, you could also join the SwiftUI lounge and Q/A sessions to get more of your questions answered for SwiftUI :slightly_smiling_face: Hi Eric! By \u201ckeyboard accessibility\u201d here I\u2019m not just referring to keyboard focus, but what happens inside a lazy grid when you press the arrow keys (or tab, actually.) Or are trying to multi-select within a grid (Shift + Arrow Key, or Shift + Option Key + Arrow Key). NSCollectionView and NSTableView can give you this out of the box (and you can see it in Finder or other native apps.) But this kind of keyboard accessibility \u2014 plus selection state handing \u2014 isn\u2019t available in SwiftUI out of the box as far as I can tell. Hopefully that clarifies my question! I hope we can make this more easier. We\u2019d love a Feedback!","title":"Currently on macOS keyboard accessibility for views like lazy grids is difficult to implement compared to views like NSCollectionView, where you can get it for free. Things like navigation (moving selection) and multi-selection have to be manually implemented, which is a very difficult and heavy task (and easy to deviate from native.)  Are there any recommendations for how we as developers can tackle this or plans to make this easier?"},{"location":"wwdc22/accessibility-lounge.html#my-goal-is-to-get-visibility-into-the-accessibility-elements-of-a-view-and-its-children-with-uikit-i-am-able-to-walk-the-view-hierarchy-in-code-and-log-all-the-accessibility-properties-of-a-view-and-its-subviews-what-would-be-the-equivalent-with-swiftui-the-goal-given-a-swiftui-view-log-all-the-accessible-elements-and-their-properties","text":"Thanks for your question! In what context are you trying to get visibility into the accessibility elements? Specifically during debug or some of your app's functionalities rely on this? <@U03HEKAJJBF> Good question. I am writing XCTests that snapshot the a11y tree for a few use cases: 1. snapshot the tree to use for regression testing (this is the major use case) 2. identify elements that are missing a11y 3. identify elements that are candidates to combine a11y Thanks for clarification! We currently don't support introspecting view hierarchies in SwiftUI views from within your process. The closest tools we have for now are Accessibility Inspector, Xcode Previews, and View Hierarchy Debug. We'd love a Feedback with your use case if you find you need more than what those tools currently offer :smiley: Ok. Unfortunately, the lack of introspection is preventing me from adopting SwiftUI since there is no way too automate a11y checks via XCTest framework.","title":"My goal is to get visibility into the accessibility elements of a view and it's children. With UIKit, I am able to walk the view hierarchy in code and log all the accessibility properties of a view and it's subviews. What would be the equivalent with SwiftUI? The goal, given a SwiftUI view, log all the accessible elements and their properties."},{"location":"wwdc22/accessibility-lounge.html#were-there-any-updates-to-the-accessibility-inspector-this-year","text":"We have some great improvements for accessibility audits on detecting non-accessible elements in Accessibility Inspector! Please try them and we are very happy to any feedback you have :smiley:","title":"Were there any updates to the Accessibility Inspector this year?"},{"location":"wwdc22/accessibility-lounge.html#how-do-all-of-the-examples-highlighted-in-this-talk-pertain-to-macos-safari-are-the-techniques-all-general-enough-to-apply-to-web-standard-ax-or-only-to-voiceover-ios-or-only-to-voiceover-on-apple-platforms-in-general","text":"Everything we went over today is supported on macOS Safari, too (along with other platforms like Watch and TV)! > Are the techniques all general enough to apply to web standard AX, or only to VoiceOver iOS, or only to VoiceOver on Apple platforms in general? These techniques definitely apply to assistive technologies of all types of Apple platforms. I'm not familiar with the level of support for these techniques in other browsers \u2014 https://caniuse.com might be helpful there. Thanks! The increment/decrement equivalents are accessible by touch or keyboard on macOS\u2026. Look up Mac VoiceOver\u2019s \u201cTrackPad Commander\u201d feature for touch gestures. To increment with a slider on macOS, you can use unmodified arrow keys in most scenarios. If the user wants to override the default key handling and force an \u201cincrement\u201d they\u2019d first \u201cinteract with\u201d slider (VO+Shift+Down) and then increment (VO+Up or VO+Right in LTR languages contexts)","title":"How do all of the examples highlighted in this talk pertain to macOS Safari? Are the techniques all general enough to apply to web standard AX, or only to VoiceOver iOS, or only to VoiceOver on Apple platforms in general?"},{"location":"wwdc22/accessibility-lounge.html#are-there-any-specific-guidelines-for-making-interfaces-accessibility-in-the-context-of-embedded-web-views-in-native-apps-does-webkitwkwebview-offer-any-mechanisms-separate-from-web-standards-to-interface-with-vo-for-example-on-an-ios-or-mac-device","text":"There are not \u2014 accessibility in embedded webviews should, in general, work just as if you had the same HTML loaded in Safari. Just remember to use semantic HTML and / or ARIA attributes on the markup in your embedded HTML :slightly_smiling_face: Great, thanks. Good session, sorry you didn't have more Q&A! Hope springs eternal ...","title":"Are there any specific guidelines for making interfaces accessibility in the context of embedded web views in native apps? Does WebKit/WKWebView offer any mechanisms separate from web standards to interface with VO for example on an iOS or Mac device?"},{"location":"wwdc22/accessibility-lounge.html#were-now-working-on-a-cross-platfrom-design-system-including-web-and-apple-platforms-we-want-to-use-this-project-to-elevate-accessibility-of-our-products-and-to-maintain-it-at-a-high-level-in-the-long-run-what-accessibility-standardsassistive-technologies-would-you-recommend-to-take-into-account-while-building-a-design-system-do-you-have-any-tips-to-make-this-project-a-success-in-terms-of-accessibility-both-on-web-and-on-apple-platforms","text":"Big question! If I'm reading this right, then I would start with making sure your design system is able to make all components accessible to the same level as the native platforms Then you need to transmute those paradigms in your system to the native platforms This is very similar to what we did with Unity accessibility plugins https://developer.apple.com/wwdc22/10151|https://developer.apple.com/wwdc22/10151 So there are Unity methods for everything from custom actions to dynamic type. Those get translated to the native iOS Thanks! :slightly_smiling_face: In addition to Chris' comments, I'll add: > What accessibility standards/assistive technologies would you recommend to take into account while building a design system? https://www.w3.org/TR/WCAG21/|WCAG has a lot of really great guidelines. If your design system includes web technologies, there are automated tools to test some aspects of accessibility, like: https://github.com/dequelabs/axe-core These tools can be run in a CI pipeline, allowing you to catch and fix accessibility issues as early as possible. As far as which assistive technologies to test, VoiceOver is a good one, since often (but not always) if a web app is accessible with VoiceOver it is also accessible with other assistive technologies. But you should still test others! Full Keyboard Access (and even general keyboard navigability) is a good one. Another one to test with is Voice Control. Thank you Tyler! I've just submitted another question precisely about automated testing solutions, just a second before seeing your reply! :slightly_smiling_face: Tyler Great minds think alike :smile: Also make sure your design system requirements account for the superset of all available accessibility features. There are API equivalents for many of these, like isReduceMotionEnabled for native Apple platforms and @media (prefers-reduced-motion) in Web. Likewise isDarkerSystemColorsEnabled vs @media (prefers-contrast: more) Not all features exist on all platforms though, so be sure to account for all in the design, and support the implementation where available.","title":"We're now working on a cross-platfrom design system (including Web and Apple platforms). We want to use this project to elevate accessibility of our products and to maintain it at a high level in the long run. What accessibility standards/assistive technologies would you recommend to take into account while building a design system? Do you have any tips to make this project a success in terms of accessibility both on Web and on Apple platforms?"},{"location":"wwdc22/accessibility-lounge.html#search-bars-now-usually-do-not-require-hitting-submit-to-search-the-results-get-updated-automatically-along-with-the-user-typing-in-their-request-would-you-say-its-a-good-practice-to-make-voiceover-announce-the-number-of-results-as-the-user-types-in-their-search-request-like-5-items-found-or-no-items-found","text":"Yes it should be announced but not immediately. There should be a period of quiescence so that your announcements are not interrupting typing feedback Thank you! You\u2019ll get that announcement-without-interruption for free if you use an ARIA \u201clive region\u201d for the \u201c5 items found\u201d announcement. You don\u2019t always need to use WebSpeech or SSML for general user interface notifications. We had a WWDC session cover live regions years ago, but it may be difficult to find now. The incomplete gist of a live region is: `&lt;div role=\"status\"&gt;As the contents of this element are changed, screen readers will announce the new text automatically.&lt;/div&gt; ` A little more here: https://www.w3.org/TR/wai-aria/#attrs_liveregions","title":"Search bars now usually do not require hitting \"Submit\" to search, the results get updated automatically along with the user typing in their request.   Would you say it's a good practice to make VoiceOver announce the number of results as the user types in their search request? Like \"5 items found\" or \"No items found\"?"},{"location":"wwdc22/accessibility-lounge.html#are-there-any-solutions-for-automated-testing-of-accessibility-would-you-recommend-apple-ui-testing-framework-for-these-purposes","text":"I know Benjamin has read this in the thread above, but for posterity I'll post it here too: https://github.com/dequelabs/axe-core This tool (and others like it) can be used to automatically catch certain types of accessibility errors in your web apps, and can be run as CI pipelines to catch errors as early as possible. These tools don't catch every accessibility issue, so make sure to still test with actual assistive technologies like VoiceOver :slightly_smiling_face: May I humbly suggest my library for XC UI testing native iOS apps :slightly_smiling_face: https://github.com/rwapp/A11yUITests (obvs not endorsed by Apple in any way) Can't miss an opportunity to say thank you to you <@U03J4DPCXGC> for all your amazing work in this area! We refer to https://mobilea11y.com/ a lot when it comes to accessibility on Apple platfroms! :heart:","title":"Are there any solutions for automated testing of accessibility? Would you recommend Apple UI testing framework for these purposes?"},{"location":"wwdc22/accessibility-lounge.html#when-adding-support-for-dynamic-type-are-there-any-guidelines-or-recommendations-on-when-its-best-to-respect-the-system-settings-vs-offering-a-screen-scoped-font-size-control-for-example-reader-apps-such-as-safari-and-books-offer-independent-font-size-controls-vs-podcasts-and-mail-do-not-the-pattern-i-see-is-apps-or-features-where-reading-content-is-the-primary-experience-which-i-personally-have-used-vs-changing-the-system-wide-setting-but-from-my-earlier-example-mail-could-also-be-classed-a-reader-app-thinking_face-thank-you","text":"Good question! I think your intuition is right here. For longer form reading content, some people prefer to adjust font size on the fly, as lighting conditions change or their eyes tire. For apps that are not for long form reading content, these controls are really not recommended in my opinion. As of iOS 15, we have per app settings where users can set font sizes per app, and they can be adjusted quickly with our control center module. So users already have a way to quickly adjust text sizes on the fly should they require it. Users are expecting to open your app and see the text size they have set globally (or per app if they've adjusted it), so adding a second control to your app just introduces unnecessary complexity","title":"When adding support for dynamic type, are there any guidelines or recommendations on when it\u2019s best to respect the system settings vs offering a screen scoped font size control?  For example reader apps such as Safari and Books offer independent font size controls vs Podcasts and Mail do not.   The pattern I see is apps or features where reading content is the primary experience (which I personally have used vs changing the system wide setting) - but from my earlier example Mail could  also be classed a reader app :thinking_face:  Thank you!"},{"location":"wwdc22/accessibility-lounge.html#is-it-possible-to-use-attributed-accessibility-label-features-language-spell-out-phonetic-notation-pitch-etc-in-swiftui-thank-you","text":"Hi Rob, there are several speech modifiers available for use with Text in SwiftUI, like speechAdjustedPitch , speechAlwaysIncludesPunctuation , speechAnnouncementsQueued , and speechSpellsOutCharacters . For changing language, you can override the locale environment on a Text view and it should do the trick. Sweet, I have never noticed these before. Thank you! Also note that since iOS 15, Text can natively support attributed string via both new markdown syntax \" Text(\"this is **bold **\") and AttributedString . So if you use those it should also work","title":"Is it possible to use attributed accessibility label features - language, spell out, phonetic notation, pitch etc - in SwiftUI? Thank you :)"},{"location":"wwdc22/accessibility-lounge.html#when-we-started-adding-accessibility-modifiers-to-our-swiftui-views-we-noticed-that-our-xctest-ui-tests-got-broken-the-tree-of-elements-changed-some-ui-elements-impacted-by-accessibility-modifiers-became-unaccessiblenon-hittable-we-didnt-find-a-way-to-use-accessibility-modifiers-and-keep-all-the-elements-accessible-for-the-purposes-of-ui-tests-is-it-expected-we-ended-up-creating-custom-modifiers-that-would-not-apply-accessibility-modifiers-in-the-ui-test-environment-is-it-the-best-solution-at-the-moment","text":"Thanks for your question! Accessibility modifiers do affect XCUITests because it may change accessibility elements tree. Currently your solution would be the best at this moment. We'd really love a Feedback from your specific case! If you would submit a Feedback and attach your example to us, we can see how we could do better! Got it, thank you!","title":"When we started adding accessibility modifiers to our SwiftUI views we noticed that our XCTest UI tests got broken. The tree of elements changed - some UI elements impacted by accessibility modifiers became unaccessible/non-hittable.   We didn't find a way to use accessibility modifiers and keep all the elements accessible for the purposes of UI tests. Is it expected?   We ended up creating custom modifiers that would not apply accessibility modifiers in the UI test environment. Is it the best solution at the moment?"},{"location":"wwdc22/accessibility-lounge.html#is-it-possible-to-turn-off-the-reading-of-mkmapview-map-poi-in-voiceover","text":"Hi Lior, thank you for your question! It is not possible with MKMapView; however if you subclass it, you might be able to by overriding accessibilityElements. Give that a try and let us know! Thank you for your answer, will do! There are several OOTB accessibility elements that I wish to disable (f.e. Legal view) their voiceover, hopefully subclass will work.","title":"is it possible to turn off the reading of MKMapView map POI in Voiceover?"},{"location":"wwdc22/accessibility-lounge.html#after-an-amazing-accessibility-design-review-am-busy-cringe-testing-with-voice-over-i-think-this-is-an-apple-bug-not-mine-the-permission-alert-on-accessing-the-camera-doesnt-read-out-the-text-explaining-why-should-i-worryfix","text":"Hi Andrew! If this alert is coming from the system, there shouldn't be anything you need to do to make it accessible. If you're finding it's not, please file a bug to us via Feedback Assistant. Note that the heading of the alert and the body are likely separate accessibility elements, and a user must move focus to the body in order to have it read, it won't all read automatically. This is by design After a solid half hour of testing, I think the problem may have been that it was iOS14 and/or it was my not understanding how to move through things with VoiceOver. I may have been doing a swipe down rather than left/right, to try to advance item. Anyway, I now am much more practiced at VoiceOver, have an even healthier respect for the people who designed it, and can not reproduce the problem. (It's also really hard to reliably trigger the state of Application has not yet been authorised for camera - if you reboot the device whilst the alert is in front, it defaults to being disabled, not left ambiguous!) Glad you were able to get more familiar with VoiceOver! Hah! I just replicated this and it was indeed my misunderstanding voiceover. I'll record a little video as a reminder but the short version is: 1. I thought that swipe down advanced to the next control 2. Whilst a slow swipe down will take you through the elements of the alert, a fast swipe down will move the focus from the title to the button you end up on, without having time to say the sub-title I managed to trigger the application state on a test device where it would show this alert then another 10 minutes of playing around to realise the above sequence is what I'd been doing Hmmmm I spoke too soon, maybe it's a thing about alerts. The swipe right for Select the next item doesn't work in them. what happens when you swipe right? It should be a quick swipe to the right (a flick) and that'll move to the next item flicks up/down will activate whatever the current rotor item is OK, yeah I was trying to do that. I will have to get back to the state where the \"Blah would like to access the camera\" alert is shown and re-record with touches captured. It seems that accessibility swipe right behaviour indeed doesn't behave normally no that particular alert ahh Rotor was on Characters not Containers I'll dive into this with more testing when have more time and log an issue if I prove out a problem. I'm confused by the Rotor, now I have it appearing. There are a lot more options on https://support.apple.com/en-au/HT204783 than I see. My rotor only has Headings Characters Words Speaking Rate Containers The rotor options are dynamic based on the type of control you're on, but you can also add a bunch more in VoiceOver settings","title":"After an amazing Accessibility Design Review am busy cringe-testing with Voice Over.  I think this is an Apple bug, not mine - the permission alert on accessing the camera doesn't read out the text explaining why  Should I worry/fix?"},{"location":"wwdc22/accessibility-lounge.html#what-is-the-recommended-accessible-way-for-infinite-scroll-in-a-feed","text":"Hi Shahla! I don't think there's a ton of special considerations you need to make for infinite scroll views. Assistive Technology users can typically discover and interact with these similarly to mainstream users. Just make sure that the content inside your scroll view is accessible! VO users can navigate via swipe and tap exploration as well. If you have specific questions about your scroll view as it pertains to accessibility, feel free to clarify your question here. :slightly_smiling_face:","title":"What is the recommended accessible way for infinite scroll in a feed?"},{"location":"wwdc22/accessibility-lounge.html#how-to-fix-the-issue-about-swiftui-textfield-with-too-small-hit-area-httpsdeveloperapplecomforumsthread703257httpsdeveloperapplecomforumsthread703257","text":"This looks like a bug for Accessibility Inspector, I have reported this feedback to our engineers. Thanks for the question! <@U03HEKAJJBF>, but if we want a more significant hit area, the current SwiftUI didn't support it yet. Is that right?","title":"How to fix the issue about SwiftUI Textfield with too small hit area? https://developer.apple.com/forums/thread/703257|https://developer.apple.com/forums/thread/703257"},{"location":"wwdc22/accessibility-lounge.html#if-i-have-a-sectioned-grid-of-items-whats-the-best-way-to-expose-those-sections-for-easy-navigation-via-voiceover-in-swiftui","text":"Thanks for the question! One thing you could do is to make sure the section headers have \"header\" accessibility traits via accessibilityAddTraits(.isHeader) The second thing you could do is check out the accessibilityRotorEntry API to provide custom rotors for VoiceOver. This can allow VoiceOver users navigate your app more efficiently. Thanks Eric!","title":"If I have a sectioned grid of items, what's the best way to expose those sections for easy navigation via Voiceover in SwiftUI?"},{"location":"wwdc22/accessibility-lounge.html#if-this-is-a-good-place-to-ask-implementation-questions-i-have-identified-whats-happening-with-the-screen-that-was-called-out-in-my-accessibility-review-lab-i-have-a-page-background-page-with-a-segmented-control-at-the-top-and-a-bunch-of-stuff-that-is-hidden-depending-on-choosing-photo-or-color-background-the-visibly-hidden-items-are-not-hidden-from-voiceover-and-so-its-selecting-them-as-you-move-through-page-items-how-do-i-hide-them-from-voiceover","text":"You can use setAccessibilityElementsHidden in UIKit or .accessibilityHidden() in SwiftUI https://developer.apple.com/documentation/objectivec/nsobject/1615080-accessibilityelementshidden?language=objc Just looked at how my UIKit code works. I have a hierarchy inside my main view (xib) UISegmentedControl UIScrollView UIView UIView backgroundImage UIView backgroundColor UIView backgroundShader I'm just toggling backgroundImage.isHidden (etc) and relying on that to hide all the innermost controls what view contains the controls? you'd want to set accessibilityElementsHidden on the view that contains all the controls you don't want VoiceOver to see yeah I'm just trying that. Sorry for not being more explicit - the three named views in my little diagram contain all the controls (it's a complex screen, as are too many of the detail editors) Flicking between this and the alert thread ... I've actually replicated the lab problem as well and confirm you were right - it's speaking the \"previous screen items\" which is just weird. but have an added nuance from testing further (after adding the accessibilityElementsHidden code and then removing it) If you tap the segmented control to change to the other elements being visible, VoiceOver is now correctly going through the elements of the page. Tapping it again to change back, it is still correct. It seems that the explicit toggling of isHidden on the parent views is enough to fix things for VoiceOver but relying on the initial isHidden from the xib, at view load, is not. I'm loading these detail editor VCs with func hostVC(_ vc:UIViewController) { for child in children { child.willMove(toParent: nil) child.view.removeFromSuperview() child.removeFromParent() } addChild(vc) vc.view.frame = view.bounds vc.view.translatesAutoresizingMaskIntoConstraints = false view.addSubview(vc.view) vc.view.leftAnchor.constraint(equalTo: view.leftAnchor).isActive = true vc.view.rightAnchor.constraint(equalTo: view.rightAnchor).isActive = true vc.view.topAnchor.constraint(equalTo: view.topAnchor).isActive = true vc.view.bottomAnchor.constraint(equalTo: view.bottomAnchor).isActive = true vc.didMove(toParent: self) }","title":"If this is a good place to ask implementation questions, I have identified what's happening with the screen that was called out in my accessibility review lab. I have a Page Background page with a segmented control at the top and a bunch of stuff that is hidden depending on choosing Photo or Color background.  The visibly hidden items are not hidden from VoiceOver and so it's selecting them as you move through page items. How do I hide them from VoiceOver?"},{"location":"wwdc22/accessibility-lounge.html#is-there-a-list-of-the-upcoming-languages-supported-by-voiceover-also-can-we-have-a-way-to-programmatically-ask-the-accessibility-framework-what-languages-are-currently-supported-were-running-into-an-issue-where-in-visual-text-we-support-vietnamese-but-voiceover-doesnt-wed-prefer-voiceover-to-use-a-fallback-like-english-in-that-case","text":"So for this year VoiceOver will add support for Arabic (World), Basque, Bengali (India), Bhojpuri (India), Bulgarian, Catalan, Croatian, Farsi, French (Belgium), Galician, Kannada, Malay, Mandarin (Liaoning, Shaanxi, Sichuan), Marathi, Shanghainese (China), Spanish (Chile), Slovenian, Tamil, Telugu, Ukrainian, Valencian, and Vietnamese! To query a list of languages, you can use the speechVoices property on AVSpeechSynthesisVoice https://developer.apple.com/documentation/avfaudio/avspeechsynthesisvoice/1619697-speechvoices?language=objc this will return you a list of all installed voices on disk. Note that a few of the new languages we are supporting do not have built in voices, so they will not show up in this list unless a user downloads them But you can basically iterate over this list and query the language property on the voice to figure out a list of languages that are available to the user :open_mouth: thank you so much! :point_up: <@U03HKTXJ4KE> and <@U03DQ584A76>!","title":"Is there a list of the upcoming languages supported by VoiceOver? Also, can we have a way to programmatically ask the accessibility framework what languages are currently supported? We're running into an issue where in visual text we support Vietnamese, but VoiceOver doesn't. We'd prefer VoiceOver to use a fallback like English in that case."},{"location":"wwdc22/accessibility-lounge.html#what-are-best-practices-for-creating-a-voiceover-experience-for-audio-recording-especially-if-that-page-includes-things-like-countdown-timers-or-textual-prompts-for-the-user","text":"I'd recommend you use announcement notifications for your prompts and countdowns. We do this in the \"Hey Siri\" training flow. Basically, whenever there is a prompt, we post an announcement notification, wait for the announcement to finish, and then start the recording session. When the next prompt is being read, we pause the recording. You can find out when an announcement is finished reading via this notification: https://developer.apple.com/documentation/uikit/uiaccessibilityannouncementdidfinishnotification?language=objc Thank you for the suggestions! I still remember the awesome djay Pro demo on iPad, pretty sure it was during the 2016 Design Award ceremony but sadly the video seems to have disappeared. May be worth looking at their workflow to see if they have other ideas? https://www.algoriddim.com/news/267-djay-pro-wins-2016-apple-design-award As audio features are important to us, am also interested in ways to make this accessible (could have spent an entire day in that accessibility design lab!). Would it make sense to use split output, with editing feedback via headphones and performance via speaker (or vice versa?). Thanks <@U03JELM0ZNV> - that's a neat reference I hadn't considered I'll definitely check it out. Designing VO feedback specifically for headphones is definitely an interesting idea :thinking_face::thinking_face::thinking_face: That way we could use announcements to call out \u201c30 seconds remaining\u201d (or whatever) and give VO users some of the same countdowns. I think really short announcements giving feedback might work too. Just have to make sure they're really targeted & not too distracting\u2026 (I work on an app for practicing public speaking, for context)","title":"What are best practices for creating a VoiceOver experience for audio recording? Especially if that page includes things like countdown timers or textual prompts for the user?"},{"location":"wwdc22/accessibility-lounge.html#would-the-assessment-mode-entitlement-be-approved-for-an-app-that-is-not-strictly-administering-an-assessment-but-where-a-valid-use-case-can-be-made-for-programatically-enabling-single-app-mode-for-non-mdm-devices-eg-another-use-intended-for-teachers-in-a-classroom-to-use-with-their-students-or-people-to-use-in-another-public-environment-that-is-not-assessment-related","text":"Hey Duncan! I encourage you to explore options of supervising the device yourself using Apple Configurator or other device management tools. If that doesn't work, I really like your use case and you should absolutely apply for an assessment entitlement. While I am not 100% positive on all of the requirements the review team goes though, I don't see harm in you applying! Here is the direct link if you need it https://developer.apple.com/contact/request/automatic-assessment-configuration/ Ok, will do. In this very specific case, while I don\u2019t regard it as \u201cassessment\u201d, I could probably argue it is formative assessment\u2026 The basic format is a quiz that lets students see if they know the names of other kids in the class/school\u2026 kinda getting increasingly important as around here (New Zealand) we now have in some schools open-plan learning environments with 90 kids and three teachers. That\u2019s a lot of new names at the start of a school year. The real reason for this is\u2026 gently\u2026 I think turning on Guided Access is a bit confusing for a user who doesn\u2019t ready know what it is. So cutting through to helping the user trigger it as part of a workflow in-app would be amazing. I\u2019ve got to run to a lab appointment. Thanks for the great work your team does. Super appreciate it. Guided Access requires manual intervention so I agree it doesn't sound perfect for your use case. Autonomous Single App Mode seems more appropriate here. Thanks for hanging out in the lounge with us, enjoy your lab!","title":"Would the Assessment Mode entitlement be approved for an app that is not strictly administering an assessment, but where a valid use case can be made for programatically enabling Single App Mode for non-MDM devices? e.g., another use intended for teachers in a classroom to use with their students, or people to use in another public environment, that is not assessment-related?"},{"location":"wwdc22/accessibility-lounge.html#i-was-playing-with-guided-access-today-and-noticed-time-limits-in-guided-access-settings-it-only-lets-me-set-a-sound-for-a-time-limit-ending-how-do-i-set-duration-for-a-time-limit","text":"(for autonomous single access mode, where guided access is entered by triple tap) Hello Jaime! :wave::skin-tone-3: When you triple-click to bring up the Guided Access \"workspace\", you can set the time limit in the options menu there. Got it! Thank you We have those options in the Guided Access workspace because you may want a different time limit depending on the app, and we want it to be easily accessible rather than making you head back to Settings. Thanks for your question! :smile:","title":"I was playing with guided access today and noticed \"Time limits\" in guided access settings. It only lets me set a sound for a time limit ending. How do I set duration for a time limit?"},{"location":"wwdc22/accessibility-lounge.html#how-do-i-find-the-work-space-on-my-iphone-so-i-can-set-a-time-limit","text":"Hey Thomas, first turn on Guided Access in Settings > Accessibility > Guided Access. Then open the app which you want to lock on to. Then triple-click to perform the accessibility shortcut. Depending on your device, it may be the home button if it has one, or the sleep/wake button. This is where you configure options for Guided Access before you begin, and Time Limit is an option in this menu. Here is more information about the Accessibility Shortcut in general if you'd like to know more https://support.apple.com/en-us/HT204390 Got it. Big thanks! Time-limit is pretty cool; :slightly_smiling_face:","title":"How do I find the \"Work space\" on my iPhone (so I can set a time limit)?"},{"location":"wwdc22/accessibility-lounge.html#do-you-have-any-favorite-apps-that-use-autonomous-single-app-mode","text":"I don\u2019t know if I would call it a favorite, but we do see a organizations making testing apps use ASAM mode effectively. When the test time starts, the device locks down, then releases access at the end of test time. There\u2019s an 8 hour limit on ASAM mode, so it can\u2019t be run forever either I\u2019m thinking more is there an app you think of that does well in guiding someone in using ASAM for the first time, or one that is clever in how it handles restrictions for guided access Ah, sorry. I do not know specific apps","title":"Do you have any favorite apps that use autonomous single app mode?"},{"location":"wwdc22/accessibility-lounge.html#is-there-a-way-to-test-more-advanced-accessibility-features-using-xc-ui-tests-eg-custom-accessibility-actions-custom-accessibility-content-custom-rotors","text":"Thanks for the question <@U03J4DPCXGC>! We don't have a great way to test this in XCUITests at the moment, but please file a request for this through Feedback Assistant!","title":"Is there a way to test more advanced accessibility features using XC UI tests? eg custom accessibility actions, custom accessibility content, custom rotors?"},{"location":"wwdc22/accessibility-lounge.html#who-uses-automatic-assessment-configuration-do-teachers-comfortably-use-it-or-is-it-only-for-it","text":"Apps that implement the Automatic Assessment Configuration frameworks can create easy-to-use UI to enter a testing session for folks like teachers to use. We also offer wide-scale solutions for teachers managing a classroom full of devices called Apple Classroom. There is a lot of information about this topic, more can be found at the link below and I recommend watching past WWDC sessions about assessment mode in particular if you are still interested. https://support.apple.com/en-us/HT206151","title":"Who uses \"Automatic Assessment Configuration\"? Do teachers comfortably use it or is it only for IT?"},{"location":"wwdc22/accessibility-lounge.html#in-an-ios-app-i-have-a-status-label-in-the-ui-but-its-too-long-and-changes-too-quickly-to-be-appropriate-for-voiceover-instead-i-want-to-announce-the-beginning-and-end-of-a-long-process-eg-preparing-completed-to-voiceover-triggered-by-a-user-action-so-not-out-of-the-blue-what-would-be-the-api-to-trigger-such-announcements","text":"you can post an announcement notification using <http://UIAccessibility.post|UIAccessibility.post> , and passing in a notification type of announcement, and a string that you want to have spoken https://developer.apple.com/documentation/uikit/1615194-uiaccessibilitypostnotification?language=objc https://developer.apple.com/documentation/uikit/uiaccessibilityannouncementnotification?language=objc Perfect. Thanks!","title":"In an iOS app, I have a status label in the UI but it's too long and changes too quickly to be appropriate for VoiceOver. Instead I want to announce the beginning and end of a long process, e.g \"Preparing ...\", \"Completed ...\" to VoiceOver (triggered by a user action so not out of the blue). What would be the API to trigger such announcements?"},{"location":"wwdc22/accessibility-lounge.html#im-building-a-drawing-app-using-pencilkit-what-kinds-of-things-can-i-do-to-make-it-accessible-using-technologies-like-voiceover-is-pencilkit-going-to-do-a-bunch-of-heavy-lifting-for-me-in-a-situation-like-this","text":"Any UI that you're getting from PencilKit should get AX for free, like the palette controls etc. As far as accessibility work for the canvas, you'll need to asses what your users are going to use your app for, and build in accommodations around those use cases. Maybe that's announcement notifications if you're doing some sort of guided drawing to tell people if they are on track. It's always good to talk to blind users of your app if you can, and see what feedback they have. If you have more specific questions, feel free to post a question on the developer forums and tag it with accessibility, as the lounge closes soon!","title":"I'm building a drawing app using PencilKit. What kinds of things can I do to make it accessible using technologies like VoiceOver? Is PencilKit going to do a bunch of heavy lifting for me in a situation like this?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html","text":"arkit-realitykit-usdz-lounge QAs by FeeTiki Hello! I am pretty new to Reality Composer. I would like to know how (if it is possible) to add textures to custom USD objects. Reality Converter makes it easy to convert, view, and customize USDZ 3D objects on Mac. For more information, visit http://developer.apple.com/augmented-reality/tools/|developer.apple.com/augmented-reality/tools/ Thanks! I am looking forward to creating AR experiences using it. Thank you very much and keep up the good work! I really appreciate the opportunity to connect with Apple engineers. Thanks, Cristian-Mihai. We love connecting with developers from around the world :relaxed: Also, congratulations on winning the Swift Student Challenge this year :tada: Is Reality Composer Available for the Mac? RComposer on macOS is part of XCode install Yes, check out the link above. <@U03JRP87THN> Any hints what\u2019s the best way to use STL objects? I am trying to use Blender, but that is \u2026 another steep learning curve I use #b3d and avoid exporting to .usd because the upAxis of Blender and Apple are incompatible. Export to .fbx or .glb instead and use RealiytConverter (link above) to convert these to proper Apple compatible .usdz The recent update of XCode made scenes in RealityComposer look much darker. Is this caused by a change in RealityKit? Hi! We are aware of a bug in macOS Ventura/iOS 16 that is causing the lighting to appear darker. Please feel free to file a bug report on Feedback Assistant about this. If you are already aware of this - will it get a higher prio if non-Apple developers also complain? Same problem with <model> on iPadOS16beta I don\u2019t think the priority would change if we get more reports since it is already a high priority issue that\u2019s been assigned to an engineer to investigate. Trying to understand how this works via Slack..., should we be able to see what everyone else is posting, what the questions are from others? Or, do you select certain questions, craft a response, then post both? Thanks. We select certain questions, craft a response, then post both :slightly_smiling_face: Thanks very much! Am trying to use LiDAR scanner to create a 3d model from capturing an object. But couldn't get enough resources for that. Any references/resources please? For creating 3D models from images captured on device, this should be a helpful resource to get more inspiration and help: https://developer.apple.com/videos/play/wwdc2022/10128/ For Object Capture specific questions, there will be a Q&A tomorrow from 3:00 - 5:00 pm that you can join! Apologies, I meant later today at 3pm! Is there a way of exporting a Reality Composer scene to a .usdz, rather than a .reality or .rcproject? If not, what are your suggested ways of leveraging Reality Composer for building animations but sharing to other devices/platforms so they can see those animations baked into the 3D model? Yes, on macOS, you can open Reality Composer, Settings, Enable USDZ export Oh, cool! I did not know that. Thank you! Same with the iPad Version this export feature has been broken for years on RC for scenes of even moderate complexity. Have you filed a bug report on Feedback Assistant about this? yes, over a year ago. still broken can you share the Feedback ID? I can check on the status My experience is: don\u2019t use 8k textures and make sure all imported and placed USDZs use exclusive texture file names (folder numbering). This can be achieved by preparing ALL of them in a single export session of RealityConverter (or do lots of manual .usda editing) FA id: FB9622407 none of my FA requests have been resolved for RK/RC, btw, that I am aware of. Same trouble on my reports <@U03J2DUEV0X> I don\u2019t see any updates on this one unfortunately :disappointed: I reported, what I consider a critical RK bug over 4 months ago, no resolution. Forum ref title: iOS 15 RealityKit AnchorEntity possible bug and I have several more...unfortunately all stuck with no resolution. FB9679173 should be resolved in iOS 16 beta 1 can you not filter out my lounge request about why RC has not advanced in almost 2 years? stuck at version 1.5 as of xcode 14 Beta. I'm building a product with your framework, so this is a healthy conversation. let me see, but we don\u2019t really have much to share regarding RC. We\u2019re aware that it hasn\u2019t been updated in a while could benefit from more features, but don\u2019t have anything to announce right now. but we can start another thread about it thank you Are there any plans (or is there any way?) to bring post-process effects and lights into Reality Composer? I'm making a short animated musical film in AR. I love how RC does so much automatically (spatial audio, object occlusion...). I just wish it was possible to amp up the cinematic-ness a little with effects. Post processing effects are not supported in RC right now, only in a RealityKit app. However, feel free to file a feature request on Feedback Assistant about this. Ok, thank you! The LookToCamera Action works different on ARQL than while testing in RealityComposer. Will this be fixed with XCode14? Please file a https://developer.apple.com/bug-reporting/|bug report for this . Any additional information you can provide, such as a video reproducing the issue would be hugely helpful. Ignored for over a year Could you provide the Feedback ID? Cannot find it in http://Feedback.app|Feedback.app , must have used user support channel instead - my bad. If RC update does not fix this, I\u2019ll add a feedback - finally. Are there guidelines or best practices for exporting a RealityKit scene to a USDZ? Is this possible? I\u2019ve seen just a little about the ModelIO framework. Is this the tool we should be using? I don\u2019t think we have any guidelines about this, since exporting/saving a scene is not supported by the current APIs. ModelIO seems like a reasonable solution to me, but you might also want to file a feature request for this on Feedback Assistant. Hello there, I am someone who is still fairly novice with Reality / AR Kit. And I want to ask what is the best way to implement Multiuser AR experiences. I\u2019ve been thinking on creating an AR app that would use this feature to allow multiple users to view a single AR view (e.g., multiple users seeing the same rendered model from their own perspectives). Not the expert here, I believe that a \u201cLocation Anchor\u201d is what you are interested in. I don\u2019t know how you would set one up without getting into coding though. I believe one of the videos from WWDC last year addressed this? Multiuser AR experiences can be created using the SynchronizationComponent , found here: https://developer.apple.com/documentation/realitykit/synchronizationcomponent This is a good tutorial (along with sample code) on building collaborative AR sessions between devices: https://developer.apple.com/documentation/arkit/creating_a_collaborative_session Also not an expert here, but I'm looking into that also. If you're comfortable with Unity, Niantic (Pokemon Go) just released a platform and some demo projects to do multi user AR. It's called Lightship. <@U03JRQ5LY2C>, here\u2019s the session from last year that I mentioned. It might help to get you started\u2026 https://developer.apple.com/videos/play/wwdc2020/10611/ Are there any examples using SwiftUI? All of the code seems to be using Storyboards, though I assume they can easily be transferred over into a SwiftUI view using either a UIViewControllerRepresentable; or generally converting the general lifecycle of a multiuser AR app to fit SwiftUI standards. The view that contains the ARView will still need to be a UIKit view but that can easily be bridged to other SwiftUI views using a UIViewRepresentable : https://developer.apple.com/documentation/swiftui/uiviewrepresentable Most of the AR collaboration code in the sample here is just Swift code that can easily be abstracted out and used in your app: https://developer.apple.com/documentation/arkit/creating_a_collaborative_session In the keynote, there's a mention about Background API in Metal. Please share documentation/resources link Are you referring to https://developer.apple.com/documentation/metal/resource_loading Yes, Thanks iOS 15.4 includes the builtInLiDARDepthCamera type in AVFoundation. Is there any advantage in implementing this camera type when doing Object Capture for better depth calculation, or does that not change the outcome of the rendered 3D model? Capturing images with LiDAR devices will give you automatic scale estimation and gravity vector information on your final usdz output Thank you, <@U03HHPAHQ3C>. Was this also the case prior to iOS 15.4? I recall the Capture Sample project from WWDC 2021 already leveraged depth data, but not sure if that depth data was already using LiDAR? Hi Brandon, yes we already supported depth in last years version of object capture. It does not have to come from LiDAR. There are different ways to capture depth on Apple devices. Please take a look at our sample code for the recommended way how to capture high quality RGB images with depth https://developer.apple.com/documentation/realitykit/taking_pictures_for_3d_object_capture Is there a method to implement scans from RoomPlan with Reality Composer? Thank you! Hi, can you clarify what integration you are looking for? What are some ideal non-product examples of good USDZs There are great USDZ examples on https://developer.apple.com/augmented-reality/quick-look/ For example you have the Lunar Rover from For All Mankind Scientific stuff! We've also added new documentation to help you generate better USD assets here: https://developer.apple.com/documentation/realitykit/creating-usd-files-for-apple-devices Is there a way to access system instances for a scene or must system updates (e.g., change the culling distance of a system) always route through a component? Hey there, thanks for your question. Generally Systems are designed to operate on Entities (within Scenes) and their Components. Each System can be updated against multiple Scenes (and the Scene\u2019s entities). If you have state that you want to be represented with a System, one method to do that is to have a root entity that holds a \u201cSystem component\u201d. There\u2019s more information on System\u2019s in last years WWDC session and on the developer documentation website: https://developer.apple.com/videos/play/wwdc2021/10074/ https://developer.apple.com/documentation/realitykit/system/update(context:)-69f86 Ok, that' what I thought, but wanted to make sure I didn't miss anything. Thanks! When do you think we will see new versions of Reality Composer and Reality Converter apps? I'm a college professor - Graduate Industrial Design, and use these as an intro to AR tools. Better, more capable versions might be nice? Thanks. Unfortunately, we don\u2019t discuss our future plans. However, we are aware that our tools haven\u2019t been updated in a few years and could use some new features. Could you share what features you are looking for us to add? FA id: FB9570859 It would be awesome to see features that are announced in Reality Kit be compatible with Reality Composer. Like videos, video textures, post processing effects. Love RC and can't wait to see it updated. Cannot apply properties to group object Cannot multi-select behaviors to delete Cannot copy multi-selected behaviors Need an object navigator to select scene objects to modify Need Opacity/Texture/Materials for objects/groups I would love to see Meshes or Objects that have Emissive lighting. Or in other words objects that would glow and be emissive similar to what we see in Unity and Unreal. I could keep going with features, but need to be mindful that RC is not in the league of Maya, for example. also Animation timeline, minimal Oh yeah, object navigator and animation timeline would be incredible. I would also love to see a visual or graphical shader editor similar to Shader Graph from Unity. It does not have to be really complex, even a basic visual editor for RealityKit / Reality Composer shaders would be awesome! <@U03JELU9E5P> use emissive texture with black as diffuseColor to achive this. It would glow (no overwhites) but can change the look a lot I\u2019d really, really like to see material importing and placement options, so that we can at least apply a simple texture to a simple object in Reality Composer, rather than having to dive into a more complex 3D app. Right now, we can only change the color and the material type. (Unless I\u2019ve missed something? We are seeing some memory leaks when adding ModelEntities to an anchor, pausing the ARSession and starting it again and adding ModelEntities again....We see memory growing in the re::SyncObject section. Does anyone have experience troubleshooting memory leaks that have happened in a similar way? Hey thanks for the question. I\u2019d recommend this year\u2019s WWDC Xcode session for what\u2019s new in debugging - ( https://developer.apple.com/wwdc22/110427 ). And there have been many other excellent sessions over the years on debugging. That said if you believe it may be RealityKit or another system framework responsible for leaking the entities we\u2019d ask you to file a Feedback Item on http://feedbackassistant.apple.com if you haven\u2019t done so already. Any plans for instant AR tracking on devices without LiDAR? This could be helpful for translation apps and other apps that overlay 2D text/images on 3D landmarks. You might want to ask this to the ARKit team, but I\u2019m not aware of any plans. A feedback item would be good though! Is there a way to localize against a scanned room from the Room Plan API (via ARKit) so that it could be used for example to setup a game in your room and share that with other people? No there is no re-localization in RoomPlan. But we expose the ARSession so you could fallback to ARKit for re-localization Gotcha, thanks! I think this would be a nice addition though. Is there a suggested manner of writing ARKit/RealityKit experiences to a video file? I'm current using RealityKit 2's post-processing to convert the source MTLTexture to a CVPixelBuffer , and writing that to an AVAssetWriter , but this occasionally ends up leading to dropped frames or random flickers in the video. Thank you for your question! We don\u2019t currently have a recommend method for doing this and as such would love to see a feedback item explaining what you need and a use case explaining it. That would be wonderful. That said your method should in theory work and we\u2019d also love to see feedback item describing the issues you\u2019re seeing. https://developer.apple.com/bug-reporting/ Is there a way to use video textures in Reality Composer? Video textures are currently not supported through Reality Composer UI. However, if your .rcproj is part of an Xcode project, you can use the RealityKit VideoMaterial api to change the material of your object in the scene at runtime. https://developer.apple.com/documentation/realitykit/videomaterial For short flipbooks, I prepare several instances of the same mesh with separate textures (one per frame) and either animate their .opacity in USD or show/hide them in RComposer Is there currently a built-in way or example of a way transform a RoomCapture from RoomPlan into a ModelEntity or other type of RealityKit entity? Instead of only the exported USDZ file? I don\u2019t believe there is a built in way, but loading a USDZ into a RealityKit scene as a ModelEntity is very simple Gotcha. What I would love to do is mark edges with measurements and such. So I think I would need to build up a ModelEntity piece by piece with the RoomCaptureData to do that Hi there! I noticed the new beta class \"ImageRenderer\" for SwiftUI, allowing SwiftUI views to be rendered into a static image and be used as a texture in ARKit. Will there be an interactive version of displaying SwiftUI views in ARKit? I am currently achieving the same effect using UIHostingViewController, but it feels hacky and creates a GLES2/Metal Rendering Issue in Simulator currently. Thanks! Hey! Thanks for your question. We don\u2019t discuss future plans, but gathering developer feedback is important to us so we\u2019d ask you to post your request to https://developer.apple.com/bug-reporting/ Okay thanks! I\u2019ll take that to mean that no such functionality currently exists at least? Definitely file a Feedback item with us - we\u2019d love to hear your use cases for any new features. In the State of the Union, there is reference to ScanKit alongside the mention of RoomPlan . Is ScanKit a SDK, or if that the same thing as RoomPlan ? RoomPlan is the name of the SDK. You\u2019ll want to refer to those APIs as RoomPlan instead of ScanKit. At last years WWDC 2021 RealityKit 2.0 got new changes to make programming with Entity Component System (ECS) easier and simpler! The current RealityKit ECS code seems too cumbersome and hard to program. Will ease of programming with ECS be a focus in the future? Hey! Thank you. While we don\u2019t discuss specific future plans, we always want to make RealityKit as easy to use for everyone as we can. We\u2019d ask you to post your issues and/or suggestions to https://developer.apple.com/bug-reporting/ I\u2019d love to find out more about what you find too cumbersome. Thanks! Collaboration frameworks for AR are important. Is Apple considering features related to remote participation in AR experiences? Unity has this capability to some extent. While we don't discuss future plans, we always hope to gather this sort of feedback during WWDC. Thanks for taking the time to share :pray: We do support Collaborative sessions over the same network, more details and sample code can be found here: https://developer.apple.com/documentation/arkit/creating_a_collaborative_session Is this what you were looking for ? Hi, I am trying to make a Battle Royal in Augmented Reality, how can I make a storm/game bounds that is visible from far away and is occluded by buildings There are challenges using ARKit outdoors, you should sign up for an ARKit lab to get some feedback from them ok, thank you This question may be better suited for tomorrow's #object-and-room-capture-lounge, but is the output CapturedRoom type able to be modified prior to export to USDZ? For example, could I remove all [.objects] types, and leave just walls/doors, or change the texture of a surface? Yes, please ask this during the object capture lounge tomorrow. But you should be able to modify after export and re-render I had this same sort of question. From my look at the docs, you could possibly create a RealityKit Entity using the RoomCaptureData. For removing elements, it looks like you should be able to remove array elements within RoomCaptureData and then write that to usdz Yes, all those things should technically be possible. You would need to use the RoomCaptureSession API and subscribe to a delegate to get those updates which contain the Surfaces and Objects. You can then process that data and render it as per your liking Thanks very much! Appreciate the replies! :slightly_smiling_face: I've noticed that when occlusion is enabled on Lidar devices, far away objects are automatically being clipped after a certain distance like 10m or so (even if there is no physically occluding them). I've tried to adjust the far parameters of the PerspectiveCameraComponent \u2013 https://developer.apple.com/documentation/realitykit/perspectivecameracomponent/far|https://developer.apple.com/documentation/realitykit/perspectivecameracomponent/far But unfortunately that didn't help. Only disabling occlusion removes the clipping. Is there a workaround for this behavior? This should be fixed in iOS 16 ~I believe this is a known issue though~ Will check again, but good to know it's on the radar. No available workaround for now then I suppose? Ahh just updated okay nice will check! yeah i found the bug report :slightly_smiling_face: Could you also have a look into this: https://developer.apple.com/forums/thread/682215 FB9517992 I just checked and it's still happening on iOS 16 Beta 1 yeah, that one hasn\u2019t been fixed yet Alright, thanks! I need SwiftUI Views in my RK experience...please and ASAP. Hi! Can you explain a little bit more what you\u2019re trying to do? If you have a use-case in mind for a new feature you\u2019re definitely also welcome to file a Feedback item with us via https://developer.apple.com/bug-reporting/ will reality kit work with swift ui views ? <@U03H3628BL7> You can host RealityKit content inside SwiftUI views with UIViewRepresentable ( https://developer.apple.com/documentation/swiftui/uiviewrepresentable ) If you\u2019re asking if you can use SwiftUI content within RealityKit - there is no direct support for that at present and we\u2019d ask you to file a feedback item explaining your use-case for that feature. I want to inject SwiftUI charts, for example into my RK scene with live updates via Observables. this will be very useful when there is a need for being surrounded in AR space with analytics about equipment telemetry, etc. thanks Oh that\u2019s a great idea. Please do file a feedback item. we filed a FA for this feature quite some time ago. no reply, will look it up. I\u2019d love to have SF Symbols renderable in AR! it actually works with RK on macOS by copy and pasting the symbols, but not available in the system font on iOS. Thanks for the suggestion! You may want to check with the SF Symbols team to confirm this is not possible yet, and also file a feature request on feedback assistant. <@U03J7GC4DEC> had a fb open for almost a year now :grin: SF Symbols in MeshResource.generateText() - FB7959655 A bit of a hack solution but you may be able to get this work via drawing the Symbol to a CGImage and passing that image in as a texture. that would work for a 2d solution, but ideally it would be a prism Ah yeah. Alright thanks for the FB item in that case. it sounds like the reason it doesn't automatically work does come down to the sf symbols team\u2026 is there a way i could get this ticket to that team? no lounge etc it looks like the bug is actually in RealityKit, so it\u2019s on the proper team right now. We\u2019ll discuss this bug internally with the team. Hi. I\u2019m Pururaj. I have been really interested in RealityKit and ARKit for the past couple of years. Where can I learn more about it? I\u2019m currently into Designing, Writing, Editing, and Management and would love to work on futuristic tech. check out this page! https://developer.apple.com/augmented-reality/ Hi Pururaj and welcome! You're already on the right track\u2014meeting with engineers and developers from around the world :grinning: To learn more about RealityKit and ARKit, I would recommend starting with our documentation and videos. Here are a few links to help you get started: \u2022 https://developer.apple.com/augmented-reality/realitykit/|RealityKit \u2022 https://developer.apple.com/augmented-reality/arkit/|More to explore with ARKit 6 \u2022 https://developer.apple.com/augmented-reality/tools/|AR Creation Tools \u2022 https://developer.apple.com/videos/augmented-reality|Videos Augmented Reality> You can also always ask questions on https://developer.apple.com/forums/|Developer Forums :speech_balloon: https://www.youtube.com/c/realityschool Our app uses RealityKit and starts multiple ARSessions throughout a user session. We\u2019re seeing an issue where the memory after the first ARSession never gets freed (which is a pretty large chunk around 200MB). We set everything to nil when the session terminates, but we can see in the memory graph that an ARSession object is still being retained. Subsequent ARSessions leak a few megabytes of memory but at the end of each session there is always only one ARSession being retained in the memory graph. Are you aware of any memory cleanup bugs either in RealityKit or ARKit that could be causing this? Or is there a way to completely free all memory from an ARSession? Here\u2019s a Feedback Assistant number for reference to a sample app 9950277 Hey Wade\u2026 looking at the issue right now and I don\u2019t have an immediate answer for you. But it looks like the issue you provided has all the data we need in it. Awesome thanks! Let me know if you need anything additional info Thank you. Hello everyone! Many of you know .glb file format (android's scene-viewer) support compression like draco. Any planning update for compress .usdz files? I would suggest filing an enhancement request on feedback assistant for this Is there a lab to ask questions about SceneKit this year? You\u2019ll probably want to ask SceneKit questions on the developer forums if no lab or lounge is available What is the recommended way to add live stream or capture capabilities with RealityKit? Do we need to build frame capture and video writers with AVFoundation? A higher level API would be a better fit for RealityKit. I would recommend using ReplayKit or ScreenCaptureKit to record your app screen to stream / share https://developer.apple.com/documentation/replaykit https://developer.apple.com/documentation/screencapturekit I'd just like to say that RealityKit and ARKit are amazing. It feels early in the world of AR, but it's amazing how easy it is to bring 3D objects into AR, and I'm excited and grateful, as a developer, to be on this journey. Thank you for these tools and your hard work! Thank you for the kind words, Brandon :pray: We appreciate you taking the time to join our digital lounges and engage with our engineers and designers! QuickLook is currently very dark (much darker than would be expected). Clients are complaining about washed-out colors, and we need to overcorrect via emission (not ideal, breaks dark rooms). (Easy to test: make a pure white material and display it in QuickLook in a bright room, it will never get white.) Are there plans to fix this? Great question - and we have good news for you :slightly_smiling_face: We are releasing new lighting to ARQL which is brighter with enhanced contrast and improved shape definition to make your assets look even better. Please check tomorrow's session - https://developer.apple.com/videos/play/wwdc2022/10141 with examples and how to implement it! Thank you! Thanks! Especially curious about the \"how to implement it\" part - as a QuickLook user I'd think there's nothing to implement :slightly_smiling_face: I'm super excited about the <model HTML tag taking shape. The examples currently list camera and animation control; that seems to be lacking examples for general interactivity / scripted access to the model similar to what three.js scenes allow for. Is this intentional, or are the examples just missing? Thanks for you interest! This is a great feature question. Are there other features you would like to see? Could you share more links to documentation about this? <@U03HMDX5C95> https://github.com/WebKit/explainers/blob/main/model/README.md Thanks <@U03JA5TGENQ>! Are you on Twitter at all? Always good to connect with other AR creators. <@U03HL4ZB05S> Definitely! 1. Seeing that USDZ and glTF are both planned to be supported, I'm of course curious about scene graph access. This kind of clashes with the notion that \"rendered model data ... is not exposed by the page\" 2. Given that our typical usecase is AR, I really wonder how the <model> tag would interact with AR in general and QuickLook in particular. This goes into areas of immersive sessions, dom-overlays, etc. - e.g. when I have styled audio control buttons in HTML, I definitely would want to have those in AR as well. <@U03HMDX5C95> sure, I'm @hybridherbst I have a model that is a .reality file, that opens in ARQL. When the user taps the model, it shrinks to show the same light in a different size. However, it's not very clear to the user that this is a possibility. If they don't tap for a while, ARQL encourages them to \"Tap the object to activate\" is there a way I can customise this message? Hi <@U03HMDX5C95> thanks for the question. That is a standard message and unfortunately there\u2019s currently no way to customize the text. Alternatively, you can create your own banner within your asset. For example, you can check out the first asset on the gallery page That's unfortunate. Should I file feedback or is this working as intended? Yes, please file a feedback report :pray: It seems a bit weird that there's currently three different implementations of USD at use across iOS / Mac. Are there plans to consolidate those into one to make testing and verification of assets across platforms easier? The shared feature subset is pretty small, resulting in less-than-ideal products for clients. There are different USD renderers across our platforms but each serve a different purpose. Here is a developer document that explains these different USD renderers and what their feature sets are https://developer.apple.com/documentation/realitykit/creating-usd-files-for-apple-devices Does Storm work on iOS? I'm (painfully!) aware of these differences, that's why I'm asking :slightly_smiling_face: Currently, testing a single USDZ model to actually work in AR across different iOS versions is really hard. Having a somewhat more unified USD renderer (or at least a way to use the same one from Mac or a browser than what's used on QuickLook) would definitely accelerate client adoption of USDZ. Gotcha, that's good feedback we can relay to the team on a more unified USD renderer :slightly_smiling_face: Is it possible to support more than one image anchor in a scene with ARQL? Feedback: FB7818339 This is not supported at this point. The team is still investigating and we'll update the report when there's additional information. Thanks. This would be really useful to have. I'm creating pendant lights for viewing in in ARQL, is it possible to anchor these to the ceiling of a room? Hi <@U03HMDX5C95>, yes this is something that is supported in AR Quick Look. You can place objects on the ceiling by dragging them there. This can be done using a regular horizontal (or vertical) anchor. However, there are potential challenges to be aware, the biggest is that ceilings usually lack a lot of feature points, which makes it difficult to detect a proper plane. Using a device with lidar can improve the results that you get. Thanks for your reply. I was more wondering about placing this file on the ceiling to begin with, rather than making the user drag the model there themselves. Is this possible? Can you describe in more detail how you would want this to work? Eg. after the asset gets loaded, what would the user need to do to see the asset on the ceiling? For example ARQL could guide them to point their device to the ceiling to view the model? It's just that for pendant lights, they come from the ceiling, so having them attached to the wall isn't super useful to people and doesn't provide a great experience currently. One way to let users know that they can move objects to the ceiling is to include a 3D banner with additional instructions. Sure, but that experience is less than ideal I think. It would be great having these land directly on the ceiling. Is it worth me filing feedback? Using ARQL, how might I add a colour picker to change between colours of a model? For example, the iMac ARQL on http://apple.com|apple.com requires users to jump in and out of ARQL to try different colours. Is there a way to have colour pickers in ARQL to try different materials or change different scenes in a .reality file? You could use Reality Composer's interactions to make an interactive USD where you can tap on different colors to change the model Thanks for your reply. Do you have any examples? It's my understanding that I wouldn't be able to do this as a 2D UI/Card above the ARQL experience, but rather would have to have it as part of the 3D AR experience? If that makes sense? Yes this needs to be done in 3D. There\u2019s a previous session that has some examples Thanks for clarifying. It would be really useful if we could provide a 2D/Card UI over the ARQL experience to do this (and it would be great for Apple to be able to show different colours of devices without jumping in and out of ARQL, too). We can already embed html in ARQL, if this could link to different scenes in a .reality file it would be amazing and make the experience so much better for users. I would file feedback for this but feel as if my other feedback requests have gone unnoticed \u2013 is there a better way to get ideas noticed? Does a prototype of what we are looking for help? Hi y\u2019all! Any thoughts about making USD/USDZ files with particle effects? Things on fire/sparking etc? Hello Matthew! This is not currently possible to do in a USD, but you should submit the idea to https://feedbackassistant.apple.com . You can however do some particle effects in an app by using RealityKit's CustomShaders. Depending on how complex your effect is, you can also bake your particle effects to regular mesh + bones animation :sparkles: In many cases you can also create a pretty convincing effect just by scaling/rotating a few planes. https://prefrontalcortex.de/labs/model-viewer/upload/kravity/|Example link (no USDZ behind that right now, but you get the idea - this is just two simple meshes for the particles) Thanks <@U03JA5TGENQ> nice idea! Is there a simple way to create a 3D object with a custom image as a texture? Reality Composer only allows a material and a color, and without that, I'll have to dip into a far more complex 3D app. I'd really, really like to use USDZ more in Motion, for pre-viz and prototyping, but without texture editing it's quite limited. Have I missed something? :) Thanks Iain. Can you explain what kind of 3D object you want to create with a textures? A complex object or a simple plane? A relatively simple object. For a recent project I needed to create playing pieces similar to dominos, but each one with a different, complex image on it. I was able to use Motion\u2019s 3D text support to extrude a rounded rectangle, but I\u2019d prefer to do this with USDZ as it\u2019s far more widely supported. Motion\u2019s 3D text support is exactly the kind of texture placement and scaling support I\u2019m looking for, by the way. There are various third-party DCCs with great USD support that let you create complex 3D object with textures and export as USD. You can then use RealityConverter to convert those to USDZ to import into Motion. Understood \u2014 though they\u2019re a huge jump up in complexity and it would be great to see this built into RC. Do you have any specific recommendations for a third-party app that\u2019s easy to use for simple objects? Another approach: three.js (web render engine) can actually create USDZs on the fly from 3D scenes. A colleague used that recently for USDZ AR files with changeable textures on https://webweb.jetzt/ar-gallery/ar-gallery.html (users have to leave AR to change the textures but then can even load custom images into it) OK, will look into three.js as a potential solution. Please consider adding texture support into RC though \u2014 it would be a great way to create simple objects and make simple edits to existing objects. Thank you! Ah, I think I understood the question a bit differently - I think you're referring to \"drag in a generic model and then change the texture in RC\" - three.js is most certainly overkill for that :slightly_smiling_face: Yes, that\u2019s pretty much it. For example, it would be great to be able to create a simple rectangular plane, then put a custom image on the front face. Simple stuff, and most 3D apps are far, far more complex. Blender in particular is a UI nightmare. Also take a look at the https://developer.apple.com/videos/play/wwdc2022/10141|Explore USD tools and rendering session tomorrow. You can now change materials properties in RealityConverter! Great! That\u2019s going to be a huge help \u2014 good to know. Another thing that might help for making quick adjustments: the browser-based three.js editor at https://threejs.org/editor . Here's how to make a plane with a custom image and export that as USDZ: Just checking \u2014 can I import USDZ into Reality Converter, just to change the texture, or should I use some intermediate format? Reality Composer is great, but our team of 3D asset modelers has found it easier to sculpt characters in Zbrush. Do ARKit and RealityKit accept models created in Zbrush, or are there intermediate steps best for preparing a model for Apple platforms? (KeyShot, etc.) Yes, if you can export your assets to FBX, glTF or OBJ, you can convert them to USDZ using Reality Converter, which is compatible with ARKit and RealityKit We\u2019ve run into problems with materials and textures when exporting them in various formats to USDZ. It seems that the pipeline from some 3d modeling applications (our use is primarily for product and transportation design) - Maya, Rhino 3d, Solidworks, isn\u2019t really smooth. Exporting files to Keyshot to assign textures seems to help in some cases. In many though the materials and textures don\u2019t really work to our satisfaction. Perhaps newer versions of Reality Converter will be more successful? Are there tools that can be used to Rig skeletons for USD characters? I have not found anything that works? Yes, there are various third-party DCCs that let you create skeletons and RealityConverter lets you convert other file formats with skeletons to USD. Autodesk Maya https://knowledge.autodesk.com/support/maya/learn-explore/caas/CloudHelp/cloudhelp/2022/ENU/Maya-USD/files/GUID-9E9D45F2-4DA9-497B-8D69-1573ED6B2BA8-html.html|https://knowledge.autodesk.com/support/maya/learn-explore/caas/CloudHelp/cloudhelp/2022/[\u2026]files/GUID-9E9D45F2-4DA9-497B-8D69-1573ED6B2BA8-html.html and Blender https://docs.blender.org/manual/en/latest/files/import_export/usd.html Are some example Digital Content Creation tools that can help you create rigged skeletons for characters exported to USD. Is Reality Composer appropriate for end-users on macOS? We'd like to export \"raw\"/unfinished USD from our app then have users use Reality Composer to put something together with multimedia. Hi Steven, what do you mean by \"raw/unfinished\" USD and multimedia? You can assemble different USDZ assets together to build out a larger scene in Reality Composer and add triggers and actions to individual assets within the project Getting late here, checking out for today - thanks for patiently answering so many hard questions! All the best :) Thanks for all the great questions, feedback and ideas from everyone and we have 15 more minutes time in this session today! Is there a way to modify ModelEntities loaded from an .usdz file on a node basis? E.g. show/hide specific nodes? Yes, if you load the USDZ with \"Entity.load(...)\" or \"Entity.loadAsync(...)\" you can traverse the hierarchy and modify the individual entities. You\u2019d want to use Entity.isEnabled in this instance to hide/show a node. Note that . loadModel will flatten the hierarchy whereas .load will show all entities Ah, so each child-node is then an entity? A prim and entity are not 1 to 1 necessarily Ok <@U03HHPAHQ3C>, I will look into your suggestion. Thank you! will there be an async await (concurrency) api to detect when entities are added to an arview? right now i'm working off combine anchors .... it's not super documented Hey there, we don\u2019t discuss future releases of Apple products. But we\u2019d love to hear your feedback and suggestions. Please file your feedback here to get it into our system. https://developer.apple.com/bug-reporting/ What\u2019s the easiest way to add user interactions (pinch to scale, rotation, transform) to an Entity loaded from a local USDZ file in RealityKit? I\u2019ve added an Entity as a child to an AnchorEntity, added the AnchorEntity to the scene, but how do I easily add gestures? You can use the installGestures function on ARView. Keep in mind that the entity will need to conform to HasCollision . https://developer.apple.com/documentation/realitykit/arview/installgestures(_:for:) Thank you, <@U03HHPAHQ3C>! Can I follow-up and ask if there is any documentation on how to make an Entity loaded from Entity.loadAsync(...) conform to HasCollision so I can use installGestures(_:for:) , as this is where I seem to be struggling to understand. You could create your own CollisionComponent with custom mesh and add it to your entity or you could simply call generateCollisionShapes(recursive: Bool) on your entity. https://developer.apple.com/documentation/realitykit/entity/generatecollisionshapes(recursive:) Thank you, <@U03HHPAHQ3C>! Just for posterity, I can call generateCollisionShapes(recursive: Bool) on my Entity before adding it as a child to the anchor, and call installGestures on ARView before adding the entity as a child to the anchor, and that should give me interactivity on the Entity itself? You can use .loadModel/.loadModelAsync , which will flatten the USDZ into a single entity. Then call generateCollisionShapes and pass that entity to the installGestures function. This will make your USDZ one single entity that you can interact with. Got it! Thank you so much for the follow-up, <@U03HHPAHQ3C>. This has been super helpful! Why are ARGeoAnchors not available in other regions? It should be usable with just gps without the localization imagery. Hey there, this session is focused on RealityKit and Reality Composer. There\u2019s an ARKit Q&A on Thursday @ 3:00PM and another on Friday @ 9:00AM. I\u2019m sure an ARKit engineer will be able to assist you. Is there any updated to Reality Composer this year? No. I know you don't discuss future releases. But is Reality Composer something we can rely on to be updated with new features? And will any features we request or file feedback on have to wait until next year, or can reality composer be updated independently, similar to reality converter? We don't discuss details about unreleased updates, but one of the things that\u2019s most helpful to us as we continue to build out our suite of augmented reality developer tools is feedback. Please continue to submit ideas or suggestions in https://feedbackassistant.apple.com|Feedback Assistant :slightly_smiling_face: What AR features/updates are you most excited for/proud of this year? Personally, I'm super excited about https://developer.apple.com/augmented-reality/roomplan/|RoomPlan . I have friends and family that work in real estate and interior design. This is going to make their lives a lot easier :slightly_smiling_face: Is there a way to have light sources in AR Quick Look files hosted on the web? For example, a client would like to have lamps in AR Quick Look. It would be awesome if we could use RC to turn off/on light sources. Is there any way to do this? I don't think that it's possible. But you should submit the idea for supporting virtual lights on https://feedbackassistant.apple.com . Can I render a snapshot of only the virtual content in RealityKit? Something similar like the snapshot functionality in SceneKit? Yes, you can use ARView.snapshot(...) : https://developer.apple.com/documentation/realitykit/arview/snapshot(savetohdr:completion:)-66jzu Thanks <@U03J7GN26C8>. But this would include the background (ARFrame), too, or? You can change the background of the ARView there: https://developer.apple.com/documentation/realitykit/arview/environment-swift.struct/background-swift.struct Perfect. Thank you <@U03J7GN26C8>! I am interested in implementing something similar to what you see in the \"Measure\" App where a 3D object tap \"transforms\" it with animation to a 2D view, any directions/hints regarding that? Hey there, not 100% sure what you\u2019re trying to do here - so maybe a Lab might be a better forum to discuss this\u2026 But based on what you\u2019re describing you can snapshot your RealityKit scene with ARView.snapshot and render it as a texture on another Entity perhaps? (we have our last labs tomorrow so you\u2019d have to sign up today) totally agree, i'll sign up for the labs. Ty! Is it possible to instance meshes in RealityKit (similar to SceneKit's clone method)? If you call https://developer.apple.com/documentation/realitykit/entity/clone(recursive:)|\".clone(...)\" on an Entity, the clone will re-use the same meshes. Good to know. I saw the draw call increase when cloning so assumed the mesh wasn't instanced. I guess \"instance\" in this case means sharing the same geometry memory Many aspects of USD are open source. Could Reality Composer also be Open-Sourced so that members of the community could work on features? Hey there, we\u2019d definitely be interested in hearing more about your idea. I\u2019d suggest submitting the suggestion at https://developer.apple.com/bug-reporting/ . In SceneKit there were shader modifiers? Is there something similar in RealityKit? We need PBR shaders but have to discard certain fragments. You can apply https://developer.apple.com/documentation/realitykit/custommaterial|CustomMaterials & https://developer.apple.com/documentation/realitykit/custommaterial/surfaceshader|CustomMaterial.SurfaceShader to achieve certain cool effects for entities! From the metal side you can call discard_fragment(); Thank you <@U03HHPAHQ3C>! Hi! Reality composer objects on top of each other, such as a vase on a table cast shadow only to the ground plane and not to one another. If baked AO textures aren't an option since the vase may be moved by the user what would you suggest in order to achieve an equally good result to the default grounding shadow given that the quality of shadows is critical for an AR experience? We don\u2019t have any materials you can apply to objects to make them participate in the same shadows as ground planes, however, you can enable shadow casting from directional and spot lights via DirectionalLightComponent.Shadows and SpotLightComponent.Shadows . This may alter the overall lighting of your scene though. Alternatively, we do have CustomMaterial , which allows you to <https://developer.apple.com/documentation/realitykit/modifying-realitykit-rendering-using-custom-materials |create custom materials via Metal>, but for this use-case may not be able to get you the desired effect. We\u2019re always looking to improve RealityKit, so would appreciate if you submitted a request for this via https://feedbackassistant.apple.com/ I\u2019ve been implementing the 1st approach and agree on your comment. I\u2019ll definitely submit a request. Thanks! Is it possible to take a snapshot of only the virtual content and a snapshot of only the real content like in SceneKit? That\u2019s a good question. I think you can get some of the way there via the ARKit apis to get the current frame. You can also toggle the mode of an ARView to switch it to .nonAR view - then use ARView.snapshot() to grab a snapshot of the virtual content. And then switch it back. However I don\u2019t believe that would give you exactly what you want - I think the ARView snapshot would not necessarily have a transparent background (if that\u2019s what you need). And even then the performance of this may not be great. I\u2019d suggest filing a feature request for this with https://developer.apple.com/bug-reporting/ I think a feedback item and an explanation of your use cases would be very welcome. instead of changing the mode to nonAR, you could replace the camera feed with a solid colour (green), then take the snapshot But I don\u2018t think the user will be happy with this green background in the time of the snapshot You certainly could try setting the Environment background color to something with 100% alpha And if it doesn\u2019t work a bug report with just that would certainly be useful. What is the best way with USDZ content to link to external website or take them to a product landing page? Hey there - if you have your USDZ content on the web you can check out the AR Quick Look functionality for such things at https://developer.apple.com/documentation/arkit/adding_an_apple_pay_button_or_a_custom_action_in_ar_quick_look Thx, appreciate the answer. So it must be wrapped in a web experience. So this can\u2019t actually exist say within the USDZ if that is sent via imessage? You would rather have to send them a link to the webpage? As far as I know there isn\u2019t currently a way to do such a thing directly from a USDZ sent from iMessage, but I can pass that request along Is there a way to capture a snapshot of only the virtual content rendered on-screen, but not the background (having the background be transparent) while continuing the on-screen experience for the user where they still see the virtual content and camera feed? The use case here is that mixing Vision's person segmentation and ARFaceTrackingConfiguration (trying to overlay 3D content on a tracked face while putting fun backgrounds behind the person in the frame) results in the 3D content typically being cut off due to the segmentation mask. It'd be great to be able to render the 3D content back on top of the composited segmented image. Hey we just answered a very similar question: https://wwdc22.slack.com/archives/C03H49QK07P/p1654727671856229 If you take a look at that question/answer - maybe it help. I don\u2019t think we can necessarily do exactly what you need but it might give you a start. Just saw that! Thanks, <@U03H3628BL7>!!! Can Reality Composer be made available as a macOS app in the App Store? While Reality Composer is available only for iOS and iPadOS on the App Store, we'll pass this feedback along. Thanks :pray: Reality Composer is available on macOS as part of Xcode as a Developer Tool. ..given it's already in Xcode yes it would be relaly cool especially when working with executives who aren't engineers. Yes please!! Sucks to have to download 10gb of Xcode just for RC is there a capture video for ARView the way there is a take snapshot() ? i see there is 4k video being hyped - will this include the ability to let users take video recordings? I'm fuzzy on this. Hi, there\u2019s no API in RealityKit to capture video. That said there are system level apis to capture screen recordings and I wonder if that would be useful for you: https://developer.apple.com/documentation/screencapturekit/ I\u2019d suggest filing a feature request with your use-case. Thanks! ( https://developer.apple.com/bug-reporting/ ) oh wow thanks! ya i've seen screen capturekit but i will file feedback and explore it too Thank you! Hello, for artist/designer only experienced with Reality Composer with no code, is there any suggestion and resources on getting started with RealityKit to make more advanced AR experiences? Hi! We have a number of WWDC sessions covering RealityKit and Reality Composer which is a great place to start. \u2022 <https://developer.apple.com/videos/play/wwdc19/609/ |Building AR Experiences with Reality Composer> \u25e6 https://developer.apple.com/documentation/realitykit/creating_a_game_with_reality_composer|Sample Project \u2022 <https://developer.apple.com/videos/play/wwdc2019/605/ |Building Apps with RealityKit> \u2022 <https://developer.apple.com/videos/play/wwdc2020/10612/ |What\u2019s new in RealityKit> \u2022 <https://developer.apple.com/videos/play/wwdc2021/10074/ |Dive into RealityKit 2> \u25e6 https://developer.apple.com/documentation/realitykit/building_an_immersive_experience_with_realitykit|Immersive App Sample \u25e6 https://developer.apple.com/documentation/realitykit/creating_an_app_for_face-painting_in_ar|Face-Painting Sample \u2022 <https://developer.apple.com/videos/play/wwdc2021/10075/ |Explore advanced rendering with RealityKit 2> There\u2019s also a great guide on building a \u2018SwiftStrike\u2019 game: <https://developer.apple.com/documentation/realitykit/swiftstrike_creating_a_game_with_realitykit |SwiftStrike: Creating a Game with RealityKit> Are world map archives something that can reasonably be used as an app asset and loaded for all users of the app? In this case, I'm thinking of scanning a room and creating an AR experience that is anchored to the specific areas of room geometry - parts of the architecture and fixtures. Hey Cameron, I think that\u2019s a question best tacked by the ARKit team directly. They have ARKit lounges tomorrow at 3PM and Friday at 9AM. I think you\u2019d probably want to talk directly to an ARKit engineer\u2026 Thank you! Is there a way to get access to more advanced materials rendering on RealityKit models? I want to \"skin\" a plane with a UIView, currently I need to fall back to ARKit and SceneKit in order to do this RealityKit has a CustomMaterial API which allows you to create custom Metal-based materials. I\u2019d recommend our https://developer.apple.com/videos/play/wwdc2021/10075/|Explore advanced rendering with RealityKit 2 WWDC talk to learn more. Saw you answer a similar question with that API, am having a look. Thanks so much! There is also a great resource on https://developer.apple.com/metal/Metal-RealityKit-APIs.pdf|Custom Shader API that gives more details on the APIs available in Metal. Is there a means of exporting a USDZ file (either from Reality Composer, Cinema 4D, etc., or programmatically), with a video texture already applied? There\u2019s no support for that in Reality Composer currently. As always a feature request filed on https://developer.apple.com/bug-reporting/ would be most appreciated. There\u2019s also no method to export USDZ from RealityKit and again feature requests appreciated. Thank you! Thanks, <@U03H3628BL7>! Much appreciated! Is it possible to show or hide only a single child node from a model entity dynamically? Hey we kind of discuss this in another thread\u2026. one second\u2026 https://wwdc22.slack.com/archives/C03H49QK07P/p1654725695867159 You can certainly load a model and preserve your hierarchy, then use the entity name or another attribute to find an entity\u2026 then hide/show it with Entity.isEnabled hope that helps. (look at https://developer.apple.com/documentation/realitykit/entityquery for finding entities efficiently) Thanks, I\u2018ll try this ;) Can I place a model in a target, such as a cover of a book or a QR, so that it doesn't move from that position by just using usdz? and how could I achieve this? You can use Reality Composer to create a scene attached to an image anchor. You can then export the scene to a USDZ or a Reality File. https://developer.apple.com/documentation/realitykit/selecting-an-anchor-for-a-reality-composer-scene Thanks a lot! Is taking the output MTLTexture from RealityKit 2's postProcessing pipeline suitable for writing to an AVAssetWriter, streaming via RTMP, etc? \u201cMaybe\u201d :slightly_smiling_face: So you can certainly take MTLTextures and convert them (if they\u2019re configured correctly) into CVPixelBuffers for AVFoundation to consume. That said it\u2019s really not the intended use case of RealityKits post processing functionality and I wouldn\u2019t be surprised if either it doesn\u2019t work as you\u2019d expect or if we break you in the future. Sounds like a great feature request though - https://developer.apple.com/bug-reporting/ Thank you, <@U03H3628BL7>! I have done this and it does work pretty well (albeit, not tried with ARKit 6's 4K resolution yet), but good to know that that's not intended. Thank you! i\u2019d still recommend filing a feedback item. Thank you! What's the best way to connect with Apple designers to talk about AR? The Design Labs? Definitely the design labs! There\u2019s also a session on designing for AR this year. https://developer.apple.com/wwdc22/10131 Thanks, it was a great session! Wish we could select an AR specific design lab There are definitely AR designers at the labs so maybe they can redirect you. Sorry I couldn\u2019t help more. Hey <@U03HMDX5C95>! When you request a design lab, be specific about your interests \u2014 we\u2019ve got designers here at Apple who work on AR technologies and we\u2019ll try to match you up accordingly! Awesome will do. Thanks <@U03H3628BL7> <@U03EBH4MA8Y> From an AR design perspective, what is best for knocking down objects? Say in a game where you knock down blocks, is it better to have the user run the device through the blocks, tap the blocks, or press a button to trigger something to hit the blocks? One of our designers answered your question: > It depends which approach is best \u2014 each have a set of pros and cons based on what you want out of the experience. It can be compelling to run through AR blocks if you want to emphasize lots of user motion in an experience and the scale of the experience is quite large \u2014 good for apps that can take advantage of wide open spaces. Tapping them is more immediate and indirect so if you wanted to destroy a tower quickly or something like that then that would be the way to go \u2014 and I could see that being very satisfying to trigger many physics objects to react at once. I think the same would apply to a button press, it\u2019s an indirect way to trigger it if the experience requires rapidly knocking them down. > > Overall I think it\u2019s up to what you want the experience to be, and maintaining internal consistency with other interactions within the app. Swiftstrike and Swiftshot are great example apps that use similar techniques: https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality https://developer.apple.com/documentation/realitykit/swiftstrike_creating_a_game_with_realitykit Is it possible to control audio media in USDZ (i.e. pause, skip, load new audio file) with a scene / behavior (using Reality Composer or other tool)? I know it\u2019s possible to trigger the start of audio. Currently Reality Composer does not support this. This sounds like a great feature request and we would appreciate if you can file feedback through Feedback Assistant. If you are willing to jump into code\u2026 You can use the https://developer.apple.com/documentation/realitykit/audioplaybackcontroller|AudioPlaybackController returned from the https://developer.apple.com/documentation/realitykit/entity/playaudio(_:)|playAudio API to play, pause, etc. You can also use https://developer.apple.com/documentation/realitykit/audiofileresource|AudioFileResource to to add / replace audio on entities. How am I able to access attributes (wall dimensions, numbers of objects such as chairs for example) of the USDZ that RoomPlan creates from within Swift? Do I need SceneKit or something else to access and modify that data? Hey Adam, that\u2019s really a question better suited for the RoomPlan and ARKit Engineers themselves. There\u2019s a RoomPlan that just started 15 minutes ago (check the Developer app). And there are ARKit lounges tomorrow at 3pm and Friday at 9am Also the documentation is up on the developer website for the RoomPlan framework itself - I\u2019d also suggest starting there: https://developer.apple.com/documentation/roomplan/ Regarding optimisations: is there support for level of detail and instancing in RealityKit? Hey there, So instancing is mostly abstracted away behind the Entity.clone() method there\u2019s another thread talking about that right now: https://wwdc22.slack.com/archives/C03H49QK07P/p1654727237961649 Level of detail is not currently exposed as API and we\u2019d recommend filing a feature suggestion on that https://developer.apple.com/bug-reporting/ That said you can implement Level of Detail yourself (probably using custom Systems and Components) although we understand that may not be ideal. Please file feature suggestions regardless! Thank you <@U03H3628BL7>! Is there a plan to have custom render passes like in SceneKit with SCNTechnique in RealityKit? While we do not currently support custom render passes, we have support for https://developer.apple.com/documentation/realitykit/arview/rendercallbacks-swift.struct/postprocess|post process effects . Please file a feature request through https://developer.apple.com/bug-reporting/|Feedback Assistant if your use case requires more customization:pray: Thank you <@U03HHPAHQ3C> Does RealityKit support light sources in objects \u2013 for example, if you wanted a light bulb. If so, is there documentation for this? Hey Emory, There\u2019s various sorts of lighting in RealityKit - you might want to start here perhaps? https://developer.apple.com/documentation/realitykit/pointlight (see the Cameras and Lighting section in the docs) Oh, that's awesome. But there's currently no way to export these lights as .usdz or .reality files? Let me bring in experts on that\u2026 I\u2019m not personally sure what we support in file. Sounds like we don\u2019t support lighting in Reality Composer unfortunately so I\u2019d suggest filing a feature suggestion https://developer.apple.com/bug-reporting/ I think it is a usdz thing, not Reality Composer. Not a question, but the RealityKit and Reality Composer teams deserve such huge kudos for giving us non-designers the ability to create 3D and AR experiences that are incredibly powerful. These are great apps and tools and make me feel very prepared for the future in learning them. I pasted your comment where the rest of the team can see it. Thank you for the kind words. In Reality Composer, a force was applied to an object. Then I wanted to animate it into another scene, starting from the post force location. Is there a way to apply a new scene using its last known position? I hacked the position by guessing the ending location and starting the next scene close to that position but it results in a slight motion jitter. Is this an rcproj that is embedded in an Xcode project? Sounds like you might be able to grab the position of the entity with the force applied to it and then set the new scene\u2019s position as that position? Started with a USDZ, and then added behaviors to it in Reality Composer. No Xcode involved. Perhaps there\u2019s a way to get the post force position in Reality Composer? I have to play the animation to finality, but then there is no way to observe the objects new XYZ coordinates, unless I stop the animation (but thus returning the object to its origin). I suppose a strong feature would be a way to view the object\u2019s live parameters during animation mode. This may be achievable if embedded in Xcode with some code. I recommend signing up for a Reality Composer lab if you would like to explore that further. But yes, being able to observe live parameters sounds like a great feature in Reality Composer. Please file a feature request using https://developer.apple.com/bug-reporting/|Feedback Assistant with your use case :slightly_smiling_face: is there a way to add gestures to an entire reality composer scene? i can add it to an individual entity it would be cool to let users place the entire scene (otherwise i lose all the reality composer behaviors when i just target the entity) A way to get the entity gestures working on an entire scene is to use visualBounds(\u2026) and create a CollisionComponent on the root entity. You can then use <https://developer.apple.com/documentation/realitykit/controlling_entity_collisions_in_realitykit |CollisionGroup> to make sure it doesn\u2019t interfere with any physics. If you\u2019re using ARView.installGestures(\u2026) you\u2019ll need the entity to conform to HasCollision , which may require you to create a new entity type for the root. Quick example: ``// New Entity type which conforms for HasCollision` class CollisionAnchorEntity: Entity, HasAnchoring, HasCollision { } // Transfer scene contents let collisionAnchor = CollisionAnchorEntity() collisionAnchor.children.append(contentsOf: originalAnchor.children) collisionAnchor.anchoring = originalAnchor.anchoring // Create CollisionComponent for bounds of scene let sceneBounds = collisionAnchor.visualBounds(recursive: true, relativeTo: collisionAnchor) let collisionShape = ShapeResource.generateBox(size: sceneBounds.extents) .offsetBy(translation: sceneBounds.center) collisionAnchor.collision = CollisionComponent(shapes: [collisionShape]) // Install gesture on new anchor arView.installGestures(for: collisionAnchor) ` This is also a great question for our RealityKit and Reality Composer lab, which is happening tomorrow @9:00 - 11:00AM. How does RoomPlan handle multiple floors? Can you extend a scan? What if room edges are not parallel, e.g. a lightly sloped roof? Hey Alex, that\u2019s probably a question a RoomPlan engineer would be better suited to answer. There\u2019s a RoomPlan Lounge that started almost an hour ago. Check the Developer app to find it. Is VisionKit / Data Scanner available in AR? Using data scanner via VisionKit is possible using ARKit. ARKit provides the captured image on the ARFrame . One can inject the ARFrame's captured image into data scanner and obtain information about text. However, the result will be two-dimensional. If the use-case is to bring the detected text into the AR world in three dimensions one needs to estimate a transform for the 2D text. ARKit does not support this natively but does support custom anchoring. When you say custom anchoring, do you mean taking the screen coordinate, doing a hit test, and placing anchor? Can we get the lidar camera position while doing a mesh in ARKit? ARMeshAnchor transforms are already aligned with the wide camera, which is also what the camera transform is relative to. Thanks :slightly_smiling_face: Is the mesh from an ARSession available through the delegate methods? Hi Stephen, Yes, once you turn on scene reconstruction by setting the sceneReconstruction property on ARWorldTrackingConfiguration . The meshes ares available as ARMeshAnchor through the ARKit's anchor delegate methods Reference APIs: ARSessionDelegate: https://developer.apple.com/documentation/arkit/arsessiondelegate ARSceneReconstruction: https://developer.apple.com/documentation/arkit/arscenereconstruction sceneReconstruction: https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3521376-scenereconstruction Thanks :slightly_smiling_face: I tried to set 4k capture based on these instructions: if let hiResCaptureVideoFormat = ARWorldTrackingConfiguration.recommendedVideoFormatForHighResolutionFrameCapturing { // Assign the video format that supports hi-res capturing. config.videoFormat = hiResCaptureVideoFormat } // Run the session. session.run(config) But it still seems to be at HD: device: <AVCaptureFigVideoDevice: 0x15283be00 [Back LiDAR Depth Camera][com.apple.avfoundation.avcapturedevice.built-in_video:9] hiResCaptureVideoFormat: <ARVideoFormat: 0x2823b00a0 imageResolution=(1920, 1440) pixelFormat=(420f) framesPerSecond=(60) captureDeviceType=AVCaptureDeviceTypeBuiltInWideAngleCamera captureDevicePosition=(1) videoFormat.imageResolution: (1920.0, 1440.0) I am using 11-in iPad Pro at iOS16, and xcode 14 beta Hi, is this iPad Pro 11-inch 5th generation? Note that this feature is only supported on iPad with M1. Model is MHQR3LL/A, with M1 Thanks, can you please report this to https://developer.apple.com/bug-reporting/ , we need to take a deeper look. Will do, thanks I figured it out. The video itself shows ARWorldTrackingConfiguration.recommendedVideoFormatFor4KResolution, but the \"code\" shows ARWorldTrackingConfiguration.recommendedVideoFormatForHighResolutionFrameCapturing Right, recommendedVideoFormatForHighResolutionFrameCapturing is used for capturing high resolution still images while the session is running. For 4K video, you should use recommendedVideoFormatFor4KResolution We are making a game called Follow the White Rabbit. About a magician whose magic suddenly works. What are some tips/best practices to prevent AR objects from shifting? We're finding a bit of drift that's most noticeable with larger virtual objects. :sparkles: :rabbit: :tophat: :sparkles: Hi Nicole, We recommend adding an ARAnchor in the position where you want to place and object and then associate your node/entity with that anchor. This should help prevent drifting. <@U03HERF817F> Thank you! Hi, I\u2019ve had some experience with Reality Composer, but for coding, I only know SwiftUI. Is it possible to create an AR App with AR Kit only with SwiftUI\uff1fif so, could you share some suggestions or links on getting started? Hi Haolun, You can use ARKit inside a SwiftUI app. You can also use RealityKit to build ARKit apps in a declarative way. Here are the links to resources and sample code to help you get started: ARKit - https://developer.apple.com/documentation/arkit RealityKit - https://developer.apple.com/documentation/realitykit Thanks! Does adding anchors everywhere force ARKit to keep a good understanding and reduce drift everywhere? If yes, will this affect the tracking quality? ARKit offers functionality to add custom anchors which is the preferred and recommended way to place content. https://developer.apple.com/documentation/arkit/arsession/2865612-addanchor Custom anchors are used internally for drift correction. We cannot guarantee absolutely no drift. However, using your own anchors will use the system's best knowledge to correct for any drift. I am working on an app that uses ARKit to guide the user around an object while semi-automatically capturing images for later (server side) 3D reconstruction. I very much appreciate the ability to control the capture session and the ability to capture high resolution images that you added in iOS 16. I believe currently we do not have much control over the high resolution image capture? It would be great if we could configure the AVCapturePhotoSettings used for the capture. For photogrammetric reconstruction purposes it would be amazing if we could for example capture a Pro RAW image during the ARKit session. Hey Hendrik! We really appreciate the feedback and are glad that you are already starting to put these API changes to good use! At the moment, we do not expose the ability to pass in AVCapturePhotoSettings through our API, but this would be a great feature request to submit via http://developer.apple.com/bug-reporting/ We want to play with the depth map. Is it possible to get the lidar camera position with the depth map? We've tried using the wide camera position and it doesn't work, because the wide camera position is not the same as the depth map's camera position. Hi Stephen, The depth map surfaced through the Scene Depth API does align with the wide angle camera and should correspond to the camera transform available through the ARFrame. Here is a sample code that generates a colored point cloud by combining the wide angle camera image and depth map https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth If you still see some issues, I recommend filing a bug through the feedback assistant at http://developer.apple.com/bug-reporting/ (edited) Thank you :slightly_smiling_face: Does ARKit track which version of USDZ Is in use? I\u2019m interested in using tools from multiple providers in my pipeline and I want to verify the format is consistent through workflow. ARKit itself has no notion of rendered content. Content (USDZ) is commonly handled by the rendering engine on top of ARKit like RealityKit, SceneKit, Metal, etc. In order to learn more about USDZ and how to efficiently use it we recommend this talk. https://developer.apple.com/videos/play/wwdc2022/10129/ I'll be sure to check out the session. Thank you. Pixar provides USDZ Python and looking at the spec here I see no recent updates. https://graphics.pixar.com/usd/release/spec_usdz.html|https://graphics.pixar.com/usd/release/spec_usdz.html But I would very much like to learn how to use it efficiently both client and server side When I read the exif data that ARKit now provide I get: \"SubjectArea\": : ( 2013, 1511, 2116, 1270 ) \"LensSpecification\": ( 4.2, 4.2, 1.6, 1.6 ) What is SubjectArea? Why does LensSpecification has twice the 4.2 (the focal I think) and twice 1.6 (the aperture I think)? The subject area is defined as rectangle with center coordinate and its dimensions. In this case, the center is at 2013, 1511 with rectangle dimensions 2116 x 1270 . For more details, you may refer to Exif standard tags. Do modifications made to configurableCaptureDeviceForPrimaryCamera while an ARSession is running change the output of captureHighResolutionFrame? What about modifications before running a new ARConfiguration? No, any modifications to ARConfiguration object does not affect a running session. You need to call run after modifying configuration for it to be used. Awesome. Any reason why it's not an instance property on the ARConfiguration itself? Seems like that would more clearly communicate this behavior. Happy to file a feedback if that helps. <@U03HWCBGUBT> I went ahead and filed a suggestion feedback: https://feedbackassistant.apple.com/feedback/10142346|FB10142346 Thanks for the feedback. You can change capture device settings such as exposure, white balance etc, that will be reflected in the output of ARSession. However, you cannot change the input/output configurations on capture session. <@U03HWCBGUBT> is changing input/output possible before starting the session? No, its not possible to change it, ARSession always adds the required inputs and outputs. Thanks Arsalan! Are there resources on how to generate a texture for the mesh generated by ARKit ? We do not have any resources for this. You should be able to use the wide angle camera and camera transform to generate texture maps for the meshes but unfortunately we do not have any resources or sample code showing that. We do have this sample code showing how to generate colored point clouds using the scene depth API, hope it is of some help. https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth Any tips for getting started in AR development with 0 coding knowledge? A mentor would also be appreciated! Hey Ramiro! Regardless of your educational background, anyone can learn how to code if you put in the effort and are passionate about it. There are tons of resources online, many of which have been produced by Apple in the form of documentation, example projects, and WWDC videos, that can help you to learn a programming language, such as Swift. I would suggest doing some tutorials, watching videos, maybe find a highly rated book on iOS programming, etc to learn how to begin building iOS apps. Once you are comfortable with that, then you can start to dive into AR specifically. Finding a good book on linear algebra would be useful if you are going to get into AR and graphics programming, but start with the basics first! For ARKit , we have all sorts of documentation and examples that you can take a look at: https://developer.apple.com/documentation/arkit/ Apple\u2019s documentation is great. I also found the site RayWenderlich to be super helpful. They even have a book specifically for AR: https://www.raywenderlich.com/books/apple-augmented-reality-by-tutorials As well as a lot of great entry level tutorials and books. Do any of the AR frameworks have hand tracking, and the ability to register a pinch between the thumb and pointer finger? ARKit does not have any hand tracking feature. The Vision framework offers functionality for hand gesture detection. https://developer.apple.com/videos/play/wwdc2020/10653/ You may find the camera's captured images on the ARFrame and can inject this into Vision. So by combining multiple frameworks you could achieve something close to the requested feature. Awesome Thank you, In an ARKit game, I\u2019m detecting collisions between the real user and walls that I build out of SCNBoxes. I have a cylinder that follows the device\u2019s pointOfView to accomplish that. I use both physicsWorld didBegin and didEnd contact delegate methods because I need to know both when collisions start and when they end. Most of the time this works fine, however, sometimes the didBegin method runs last, even though I clearly moved away and there are no more collisions so my app still thinks that we\u2019re in a collision state. Any idea why this could happen?\u2028 Thanks! Hi <@U03JXRWDTDG>. In this lounge we are focused on questions to ARKit, it looks like your request is specific to SceneKit. Please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Thanks. Indeed, it\u2019s SceneKit inside ARKit. What I ask can only happen in AR where the actual user moves in the world and interacts with nodes that I build. I will definitely ask the question in the Developer Forums. Thanks! Does triangles vs quads matter for ARKit Hey Adrien, I\u2019m not sure of the context of this question, would you mind reformulating it / adding more context and post it again? Does ARKit give any confidence score for each camera position it estimates during camera tracking? If any camera position is not estimated correctly do you suggest any option to improve it? Yes, ARKit returns tracking state for every frame update. You can read more about it here: https://developer.apple.com/documentation/arkit/managing_session_life_cycle_and_tracking_quality It is highly recommended to integrate standard coaching view in your app to guide users when tracking is limited. More details at: https://developer.apple.com/documentation/arkit/arcoachingoverlayview Can you explain a little more about the confidence score and what the physical meaning is? How do you define the confidence score? You can find details at https://developer.apple.com/documentation/arkit/managing_session_life_cycle_and_tracking_quality Basically, ARKit provides discrete tracking states only. Is it possible to do body tracking while being in an ARWorldTrackingConfiguration? I believe it isn\u2019t possible but to make sure. Correct, 3D body tracking is only supported using the ARBodyTrackingConfiguration . However, we support the detection of 2D bodies on multiple configurations; the ARWorldTrackingConfiguration is one of them. In order to check which configuration supports this you may use the supportsFrameSemantics: function. https://developer.apple.com/documentation/arkit/arconfiguration/3089122-supportsframesemantics?language=objc Thanks! I didn\u2019t know that 2D tracking was supported on ARWorldTrackingConfiguration Is there a way to select wish rear camera to use for ARView? (Wide, ultrawide, panoramic) Hi Hassan, ARKit only supports wide camera as the primary camera. It is not possible use other cameras for rendering. Thank you . It have be nice to have ultrawide too it gets more content of real world that users can film Thanks for the suggestion. Please file a feature request through the feedback assistant at http://developer.apple.com/bug-reporting/ Is it possible to use the front facing camera with ARKit? The face tracking configuration does use the front facing camera https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration Can I use the MapKit 3d model of a city, and anchor it as a child of an anchor using lidar geotracking? For long distance occlusion and collision purposes? There is no integration of MapKit into the ARView. If you know the building footprint (i.e. the polygon in lat/lon coordinates) or even exact geometry anchored to a lat/lon coordinate you can transform these coordinates by placing a GeoAnchor at a that location and work with the local coordinates for the purposes of occlusion and collision. I wanted to occlude something big like buildings in front of a dinosaur terrorizing downtown I think this might work, when the players keep their vision down. So what I meant is if you know the four corners of that building you can create four Location Anchors at those lat/lon coordinates. If they are tracked in ARKit you get the local coordinates and can build an occlusion/collision mesh. Wow, thank you for the help. I didn't think about pushing it that far with geoanchors I've been trying to build the Hydra sample, will it always require CMake? Hi Steven. In this lounge we are focused on questions to ARKit, it looks like your request is specific to USD. As the USD lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ What\u2019s the maximum dimensions RoomPlan support ? The recommended maximum size of the room is 30 x 30 feet. Thank you One we have a RoomPlan scan, can we use it next time as a anchor so we can always Paint Model in same place ? RoomPlan is not an ARAnchor in current design. Thanks for suggestion. We will take into consideration. Thank you <@U03J1MVSDBP> I created a demo where custom ARAnchors are created for RoomPlan objects. The same could be done for surfaces and then saved to a world map: https://github.com/jmousseau/RoomObjectReplicatorDemo Thank you Does setting ARKit to use 4K resolution affect the battery longevity ? Does it increase the risk to get the device too hot, even if the fps is limited at 30 fps instead of 60 fps ? Is there a way to get 60 fps at 4K resolution ? Hey Gaetan! Yes, using 4k resolution may result in more power being consumed. It may also result in thermal mitigation engaging to keep the device from getting too hot, which may impact performance. At the moment, we are only supporting 4k @ 30hz. Hi, ARSession has the getGeoLocation(forPoint: \u2026 method. Is there also a way to obtain the heading relative to north given a directional vector within the scene or for the device (point of view)? We are not exposing the heading directly. You can create any GeoAnchor in your vicinity and then compare its transform with your camera\u2019s transform. Since GeoAnchors are always aligned to East-Up-South, you can derive any global camera orientation by comparing the camera\u2019s transform to the GeoAnchor transform. Gotcha, thank you <@U03J7GC2A2C>! Do you think this would be worth a feature requests to avoid the additional anchor or is it rather unlikely that this will get exposed? Anything you need is important to us :heart:, please file it. Might there be more example projects showcasing pure Metal with ARKit? SceneKit is cool, but admittedly, I'd love to see more low-level examples. :) Alternatively, is anyone working on some open source projects showcasing something like this? I think it would be a big win for Apple-platform development to build-up a lot more examples. Hi Karl, Thanks for the suggestion. Here are some existing sample code that uses Metal with ARKit: https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth https://developer.apple.com/documentation/arkit/environmental_analysis/creating_a_fog_effect_using_scene_depth https://developer.apple.com/documentation/arkit/displaying_an_ar_experience_with_metal?language=objc Hello, and thanks! Is the \"new lighting\" capability only in AR Quick Look or is it also available in RealityKit and SceneKit renderers? Hi George. In this lounge we are focused on questions to ARKit, it looks like your request is specific to RealityKit / SceneKit. As those lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Do any of the AR frameworks accept location or hand position from Apple Watch? No, ARKit runs standalone on iPhone and iPad devices only and does not take any external inputs. <@U03HWCBGUBT> thanks for doing the legwork. I see the other response regarding Vision and World tracking. I'll work with that. I have a strange ARKit+SceneKit rendering bug and wanted to see if any of the engineers ARKit engineers participating here might have an idea as to what might be causing this and how to fix it? I am rendering a semi-transparent object using SceneKit. The material uses the constant lighting model, a texture controls transparency. If the scene is fairly bright, then the transparent material will initially show less transparent than it should. If the camera gets pointed at something less bright (or the bright part is blocked as in the example video linked below) then the rendering suddenly switches to be more transparent. This switch is permanent, it does not switch back again. See example video here: https://www.dropbox.com/s/8wpo83loxg51o43/ARKit_rendering_bug.mov?dl=0|https://www.dropbox.com/s/8wpo83loxg51o43/ARKit_rendering_bug.mov?dl=0 Any idea what might be causing this? Hi Hendrik. In this lounge we are focused on questions to ARKit, it looks like your request is specific to SceneKit. Please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ I think this is actually more of an ARKit issue though. Because the rendering changes based on the camera feed brightness. SceneKit does not know about ARKit\u2019s camera feed. Also: Are there still SceneKit engineers at Apple? I would be surprised if there were. Somehow a switch seems to be flipped that changes SceneKit\u2019s rendering dramatically. But based on my understanding I would think it has to be ARKit that is flipping that switch (intentionally or accidentally) based on the scene brightness. Where by \u2018scene\u2019 I am referring to the real world surrounding.. You can try setting automaticallyUpdatesLighting to false in ARSCNView to disable the behavior. We can capture session events (namely anchors add/remove) by implementing ARSessionDelegate (not RealityKit), is it possible get similar or part of this events with RealityKit? (To avoid converting a from ARAnchor to AnchorEntity) RealityKit exposes the ARSession through this API https://developer.apple.com/documentation/realitykit/arview/session You can set the delegate on it to listen to ARKit delegate events Thank you I have an ARKit app that uses SceneKit and PBR. Is there a simple process to migrate ~70K lines of Swift to use RealityKit? I need to resolve the rendering differences between my ARKit app that uses the SceneKit renderer and the renders that come from AR Quick Look's use of the RealityKit renderer. Hi George. In this lounge we are focused on questions to ARKit, it looks like your request is specific to SceneKit / RealityKit and Swift. I\u2019m not aware of any tool for porting large chunks of swift code. Please use the developer forums or the feedback system to get connected to an engineer from other team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Is there a talk that goes into any detail about using the new Spatial framework - https://developer.apple.com/documentation/spatial|https://developer.apple.com/documentation/spatial - and how it works with ARKit, SceneKit, and/or RealityKit? There is no dedicated talk about Spatial framework. It provides core functions that can be used with any 2D/3D primitive data. Thank you When using the new 4K resolution in ARKit for a post-production (film/television) workflow, what is the suggested way to take the AR experience and output to a video file? Hi Brandon, To capture and replay an ARKit session, see an example here: https://developer.apple.com/documentation/arkit/arsession/recording_and_replaying_ar_session_data If you want to capture video in your app in order to do post processing later, you could use and configure an AVAssetWriter to capture a video. https://developer.apple.com/documentation/avfoundation/avassetwriter Thanks, <@U03K4D9U7GU>! Just to follow-up on that comment; do you have a suggest way of getting the output of the ARSession into a format that can be written to an AVAssetWriter (I'm imagining this would require a custom Metal processing pipeline to convert the MTLTextures to CVPixelBuffers, as I don't think there's a way of getting CVPixelBuffers out of ARKit directly)? We do provide a camera frame with every ARFrame, see: https://developer.apple.com/documentation/arkit/arframe/2867984-capturedimage Thanks, <@U03K4D9U7GU>! Just for posterity; the ARFrame's CVPixelBuffer (rather, capturedImage property) doesn't include the overlaid AR content, as the user might see through the phone, though, correct? This is the background video feed only? Yes, ARFrame\u2019s capturedImage is just the \u2018clean slate\u2019, it doesn\u2019t contain any virtual content rendered on top of it. Yeah, the output from ARKit is just the camera frame without virtual content. If you are doing your own rendering and your metal textures are backed by IOSurface s. then you can easily create CVPixelBuffer s using the IOSurface s and then pass those to AVFoundation for recording. Got it, thanks, <@U03J7GC2A2C> <@U03H35XE3N3>. I thought that was the case! I appreciate your replies! If I create a USDZ with SCNScene.write(...usdzURL...) will it use the \"new lighting\" and contrast adjustment in AR Quick Look? Hi George. In this lounge we are focused on questions to ARKit, it looks like your request is specific to Quick Look and USD. Those lounges are closed for WWDC, so please use the developer forums or the feedback system to get connected to an engineer from other team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Is there a way to get notified when ARKit relocate itself when it finds out that it has drifted ? From my experience, the tracking status does not change when this happens. Also is there a way to ask ARKit to not try to relocate itself after a drift ? Hi Gaetan, We recommend adding ARAnchors and associating your virtual content with ARAnchors. In case there is a drift, the anchor delegate didUpdateAnchor would update the anchor such that the virtual content stays in the same location in the real world. In recent years I read and partially experimented with the latest \"graphics\" frameworks - but somehow I got lost over a cohesive developer experience when to use which framework (and how to integrate them into a good product). The are amazing \"vertical\" solutions in these frameworks but I see only few strong stories/app/solutions around them. Does Apple has a \"big picture\" guide when to use which framework, how to interact between them? Many of them work together...are there particular corner cases you had? Hi Oliver, We understand that the number of frameworks can be daunting sometimes. However as you alluded to, we try and offer \"high level\" frameworks to try and meet developers' needs out of the box, for example, being able to use RealityKit for rendering instead of the lower level Metal. That said, Apple provides several tutorials and code samples to introduce developers into the various frameworks, e.g. https://developer.apple.com/documentation/realitykit/building_an_immersive_experience_with_realitykit . Another great resource are WWDC videos, which go back several years in order to build a solid understanding of a particular framework or technology. https://developer.apple.com/videos/all-videos/ For ARKit specific, see Ryan's great answer here: https://wwdc22.slack.com/archives/C03H49QK07P/p1654815037708349?thread_ts=1654815015.825259&cid=C03H49QK07P <@U03K4D9U7GU> thank you, these are fine vertical guides but I\u2019m looking for a more horizontal approach. (super specific example, probably needing, camera, vision, realitykit, arkit\u2026): can we let the user take a picture of a tree and map it as a texture easily onto a pair of glasses frame for AR preview when he is looking into the mirror? the solution would probably need a USDZ frame model to be rendered in RealityKit, but can we map a UIImage onto an element in USDZ programatically? I'm pretty sure RealityKit has TextureResource which can take a CGImage (the base decoding class) and goes \"up\" to a MetalTexture Knowing the basic data flow helps here because taking a photo means a photo file which has to be decoded first into some kind of pixel buffer Then that has to go to the GPU, usually as a texture Any guidance on how to build a bridge between ARKit and Spatial Audio? Say you're viewing an object and the audio evolves as you change the object's perspective We do not have a sample code that uses ARKit together with spatial audio (PHASE). However, this is a great question, can you please send us a request through https://developer.apple.com/bug-reporting/ <@U03HWCBGUBT> yes, will do! thank you I'm continuing to have issues with the performance of adding interactivity to an entity added via RealityKit ( Entity.loadAsync(named: \"MyEntity\") ). - I create a ModelEntity. - I added my async loaded Entity as a child to the ModelEntity. - I add the ModelEntity as a child to the AnchorEntity. - I add the AnchorEntity to the ARView's scenes. - I get the bounds of the entity using entity.visualBounds(relativeTo: parentEntity) - I add Collision to the ParentEntity with parentEntity.collision = CollisionComponent(shapes: [ShapeResource.generateBox(size: entityBounds.extents).offsetBy(translation: entityBounds.center)]) - I install gestures .all on the ARView. This results in an Entity I can pinch and scale, but no manner to rotate or transform its position. Am I doing something wrong? Hi Brandon. In this lounge we are focused on questions to ARKit, it looks like your request is specific to RealityKit. Those lounges are closed for WWDC, so please use the developer forums or the feedback system to get connected to an engineer from that team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ With body motion tracking, can a dev specify the sample rate of the sample(every few ms) and write out that sample in a continues manner. eg a basic motion recorder. Please ignore the question if this is the wrong place to ask Body tracking runs at 60 Hz at the same cadence of the camera. We cannot compute this faster than this. However, by changing the ARVideoFormat you may change this to 30 Hz or other supported frame rates. We do not offer a functionality to write the motion capture data to file. However, our data is very well compatible with a format called BVH. By following the topology of the skeleton given by the ARSkeletonDefinition and the data coming from an ARBodyAnchor one could generate such a file output. https://en.wikipedia.org/wiki/Biovision_Hierarchy GOLD! solid gold! thanks for the help Have there been any changes to the light estimation APIs? For example, is directional light available with a world tracking config? No, there haven\u2019t been changes to light estimation in ARKit this year. Thank you! Is there any way to protect usdz files so that they cannot be downloaded from the web, but still be seen in ar quick look? Hi Jose Mariano. In this lounge we are focused on questions to ARKit, it looks like your request is specific to QuickLook/USDz. As those lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ In \"Discover ARKit 6\" there's a cool demo of setting a point in AR where a picture was taken, and guiding the user there. Is sample code for this available somewhere? Thanks for your excitement about that app idea. We do not have the sample code, but I recommend going to our ARKit 4 session where we explain how to pick a coordinate in Maps and create a GeoAnchor based on it. For alignment with the real world we have the example with the Ferry Building in SF. We followed that exact workflow with the focus square example. Hi ! Does adding custom anchors for drift correction is pertinent on a lidar enable device ? In general we recommend to use/add anchors independent from the device you are using. Hello USD team, Thanks for the great demo USD and Hydra/Storm demo! Is it possible to build an app with USD Hydra/Storm - SwiftUI interactions? Ex: a car configurator that has: - A car as a USD file - USD PhysicsRigidBody joint applied on the door of the car USD file. - SwiftUI button to trigger opening the doors - Everything rendered in Hydra/Storm. as the demo app Hi Alexandre, During this lounge we are focused on questions on ARKit. The USD lounges are already closed for this WWDC. So for those question I encourage you to use the developer forums or the feedback system to get connected to an engineer from those team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ ok, thanks I noticed that the built-in Camera app can detect very small QR codes compared to 4K AR. Why is that? Is there a workaround? Hi Ivan, we don\u2019t have QR code detection in ARKit. However, you can use the Vision APIs to do QR code detection on the captured image. This VisionKit talk and sample code might be of interest to you: https://developer.apple.com/videos/play/wwdc2022/10025/ https://developer.apple.com/documentation/visionkit/scanning_data_with_the_camera I am really struggling to build user interactions in RealityKit that are as \u201csmooth\u201d as AR QuickLook (in terms of a user placing, moving, rotating, or scaling a model). I\u2019m using \u201cinstallGestures\u201d on the ARView but it\u2019s nowhere near as smooth as the AR QuickLook experience. Is there any suggestions of how to get AR QuickLook style integrations? Hi Brandon. In this lounge we are focused on questions to ARKit, it looks like your request is specific to QuickLook/RealityKit. As those lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Regarding the new ARKit 6 api that takes a 4k photo of the AR scene, is there a limit to how many times it can be called? Can I take say 30 photos within a second? You can take the next photo right after the completion handler of your previous captureHighResolutionFrame call - or even from within the completion handler. If you try taking a new photo before the previous call completed, you will receive an highResolutionFrameCaptureInProgress error in the completion handler. Thank you <@U03HWCCTAKT>! Thats good to know. I ask that to see if I can use the same workflow to capture short videos as well if possible. We'd like to use an object both as a source for a USDZ based on the PhotogrammetrySession and as an ARReferenceObject, so that we can overlay information at the same position on both the real object and the created model. Is there any guidance on how to align these coordinate systems, e.g. by aligning the point clouds from the photogrammetry session and reference object? Or can we make assumptions on the origin of the resulting USDZ from the PhotogrammetrySession? Creating a model for ObjectDetection and creating a textured mesh with ObjectCapture are two different use cases with separate workflows, we do not offer a tool to convert from one to another. That sounds like a great use case though, I encourage you to file a feature request. Feedback Assistant: https://feedbackassistant.apple.com FB10160265 :100: Is there a maximum number of 2D bodies that can be tracked in an ARWorldTrackingConfiguration? Hi Brandon! ARKit detects one body at a time. If multiple people are in the scene, the most prominent one is returned. Thank you, <@U03HWCCTAKT>! Is it now possible to do AR with the ultra wide angle 0.5 camera? Unfortunately not. ARKit consumes the UW camera internally for certain processing tasks in a specific configuration. Though I encourage you to file a feature request. Feedback Assistant: https://feedbackassistant.apple.com We're planning to integrate an AR distance measuring view into our app. Does ARKit now provide the necessary technology to achieve this, or is RealityKit a better match? Are there any useful docs to look at? Hi Yurii, ARKit offers several ways to measure distances. You can either evaluate distances from the device to its environment or between ARAnchors. Please see this documentation to get an overview: https://developer.apple.com/documentation/arkit/environmental_analysis It looks like your question is also tied to RealityKit, but in this lounge we are focused on questions to ARKit. As RealityKit lounge is closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Thank you <@U03HB4VGW30> Are there any good resources on getting started with estimated object dimensions? Similar to the measurable app but to do height and width. Hi Eduardo, I recommend checking out our documentation of our SceneGeometry API that we presented in ARKit 3.5. A good overview is given in this tech talk: https://developer.apple.com/videos/play/tech-talks/609 After getting a geometry that is good enough you still have to solve the task of isolating your object of choice and computing its volume. There would be several ways of doing it. For example cutting everything off above the ground level, or letting the user create a cube objects and then intersect it with the scene geometry, we do not have any code sample for these tasks, though. Thanks <@U03J7GC2A2C> Would it be possible to get the dimension of small objects? Ex. Ball, pillow, cup, etc. Video feed is always overexposed using ARKit. Trying to enable HDR for ARSession doesn't seem to work. Setting videoHDRAllowed to true on ARWorldTrackingConfiguration does not change video rendering. Also when accessing the AVCaptureDevice with ARWorldTrackingConfiguration.configurableCaptureDeviceForPrimaryCamera, activeFormat.isVideoHDRSupported returns false (on iPhone 12 Pro Max) so I cannot set captureDevice.isVideoHDREnabled to true. Also when using setExposureModeCustom and setting iso to activeFormat.minISO, the image rendered by ARKit has always a way greater exposure than when running an AVCaptureSession. The use case is for using ARKit in a Basketball stadium: the pitch always appears totally white with ARKit so we cannot see any player while with AVCaptureSession (or just the iOS camera app) the pitch and players appear clearly thanks to HDR. Hi Brian! Setting videoHDRAllowed means that HDR will be enabled on the formats supporting it; however this is not the case for all video formats. In iOS 16, ARVideoFormat has a new property isVideoHDRSupported . You can filter the list of the configuration\u2019s supportedVideoFormats to find one where videoHDRSupported is true , and set this format as the configuration\u2019s videoFormat before running the session. Thank you! I'll try it out! I just tried it, and it worked! Thanks :smile: This is a great addition to ARKit! A big thank you to the ARKit team for giving us access to those video options! Does ARkit or RealityKit support rigid body physics defined in a USD file? Hi Alexandre, ARKit doesn\u2019t support physics but rather detects the surrounding scene to allow RealityKit to handle virtual objects. RealityKit does support rigid body physics and a good place to start looking is at the physics APIs here: https://developer.apple.com/documentation/arkit/usdz_schemas_for_ar/simulated_physical_interaction/preliminary_physicsrigidbodyapi Not supported yet by RKit, but FYI: https://graphics.pixar.com/usd/release/api/usd_physics_page_front.html and https://graphics.pixar.com/usd/release/wp_rigid_body_physics.html This request is specific to the interaction between USD files and RealityKit, today we are focusing on ARKit question in our lounge. I encourage you to file a feature request through the feedback assistant. Feedback Assistant: https://feedbackassistant.apple.com Hi, and thanks for your work! I\u2019d like to ask what might be the possible causes for the ARSessionDelegate is retaining ARFrames console warning. I use the session:didUpdateframe: delegate method to just check whether the AnchorEntity(plane:) I\u2019m looking for is in a sufficient distance from the camera. Thanks. Hi Ioannis, we have a limited pool of resources for our ARFrames and in order to keep some available for ARKit to process, we recommend processing the frames as quickly as possible. If you need to perform longer computations, you can copy an ARFrame and release the ARFrame from the delegate method. Makes sense. Thank you! Hello, I would like to know if it's possible to use shareplay with a ARKit app? When i try there is no video on the facetime call if the back camera is started. Is it possible to have both camera at the same time (front for facetime and back for my AR app)? Thanks Hi Gil! ARKit configures the cameras according to the selected configuration. Capturing from another camera while an ARKit session is running is not supported. What can you share about this report FB9184883 ? Is it marked as a highest priority issue ? For those wondering, a post on the forum explains it : https://developer.apple.com/forums/thread/705860|https://developer.apple.com/forums/thread/705860 Hi Gaetan, I did check with the request you filed and pinged the engineer assigned to it. I also encourage you to attach an ARKit recording (one in portrait, one in landscape) so we can rerun and reproduce your exact results. If you\u2019re not familiar with the process, here\u2019s the link to the documentation: https://developer.apple.com/documentation/arkit/arsession/recording_and_replaying_ar_session_data?language=objc Alright, thanks <@U03J7GC2A2C> ! Is there a native 3D processing tool coming the iOS/macOS any time soon? We needed to implement USDZ cropping on iOS, and the 3rd party tools don't deliver the quality we want. Hi Yurii, In this lounge we are focused on questions to ARKit, it looks like your request is specific to USDZ and/or other tools. As the related lounge is closed for WWDC, please use the developer forums or the feedback system: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Is it possible to do perspective correction in ArKit using the captured depth map? like on the continuity camera \"desk view\" for ex Glad you\u2019re also a fan of the new desk view feature. There are potentially two solutions to this: You might just try to to a single perspective projections for the whole image The second solution is to use a per-pixel correction like you suggested. Both come with their own benefits and drawbacks. Please check out our documentation for implementing the second approach: https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth Is there any plan to allow built-in hands and finger detection within ARKit to let the user interact with an object directly with his hands and not only though touch events on the device screen? Hi Brian! ARKit has no built-in hand or finger detection, but you can use Vision to track hands or detect hand poses. Here is a developer sample illustrating this: https://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision . For ARKit feature requests, we encourage you to send us a report in Feedback Assistant: https://feedbackassistant.apple.com Hi Andreas! Thank you for your reply and for these informations. \"So what I meant is if you know the four corners of that building you can create four Location Anchors at those lat/lon coordinates. If they are tracked in ARKit you get the local coordinates and can build an occlusion/collision mesh.\" Hi Christian, did you mean I should build an occlusion mesh for every building I want occlusion/collision. I am kind of new to ARKit but I saw that I can create a metal renderer to work with ARKit. can I get depth information using a convolutional neural network from metal. sorry if this is off topic. Hey Abraham, throwing machine learning at it sounds like a super fun project, I would recommend to start a bit simpler. So as a small experiment you can take the four corners of a building from the Maps app and then create four Location Anchors based on these coordinates. As soon as those are tracked you can look at the local coordinates (in x,y,z) and then build a polygon based on it, you can extrude it towards the sky (y up) to get a nice collision/occlusion mesh. I will try Christian. I will have the code publicly hosted on GitHub https://github.com/Ansel7/Mobius ) and probably create some kind of tutorial on this journey. I have a lab scheduled for later today, I will present this solution, and ask for help on the implementation, Thank you ARKit team Using additional cameras in ARKit - are there any resources to show how this iss setup? Hi Dan! ARKit allows streaming video from only one camera at a time. Which camera is used is determined by your configuration (e.g. ARFaceTrackingConfiguration will use the front facing camera, ARWorldTrackingConfiguration will use the back wide camera). You can, however, enable face anchors detected by the front camera in an ARWorldTrackingConfiguration . Vice versa, you can enable isWorldTrackingEnabled in an ARFaceTrackingConfiguration to benefit from 6DOF world tracking. You can learn more about these APIs here: https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3223422-userfacetrackingenabled https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/3175413-isworldtrackingenabled . Or check out this developer sample: https://developer.apple.com/documentation/arkit/content_anchors/combining_user_face-tracking_and_world_tracking Am I able to capture frames off additional feeds at the same time (not necessarily exactly synchronous). Eg main at session happening on rear facing camera, additional video frames captured occasionally from front facing. Maybe I've been fooled, see an ARKit 6 breakdown that says \u201cSimultaneous front and back cameras with AR\u201d - is this real? We introduced new API to capture frames in higher resolution than your configuration\u2019s video format: https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframe . Those frames are captured from the same camera. Setting up additional cameras is not supported. We encourage you to file a feature request in Feedback Assistant: http://feedbackassistant.apple.com/ The sentence you quoted refers to the API I mentioned above. Not all of the features listed in the lower part on https://developer.apple.com/augmented-reality/arkit/ are new additions in ARKit 6; this is meant more as highlighting ARKit\u2019s capabilities in general. Ok\u2026 that's ancient news. Thanks for finding the source of my misconception! Great work guys! Hi! We're working on an AR experience that allows user to put AR objects in their surroundings and replay it later. We're saving the data on an ARWorldMap and archive it on the filesystem to be retrieved later. Everything works great on smaller areas with small ARWorldMap file sizes. However as user adds more stuff, the ARWorldMap file gets bigger and at some point, it takes so long or even impossible to relocalize using the big ARWorldMap files. I'm seeing slower relocalization on ARWorldMap files with 10 mb size. Question: Is there like a known cap of how big ARWorldMap files can be to retain effectivenes of relocalization and the AR experience? What can impact performance for AR relocalization other than lighting condition and the object textures that we're rendering (maybe area size? camera motion? features in the area?) since we're seeing frame drops on bigger ARWorldMap files. Thanks! ARWorldMaps are optimized for room sized scenarios. If you exceed that limit then relocalization will stop working in certain areas as the map is not big enough any more to cover the whole area. The frame drops sound related to the amount of content being displayed though. For that, feel free to provide more details through Feedback Assistant: https://feedbackassistant.apple.com What\u2019s the current upper limit on SKNode objects? Say you wanted to create several rows of image sprites/emoji for eg a virtual interactive bookshelf layout. Also if you wanted to animate these sprites would that be feasible? Hi <@U03J97X79CL>, In this lounge we are focused on questions to ARKit, it seems like your request is specific to SpriteKit. You might be able to find some answers in the SpriteKit performance documentation: https://developer.apple.com/documentation/spritekit/nodes_for_scene_building/maximizing_node_drawing_performance Otherwise, please use the developer forums or the feedback system to get connected to an engineer from SpriteKit. Is there a way to force ARWorldMap to relocalize on our position instead of inferring from the features around us? For example, since ARWorldMap has its own root anchor, can we do something like \"load this ARWorldMap using my current position/transform in the real world as the root anchor\"? From my understanding we can do this with a single/multiple ARObjects but haven't found any resources about collections of ARAnchors in an ARWorldMap This is not supported out of the box. What you could do is compute the offset between your current location (before relocalization) and after relocalization and apply that accordingly.","title":"arkit realitykit"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#arkit-realitykit-usdz-lounge-qas","text":"","title":"arkit-realitykit-usdz-lounge QAs"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hello-i-am-pretty-new-to-reality-composer-i-would-like-to-know-how-if-it-is-possible-to-add-textures-to-custom-usd-objects","text":"Reality Converter makes it easy to convert, view, and customize USDZ 3D objects on Mac. For more information, visit http://developer.apple.com/augmented-reality/tools/|developer.apple.com/augmented-reality/tools/ Thanks! I am looking forward to creating AR experiences using it. Thank you very much and keep up the good work! I really appreciate the opportunity to connect with Apple engineers. Thanks, Cristian-Mihai. We love connecting with developers from around the world :relaxed: Also, congratulations on winning the Swift Student Challenge this year :tada: Is Reality Composer Available for the Mac? RComposer on macOS is part of XCode install Yes, check out the link above. <@U03JRP87THN> Any hints what\u2019s the best way to use STL objects? I am trying to use Blender, but that is \u2026 another steep learning curve I use #b3d and avoid exporting to .usd because the upAxis of Blender and Apple are incompatible. Export to .fbx or .glb instead and use RealiytConverter (link above) to convert these to proper Apple compatible .usdz","title":"Hello! I am pretty new to Reality Composer. I would like to know how (if it is possible) to add textures to custom USD objects."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#the-recent-update-of-xcode-made-scenes-in-realitycomposer-look-much-darker-is-this-caused-by-a-change-in-realitykit","text":"Hi! We are aware of a bug in macOS Ventura/iOS 16 that is causing the lighting to appear darker. Please feel free to file a bug report on Feedback Assistant about this. If you are already aware of this - will it get a higher prio if non-Apple developers also complain? Same problem with <model> on iPadOS16beta I don\u2019t think the priority would change if we get more reports since it is already a high priority issue that\u2019s been assigned to an engineer to investigate.","title":"The recent update of XCode made scenes in RealityComposer look much darker. Is this caused by a change in RealityKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#trying-to-understand-how-this-works-via-slack-should-we-be-able-to-see-what-everyone-else-is-posting-what-the-questions-are-from-others-or-do-you-select-certain-questions-craft-a-response-then-post-both-thanks","text":"We select certain questions, craft a response, then post both :slightly_smiling_face: Thanks very much!","title":"Trying to understand how this works via Slack..., should we be able to see what everyone else is posting, what the questions are from others? Or, do you select certain questions, craft a response, then post both? Thanks."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#am-trying-to-use-lidar-scanner-to-create-a-3d-model-from-capturing-an-object-but-couldnt-get-enough-resources-for-that-any-referencesresources-please","text":"For creating 3D models from images captured on device, this should be a helpful resource to get more inspiration and help: https://developer.apple.com/videos/play/wwdc2022/10128/ For Object Capture specific questions, there will be a Q&A tomorrow from 3:00 - 5:00 pm that you can join! Apologies, I meant later today at 3pm!","title":"Am trying to use LiDAR scanner to create a 3d model from capturing an object. But couldn't get enough resources for that. Any references/resources please?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-of-exporting-a-reality-composer-scene-to-a-usdz-rather-than-a-reality-or-rcproject-if-not-what-are-your-suggested-ways-of-leveraging-reality-composer-for-building-animations-but-sharing-to-other-devicesplatforms-so-they-can-see-those-animations-baked-into-the-3d-model","text":"Yes, on macOS, you can open Reality Composer, Settings, Enable USDZ export Oh, cool! I did not know that. Thank you! Same with the iPad Version this export feature has been broken for years on RC for scenes of even moderate complexity. Have you filed a bug report on Feedback Assistant about this? yes, over a year ago. still broken can you share the Feedback ID? I can check on the status My experience is: don\u2019t use 8k textures and make sure all imported and placed USDZs use exclusive texture file names (folder numbering). This can be achieved by preparing ALL of them in a single export session of RealityConverter (or do lots of manual .usda editing) FA id: FB9622407 none of my FA requests have been resolved for RK/RC, btw, that I am aware of. Same trouble on my reports <@U03J2DUEV0X> I don\u2019t see any updates on this one unfortunately :disappointed: I reported, what I consider a critical RK bug over 4 months ago, no resolution. Forum ref title: iOS 15 RealityKit AnchorEntity possible bug and I have several more...unfortunately all stuck with no resolution. FB9679173 should be resolved in iOS 16 beta 1 can you not filter out my lounge request about why RC has not advanced in almost 2 years? stuck at version 1.5 as of xcode 14 Beta. I'm building a product with your framework, so this is a healthy conversation. let me see, but we don\u2019t really have much to share regarding RC. We\u2019re aware that it hasn\u2019t been updated in a while could benefit from more features, but don\u2019t have anything to announce right now. but we can start another thread about it thank you","title":"Is there a way of exporting a Reality Composer scene to a .usdz, rather than a .reality or .rcproject?  If not, what are your suggested ways of leveraging Reality Composer for building animations but sharing to other devices/platforms so they can see those animations baked into the 3D model?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#are-there-any-plans-or-is-there-any-way-to-bring-post-process-effects-and-lights-into-reality-composer-im-making-a-short-animated-musical-film-in-ar-i-love-how-rc-does-so-much-automatically-spatial-audio-object-occlusion-i-just-wish-it-was-possible-to-amp-up-the-cinematic-ness-a-little-with-effects","text":"Post processing effects are not supported in RC right now, only in a RealityKit app. However, feel free to file a feature request on Feedback Assistant about this. Ok, thank you!","title":"Are there any plans (or is there any way?) to bring post-process effects and lights into Reality Composer?  I'm making a short animated musical film in AR. I love how RC does so much automatically (spatial audio, object occlusion...). I just wish it was possible to amp up the cinematic-ness a little with effects."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#the-looktocamera-action-works-different-on-arql-than-while-testing-in-realitycomposer-will-this-be-fixed-with-xcode14","text":"Please file a https://developer.apple.com/bug-reporting/|bug report for this . Any additional information you can provide, such as a video reproducing the issue would be hugely helpful. Ignored for over a year Could you provide the Feedback ID? Cannot find it in http://Feedback.app|Feedback.app , must have used user support channel instead - my bad. If RC update does not fix this, I\u2019ll add a feedback - finally.","title":"The LookToCamera Action works different on ARQL than while testing in RealityComposer. Will this be fixed with XCode14?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#are-there-guidelines-or-best-practices-for-exporting-a-realitykit-scene-to-a-usdz-is-this-possible-ive-seen-just-a-little-about-the-modelio-framework-is-this-the-tool-we-should-be-using","text":"I don\u2019t think we have any guidelines about this, since exporting/saving a scene is not supported by the current APIs. ModelIO seems like a reasonable solution to me, but you might also want to file a feature request for this on Feedback Assistant.","title":"Are there guidelines or best practices for exporting a RealityKit scene to a USDZ? Is this possible? I\u2019ve seen just a little about the ModelIO framework. Is this the tool we should be using?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hello-there-i-am-someone-who-is-still-fairly-novice-with-reality-ar-kit-and-i-want-to-ask-what-is-the-best-way-to-implement-multiuser-ar-experiences-ive-been-thinking-on-creating-an-ar-app-that-would-use-this-feature-to-allow-multiple-users-to-view-a-single-ar-view-eg-multiple-users-seeing-the-same-rendered-model-from-their-own-perspectives","text":"Not the expert here, I believe that a \u201cLocation Anchor\u201d is what you are interested in. I don\u2019t know how you would set one up without getting into coding though. I believe one of the videos from WWDC last year addressed this? Multiuser AR experiences can be created using the SynchronizationComponent , found here: https://developer.apple.com/documentation/realitykit/synchronizationcomponent This is a good tutorial (along with sample code) on building collaborative AR sessions between devices: https://developer.apple.com/documentation/arkit/creating_a_collaborative_session Also not an expert here, but I'm looking into that also. If you're comfortable with Unity, Niantic (Pokemon Go) just released a platform and some demo projects to do multi user AR. It's called Lightship. <@U03JRQ5LY2C>, here\u2019s the session from last year that I mentioned. It might help to get you started\u2026 https://developer.apple.com/videos/play/wwdc2020/10611/ Are there any examples using SwiftUI? All of the code seems to be using Storyboards, though I assume they can easily be transferred over into a SwiftUI view using either a UIViewControllerRepresentable; or generally converting the general lifecycle of a multiuser AR app to fit SwiftUI standards. The view that contains the ARView will still need to be a UIKit view but that can easily be bridged to other SwiftUI views using a UIViewRepresentable : https://developer.apple.com/documentation/swiftui/uiviewrepresentable Most of the AR collaboration code in the sample here is just Swift code that can easily be abstracted out and used in your app: https://developer.apple.com/documentation/arkit/creating_a_collaborative_session","title":"Hello there, I am someone who is still fairly novice with Reality / AR Kit. And I want to ask what is the best way to implement Multiuser AR experiences. I\u2019ve been thinking on creating an AR app that would use this feature to allow multiple users to view a single AR view (e.g., multiple users seeing the same rendered model from their own perspectives)."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-the-keynote-theres-a-mention-about-background-api-in-metal-please-share-documentationresources-link","text":"Are you referring to https://developer.apple.com/documentation/metal/resource_loading Yes, Thanks","title":"In the keynote, there's a mention about Background API in Metal. Please share documentation/resources link"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#ios-154-includes-the-builtinlidardepthcamera-type-in-avfoundation-is-there-any-advantage-in-implementing-this-camera-type-when-doing-object-capture-for-better-depth-calculation-or-does-that-not-change-the-outcome-of-the-rendered-3d-model","text":"Capturing images with LiDAR devices will give you automatic scale estimation and gravity vector information on your final usdz output Thank you, <@U03HHPAHQ3C>. Was this also the case prior to iOS 15.4? I recall the Capture Sample project from WWDC 2021 already leveraged depth data, but not sure if that depth data was already using LiDAR? Hi Brandon, yes we already supported depth in last years version of object capture. It does not have to come from LiDAR. There are different ways to capture depth on Apple devices. Please take a look at our sample code for the recommended way how to capture high quality RGB images with depth https://developer.apple.com/documentation/realitykit/taking_pictures_for_3d_object_capture","title":"iOS 15.4 includes the builtInLiDARDepthCamera type in AVFoundation.  Is there any advantage in implementing this camera type when doing Object Capture for better depth calculation, or does that not change the outcome of the rendered 3D model?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-method-to-implement-scans-from-roomplan-with-reality-composer-thank-you","text":"Hi, can you clarify what integration you are looking for?","title":"Is there a method to implement scans from RoomPlan with Reality Composer? Thank you!"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#what-are-some-ideal-non-product-examples-of-good-usdzs","text":"There are great USDZ examples on https://developer.apple.com/augmented-reality/quick-look/ For example you have the Lunar Rover from For All Mankind Scientific stuff! We've also added new documentation to help you generate better USD assets here: https://developer.apple.com/documentation/realitykit/creating-usd-files-for-apple-devices","title":"What are some ideal non-product examples of good USDZs"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-access-system-instances-for-a-scene-or-must-system-updates-eg-change-the-culling-distance-of-a-system-always-route-through-a-component","text":"Hey there, thanks for your question. Generally Systems are designed to operate on Entities (within Scenes) and their Components. Each System can be updated against multiple Scenes (and the Scene\u2019s entities). If you have state that you want to be represented with a System, one method to do that is to have a root entity that holds a \u201cSystem component\u201d. There\u2019s more information on System\u2019s in last years WWDC session and on the developer documentation website: https://developer.apple.com/videos/play/wwdc2021/10074/ https://developer.apple.com/documentation/realitykit/system/update(context:)-69f86 Ok, that' what I thought, but wanted to make sure I didn't miss anything. Thanks!","title":"Is there a way to access system instances for a scene or must system updates (e.g., change the culling distance of a system) always route through a component?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#when-do-you-think-we-will-see-new-versions-of-reality-composer-and-reality-converter-apps-im-a-college-professor-graduate-industrial-design-and-use-these-as-an-intro-to-ar-tools-better-more-capable-versions-might-be-nice-thanks","text":"Unfortunately, we don\u2019t discuss our future plans. However, we are aware that our tools haven\u2019t been updated in a few years and could use some new features. Could you share what features you are looking for us to add? FA id: FB9570859 It would be awesome to see features that are announced in Reality Kit be compatible with Reality Composer. Like videos, video textures, post processing effects. Love RC and can't wait to see it updated. Cannot apply properties to group object Cannot multi-select behaviors to delete Cannot copy multi-selected behaviors Need an object navigator to select scene objects to modify Need Opacity/Texture/Materials for objects/groups I would love to see Meshes or Objects that have Emissive lighting. Or in other words objects that would glow and be emissive similar to what we see in Unity and Unreal. I could keep going with features, but need to be mindful that RC is not in the league of Maya, for example. also Animation timeline, minimal Oh yeah, object navigator and animation timeline would be incredible. I would also love to see a visual or graphical shader editor similar to Shader Graph from Unity. It does not have to be really complex, even a basic visual editor for RealityKit / Reality Composer shaders would be awesome! <@U03JELU9E5P> use emissive texture with black as diffuseColor to achive this. It would glow (no overwhites) but can change the look a lot I\u2019d really, really like to see material importing and placement options, so that we can at least apply a simple texture to a simple object in Reality Composer, rather than having to dive into a more complex 3D app. Right now, we can only change the color and the material type. (Unless I\u2019ve missed something?","title":"When do you think we will see new versions of Reality Composer and Reality Converter apps? I'm a college professor - Graduate Industrial Design, and use these as an intro to AR tools. Better, more capable versions might be nice? Thanks."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#we-are-seeing-some-memory-leaks-when-adding-modelentities-to-an-anchor-pausing-the-arsession-and-starting-it-again-and-adding-modelentities-againwe-see-memory-growing-in-the-resyncobject-section-does-anyone-have-experience-troubleshooting-memory-leaks-that-have-happened-in-a-similar-way","text":"Hey thanks for the question. I\u2019d recommend this year\u2019s WWDC Xcode session for what\u2019s new in debugging - ( https://developer.apple.com/wwdc22/110427 ). And there have been many other excellent sessions over the years on debugging. That said if you believe it may be RealityKit or another system framework responsible for leaking the entities we\u2019d ask you to file a Feedback Item on http://feedbackassistant.apple.com if you haven\u2019t done so already.","title":"We are seeing some memory leaks when adding ModelEntities to an anchor, pausing the ARSession and starting it again and adding ModelEntities again....We see memory growing in the re::SyncObject section.  Does anyone have experience troubleshooting memory leaks that have happened in a similar way?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#any-plans-for-instant-ar-tracking-on-devices-without-lidar-this-could-be-helpful-for-translation-apps-and-other-apps-that-overlay-2d-textimages-on-3d-landmarks","text":"You might want to ask this to the ARKit team, but I\u2019m not aware of any plans. A feedback item would be good though!","title":"Any plans for instant AR tracking on devices without LiDAR? This could be helpful for translation apps and other apps that overlay 2D  text/images on 3D landmarks."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-localize-against-a-scanned-room-from-the-room-plan-api-via-arkit-so-that-it-could-be-used-for-example-to-setup-a-game-in-your-room-and-share-that-with-other-people","text":"No there is no re-localization in RoomPlan. But we expose the ARSession so you could fallback to ARKit for re-localization Gotcha, thanks! I think this would be a nice addition though.","title":"Is there a way to localize against a scanned room from the Room Plan API (via ARKit) so that it could be used for example to setup a game in your room and share that with other people?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-suggested-manner-of-writing-arkitrealitykit-experiences-to-a-video-file-im-current-using-realitykit-2s-post-processing-to-convert-the-source-mtltexture-to-a-cvpixelbuffer-and-writing-that-to-an-avassetwriter-but-this-occasionally-ends-up-leading-to-dropped-frames-or-random-flickers-in-the-video","text":"Thank you for your question! We don\u2019t currently have a recommend method for doing this and as such would love to see a feedback item explaining what you need and a use case explaining it. That would be wonderful. That said your method should in theory work and we\u2019d also love to see feedback item describing the issues you\u2019re seeing. https://developer.apple.com/bug-reporting/","title":"Is there a suggested manner of writing ARKit/RealityKit experiences to a video file?  I'm current using RealityKit 2's post-processing to convert the source MTLTexture to a CVPixelBuffer, and writing that to an AVAssetWriter, but this occasionally ends up leading to dropped frames or random flickers in the video."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-use-video-textures-in-reality-composer","text":"Video textures are currently not supported through Reality Composer UI. However, if your .rcproj is part of an Xcode project, you can use the RealityKit VideoMaterial api to change the material of your object in the scene at runtime. https://developer.apple.com/documentation/realitykit/videomaterial For short flipbooks, I prepare several instances of the same mesh with separate textures (one per frame) and either animate their .opacity in USD or show/hide them in RComposer","title":"Is there a way to use video textures in Reality Composer?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-currently-a-built-in-way-or-example-of-a-way-transform-a-roomcapture-from-roomplan-into-a-modelentity-or-other-type-of-realitykit-entity-instead-of-only-the-exported-usdz-file","text":"I don\u2019t believe there is a built in way, but loading a USDZ into a RealityKit scene as a ModelEntity is very simple Gotcha. What I would love to do is mark edges with measurements and such. So I think I would need to build up a ModelEntity piece by piece with the RoomCaptureData to do that","title":"Is there currently a built-in way or example of a way transform a RoomCapture from RoomPlan into a ModelEntity or other type of RealityKit entity? Instead of only the exported USDZ file?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-there-i-noticed-the-new-beta-class-imagerenderer-for-swiftui-allowing-swiftui-views-to-be-rendered-into-a-static-image-and-be-used-as-a-texture-in-arkit-will-there-be-an-interactive-version-of-displaying-swiftui-views-in-arkit-i-am-currently-achieving-the-same-effect-using-uihostingviewcontroller-but-it-feels-hacky-and-creates-a-gles2metal-rendering-issue-in-simulator-currently-thanks","text":"Hey! Thanks for your question. We don\u2019t discuss future plans, but gathering developer feedback is important to us so we\u2019d ask you to post your request to https://developer.apple.com/bug-reporting/ Okay thanks! I\u2019ll take that to mean that no such functionality currently exists at least? Definitely file a Feedback item with us - we\u2019d love to hear your use cases for any new features.","title":"Hi there! I noticed the new beta class \"ImageRenderer\" for SwiftUI, allowing SwiftUI views to be rendered into a static image and be used as a texture in ARKit.  Will there be an interactive version of displaying SwiftUI views in ARKit?   I am currently achieving the same effect using UIHostingViewController, but it feels hacky and creates a GLES2/Metal Rendering Issue in Simulator currently.  Thanks!"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-the-state-of-the-union-there-is-reference-to-scankit-alongside-the-mention-of-roomplan-is-scankit-a-sdk-or-if-that-the-same-thing-as-roomplan","text":"RoomPlan is the name of the SDK. You\u2019ll want to refer to those APIs as RoomPlan instead of ScanKit.","title":"In the State of the Union, there is reference to ScanKit alongside the mention of RoomPlan.  Is ScanKit a SDK, or if that the same thing as RoomPlan?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#at-last-years-wwdc-2021-realitykit-20-got-new-changes-to-make-programming-with-entity-component-system-ecs-easier-and-simpler-the-current-realitykit-ecs-code-seems-too-cumbersome-and-hard-to-program-will-ease-of-programming-with-ecs-be-a-focus-in-the-future","text":"Hey! Thank you. While we don\u2019t discuss specific future plans, we always want to make RealityKit as easy to use for everyone as we can. We\u2019d ask you to post your issues and/or suggestions to https://developer.apple.com/bug-reporting/ I\u2019d love to find out more about what you find too cumbersome. Thanks!","title":"At last years WWDC 2021 RealityKit 2.0 got new changes to make programming with Entity Component System (ECS) easier and simpler!  The current RealityKit ECS code seems too cumbersome and hard to program.   Will ease of programming with ECS be a focus in the future?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#collaboration-frameworks-for-ar-are-important-is-apple-considering-features-related-to-remote-participation-in-ar-experiences-unity-has-this-capability-to-some-extent","text":"While we don't discuss future plans, we always hope to gather this sort of feedback during WWDC. Thanks for taking the time to share :pray: We do support Collaborative sessions over the same network, more details and sample code can be found here: https://developer.apple.com/documentation/arkit/creating_a_collaborative_session Is this what you were looking for ?","title":"Collaboration frameworks for AR are important. Is Apple considering features related to remote participation in AR experiences? Unity has this capability to some extent."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-i-am-trying-to-make-a-battle-royal-in-augmented-reality-how-can-i-make-a-stormgame-bounds-that-is-visible-from-far-away-and-is-occluded-by-buildings","text":"There are challenges using ARKit outdoors, you should sign up for an ARKit lab to get some feedback from them ok, thank you","title":"Hi, I am trying to make a Battle Royal in Augmented Reality, how can I make a storm/game bounds that is visible from far away and is occluded by buildings"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#this-question-may-be-better-suited-for-tomorrows-object-and-room-capture-lounge-but-is-the-output-capturedroom-type-able-to-be-modified-prior-to-export-to-usdz-for-example-could-i-remove-all-objects-types-and-leave-just-wallsdoors-or-change-the-texture-of-a-surface","text":"Yes, please ask this during the object capture lounge tomorrow. But you should be able to modify after export and re-render I had this same sort of question. From my look at the docs, you could possibly create a RealityKit Entity using the RoomCaptureData. For removing elements, it looks like you should be able to remove array elements within RoomCaptureData and then write that to usdz Yes, all those things should technically be possible. You would need to use the RoomCaptureSession API and subscribe to a delegate to get those updates which contain the Surfaces and Objects. You can then process that data and render it as per your liking Thanks very much! Appreciate the replies! :slightly_smiling_face:","title":"This question may be better suited for tomorrow's #object-and-room-capture-lounge, but is the output CapturedRoom type able to be modified prior to export to USDZ?  For example, could I remove all [.objects] types, and leave just walls/doors, or change the texture of a surface?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#ive-noticed-that-when-occlusion-is-enabled-on-lidar-devices-far-away-objects-are-automatically-being-clipped-after-a-certain-distance-like-10m-or-so-even-if-there-is-no-physically-occluding-them-ive-tried-to-adjust-the-far-parameters-of-the-perspectivecameracomponent-httpsdeveloperapplecomdocumentationrealitykitperspectivecameracomponentfarhttpsdeveloperapplecomdocumentationrealitykitperspectivecameracomponentfar-but-unfortunately-that-didnt-help-only-disabling-occlusion-removes-the-clipping-is-there-a-workaround-for-this-behavior","text":"This should be fixed in iOS 16 ~I believe this is a known issue though~ Will check again, but good to know it's on the radar. No available workaround for now then I suppose? Ahh just updated okay nice will check! yeah i found the bug report :slightly_smiling_face: Could you also have a look into this: https://developer.apple.com/forums/thread/682215 FB9517992 I just checked and it's still happening on iOS 16 Beta 1 yeah, that one hasn\u2019t been fixed yet Alright, thanks!","title":"I've noticed that when occlusion is enabled on Lidar devices, far away objects are automatically being clipped after a certain distance like 10m or so (even if there is no physically occluding them). I've tried to adjust the far parameters of the PerspectiveCameraComponent \u2013 https://developer.apple.com/documentation/realitykit/perspectivecameracomponent/far|https://developer.apple.com/documentation/realitykit/perspectivecameracomponent/far But unfortunately that didn't help. Only disabling occlusion removes the clipping. Is there a workaround for this behavior?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-need-swiftui-views-in-my-rk-experienceplease-and-asap","text":"Hi! Can you explain a little bit more what you\u2019re trying to do? If you have a use-case in mind for a new feature you\u2019re definitely also welcome to file a Feedback item with us via https://developer.apple.com/bug-reporting/ will reality kit work with swift ui views ? <@U03H3628BL7> You can host RealityKit content inside SwiftUI views with UIViewRepresentable ( https://developer.apple.com/documentation/swiftui/uiviewrepresentable ) If you\u2019re asking if you can use SwiftUI content within RealityKit - there is no direct support for that at present and we\u2019d ask you to file a feedback item explaining your use-case for that feature. I want to inject SwiftUI charts, for example into my RK scene with live updates via Observables. this will be very useful when there is a need for being surrounded in AR space with analytics about equipment telemetry, etc. thanks Oh that\u2019s a great idea. Please do file a feedback item. we filed a FA for this feature quite some time ago. no reply, will look it up.","title":"I need SwiftUI Views in my RK experience...please and ASAP."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#id-love-to-have-sf-symbols-renderable-in-ar-it-actually-works-with-rk-on-macos-by-copy-and-pasting-the-symbols-but-not-available-in-the-system-font-on-ios","text":"Thanks for the suggestion! You may want to check with the SF Symbols team to confirm this is not possible yet, and also file a feature request on feedback assistant. <@U03J7GC4DEC> had a fb open for almost a year now :grin: SF Symbols in MeshResource.generateText() - FB7959655 A bit of a hack solution but you may be able to get this work via drawing the Symbol to a CGImage and passing that image in as a texture. that would work for a 2d solution, but ideally it would be a prism Ah yeah. Alright thanks for the FB item in that case. it sounds like the reason it doesn't automatically work does come down to the sf symbols team\u2026 is there a way i could get this ticket to that team? no lounge etc it looks like the bug is actually in RealityKit, so it\u2019s on the proper team right now. We\u2019ll discuss this bug internally with the team.","title":"I\u2019d love to have SF Symbols renderable in AR! it actually works with RK on macOS by copy and pasting the symbols, but not available in the system font on iOS."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-im-pururaj-i-have-been-really-interested-in-realitykit-and-arkit-for-the-past-couple-of-years-where-can-i-learn-more-about-it-im-currently-into-designing-writing-editing-and-management-and-would-love-to-work-on-futuristic-tech","text":"check out this page! https://developer.apple.com/augmented-reality/ Hi Pururaj and welcome! You're already on the right track\u2014meeting with engineers and developers from around the world :grinning: To learn more about RealityKit and ARKit, I would recommend starting with our documentation and videos. Here are a few links to help you get started: \u2022 https://developer.apple.com/augmented-reality/realitykit/|RealityKit \u2022 https://developer.apple.com/augmented-reality/arkit/|More to explore with ARKit 6 \u2022 https://developer.apple.com/augmented-reality/tools/|AR Creation Tools \u2022 https://developer.apple.com/videos/augmented-reality|Videos Augmented Reality> You can also always ask questions on https://developer.apple.com/forums/|Developer Forums :speech_balloon: https://www.youtube.com/c/realityschool","title":"Hi. I\u2019m Pururaj. I have been really interested in RealityKit and ARKit for the past couple of years. Where can I learn more about it? I\u2019m currently into Designing, Writing, Editing, and Management and would love to work on futuristic tech."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#our-app-uses-realitykit-and-starts-multiple-arsessions-throughout-a-user-session-were-seeing-an-issue-where-the-memory-after-the-first-arsession-never-gets-freed-which-is-a-pretty-large-chunk-around-200mb-we-set-everything-to-nil-when-the-session-terminates-but-we-can-see-in-the-memory-graph-that-an-arsession-object-is-still-being-retained-subsequent-arsessions-leak-a-few-megabytes-of-memory-but-at-the-end-of-each-session-there-is-always-only-one-arsession-being-retained-in-the-memory-graph-are-you-aware-of-any-memory-cleanup-bugs-either-in-realitykit-or-arkit-that-could-be-causing-this-or-is-there-a-way-to-completely-free-all-memory-from-an-arsession-heres-a-feedback-assistant-number-for-reference-to-a-sample-app-9950277","text":"Hey Wade\u2026 looking at the issue right now and I don\u2019t have an immediate answer for you. But it looks like the issue you provided has all the data we need in it. Awesome thanks! Let me know if you need anything additional info Thank you.","title":"Our app uses RealityKit and starts multiple ARSessions throughout a user session. We\u2019re seeing an issue where the memory after the first ARSession never gets freed (which is a pretty large chunk around 200MB). We set everything to nil when the session terminates, but we can see in the memory graph that an ARSession object is still being retained. Subsequent ARSessions leak a few megabytes of memory but at the end of each session there is always only one ARSession being retained in the memory graph. Are you aware of any memory cleanup bugs either in RealityKit or ARKit that could be causing this? Or is there a way to completely free all memory from an ARSession? Here\u2019s a Feedback Assistant number for reference to a sample app 9950277"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hello-everyone-many-of-you-know-glb-file-format-androids-scene-viewer-support-compression-like-draco-any-planning-update-for-compress-usdz-files","text":"I would suggest filing an enhancement request on feedback assistant for this","title":"Hello everyone! Many of you know .glb file format (android's scene-viewer) support compression like draco. Any planning update for compress .usdz files?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-lab-to-ask-questions-about-scenekit-this-year","text":"You\u2019ll probably want to ask SceneKit questions on the developer forums if no lab or lounge is available","title":"Is there a lab to ask questions about SceneKit this year?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#what-is-the-recommended-way-to-add-live-stream-or-capture-capabilities-with-realitykit-do-we-need-to-build-frame-capture-and-video-writers-with-avfoundation-a-higher-level-api-would-be-a-better-fit-for-realitykit","text":"I would recommend using ReplayKit or ScreenCaptureKit to record your app screen to stream / share https://developer.apple.com/documentation/replaykit https://developer.apple.com/documentation/screencapturekit","title":"What is the recommended way to add live stream or capture capabilities with RealityKit? Do we need to build frame capture and video writers with AVFoundation? A higher level API would be a better fit for RealityKit."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#id-just-like-to-say-that-realitykit-and-arkit-are-amazing-it-feels-early-in-the-world-of-ar-but-its-amazing-how-easy-it-is-to-bring-3d-objects-into-ar-and-im-excited-and-grateful-as-a-developer-to-be-on-this-journey-thank-you-for-these-tools-and-your-hard-work","text":"Thank you for the kind words, Brandon :pray: We appreciate you taking the time to join our digital lounges and engage with our engineers and designers!","title":"I'd just like to say that RealityKit and ARKit are amazing.  It feels early in the world of AR, but it's amazing how easy it is to bring 3D objects into AR, and I'm excited and grateful, as a developer, to be on this journey.  Thank you for these tools and your hard work!"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#quicklook-is-currently-very-dark-much-darker-than-would-be-expected-clients-are-complaining-about-washed-out-colors-and-we-need-to-overcorrect-via-emission-not-ideal-breaks-dark-rooms-easy-to-test-make-a-pure-white-material-and-display-it-in-quicklook-in-a-bright-room-it-will-never-get-white-are-there-plans-to-fix-this","text":"Great question - and we have good news for you :slightly_smiling_face: We are releasing new lighting to ARQL which is brighter with enhanced contrast and improved shape definition to make your assets look even better. Please check tomorrow's session - https://developer.apple.com/videos/play/wwdc2022/10141 with examples and how to implement it! Thank you! Thanks! Especially curious about the \"how to implement it\" part - as a QuickLook user I'd think there's nothing to implement :slightly_smiling_face:","title":"QuickLook is currently very dark (much darker than would be expected). Clients are complaining about washed-out colors, and we need to overcorrect via emission (not ideal, breaks dark rooms). (Easy to test: make a pure white material and display it in QuickLook in a bright room, it will never get white.) Are there plans to fix this?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#im-super-excited-about-the-model-html-tag-taking-shape-the-examples-currently-list-camera-and-animation-control-that-seems-to-be-lacking-examples-for-general-interactivity-scripted-access-to-the-model-similar-to-what-threejs-scenes-allow-for-is-this-intentional-or-are-the-examples-just-missing","text":"Thanks for you interest! This is a great feature question. Are there other features you would like to see? Could you share more links to documentation about this? <@U03HMDX5C95> https://github.com/WebKit/explainers/blob/main/model/README.md Thanks <@U03JA5TGENQ>! Are you on Twitter at all? Always good to connect with other AR creators. <@U03HL4ZB05S> Definitely! 1. Seeing that USDZ and glTF are both planned to be supported, I'm of course curious about scene graph access. This kind of clashes with the notion that \"rendered model data ... is not exposed by the page\" 2. Given that our typical usecase is AR, I really wonder how the <model> tag would interact with AR in general and QuickLook in particular. This goes into areas of immersive sessions, dom-overlays, etc. - e.g. when I have styled audio control buttons in HTML, I definitely would want to have those in AR as well. <@U03HMDX5C95> sure, I'm @hybridherbst","title":"I'm super excited about the &lt;model HTML tag taking shape. The examples currently list camera and animation control; that seems to be lacking examples for general interactivity / scripted access to the model similar to what three.js scenes allow for. Is this intentional, or are the examples just missing?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-have-a-model-that-is-a-reality-file-that-opens-in-arql-when-the-user-taps-the-model-it-shrinks-to-show-the-same-light-in-a-different-size-however-its-not-very-clear-to-the-user-that-this-is-a-possibility-if-they-dont-tap-for-a-while-arql-encourages-them-to-tap-the-object-to-activate-is-there-a-way-i-can-customise-this-message","text":"Hi <@U03HMDX5C95> thanks for the question. That is a standard message and unfortunately there\u2019s currently no way to customize the text. Alternatively, you can create your own banner within your asset. For example, you can check out the first asset on the gallery page That's unfortunate. Should I file feedback or is this working as intended? Yes, please file a feedback report :pray:","title":"I have a model that is a .reality file, that opens in ARQL. When the user taps the model, it shrinks to show the same light in a different size. However, it's not very clear to the user that this is a possibility. If they don't tap for a while, ARQL encourages them to \"Tap the object to activate\" is there a way I can customise this message?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#it-seems-a-bit-weird-that-theres-currently-three-different-implementations-of-usd-at-use-across-ios-mac-are-there-plans-to-consolidate-those-into-one-to-make-testing-and-verification-of-assets-across-platforms-easier-the-shared-feature-subset-is-pretty-small-resulting-in-less-than-ideal-products-for-clients","text":"There are different USD renderers across our platforms but each serve a different purpose. Here is a developer document that explains these different USD renderers and what their feature sets are https://developer.apple.com/documentation/realitykit/creating-usd-files-for-apple-devices Does Storm work on iOS? I'm (painfully!) aware of these differences, that's why I'm asking :slightly_smiling_face: Currently, testing a single USDZ model to actually work in AR across different iOS versions is really hard. Having a somewhat more unified USD renderer (or at least a way to use the same one from Mac or a browser than what's used on QuickLook) would definitely accelerate client adoption of USDZ. Gotcha, that's good feedback we can relay to the team on a more unified USD renderer :slightly_smiling_face:","title":"It seems a bit weird that there's currently three different implementations of USD at use across iOS / Mac. Are there plans to consolidate those into one to make testing and verification of assets across platforms easier?  The shared feature subset is pretty small, resulting in less-than-ideal products for clients."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-support-more-than-one-image-anchor-in-a-scene-with-arql-feedback-fb7818339","text":"This is not supported at this point. The team is still investigating and we'll update the report when there's additional information. Thanks. This would be really useful to have.","title":"Is it possible to support more than one image anchor in a scene with ARQL? Feedback: FB7818339"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#im-creating-pendant-lights-for-viewing-in-in-arql-is-it-possible-to-anchor-these-to-the-ceiling-of-a-room","text":"Hi <@U03HMDX5C95>, yes this is something that is supported in AR Quick Look. You can place objects on the ceiling by dragging them there. This can be done using a regular horizontal (or vertical) anchor. However, there are potential challenges to be aware, the biggest is that ceilings usually lack a lot of feature points, which makes it difficult to detect a proper plane. Using a device with lidar can improve the results that you get. Thanks for your reply. I was more wondering about placing this file on the ceiling to begin with, rather than making the user drag the model there themselves. Is this possible? Can you describe in more detail how you would want this to work? Eg. after the asset gets loaded, what would the user need to do to see the asset on the ceiling? For example ARQL could guide them to point their device to the ceiling to view the model? It's just that for pendant lights, they come from the ceiling, so having them attached to the wall isn't super useful to people and doesn't provide a great experience currently. One way to let users know that they can move objects to the ceiling is to include a 3D banner with additional instructions. Sure, but that experience is less than ideal I think. It would be great having these land directly on the ceiling. Is it worth me filing feedback?","title":"I'm creating pendant lights for viewing in in ARQL, is it possible to anchor these to the ceiling of a room?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#using-arql-how-might-i-add-a-colour-picker-to-change-between-colours-of-a-model-for-example-the-imac-arql-on-httpapplecomapplecom-requires-users-to-jump-in-and-out-of-arql-to-try-different-colours-is-there-a-way-to-have-colour-pickers-in-arql-to-try-different-materials-or-change-different-scenes-in-a-reality-file","text":"You could use Reality Composer's interactions to make an interactive USD where you can tap on different colors to change the model Thanks for your reply. Do you have any examples? It's my understanding that I wouldn't be able to do this as a 2D UI/Card above the ARQL experience, but rather would have to have it as part of the 3D AR experience? If that makes sense? Yes this needs to be done in 3D. There\u2019s a previous session that has some examples Thanks for clarifying. It would be really useful if we could provide a 2D/Card UI over the ARQL experience to do this (and it would be great for Apple to be able to show different colours of devices without jumping in and out of ARQL, too). We can already embed html in ARQL, if this could link to different scenes in a .reality file it would be amazing and make the experience so much better for users. I would file feedback for this but feel as if my other feedback requests have gone unnoticed \u2013 is there a better way to get ideas noticed? Does a prototype of what we are looking for help?","title":"Using ARQL, how might I add a colour picker to change between colours of a model? For example, the iMac ARQL on http://apple.com|apple.com requires users to jump in and out of ARQL to try different colours. Is there a way to have colour pickers in ARQL to try different materials or change different scenes in a .reality file?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-yall-any-thoughts-about-making-usdusdz-files-with-particle-effects-things-on-firesparking-etc","text":"Hello Matthew! This is not currently possible to do in a USD, but you should submit the idea to https://feedbackassistant.apple.com . You can however do some particle effects in an app by using RealityKit's CustomShaders. Depending on how complex your effect is, you can also bake your particle effects to regular mesh + bones animation :sparkles: In many cases you can also create a pretty convincing effect just by scaling/rotating a few planes. https://prefrontalcortex.de/labs/model-viewer/upload/kravity/|Example link (no USDZ behind that right now, but you get the idea - this is just two simple meshes for the particles) Thanks <@U03JA5TGENQ> nice idea!","title":"Hi y\u2019all! Any thoughts about making USD/USDZ files with particle effects? Things on fire/sparking etc?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-simple-way-to-create-a-3d-object-with-a-custom-image-as-a-texture-reality-composer-only-allows-a-material-and-a-color-and-without-that-ill-have-to-dip-into-a-far-more-complex-3d-app-id-really-really-like-to-use-usdz-more-in-motion-for-pre-viz-and-prototyping-but-without-texture-editing-its-quite-limited-have-i-missed-something","text":"Thanks Iain. Can you explain what kind of 3D object you want to create with a textures? A complex object or a simple plane? A relatively simple object. For a recent project I needed to create playing pieces similar to dominos, but each one with a different, complex image on it. I was able to use Motion\u2019s 3D text support to extrude a rounded rectangle, but I\u2019d prefer to do this with USDZ as it\u2019s far more widely supported. Motion\u2019s 3D text support is exactly the kind of texture placement and scaling support I\u2019m looking for, by the way. There are various third-party DCCs with great USD support that let you create complex 3D object with textures and export as USD. You can then use RealityConverter to convert those to USDZ to import into Motion. Understood \u2014 though they\u2019re a huge jump up in complexity and it would be great to see this built into RC. Do you have any specific recommendations for a third-party app that\u2019s easy to use for simple objects? Another approach: three.js (web render engine) can actually create USDZs on the fly from 3D scenes. A colleague used that recently for USDZ AR files with changeable textures on https://webweb.jetzt/ar-gallery/ar-gallery.html (users have to leave AR to change the textures but then can even load custom images into it) OK, will look into three.js as a potential solution. Please consider adding texture support into RC though \u2014 it would be a great way to create simple objects and make simple edits to existing objects. Thank you! Ah, I think I understood the question a bit differently - I think you're referring to \"drag in a generic model and then change the texture in RC\" - three.js is most certainly overkill for that :slightly_smiling_face: Yes, that\u2019s pretty much it. For example, it would be great to be able to create a simple rectangular plane, then put a custom image on the front face. Simple stuff, and most 3D apps are far, far more complex. Blender in particular is a UI nightmare. Also take a look at the https://developer.apple.com/videos/play/wwdc2022/10141|Explore USD tools and rendering session tomorrow. You can now change materials properties in RealityConverter! Great! That\u2019s going to be a huge help \u2014 good to know. Another thing that might help for making quick adjustments: the browser-based three.js editor at https://threejs.org/editor . Here's how to make a plane with a custom image and export that as USDZ: Just checking \u2014 can I import USDZ into Reality Converter, just to change the texture, or should I use some intermediate format?","title":"Is there a simple way to create a 3D object with a custom image as a texture? Reality Composer only allows a material and a color, and without that, I'll have to dip into a far more complex 3D app. I'd really, really like to use USDZ more in Motion, for pre-viz and prototyping, but without texture editing it's quite limited. Have I missed something? :)"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#reality-composer-is-great-but-our-team-of-3d-asset-modelers-has-found-it-easier-to-sculpt-characters-in-zbrush-do-arkit-and-realitykit-accept-models-created-in-zbrush-or-are-there-intermediate-steps-best-for-preparing-a-model-for-apple-platforms-keyshot-etc","text":"Yes, if you can export your assets to FBX, glTF or OBJ, you can convert them to USDZ using Reality Converter, which is compatible with ARKit and RealityKit We\u2019ve run into problems with materials and textures when exporting them in various formats to USDZ. It seems that the pipeline from some 3d modeling applications (our use is primarily for product and transportation design) - Maya, Rhino 3d, Solidworks, isn\u2019t really smooth. Exporting files to Keyshot to assign textures seems to help in some cases. In many though the materials and textures don\u2019t really work to our satisfaction. Perhaps newer versions of Reality Converter will be more successful?","title":"Reality Composer is great, but our team of 3D asset modelers has found it easier to sculpt characters in Zbrush.   Do ARKit and RealityKit accept models created in Zbrush, or are there intermediate steps best for preparing a model for Apple platforms? (KeyShot, etc.)"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#are-there-tools-that-can-be-used-to-rig-skeletons-for-usd-characters-i-have-not-found-anything-that-works","text":"Yes, there are various third-party DCCs that let you create skeletons and RealityConverter lets you convert other file formats with skeletons to USD. Autodesk Maya https://knowledge.autodesk.com/support/maya/learn-explore/caas/CloudHelp/cloudhelp/2022/ENU/Maya-USD/files/GUID-9E9D45F2-4DA9-497B-8D69-1573ED6B2BA8-html.html|https://knowledge.autodesk.com/support/maya/learn-explore/caas/CloudHelp/cloudhelp/2022/[\u2026]files/GUID-9E9D45F2-4DA9-497B-8D69-1573ED6B2BA8-html.html and Blender https://docs.blender.org/manual/en/latest/files/import_export/usd.html Are some example Digital Content Creation tools that can help you create rigged skeletons for characters exported to USD.","title":"Are there tools that can be used to Rig skeletons for USD characters?   I have not found anything that works?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-reality-composer-appropriate-for-end-users-on-macos-wed-like-to-export-rawunfinished-usd-from-our-app-then-have-users-use-reality-composer-to-put-something-together-with-multimedia","text":"Hi Steven, what do you mean by \"raw/unfinished\" USD and multimedia? You can assemble different USDZ assets together to build out a larger scene in Reality Composer and add triggers and actions to individual assets within the project","title":"Is Reality Composer appropriate for end-users on macOS? We'd like to export \"raw\"/unfinished USD from our app then have users use Reality Composer to put something together with multimedia."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#getting-late-here-checking-out-for-today-thanks-for-patiently-answering-so-many-hard-questions-all-the-best","text":"Thanks for all the great questions, feedback and ideas from everyone and we have 15 more minutes time in this session today!","title":"Getting late here, checking out for today - thanks for patiently answering so many hard questions! All the best :)"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-modify-modelentities-loaded-from-an-usdz-file-on-a-node-basis-eg-showhide-specific-nodes","text":"Yes, if you load the USDZ with \"Entity.load(...)\" or \"Entity.loadAsync(...)\" you can traverse the hierarchy and modify the individual entities. You\u2019d want to use Entity.isEnabled in this instance to hide/show a node. Note that . loadModel will flatten the hierarchy whereas .load will show all entities Ah, so each child-node is then an entity? A prim and entity are not 1 to 1 necessarily Ok <@U03HHPAHQ3C>, I will look into your suggestion. Thank you!","title":"Is there a way to modify ModelEntities loaded from an .usdz file on a node basis? E.g. show/hide specific nodes?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#will-there-be-an-async-await-concurrency-api-to-detect-when-entities-are-added-to-an-arview-right-now-im-working-off-combine-anchors-its-not-super-documented","text":"Hey there, we don\u2019t discuss future releases of Apple products. But we\u2019d love to hear your feedback and suggestions. Please file your feedback here to get it into our system. https://developer.apple.com/bug-reporting/","title":"will there be an async await (concurrency) api to detect when entities are added to an arview? right now i'm working off combine anchors .... it's not super documented"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#whats-the-easiest-way-to-add-user-interactions-pinch-to-scale-rotation-transform-to-an-entity-loaded-from-a-local-usdz-file-in-realitykit-ive-added-an-entity-as-a-child-to-an-anchorentity-added-the-anchorentity-to-the-scene-but-how-do-i-easily-add-gestures","text":"You can use the installGestures function on ARView. Keep in mind that the entity will need to conform to HasCollision . https://developer.apple.com/documentation/realitykit/arview/installgestures(_:for:) Thank you, <@U03HHPAHQ3C>! Can I follow-up and ask if there is any documentation on how to make an Entity loaded from Entity.loadAsync(...) conform to HasCollision so I can use installGestures(_:for:) , as this is where I seem to be struggling to understand. You could create your own CollisionComponent with custom mesh and add it to your entity or you could simply call generateCollisionShapes(recursive: Bool) on your entity. https://developer.apple.com/documentation/realitykit/entity/generatecollisionshapes(recursive:) Thank you, <@U03HHPAHQ3C>! Just for posterity, I can call generateCollisionShapes(recursive: Bool) on my Entity before adding it as a child to the anchor, and call installGestures on ARView before adding the entity as a child to the anchor, and that should give me interactivity on the Entity itself? You can use .loadModel/.loadModelAsync , which will flatten the USDZ into a single entity. Then call generateCollisionShapes and pass that entity to the installGestures function. This will make your USDZ one single entity that you can interact with. Got it! Thank you so much for the follow-up, <@U03HHPAHQ3C>. This has been super helpful!","title":"What\u2019s the easiest way to add user interactions (pinch to scale, rotation, transform) to an Entity loaded from a local USDZ file in RealityKit?  I\u2019ve added an Entity as a child to an AnchorEntity, added the AnchorEntity to the scene, but how do I easily add gestures?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#why-are-argeoanchors-not-available-in-other-regions-it-should-be-usable-with-just-gps-without-the-localization-imagery","text":"Hey there, this session is focused on RealityKit and Reality Composer. There\u2019s an ARKit Q&A on Thursday @ 3:00PM and another on Friday @ 9:00AM. I\u2019m sure an ARKit engineer will be able to assist you.","title":"Why are ARGeoAnchors not available in other regions? It should be usable with just gps without the localization imagery."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-any-updated-to-reality-composer-this-year","text":"No. I know you don't discuss future releases. But is Reality Composer something we can rely on to be updated with new features? And will any features we request or file feedback on have to wait until next year, or can reality composer be updated independently, similar to reality converter? We don't discuss details about unreleased updates, but one of the things that\u2019s most helpful to us as we continue to build out our suite of augmented reality developer tools is feedback. Please continue to submit ideas or suggestions in https://feedbackassistant.apple.com|Feedback Assistant :slightly_smiling_face:","title":"Is there any updated to Reality Composer this year?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#what-ar-featuresupdates-are-you-most-excited-forproud-of-this-year","text":"Personally, I'm super excited about https://developer.apple.com/augmented-reality/roomplan/|RoomPlan . I have friends and family that work in real estate and interior design. This is going to make their lives a lot easier :slightly_smiling_face:","title":"What AR features/updates are you most excited for/proud of this year?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-have-light-sources-in-ar-quick-look-files-hosted-on-the-web-for-example-a-client-would-like-to-have-lamps-in-ar-quick-look-it-would-be-awesome-if-we-could-use-rc-to-turn-offon-light-sources-is-there-any-way-to-do-this","text":"I don't think that it's possible. But you should submit the idea for supporting virtual lights on https://feedbackassistant.apple.com .","title":"Is there a way to have light sources in AR Quick Look files hosted on the web? For example, a client would like to have lamps in AR Quick Look. It would be awesome if we could use RC to turn off/on light sources. Is there any way to do this?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#can-i-render-a-snapshot-of-only-the-virtual-content-in-realitykit-something-similar-like-the-snapshot-functionality-in-scenekit","text":"Yes, you can use ARView.snapshot(...) : https://developer.apple.com/documentation/realitykit/arview/snapshot(savetohdr:completion:)-66jzu Thanks <@U03J7GN26C8>. But this would include the background (ARFrame), too, or? You can change the background of the ARView there: https://developer.apple.com/documentation/realitykit/arview/environment-swift.struct/background-swift.struct Perfect. Thank you <@U03J7GN26C8>!","title":"Can I render a snapshot of only the virtual content in RealityKit? Something similar like the snapshot functionality in SceneKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-am-interested-in-implementing-something-similar-to-what-you-see-in-the-measure-app-where-a-3d-object-tap-transforms-it-with-animation-to-a-2d-view-any-directionshints-regarding-that","text":"Hey there, not 100% sure what you\u2019re trying to do here - so maybe a Lab might be a better forum to discuss this\u2026 But based on what you\u2019re describing you can snapshot your RealityKit scene with ARView.snapshot and render it as a texture on another Entity perhaps? (we have our last labs tomorrow so you\u2019d have to sign up today) totally agree, i'll sign up for the labs. Ty!","title":"I am interested in implementing something similar to what you see in the \"Measure\" App where a 3D object tap \"transforms\" it with animation to a 2D view, any directions/hints regarding that?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-instance-meshes-in-realitykit-similar-to-scenekits-clone-method","text":"If you call https://developer.apple.com/documentation/realitykit/entity/clone(recursive:)|\".clone(...)\" on an Entity, the clone will re-use the same meshes. Good to know. I saw the draw call increase when cloning so assumed the mesh wasn't instanced. I guess \"instance\" in this case means sharing the same geometry memory","title":"Is it possible to instance meshes in RealityKit (similar to SceneKit's clone method)?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#many-aspects-of-usd-are-open-source-could-reality-composer-also-be-open-sourced-so-that-members-of-the-community-could-work-on-features","text":"Hey there, we\u2019d definitely be interested in hearing more about your idea. I\u2019d suggest submitting the suggestion at https://developer.apple.com/bug-reporting/ .","title":"Many aspects of USD are open source. Could Reality Composer also be Open-Sourced so that members of the community could work on features?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-scenekit-there-were-shader-modifiers-is-there-something-similar-in-realitykit-we-need-pbr-shaders-but-have-to-discard-certain-fragments","text":"You can apply https://developer.apple.com/documentation/realitykit/custommaterial|CustomMaterials & https://developer.apple.com/documentation/realitykit/custommaterial/surfaceshader|CustomMaterial.SurfaceShader to achieve certain cool effects for entities! From the metal side you can call discard_fragment(); Thank you <@U03HHPAHQ3C>!","title":"In SceneKit there were shader modifiers? Is there something similar in RealityKit? We need PBR shaders but have to discard certain fragments."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-reality-composer-objects-on-top-of-each-other-such-as-a-vase-on-a-table-cast-shadow-only-to-the-ground-plane-and-not-to-one-another-if-baked-ao-textures-arent-an-option-since-the-vase-may-be-moved-by-the-user-what-would-you-suggest-in-order-to-achieve-an-equally-good-result-to-the-default-grounding-shadow-given-that-the-quality-of-shadows-is-critical-for-an-ar-experience","text":"We don\u2019t have any materials you can apply to objects to make them participate in the same shadows as ground planes, however, you can enable shadow casting from directional and spot lights via DirectionalLightComponent.Shadows and SpotLightComponent.Shadows . This may alter the overall lighting of your scene though. Alternatively, we do have CustomMaterial , which allows you to <https://developer.apple.com/documentation/realitykit/modifying-realitykit-rendering-using-custom-materials |create custom materials via Metal>, but for this use-case may not be able to get you the desired effect. We\u2019re always looking to improve RealityKit, so would appreciate if you submitted a request for this via https://feedbackassistant.apple.com/ I\u2019ve been implementing the 1st approach and agree on your comment. I\u2019ll definitely submit a request. Thanks!","title":"Hi! Reality composer objects on top of each other, such as a vase on a table cast shadow only to the ground plane and not to one another. If baked AO textures aren't an option since the vase may be moved by the user what would you suggest in order to achieve an equally good result to the default grounding shadow given that the quality of shadows is critical for an AR experience?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-take-a-snapshot-of-only-the-virtual-content-and-a-snapshot-of-only-the-real-content-like-in-scenekit","text":"That\u2019s a good question. I think you can get some of the way there via the ARKit apis to get the current frame. You can also toggle the mode of an ARView to switch it to .nonAR view - then use ARView.snapshot() to grab a snapshot of the virtual content. And then switch it back. However I don\u2019t believe that would give you exactly what you want - I think the ARView snapshot would not necessarily have a transparent background (if that\u2019s what you need). And even then the performance of this may not be great. I\u2019d suggest filing a feature request for this with https://developer.apple.com/bug-reporting/ I think a feedback item and an explanation of your use cases would be very welcome. instead of changing the mode to nonAR, you could replace the camera feed with a solid colour (green), then take the snapshot But I don\u2018t think the user will be happy with this green background in the time of the snapshot You certainly could try setting the Environment background color to something with 100% alpha And if it doesn\u2019t work a bug report with just that would certainly be useful.","title":"Is it possible to take a snapshot of only the virtual content and a snapshot of only the real content like in SceneKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#what-is-the-best-way-with-usdz-content-to-link-to-external-website-or-take-them-to-a-product-landing-page","text":"Hey there - if you have your USDZ content on the web you can check out the AR Quick Look functionality for such things at https://developer.apple.com/documentation/arkit/adding_an_apple_pay_button_or_a_custom_action_in_ar_quick_look Thx, appreciate the answer. So it must be wrapped in a web experience. So this can\u2019t actually exist say within the USDZ if that is sent via imessage? You would rather have to send them a link to the webpage? As far as I know there isn\u2019t currently a way to do such a thing directly from a USDZ sent from iMessage, but I can pass that request along","title":"What is the best way with USDZ content to link to external website or take them to a product landing page?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-capture-a-snapshot-of-only-the-virtual-content-rendered-on-screen-but-not-the-background-having-the-background-be-transparent-while-continuing-the-on-screen-experience-for-the-user-where-they-still-see-the-virtual-content-and-camera-feed-the-use-case-here-is-that-mixing-visions-person-segmentation-and-arfacetrackingconfiguration-trying-to-overlay-3d-content-on-a-tracked-face-while-putting-fun-backgrounds-behind-the-person-in-the-frame-results-in-the-3d-content-typically-being-cut-off-due-to-the-segmentation-mask-itd-be-great-to-be-able-to-render-the-3d-content-back-on-top-of-the-composited-segmented-image","text":"Hey we just answered a very similar question: https://wwdc22.slack.com/archives/C03H49QK07P/p1654727671856229 If you take a look at that question/answer - maybe it help. I don\u2019t think we can necessarily do exactly what you need but it might give you a start. Just saw that! Thanks, <@U03H3628BL7>!!!","title":"Is there a way to capture a snapshot of only the virtual content rendered on-screen, but not the background (having the background be transparent) while continuing the on-screen experience for the user where they still see the virtual content and camera feed?  The use case here is that mixing Vision's person segmentation and ARFaceTrackingConfiguration (trying to overlay 3D content on a tracked face while putting fun backgrounds behind the person in the frame) results in the 3D content typically being cut off due to the segmentation mask.  It'd be great to be able to render the 3D content back on top of the composited segmented image."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#can-reality-composer-be-made-available-as-a-macos-app-in-the-app-store","text":"While Reality Composer is available only for iOS and iPadOS on the App Store, we'll pass this feedback along. Thanks :pray: Reality Composer is available on macOS as part of Xcode as a Developer Tool. ..given it's already in Xcode yes it would be relaly cool especially when working with executives who aren't engineers. Yes please!! Sucks to have to download 10gb of Xcode just for RC","title":"Can Reality Composer be made available as a macOS app in the App Store?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-capture-video-for-arview-the-way-there-is-a-take-snapshot-i-see-there-is-4k-video-being-hyped-will-this-include-the-ability-to-let-users-take-video-recordings-im-fuzzy-on-this","text":"Hi, there\u2019s no API in RealityKit to capture video. That said there are system level apis to capture screen recordings and I wonder if that would be useful for you: https://developer.apple.com/documentation/screencapturekit/ I\u2019d suggest filing a feature request with your use-case. Thanks! ( https://developer.apple.com/bug-reporting/ ) oh wow thanks! ya i've seen screen capturekit but i will file feedback and explore it too Thank you!","title":"is there a capture video for ARView the way there is a take snapshot() ? i see there is 4k video being hyped - will this include the ability to let users take video recordings? I'm fuzzy on this."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hello-for-artistdesigner-only-experienced-with-reality-composer-with-no-code-is-there-any-suggestion-and-resources-on-getting-started-with-realitykit-to-make-more-advanced-ar-experiences","text":"Hi! We have a number of WWDC sessions covering RealityKit and Reality Composer which is a great place to start. \u2022 <https://developer.apple.com/videos/play/wwdc19/609/ |Building AR Experiences with Reality Composer> \u25e6 https://developer.apple.com/documentation/realitykit/creating_a_game_with_reality_composer|Sample Project \u2022 <https://developer.apple.com/videos/play/wwdc2019/605/ |Building Apps with RealityKit> \u2022 <https://developer.apple.com/videos/play/wwdc2020/10612/ |What\u2019s new in RealityKit> \u2022 <https://developer.apple.com/videos/play/wwdc2021/10074/ |Dive into RealityKit 2> \u25e6 https://developer.apple.com/documentation/realitykit/building_an_immersive_experience_with_realitykit|Immersive App Sample \u25e6 https://developer.apple.com/documentation/realitykit/creating_an_app_for_face-painting_in_ar|Face-Painting Sample \u2022 <https://developer.apple.com/videos/play/wwdc2021/10075/ |Explore advanced rendering with RealityKit 2> There\u2019s also a great guide on building a \u2018SwiftStrike\u2019 game: <https://developer.apple.com/documentation/realitykit/swiftstrike_creating_a_game_with_realitykit |SwiftStrike: Creating a Game with RealityKit>","title":"Hello, for artist/designer only experienced with Reality Composer with no code, is there any suggestion and resources on getting started with RealityKit to make more advanced AR experiences?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#are-world-map-archives-something-that-can-reasonably-be-used-as-an-app-asset-and-loaded-for-all-users-of-the-app-in-this-case-im-thinking-of-scanning-a-room-and-creating-an-ar-experience-that-is-anchored-to-the-specific-areas-of-room-geometry-parts-of-the-architecture-and-fixtures","text":"Hey Cameron, I think that\u2019s a question best tacked by the ARKit team directly. They have ARKit lounges tomorrow at 3PM and Friday at 9AM. I think you\u2019d probably want to talk directly to an ARKit engineer\u2026 Thank you!","title":"Are world map archives something that can reasonably be  used as an app asset and loaded for all users of the app? In this case, I'm thinking of scanning a room and creating an AR experience that is anchored to the specific areas of room geometry - parts of the architecture and fixtures."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-get-access-to-more-advanced-materials-rendering-on-realitykit-models-i-want-to-skin-a-plane-with-a-uiview-currently-i-need-to-fall-back-to-arkit-and-scenekit-in-order-to-do-this","text":"RealityKit has a CustomMaterial API which allows you to create custom Metal-based materials. I\u2019d recommend our https://developer.apple.com/videos/play/wwdc2021/10075/|Explore advanced rendering with RealityKit 2 WWDC talk to learn more. Saw you answer a similar question with that API, am having a look. Thanks so much! There is also a great resource on https://developer.apple.com/metal/Metal-RealityKit-APIs.pdf|Custom Shader API that gives more details on the APIs available in Metal.","title":"Is there a way to get access to more advanced materials rendering on RealityKit models? I want to \"skin\" a plane with a UIView, currently I need to fall back to ARKit and SceneKit in order to do this"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-means-of-exporting-a-usdz-file-either-from-reality-composer-cinema-4d-etc-or-programmatically-with-a-video-texture-already-applied","text":"There\u2019s no support for that in Reality Composer currently. As always a feature request filed on https://developer.apple.com/bug-reporting/ would be most appreciated. There\u2019s also no method to export USDZ from RealityKit and again feature requests appreciated. Thank you! Thanks, <@U03H3628BL7>! Much appreciated!","title":"Is there a means of exporting a USDZ file (either from Reality Composer, Cinema 4D, etc., or programmatically), with a video texture already applied?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-show-or-hide-only-a-single-child-node-from-a-model-entity-dynamically","text":"Hey we kind of discuss this in another thread\u2026. one second\u2026 https://wwdc22.slack.com/archives/C03H49QK07P/p1654725695867159 You can certainly load a model and preserve your hierarchy, then use the entity name or another attribute to find an entity\u2026 then hide/show it with Entity.isEnabled hope that helps. (look at https://developer.apple.com/documentation/realitykit/entityquery for finding entities efficiently) Thanks, I\u2018ll try this ;)","title":"Is it possible to show or hide only a single child node from a model entity dynamically?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#can-i-place-a-model-in-a-target-such-as-a-cover-of-a-book-or-a-qr-so-that-it-doesnt-move-from-that-position-by-just-using-usdz-and-how-could-i-achieve-this","text":"You can use Reality Composer to create a scene attached to an image anchor. You can then export the scene to a USDZ or a Reality File. https://developer.apple.com/documentation/realitykit/selecting-an-anchor-for-a-reality-composer-scene Thanks a lot!","title":"Can I place a model in a target, such as a cover of a book or a QR, so that it doesn't move from that position by just using usdz? and how could I achieve this?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-taking-the-output-mtltexture-from-realitykit-2s-postprocessing-pipeline-suitable-for-writing-to-an-avassetwriter-streaming-via-rtmp-etc","text":"\u201cMaybe\u201d :slightly_smiling_face: So you can certainly take MTLTextures and convert them (if they\u2019re configured correctly) into CVPixelBuffers for AVFoundation to consume. That said it\u2019s really not the intended use case of RealityKits post processing functionality and I wouldn\u2019t be surprised if either it doesn\u2019t work as you\u2019d expect or if we break you in the future. Sounds like a great feature request though - https://developer.apple.com/bug-reporting/ Thank you, <@U03H3628BL7>! I have done this and it does work pretty well (albeit, not tried with ARKit 6's 4K resolution yet), but good to know that that's not intended. Thank you! i\u2019d still recommend filing a feedback item. Thank you!","title":"Is taking the output MTLTexture from RealityKit 2's postProcessing pipeline suitable for writing to an AVAssetWriter, streaming via RTMP, etc?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#whats-the-best-way-to-connect-with-apple-designers-to-talk-about-ar-the-design-labs","text":"Definitely the design labs! There\u2019s also a session on designing for AR this year. https://developer.apple.com/wwdc22/10131 Thanks, it was a great session! Wish we could select an AR specific design lab There are definitely AR designers at the labs so maybe they can redirect you. Sorry I couldn\u2019t help more. Hey <@U03HMDX5C95>! When you request a design lab, be specific about your interests \u2014 we\u2019ve got designers here at Apple who work on AR technologies and we\u2019ll try to match you up accordingly! Awesome will do. Thanks <@U03H3628BL7> <@U03EBH4MA8Y>","title":"What's the best way to connect with Apple designers to talk about AR? The Design Labs?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#from-an-ar-design-perspective-what-is-best-for-knocking-down-objects-say-in-a-game-where-you-knock-down-blocks-is-it-better-to-have-the-user-run-the-device-through-the-blocks-tap-the-blocks-or-press-a-button-to-trigger-something-to-hit-the-blocks","text":"One of our designers answered your question: > It depends which approach is best \u2014 each have a set of pros and cons based on what you want out of the experience. It can be compelling to run through AR blocks if you want to emphasize lots of user motion in an experience and the scale of the experience is quite large \u2014 good for apps that can take advantage of wide open spaces. Tapping them is more immediate and indirect so if you wanted to destroy a tower quickly or something like that then that would be the way to go \u2014 and I could see that being very satisfying to trigger many physics objects to react at once. I think the same would apply to a button press, it\u2019s an indirect way to trigger it if the experience requires rapidly knocking them down. > > Overall I think it\u2019s up to what you want the experience to be, and maintaining internal consistency with other interactions within the app. Swiftstrike and Swiftshot are great example apps that use similar techniques: https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality https://developer.apple.com/documentation/realitykit/swiftstrike_creating_a_game_with_realitykit","title":"From an AR design perspective, what is best for knocking down objects? Say in a game where you knock down blocks, is it better to have the user run the device through the blocks, tap the blocks, or press a button to trigger something to hit the blocks?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-control-audio-media-in-usdz-ie-pause-skip-load-new-audio-file-with-a-scene-behavior-using-reality-composer-or-other-tool-i-know-its-possible-to-trigger-the-start-of-audio","text":"Currently Reality Composer does not support this. This sounds like a great feature request and we would appreciate if you can file feedback through Feedback Assistant. If you are willing to jump into code\u2026 You can use the https://developer.apple.com/documentation/realitykit/audioplaybackcontroller|AudioPlaybackController returned from the https://developer.apple.com/documentation/realitykit/entity/playaudio(_:)|playAudio API to play, pause, etc. You can also use https://developer.apple.com/documentation/realitykit/audiofileresource|AudioFileResource to to add / replace audio on entities.","title":"Is it possible to control audio media in USDZ (i.e. pause, skip, load new audio file) with a scene / behavior (using Reality Composer or other tool)?  I know it\u2019s possible to trigger the start of audio."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#how-am-i-able-to-access-attributes-wall-dimensions-numbers-of-objects-such-as-chairs-for-example-of-the-usdz-that-roomplan-creates-from-within-swift-do-i-need-scenekit-or-something-else-to-access-and-modify-that-data","text":"Hey Adam, that\u2019s really a question better suited for the RoomPlan and ARKit Engineers themselves. There\u2019s a RoomPlan that just started 15 minutes ago (check the Developer app). And there are ARKit lounges tomorrow at 3pm and Friday at 9am Also the documentation is up on the developer website for the RoomPlan framework itself - I\u2019d also suggest starting there: https://developer.apple.com/documentation/roomplan/","title":"How am I able to access attributes (wall dimensions, numbers of objects such as chairs for example) of the USDZ that RoomPlan creates from within Swift? Do I need SceneKit or something else to access and modify that data?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#regarding-optimisations-is-there-support-for-level-of-detail-and-instancing-in-realitykit","text":"Hey there, So instancing is mostly abstracted away behind the Entity.clone() method there\u2019s another thread talking about that right now: https://wwdc22.slack.com/archives/C03H49QK07P/p1654727237961649 Level of detail is not currently exposed as API and we\u2019d recommend filing a feature suggestion on that https://developer.apple.com/bug-reporting/ That said you can implement Level of Detail yourself (probably using custom Systems and Components) although we understand that may not be ideal. Please file feature suggestions regardless! Thank you <@U03H3628BL7>!","title":"Regarding optimisations: is there support for level of detail and instancing in RealityKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-plan-to-have-custom-render-passes-like-in-scenekit-with-scntechnique-in-realitykit","text":"While we do not currently support custom render passes, we have support for https://developer.apple.com/documentation/realitykit/arview/rendercallbacks-swift.struct/postprocess|post process effects . Please file a feature request through https://developer.apple.com/bug-reporting/|Feedback Assistant if your use case requires more customization:pray: Thank you <@U03HHPAHQ3C>","title":"Is there a plan to have custom render passes like in SceneKit with SCNTechnique in RealityKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-realitykit-support-light-sources-in-objects-for-example-if-you-wanted-a-light-bulb-if-so-is-there-documentation-for-this","text":"Hey Emory, There\u2019s various sorts of lighting in RealityKit - you might want to start here perhaps? https://developer.apple.com/documentation/realitykit/pointlight (see the Cameras and Lighting section in the docs) Oh, that's awesome. But there's currently no way to export these lights as .usdz or .reality files? Let me bring in experts on that\u2026 I\u2019m not personally sure what we support in file. Sounds like we don\u2019t support lighting in Reality Composer unfortunately so I\u2019d suggest filing a feature suggestion https://developer.apple.com/bug-reporting/ I think it is a usdz thing, not Reality Composer.","title":"Does RealityKit support light sources in objects \u2013 for example, if you wanted a light bulb. If so, is there documentation for this?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#not-a-question-but-the-realitykit-and-reality-composer-teams-deserve-such-huge-kudos-for-giving-us-non-designers-the-ability-to-create-3d-and-ar-experiences-that-are-incredibly-powerful-these-are-great-apps-and-tools-and-make-me-feel-very-prepared-for-the-future-in-learning-them","text":"I pasted your comment where the rest of the team can see it. Thank you for the kind words.","title":"Not a question, but the RealityKit and Reality Composer teams deserve such huge kudos for giving us non-designers the ability to create 3D and AR experiences that are incredibly powerful.  These are great apps and tools and make me feel very prepared for the future in learning them."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-reality-composer-a-force-was-applied-to-an-object-then-i-wanted-to-animate-it-into-another-scene-starting-from-the-post-force-location-is-there-a-way-to-apply-a-new-scene-using-its-last-known-position-i-hacked-the-position-by-guessing-the-ending-location-and-starting-the-next-scene-close-to-that-position-but-it-results-in-a-slight-motion-jitter","text":"Is this an rcproj that is embedded in an Xcode project? Sounds like you might be able to grab the position of the entity with the force applied to it and then set the new scene\u2019s position as that position? Started with a USDZ, and then added behaviors to it in Reality Composer. No Xcode involved. Perhaps there\u2019s a way to get the post force position in Reality Composer? I have to play the animation to finality, but then there is no way to observe the objects new XYZ coordinates, unless I stop the animation (but thus returning the object to its origin). I suppose a strong feature would be a way to view the object\u2019s live parameters during animation mode. This may be achievable if embedded in Xcode with some code. I recommend signing up for a Reality Composer lab if you would like to explore that further. But yes, being able to observe live parameters sounds like a great feature in Reality Composer. Please file a feature request using https://developer.apple.com/bug-reporting/|Feedback Assistant with your use case :slightly_smiling_face:","title":"In Reality Composer, a force was applied to an object.  Then I wanted to animate it into another scene, starting from the post force location.  Is there a way to apply a new scene using its last known position?  I hacked the position by guessing the ending location and starting the next scene close to that position but it results in a slight motion jitter."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-add-gestures-to-an-entire-reality-composer-scene-i-can-add-it-to-an-individual-entity-it-would-be-cool-to-let-users-place-the-entire-scene-otherwise-i-lose-all-the-reality-composer-behaviors-when-i-just-target-the-entity","text":"A way to get the entity gestures working on an entire scene is to use visualBounds(\u2026) and create a CollisionComponent on the root entity. You can then use <https://developer.apple.com/documentation/realitykit/controlling_entity_collisions_in_realitykit |CollisionGroup> to make sure it doesn\u2019t interfere with any physics. If you\u2019re using ARView.installGestures(\u2026) you\u2019ll need the entity to conform to HasCollision , which may require you to create a new entity type for the root. Quick example: ``// New Entity type which conforms for HasCollision` class CollisionAnchorEntity: Entity, HasAnchoring, HasCollision { } // Transfer scene contents let collisionAnchor = CollisionAnchorEntity() collisionAnchor.children.append(contentsOf: originalAnchor.children) collisionAnchor.anchoring = originalAnchor.anchoring // Create CollisionComponent for bounds of scene let sceneBounds = collisionAnchor.visualBounds(recursive: true, relativeTo: collisionAnchor) let collisionShape = ShapeResource.generateBox(size: sceneBounds.extents) .offsetBy(translation: sceneBounds.center) collisionAnchor.collision = CollisionComponent(shapes: [collisionShape]) // Install gesture on new anchor arView.installGestures(for: collisionAnchor) ` This is also a great question for our RealityKit and Reality Composer lab, which is happening tomorrow @9:00 - 11:00AM.","title":"is there a way to add gestures to an entire reality composer scene? i can add it to an individual entity it would be cool to let users place the entire scene (otherwise i lose all the reality composer behaviors when i just target the entity)"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#how-does-roomplan-handle-multiple-floors-can-you-extend-a-scan-what-if-room-edges-are-not-parallel-eg-a-lightly-sloped-roof","text":"Hey Alex, that\u2019s probably a question a RoomPlan engineer would be better suited to answer. There\u2019s a RoomPlan Lounge that started almost an hour ago. Check the Developer app to find it.","title":"How does RoomPlan handle multiple floors? Can you extend a scan? What if room edges are not parallel, e.g. a lightly sloped roof?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-visionkit-data-scanner-available-in-ar","text":"Using data scanner via VisionKit is possible using ARKit. ARKit provides the captured image on the ARFrame . One can inject the ARFrame's captured image into data scanner and obtain information about text. However, the result will be two-dimensional. If the use-case is to bring the detected text into the AR world in three dimensions one needs to estimate a transform for the 2D text. ARKit does not support this natively but does support custom anchoring. When you say custom anchoring, do you mean taking the screen coordinate, doing a hit test, and placing anchor?","title":"Is VisionKit / Data Scanner available in AR?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#can-we-get-the-lidar-camera-position-while-doing-a-mesh-in-arkit","text":"ARMeshAnchor transforms are already aligned with the wide camera, which is also what the camera transform is relative to. Thanks :slightly_smiling_face:","title":"Can we get the lidar camera position while doing a mesh in ARKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-the-mesh-from-an-arsession-available-through-the-delegate-methods","text":"Hi Stephen, Yes, once you turn on scene reconstruction by setting the sceneReconstruction property on ARWorldTrackingConfiguration . The meshes ares available as ARMeshAnchor through the ARKit's anchor delegate methods Reference APIs: ARSessionDelegate: https://developer.apple.com/documentation/arkit/arsessiondelegate ARSceneReconstruction: https://developer.apple.com/documentation/arkit/arscenereconstruction sceneReconstruction: https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3521376-scenereconstruction Thanks :slightly_smiling_face:","title":"Is the mesh from an ARSession available through the delegate methods?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-tried-to-set-4k-capture-based-on-these-instructions-if-let-hirescapturevideoformat-arworldtrackingconfigurationrecommendedvideoformatforhighresolutionframecapturing-assign-the-video-format-that-supports-hi-res-capturing-configvideoformat-hirescapturevideoformat-run-the-session-sessionrunconfig-but-it-still-seems-to-be-at-hd-device-avcapturefigvideodevice-0x15283be00-back-lidar-depth-cameracomappleavfoundationavcapturedevicebuilt-in_video9-hirescapturevideoformat-arvideoformat-0x2823b00a0-imageresolution1920-1440-pixelformat420f-framespersecond60-capturedevicetypeavcapturedevicetypebuiltinwideanglecamera-capturedeviceposition1-videoformatimageresolution-19200-14400-i-am-using-11-in-ipad-pro-at-ios16-and-xcode-14-beta","text":"Hi, is this iPad Pro 11-inch 5th generation? Note that this feature is only supported on iPad with M1. Model is MHQR3LL/A, with M1 Thanks, can you please report this to https://developer.apple.com/bug-reporting/ , we need to take a deeper look. Will do, thanks I figured it out. The video itself shows ARWorldTrackingConfiguration.recommendedVideoFormatFor4KResolution, but the \"code\" shows ARWorldTrackingConfiguration.recommendedVideoFormatForHighResolutionFrameCapturing Right, recommendedVideoFormatForHighResolutionFrameCapturing is used for capturing high resolution still images while the session is running. For 4K video, you should use recommendedVideoFormatFor4KResolution","title":"I tried to set 4k capture based on these instructions:  if let hiResCaptureVideoFormat = ARWorldTrackingConfiguration.recommendedVideoFormatForHighResolutionFrameCapturing {     // Assign the video format that supports hi-res capturing. config.videoFormat = hiResCaptureVideoFormat } // Run the session. session.run(config)  But it still seems to be at HD:  device: &lt;AVCaptureFigVideoDevice: 0x15283be00 [Back LiDAR Depth Camera][com.apple.avfoundation.avcapturedevice.built-in_video:9] hiResCaptureVideoFormat: &lt;ARVideoFormat: 0x2823b00a0 imageResolution=(1920, 1440) pixelFormat=(420f) framesPerSecond=(60) captureDeviceType=AVCaptureDeviceTypeBuiltInWideAngleCamera captureDevicePosition=(1) videoFormat.imageResolution: (1920.0, 1440.0)  I am using 11-in iPad Pro at iOS16, and xcode 14 beta"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#we-are-making-a-game-called-follow-the-white-rabbit-about-a-magician-whose-magic-suddenly-works-what-are-some-tipsbest-practices-to-prevent-ar-objects-from-shifting-were-finding-a-bit-of-drift-thats-most-noticeable-with-larger-virtual-objects-sparkles-rabbit-tophat-sparkles","text":"Hi Nicole, We recommend adding an ARAnchor in the position where you want to place and object and then associate your node/entity with that anchor. This should help prevent drifting. <@U03HERF817F> Thank you!","title":"We are making a game called Follow the White Rabbit. About a magician whose magic suddenly works. What are some tips/best practices to prevent AR objects from shifting? We're finding a bit of drift that's most noticeable with larger virtual objects. :sparkles: :rabbit: :tophat: :sparkles:"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-ive-had-some-experience-with-reality-composer-but-for-coding-i-only-know-swiftui-is-it-possible-to-create-an-ar-app-with-ar-kit-only-with-swiftuiif-so-could-you-share-some-suggestions-or-links-on-getting-started","text":"Hi Haolun, You can use ARKit inside a SwiftUI app. You can also use RealityKit to build ARKit apps in a declarative way. Here are the links to resources and sample code to help you get started: ARKit - https://developer.apple.com/documentation/arkit RealityKit - https://developer.apple.com/documentation/realitykit Thanks!","title":"Hi, I\u2019ve had some experience with Reality Composer, but for coding, I only know SwiftUI. Is it possible to create an AR App with AR Kit only with SwiftUI\uff1fif so, could you share some suggestions or links on getting started?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-adding-anchors-everywhere-force-arkit-to-keep-a-good-understanding-and-reduce-drift-everywhere-if-yes-will-this-affect-the-tracking-quality","text":"ARKit offers functionality to add custom anchors which is the preferred and recommended way to place content. https://developer.apple.com/documentation/arkit/arsession/2865612-addanchor Custom anchors are used internally for drift correction. We cannot guarantee absolutely no drift. However, using your own anchors will use the system's best knowledge to correct for any drift.","title":"Does adding anchors everywhere force ARKit to keep a good understanding and reduce drift everywhere? If yes, will this affect the tracking quality?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-am-working-on-an-app-that-uses-arkit-to-guide-the-user-around-an-object-while-semi-automatically-capturing-images-for-later-server-side-3d-reconstruction-i-very-much-appreciate-the-ability-to-control-the-capture-session-and-the-ability-to-capture-high-resolution-images-that-you-added-in-ios-16-i-believe-currently-we-do-not-have-much-control-over-the-high-resolution-image-capture-it-would-be-great-if-we-could-configure-the-avcapturephotosettings-used-for-the-capture-for-photogrammetric-reconstruction-purposes-it-would-be-amazing-if-we-could-for-example-capture-a-pro-raw-image-during-the-arkit-session","text":"Hey Hendrik! We really appreciate the feedback and are glad that you are already starting to put these API changes to good use! At the moment, we do not expose the ability to pass in AVCapturePhotoSettings through our API, but this would be a great feature request to submit via http://developer.apple.com/bug-reporting/","title":"I am working on an app that uses ARKit to guide the user around an object while semi-automatically capturing images for later (server side) 3D reconstruction. I very much appreciate the ability to control the capture session and the ability to capture high resolution images that you added in iOS 16.  I believe currently we do not have much control over the high resolution image capture? It would be great if we could configure the AVCapturePhotoSettings used for the capture.  For photogrammetric reconstruction purposes it would be amazing if we could for example capture a Pro RAW image during the ARKit session."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#we-want-to-play-with-the-depth-map-is-it-possible-to-get-the-lidar-camera-position-with-the-depth-map-weve-tried-using-the-wide-camera-position-and-it-doesnt-work-because-the-wide-camera-position-is-not-the-same-as-the-depth-maps-camera-position","text":"Hi Stephen, The depth map surfaced through the Scene Depth API does align with the wide angle camera and should correspond to the camera transform available through the ARFrame. Here is a sample code that generates a colored point cloud by combining the wide angle camera image and depth map https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth If you still see some issues, I recommend filing a bug through the feedback assistant at http://developer.apple.com/bug-reporting/ (edited) Thank you :slightly_smiling_face:","title":"We want to play with the depth map. Is it possible to get the lidar camera position with the depth map?  We've tried using the wide camera position and it doesn't work, because the wide camera position is not the same as the depth map's camera position."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-arkit-track-which-version-of-usdz-is-in-use-im-interested-in-using-tools-from-multiple-providers-in-my-pipeline-and-i-want-to-verify-the-format-is-consistent-through-workflow","text":"ARKit itself has no notion of rendered content. Content (USDZ) is commonly handled by the rendering engine on top of ARKit like RealityKit, SceneKit, Metal, etc. In order to learn more about USDZ and how to efficiently use it we recommend this talk. https://developer.apple.com/videos/play/wwdc2022/10129/ I'll be sure to check out the session. Thank you. Pixar provides USDZ Python and looking at the spec here I see no recent updates. https://graphics.pixar.com/usd/release/spec_usdz.html|https://graphics.pixar.com/usd/release/spec_usdz.html But I would very much like to learn how to use it efficiently both client and server side","title":"Does ARKit track which version of USDZ Is in use?  I\u2019m interested in using tools from multiple providers in my pipeline and I want to verify the format is consistent through workflow."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#when-i-read-the-exif-data-that-arkit-now-provide-i-get-subjectarea-2013-1511-2116-1270-lensspecification-42-42-16-16-what-is-subjectarea-why-does-lensspecification-has-twice-the-42-the-focal-i-think-and-twice-16-the-aperture-i-think","text":"The subject area is defined as rectangle with center coordinate and its dimensions. In this case, the center is at 2013, 1511 with rectangle dimensions 2116 x 1270 . For more details, you may refer to Exif standard tags.","title":"When I read the exif data that ARKit now provide I get:  \"SubjectArea\": : ( 2013, 1511, 2116, 1270 ) \"LensSpecification\": ( 4.2, 4.2, 1.6, 1.6 )  What is SubjectArea? Why does LensSpecification has twice  the 4.2 (the focal I think) and twice 1.6 (the aperture I think)?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#do-modifications-made-to-configurablecapturedeviceforprimarycamera-while-an-arsession-is-running-change-the-output-of-capturehighresolutionframe-what-about-modifications-before-running-a-new-arconfiguration","text":"No, any modifications to ARConfiguration object does not affect a running session. You need to call run after modifying configuration for it to be used. Awesome. Any reason why it's not an instance property on the ARConfiguration itself? Seems like that would more clearly communicate this behavior. Happy to file a feedback if that helps. <@U03HWCBGUBT> I went ahead and filed a suggestion feedback: https://feedbackassistant.apple.com/feedback/10142346|FB10142346 Thanks for the feedback. You can change capture device settings such as exposure, white balance etc, that will be reflected in the output of ARSession. However, you cannot change the input/output configurations on capture session. <@U03HWCBGUBT> is changing input/output possible before starting the session? No, its not possible to change it, ARSession always adds the required inputs and outputs. Thanks Arsalan!","title":"Do modifications made to configurableCaptureDeviceForPrimaryCamera while an ARSession is running change the output of captureHighResolutionFrame? What about modifications before running a new ARConfiguration?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#are-there-resources-on-how-to-generate-a-texture-for-the-mesh-generated-by-arkit","text":"We do not have any resources for this. You should be able to use the wide angle camera and camera transform to generate texture maps for the meshes but unfortunately we do not have any resources or sample code showing that. We do have this sample code showing how to generate colored point clouds using the scene depth API, hope it is of some help. https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth","title":"Are there resources on how to generate a texture for the mesh generated by ARKit ?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#any-tips-for-getting-started-in-ar-development-with-0-coding-knowledge-a-mentor-would-also-be-appreciated","text":"Hey Ramiro! Regardless of your educational background, anyone can learn how to code if you put in the effort and are passionate about it. There are tons of resources online, many of which have been produced by Apple in the form of documentation, example projects, and WWDC videos, that can help you to learn a programming language, such as Swift. I would suggest doing some tutorials, watching videos, maybe find a highly rated book on iOS programming, etc to learn how to begin building iOS apps. Once you are comfortable with that, then you can start to dive into AR specifically. Finding a good book on linear algebra would be useful if you are going to get into AR and graphics programming, but start with the basics first! For ARKit , we have all sorts of documentation and examples that you can take a look at: https://developer.apple.com/documentation/arkit/ Apple\u2019s documentation is great. I also found the site RayWenderlich to be super helpful. They even have a book specifically for AR: https://www.raywenderlich.com/books/apple-augmented-reality-by-tutorials As well as a lot of great entry level tutorials and books.","title":"Any tips for getting started in AR development with 0 coding knowledge? A mentor would also be appreciated!"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#do-any-of-the-ar-frameworks-have-hand-tracking-and-the-ability-to-register-a-pinch-between-the-thumb-and-pointer-finger","text":"ARKit does not have any hand tracking feature. The Vision framework offers functionality for hand gesture detection. https://developer.apple.com/videos/play/wwdc2020/10653/ You may find the camera's captured images on the ARFrame and can inject this into Vision. So by combining multiple frameworks you could achieve something close to the requested feature. Awesome Thank you,","title":"Do any of the AR frameworks have hand tracking, and the ability to register a pinch between the thumb and pointer finger?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-an-arkit-game-im-detecting-collisions-between-the-real-user-and-walls-that-i-build-out-of-scnboxes-i-have-a-cylinder-that-follows-the-devices-pointofview-to-accomplish-that-i-use-both-physicsworld-didbegin-and-didend-contact-delegate-methods-because-i-need-to-know-both-when-collisions-start-and-when-they-end-most-of-the-time-this-works-fine-however-sometimes-the-didbegin-method-runs-last-even-though-i-clearly-moved-away-and-there-are-no-more-collisions-so-my-app-still-thinks-that-were-in-a-collision-state-any-idea-why-this-could-happen-thanks","text":"Hi <@U03JXRWDTDG>. In this lounge we are focused on questions to ARKit, it looks like your request is specific to SceneKit. Please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Thanks. Indeed, it\u2019s SceneKit inside ARKit. What I ask can only happen in AR where the actual user moves in the world and interacts with nodes that I build. I will definitely ask the question in the Developer Forums. Thanks!","title":"In an ARKit game, I\u2019m detecting collisions between the real user and walls that I build out of SCNBoxes. I have a cylinder that follows the device\u2019s pointOfView to accomplish that. I use both physicsWorld didBegin and didEnd contact delegate methods because I need to know both when collisions start and when they end. Most of the time this works fine, however, sometimes the didBegin method runs last, even though I clearly moved away and there are no more collisions so my app still thinks that we\u2019re in a collision state. Any idea why this could happen?\u2028 Thanks!"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-triangles-vs-quads-matter-for-arkit","text":"Hey Adrien, I\u2019m not sure of the context of this question, would you mind reformulating it / adding more context and post it again?","title":"Does triangles vs quads matter for ARKit"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-arkit-give-any-confidence-score-for-each-camera-position-it-estimates-during-camera-tracking-if-any-camera-position-is-not-estimated-correctly-do-you-suggest-any-option-to-improve-it","text":"Yes, ARKit returns tracking state for every frame update. You can read more about it here: https://developer.apple.com/documentation/arkit/managing_session_life_cycle_and_tracking_quality It is highly recommended to integrate standard coaching view in your app to guide users when tracking is limited. More details at: https://developer.apple.com/documentation/arkit/arcoachingoverlayview Can you explain a little more about the confidence score and what the physical meaning is? How do you define the confidence score? You can find details at https://developer.apple.com/documentation/arkit/managing_session_life_cycle_and_tracking_quality Basically, ARKit provides discrete tracking states only.","title":"Does ARKit give any confidence score for each camera position it estimates during camera tracking? If any camera position is not estimated correctly do you suggest any option to improve it?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-do-body-tracking-while-being-in-an-arworldtrackingconfiguration-i-believe-it-isnt-possible-but-to-make-sure","text":"Correct, 3D body tracking is only supported using the ARBodyTrackingConfiguration . However, we support the detection of 2D bodies on multiple configurations; the ARWorldTrackingConfiguration is one of them. In order to check which configuration supports this you may use the supportsFrameSemantics: function. https://developer.apple.com/documentation/arkit/arconfiguration/3089122-supportsframesemantics?language=objc Thanks! I didn\u2019t know that 2D tracking was supported on ARWorldTrackingConfiguration","title":"Is it possible to do body tracking while being in an ARWorldTrackingConfiguration? I believe it isn\u2019t possible but to make sure."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-select-wish-rear-camera-to-use-for-arview-wide-ultrawide-panoramic","text":"Hi Hassan, ARKit only supports wide camera as the primary camera. It is not possible use other cameras for rendering. Thank you . It have be nice to have ultrawide too it gets more content of real world that users can film Thanks for the suggestion. Please file a feature request through the feedback assistant at http://developer.apple.com/bug-reporting/ Is it possible to use the front facing camera with ARKit? The face tracking configuration does use the front facing camera https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration","title":"Is there a way to select wish rear camera to use for ARView? (Wide, ultrawide, panoramic)"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#can-i-use-the-mapkit-3d-model-of-a-city-and-anchor-it-as-a-child-of-an-anchor-using-lidar-geotracking-for-long-distance-occlusion-and-collision-purposes","text":"There is no integration of MapKit into the ARView. If you know the building footprint (i.e. the polygon in lat/lon coordinates) or even exact geometry anchored to a lat/lon coordinate you can transform these coordinates by placing a GeoAnchor at a that location and work with the local coordinates for the purposes of occlusion and collision. I wanted to occlude something big like buildings in front of a dinosaur terrorizing downtown I think this might work, when the players keep their vision down. So what I meant is if you know the four corners of that building you can create four Location Anchors at those lat/lon coordinates. If they are tracked in ARKit you get the local coordinates and can build an occlusion/collision mesh. Wow, thank you for the help. I didn't think about pushing it that far with geoanchors","title":"Can I use the MapKit 3d model of a city, and anchor it as a child of an anchor using lidar geotracking? For long distance occlusion and collision purposes?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#ive-been-trying-to-build-the-hydra-sample-will-it-always-require-cmake","text":"Hi Steven. In this lounge we are focused on questions to ARKit, it looks like your request is specific to USD. As the USD lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"I've been trying to build the Hydra sample, will it always require CMake?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#whats-the-maximum-dimensions-roomplan-support","text":"The recommended maximum size of the room is 30 x 30 feet. Thank you","title":"What\u2019s the maximum dimensions RoomPlan support ?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#one-we-have-a-roomplan-scan-can-we-use-it-next-time-as-a-anchor-so-we-can-always-paint-model-in-same-place","text":"RoomPlan is not an ARAnchor in current design. Thanks for suggestion. We will take into consideration. Thank you <@U03J1MVSDBP> I created a demo where custom ARAnchors are created for RoomPlan objects. The same could be done for surfaces and then saved to a world map: https://github.com/jmousseau/RoomObjectReplicatorDemo Thank you","title":"One we have a RoomPlan scan, can we use it next time as a anchor so we can always Paint Model in same place ?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-setting-arkit-to-use-4k-resolution-affect-the-battery-longevity-does-it-increase-the-risk-to-get-the-device-too-hot-even-if-the-fps-is-limited-at-30-fps-instead-of-60-fps-is-there-a-way-to-get-60-fps-at-4k-resolution","text":"Hey Gaetan! Yes, using 4k resolution may result in more power being consumed. It may also result in thermal mitigation engaging to keep the device from getting too hot, which may impact performance. At the moment, we are only supporting 4k @ 30hz.","title":"Does setting ARKit to use 4K resolution affect the battery longevity ? Does it increase the risk to get the device too hot, even if the fps is limited at 30 fps instead of 60 fps ? Is there a way to get 60 fps at 4K resolution ?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-arsession-has-the-getgeolocationforpoint-method-is-there-also-a-way-to-obtain-the-heading-relative-to-north-given-a-directional-vector-within-the-scene-or-for-the-device-point-of-view","text":"We are not exposing the heading directly. You can create any GeoAnchor in your vicinity and then compare its transform with your camera\u2019s transform. Since GeoAnchors are always aligned to East-Up-South, you can derive any global camera orientation by comparing the camera\u2019s transform to the GeoAnchor transform. Gotcha, thank you <@U03J7GC2A2C>! Do you think this would be worth a feature requests to avoid the additional anchor or is it rather unlikely that this will get exposed? Anything you need is important to us :heart:, please file it.","title":"Hi, ARSession has the getGeoLocation(forPoint: \u2026 method. Is there also a way to obtain the heading relative to north given a directional vector within the scene or for the device (point of view)?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#might-there-be-more-example-projects-showcasing-pure-metal-with-arkit-scenekit-is-cool-but-admittedly-id-love-to-see-more-low-level-examples-alternatively-is-anyone-working-on-some-open-source-projects-showcasing-something-like-this-i-think-it-would-be-a-big-win-for-apple-platform-development-to-build-up-a-lot-more-examples","text":"Hi Karl, Thanks for the suggestion. Here are some existing sample code that uses Metal with ARKit: https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth https://developer.apple.com/documentation/arkit/environmental_analysis/creating_a_fog_effect_using_scene_depth https://developer.apple.com/documentation/arkit/displaying_an_ar_experience_with_metal?language=objc Hello, and thanks!","title":"Might there be more example projects showcasing pure Metal with ARKit? SceneKit is cool, but admittedly, I'd love to see more low-level examples.  :) Alternatively, is anyone working on some open source projects showcasing something like this? I think it would be a big win for Apple-platform development to build-up a lot more examples."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-the-new-lighting-capability-only-in-ar-quick-look-or-is-it-also-available-in-realitykit-and-scenekit-renderers","text":"Hi George. In this lounge we are focused on questions to ARKit, it looks like your request is specific to RealityKit / SceneKit. As those lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"Is the \"new lighting\" capability only in AR Quick Look or is it also available in RealityKit and SceneKit renderers?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#do-any-of-the-ar-frameworks-accept-location-or-hand-position-from-apple-watch","text":"No, ARKit runs standalone on iPhone and iPad devices only and does not take any external inputs. <@U03HWCBGUBT> thanks for doing the legwork. I see the other response regarding Vision and World tracking. I'll work with that.","title":"Do any of the AR frameworks accept location or hand position from Apple Watch?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-have-a-strange-arkitscenekit-rendering-bug-and-wanted-to-see-if-any-of-the-engineers-arkit-engineers-participating-here-might-have-an-idea-as-to-what-might-be-causing-this-and-how-to-fix-it-i-am-rendering-a-semi-transparent-object-using-scenekit-the-material-uses-the-constant-lighting-model-a-texture-controls-transparency-if-the-scene-is-fairly-bright-then-the-transparent-material-will-initially-show-less-transparent-than-it-should-if-the-camera-gets-pointed-at-something-less-bright-or-the-bright-part-is-blocked-as-in-the-example-video-linked-below-then-the-rendering-suddenly-switches-to-be-more-transparent-this-switch-is-permanent-it-does-not-switch-back-again-see-example-video-here-httpswwwdropboxcoms8wpo83loxg51o43arkit_rendering_bugmovdl0httpswwwdropboxcoms8wpo83loxg51o43arkit_rendering_bugmovdl0-any-idea-what-might-be-causing-this","text":"Hi Hendrik. In this lounge we are focused on questions to ARKit, it looks like your request is specific to SceneKit. Please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ I think this is actually more of an ARKit issue though. Because the rendering changes based on the camera feed brightness. SceneKit does not know about ARKit\u2019s camera feed. Also: Are there still SceneKit engineers at Apple? I would be surprised if there were. Somehow a switch seems to be flipped that changes SceneKit\u2019s rendering dramatically. But based on my understanding I would think it has to be ARKit that is flipping that switch (intentionally or accidentally) based on the scene brightness. Where by \u2018scene\u2019 I am referring to the real world surrounding.. You can try setting automaticallyUpdatesLighting to false in ARSCNView to disable the behavior.","title":"I have a strange ARKit+SceneKit rendering bug and wanted to see if any of the engineers ARKit engineers participating here might have an idea as to what might be causing this and how to fix it? I am rendering a semi-transparent object using SceneKit. The material uses the constant lighting model, a texture controls transparency.  If the scene is fairly bright, then the transparent material will initially show less transparent than it should. If the camera gets pointed at something less bright (or the bright part is blocked as in the example video linked below) then the rendering suddenly switches to be more transparent. This switch is permanent, it does not switch back again.  See example video here:  https://www.dropbox.com/s/8wpo83loxg51o43/ARKit_rendering_bug.mov?dl=0|https://www.dropbox.com/s/8wpo83loxg51o43/ARKit_rendering_bug.mov?dl=0 Any idea what might be causing this?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#we-can-capture-session-events-namely-anchors-addremove-by-implementing-arsessiondelegate-not-realitykit-is-it-possible-get-similar-or-part-of-this-events-with-realitykit-to-avoid-converting-a-from-aranchor-to-anchorentity","text":"RealityKit exposes the ARSession through this API https://developer.apple.com/documentation/realitykit/arview/session You can set the delegate on it to listen to ARKit delegate events Thank you","title":"We can capture session events (namely anchors add/remove) by implementing ARSessionDelegate (not RealityKit), is it possible get similar or part of this events with RealityKit? (To avoid converting a from ARAnchor to AnchorEntity)"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-have-an-arkit-app-that-uses-scenekit-and-pbr-is-there-a-simple-process-to-migrate-70k-lines-of-swift-to-use-realitykit-i-need-to-resolve-the-rendering-differences-between-my-arkit-app-that-uses-the-scenekit-renderer-and-the-renders-that-come-from-ar-quick-looks-use-of-the-realitykit-renderer","text":"Hi George. In this lounge we are focused on questions to ARKit, it looks like your request is specific to SceneKit / RealityKit and Swift. I\u2019m not aware of any tool for porting large chunks of swift code. Please use the developer forums or the feedback system to get connected to an engineer from other team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"I have an ARKit app that uses SceneKit and PBR. Is there a simple process to migrate ~70K lines of Swift to use RealityKit? I need to resolve the rendering differences between my ARKit app that uses the SceneKit renderer and the renders that come from AR Quick Look's use of the RealityKit renderer."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-talk-that-goes-into-any-detail-about-using-the-new-spatial-framework-httpsdeveloperapplecomdocumentationspatialhttpsdeveloperapplecomdocumentationspatial-and-how-it-works-with-arkit-scenekit-andor-realitykit","text":"There is no dedicated talk about Spatial framework. It provides core functions that can be used with any 2D/3D primitive data. Thank you","title":"Is there a talk that goes into any detail about using the new Spatial framework - https://developer.apple.com/documentation/spatial|https://developer.apple.com/documentation/spatial - and how it works with ARKit, SceneKit, and/or RealityKit?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#when-using-the-new-4k-resolution-in-arkit-for-a-post-production-filmtelevision-workflow-what-is-the-suggested-way-to-take-the-ar-experience-and-output-to-a-video-file","text":"Hi Brandon, To capture and replay an ARKit session, see an example here: https://developer.apple.com/documentation/arkit/arsession/recording_and_replaying_ar_session_data If you want to capture video in your app in order to do post processing later, you could use and configure an AVAssetWriter to capture a video. https://developer.apple.com/documentation/avfoundation/avassetwriter Thanks, <@U03K4D9U7GU>! Just to follow-up on that comment; do you have a suggest way of getting the output of the ARSession into a format that can be written to an AVAssetWriter (I'm imagining this would require a custom Metal processing pipeline to convert the MTLTextures to CVPixelBuffers, as I don't think there's a way of getting CVPixelBuffers out of ARKit directly)? We do provide a camera frame with every ARFrame, see: https://developer.apple.com/documentation/arkit/arframe/2867984-capturedimage Thanks, <@U03K4D9U7GU>! Just for posterity; the ARFrame's CVPixelBuffer (rather, capturedImage property) doesn't include the overlaid AR content, as the user might see through the phone, though, correct? This is the background video feed only? Yes, ARFrame\u2019s capturedImage is just the \u2018clean slate\u2019, it doesn\u2019t contain any virtual content rendered on top of it. Yeah, the output from ARKit is just the camera frame without virtual content. If you are doing your own rendering and your metal textures are backed by IOSurface s. then you can easily create CVPixelBuffer s using the IOSurface s and then pass those to AVFoundation for recording. Got it, thanks, <@U03J7GC2A2C> <@U03H35XE3N3>. I thought that was the case! I appreciate your replies!","title":"When using the new 4K resolution in ARKit for a post-production (film/television) workflow, what is the suggested way to take the AR experience and output to a video file?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#if-i-create-a-usdz-with-scnscenewriteusdzurl-will-it-use-the-new-lighting-and-contrast-adjustment-in-ar-quick-look","text":"Hi George. In this lounge we are focused on questions to ARKit, it looks like your request is specific to Quick Look and USD. Those lounges are closed for WWDC, so please use the developer forums or the feedback system to get connected to an engineer from other team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"If I create a USDZ with SCNScene.write(...usdzURL...) will it use the \"new lighting\" and contrast adjustment in AR Quick Look?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-get-notified-when-arkit-relocate-itself-when-it-finds-out-that-it-has-drifted-from-my-experience-the-tracking-status-does-not-change-when-this-happens-also-is-there-a-way-to-ask-arkit-to-not-try-to-relocate-itself-after-a-drift","text":"Hi Gaetan, We recommend adding ARAnchors and associating your virtual content with ARAnchors. In case there is a drift, the anchor delegate didUpdateAnchor would update the anchor such that the virtual content stays in the same location in the real world.","title":"Is there a way to get notified when ARKit relocate itself when it finds out that it has drifted ? From my experience, the tracking status does not change when this happens. Also is there a way to ask ARKit to not try to relocate itself after a drift ?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-recent-years-i-read-and-partially-experimented-with-the-latest-graphics-frameworks-but-somehow-i-got-lost-over-a-cohesive-developer-experience-when-to-use-which-framework-and-how-to-integrate-them-into-a-good-product-the-are-amazing-vertical-solutions-in-these-frameworks-but-i-see-only-few-strong-storiesappsolutions-around-them-does-apple-has-a-big-picture-guide-when-to-use-which-framework-how-to-interact-between-them","text":"Many of them work together...are there particular corner cases you had? Hi Oliver, We understand that the number of frameworks can be daunting sometimes. However as you alluded to, we try and offer \"high level\" frameworks to try and meet developers' needs out of the box, for example, being able to use RealityKit for rendering instead of the lower level Metal. That said, Apple provides several tutorials and code samples to introduce developers into the various frameworks, e.g. https://developer.apple.com/documentation/realitykit/building_an_immersive_experience_with_realitykit . Another great resource are WWDC videos, which go back several years in order to build a solid understanding of a particular framework or technology. https://developer.apple.com/videos/all-videos/ For ARKit specific, see Ryan's great answer here: https://wwdc22.slack.com/archives/C03H49QK07P/p1654815037708349?thread_ts=1654815015.825259&cid=C03H49QK07P <@U03K4D9U7GU> thank you, these are fine vertical guides but I\u2019m looking for a more horizontal approach. (super specific example, probably needing, camera, vision, realitykit, arkit\u2026): can we let the user take a picture of a tree and map it as a texture easily onto a pair of glasses frame for AR preview when he is looking into the mirror? the solution would probably need a USDZ frame model to be rendered in RealityKit, but can we map a UIImage onto an element in USDZ programatically? I'm pretty sure RealityKit has TextureResource which can take a CGImage (the base decoding class) and goes \"up\" to a MetalTexture Knowing the basic data flow helps here because taking a photo means a photo file which has to be decoded first into some kind of pixel buffer Then that has to go to the GPU, usually as a texture","title":"In recent years I read and partially experimented with the latest \"graphics\" frameworks - but somehow I got lost over a cohesive developer experience when to use which framework (and how to integrate them into a good product). The are amazing \"vertical\" solutions in these frameworks but I see only few strong stories/app/solutions around them. Does Apple has a \"big picture\" guide when to use which framework, how to interact between them?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#any-guidance-on-how-to-build-a-bridge-between-arkit-and-spatial-audio-say-youre-viewing-an-object-and-the-audio-evolves-as-you-change-the-objects-perspective","text":"We do not have a sample code that uses ARKit together with spatial audio (PHASE). However, this is a great question, can you please send us a request through https://developer.apple.com/bug-reporting/ <@U03HWCBGUBT> yes, will do! thank you","title":"Any guidance on how to build a bridge between ARKit and Spatial Audio?  Say you're viewing an object and the audio evolves as you change the object's perspective"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#im-continuing-to-have-issues-with-the-performance-of-adding-interactivity-to-an-entity-added-via-realitykit-entityloadasyncnamed-myentity-i-create-a-modelentity-i-added-my-async-loaded-entity-as-a-child-to-the-modelentity-i-add-the-modelentity-as-a-child-to-the-anchorentity-i-add-the-anchorentity-to-the-arviews-scenes-i-get-the-bounds-of-the-entity-using-entityvisualboundsrelativeto-parententity-i-add-collision-to-the-parententity-with-parententitycollision-collisioncomponentshapes-shaperesourcegenerateboxsize-entityboundsextentsoffsetbytranslation-entityboundscenter-i-install-gestures-all-on-the-arview-this-results-in-an-entity-i-can-pinch-and-scale-but-no-manner-to-rotate-or-transform-its-position-am-i-doing-something-wrong","text":"Hi Brandon. In this lounge we are focused on questions to ARKit, it looks like your request is specific to RealityKit. Those lounges are closed for WWDC, so please use the developer forums or the feedback system to get connected to an engineer from that team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"I'm continuing to have issues with the performance of adding interactivity to an entity added via RealityKit (Entity.loadAsync(named: \"MyEntity\")).  - I create a ModelEntity. - I added my async loaded Entity as a child to the ModelEntity. - I add the ModelEntity as a child to the AnchorEntity. - I add the AnchorEntity to the ARView's scenes. - I get the bounds of the entity using entity.visualBounds(relativeTo: parentEntity) - I add Collision to the ParentEntity with parentEntity.collision = CollisionComponent(shapes: [ShapeResource.generateBox(size: entityBounds.extents).offsetBy(translation: entityBounds.center)]) - I install gestures .all on the ARView.  This results in an Entity I can pinch and scale, but no manner to rotate or transform its position.  Am I doing something wrong?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#with-body-motion-tracking-can-a-dev-specify-the-sample-rate-of-the-sampleevery-few-ms-and-write-out-that-sample-in-a-continues-manner-eg-a-basic-motion-recorder-please-ignore-the-question-if-this-is-the-wrong-place-to-ask","text":"Body tracking runs at 60 Hz at the same cadence of the camera. We cannot compute this faster than this. However, by changing the ARVideoFormat you may change this to 30 Hz or other supported frame rates. We do not offer a functionality to write the motion capture data to file. However, our data is very well compatible with a format called BVH. By following the topology of the skeleton given by the ARSkeletonDefinition and the data coming from an ARBodyAnchor one could generate such a file output. https://en.wikipedia.org/wiki/Biovision_Hierarchy GOLD! solid gold! thanks for the help","title":"With body motion tracking, can a dev specify the sample rate of the sample(every few ms) and write out that sample in a continues manner. eg a basic motion recorder. Please ignore the question if this is the wrong place to ask"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#have-there-been-any-changes-to-the-light-estimation-apis-for-example-is-directional-light-available-with-a-world-tracking-config","text":"No, there haven\u2019t been changes to light estimation in ARKit this year. Thank you!","title":"Have there been any changes to the light estimation APIs? For example, is directional light available with a world tracking config?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-any-way-to-protect-usdz-files-so-that-they-cannot-be-downloaded-from-the-web-but-still-be-seen-in-ar-quick-look","text":"Hi Jose Mariano. In this lounge we are focused on questions to ARKit, it looks like your request is specific to QuickLook/USDz. As those lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"Is there any way to protect usdz files so that they cannot be downloaded from the web, but still be seen in ar quick look?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#in-discover-arkit-6-theres-a-cool-demo-of-setting-a-point-in-ar-where-a-picture-was-taken-and-guiding-the-user-there-is-sample-code-for-this-available-somewhere","text":"Thanks for your excitement about that app idea. We do not have the sample code, but I recommend going to our ARKit 4 session where we explain how to pick a coordinate in Maps and create a GeoAnchor based on it. For alignment with the real world we have the example with the Ferry Building in SF. We followed that exact workflow with the focus square example.","title":"In \"Discover ARKit 6\" there's a cool demo of setting a point in AR where a picture was taken, and guiding the user there. Is sample code for this available somewhere?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-does-adding-custom-anchors-for-drift-correction-is-pertinent-on-a-lidar-enable-device","text":"In general we recommend to use/add anchors independent from the device you are using.","title":"Hi ! Does adding custom anchors for drift correction is pertinent on a lidar enable device ?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hello-usd-team-thanks-for-the-great-demo-usd-and-hydrastorm-demo-is-it-possible-to-build-an-app-with-usd-hydrastorm-swiftui-interactions-ex-a-car-configurator-that-has-a-car-as-a-usd-file-usd-physicsrigidbody-joint-applied-on-the-door-of-the-car-usd-file-swiftui-button-to-trigger-opening-the-doors-everything-rendered-in-hydrastorm-as-the-demo-app","text":"Hi Alexandre, During this lounge we are focused on questions on ARKit. The USD lounges are already closed for this WWDC. So for those question I encourage you to use the developer forums or the feedback system to get connected to an engineer from those team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ ok, thanks","title":"Hello USD team, Thanks for the great demo USD and Hydra/Storm demo!  Is it possible to build an app with USD Hydra/Storm - SwiftUI interactions?  Ex: a car configurator that has: - A car as a USD file - USD PhysicsRigidBody joint applied on the door of the car USD file. - SwiftUI button to trigger opening the doors  - Everything rendered in Hydra/Storm. as the demo app"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-noticed-that-the-built-in-camera-app-can-detect-very-small-qr-codes-compared-to-4k-ar-why-is-that-is-there-a-workaround","text":"Hi Ivan, we don\u2019t have QR code detection in ARKit. However, you can use the Vision APIs to do QR code detection on the captured image. This VisionKit talk and sample code might be of interest to you: https://developer.apple.com/videos/play/wwdc2022/10025/ https://developer.apple.com/documentation/visionkit/scanning_data_with_the_camera","title":"I noticed that the built-in Camera app can detect very small QR codes compared to 4K AR. Why is that? Is there a workaround?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#i-am-really-struggling-to-build-user-interactions-in-realitykit-that-are-as-smooth-as-ar-quicklook-in-terms-of-a-user-placing-moving-rotating-or-scaling-a-model-im-using-installgestures-on-the-arview-but-its-nowhere-near-as-smooth-as-the-ar-quicklook-experience-is-there-any-suggestions-of-how-to-get-ar-quicklook-style-integrations","text":"Hi Brandon. In this lounge we are focused on questions to ARKit, it looks like your request is specific to QuickLook/RealityKit. As those lounges are closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"I am really struggling to build user interactions in RealityKit that are as \u201csmooth\u201d as AR QuickLook (in terms of a user placing, moving, rotating, or scaling a model).  I\u2019m using \u201cinstallGestures\u201d on the ARView but it\u2019s nowhere near as smooth as the AR QuickLook experience.  Is there any suggestions of how to get AR QuickLook style integrations?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#regarding-the-new-arkit-6-api-that-takes-a-4k-photo-of-the-ar-scene-is-there-a-limit-to-how-many-times-it-can-be-called-can-i-take-say-30-photos-within-a-second","text":"You can take the next photo right after the completion handler of your previous captureHighResolutionFrame call - or even from within the completion handler. If you try taking a new photo before the previous call completed, you will receive an highResolutionFrameCaptureInProgress error in the completion handler. Thank you <@U03HWCCTAKT>! Thats good to know. I ask that to see if I can use the same workflow to capture short videos as well if possible.","title":"Regarding the new ARKit 6 api that takes a 4k photo of the AR scene, is there a limit to how many times it can be called? Can I take say 30 photos within a second?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#wed-like-to-use-an-object-both-as-a-source-for-a-usdz-based-on-the-photogrammetrysession-and-as-an-arreferenceobject-so-that-we-can-overlay-information-at-the-same-position-on-both-the-real-object-and-the-created-model-is-there-any-guidance-on-how-to-align-these-coordinate-systems-eg-by-aligning-the-point-clouds-from-the-photogrammetry-session-and-reference-object-or-can-we-make-assumptions-on-the-origin-of-the-resulting-usdz-from-the-photogrammetrysession","text":"Creating a model for ObjectDetection and creating a textured mesh with ObjectCapture are two different use cases with separate workflows, we do not offer a tool to convert from one to another. That sounds like a great use case though, I encourage you to file a feature request. Feedback Assistant: https://feedbackassistant.apple.com FB10160265 :100:","title":"We'd like to use an object both as a source for a USDZ based on the PhotogrammetrySession and as an ARReferenceObject, so that we can overlay information at the same position on both the real object and the created model.  Is there any guidance on how to align these coordinate systems, e.g. by aligning the point clouds from the photogrammetry session and reference object? Or can we make assumptions on the origin of the resulting USDZ from the PhotogrammetrySession?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-maximum-number-of-2d-bodies-that-can-be-tracked-in-an-arworldtrackingconfiguration","text":"Hi Brandon! ARKit detects one body at a time. If multiple people are in the scene, the most prominent one is returned. Thank you, <@U03HWCCTAKT>!","title":"Is there a maximum number of 2D bodies that can be tracked in an ARWorldTrackingConfiguration?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-now-possible-to-do-ar-with-the-ultra-wide-angle-05-camera","text":"Unfortunately not. ARKit consumes the UW camera internally for certain processing tasks in a specific configuration. Though I encourage you to file a feature request. Feedback Assistant: https://feedbackassistant.apple.com","title":"Is it now possible to do AR with the ultra wide angle 0.5  camera?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#were-planning-to-integrate-an-ar-distance-measuring-view-into-our-app-does-arkit-now-provide-the-necessary-technology-to-achieve-this-or-is-realitykit-a-better-match-are-there-any-useful-docs-to-look-at","text":"Hi Yurii, ARKit offers several ways to measure distances. You can either evaluate distances from the device to its environment or between ARAnchors. Please see this documentation to get an overview: https://developer.apple.com/documentation/arkit/environmental_analysis It looks like your question is also tied to RealityKit, but in this lounge we are focused on questions to ARKit. As RealityKit lounge is closed for WWDC, please use the developer forums or the feedback system to get connected to an engineer from this team: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/ Thank you <@U03HB4VGW30>","title":"We're planning to integrate an AR distance measuring view into our app. Does ARKit now provide the necessary technology to achieve this, or is RealityKit a better match? Are there any useful docs to look at?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#are-there-any-good-resources-on-getting-started-with-estimated-object-dimensions-similar-to-the-measurable-app-but-to-do-height-and-width","text":"Hi Eduardo, I recommend checking out our documentation of our SceneGeometry API that we presented in ARKit 3.5. A good overview is given in this tech talk: https://developer.apple.com/videos/play/tech-talks/609 After getting a geometry that is good enough you still have to solve the task of isolating your object of choice and computing its volume. There would be several ways of doing it. For example cutting everything off above the ground level, or letting the user create a cube objects and then intersect it with the scene geometry, we do not have any code sample for these tasks, though. Thanks <@U03J7GC2A2C> Would it be possible to get the dimension of small objects? Ex. Ball, pillow, cup, etc.","title":"Are there any good resources on getting started with estimated object dimensions?  Similar to the measurable app but to do height and width."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#video-feed-is-always-overexposed-using-arkit-trying-to-enable-hdr-for-arsession-doesnt-seem-to-work-setting-videohdrallowed-to-true-on-arworldtrackingconfiguration-does-not-change-video-rendering-also-when-accessing-the-avcapturedevice-with-arworldtrackingconfigurationconfigurablecapturedeviceforprimarycamera-activeformatisvideohdrsupported-returns-false-on-iphone-12-pro-max-so-i-cannot-set-capturedeviceisvideohdrenabled-to-true-also-when-using-setexposuremodecustom-and-setting-iso-to-activeformatminiso-the-image-rendered-by-arkit-has-always-a-way-greater-exposure-than-when-running-an-avcapturesession-the-use-case-is-for-using-arkit-in-a-basketball-stadium-the-pitch-always-appears-totally-white-with-arkit-so-we-cannot-see-any-player-while-with-avcapturesession-or-just-the-ios-camera-app-the-pitch-and-players-appear-clearly-thanks-to-hdr","text":"Hi Brian! Setting videoHDRAllowed means that HDR will be enabled on the formats supporting it; however this is not the case for all video formats. In iOS 16, ARVideoFormat has a new property isVideoHDRSupported . You can filter the list of the configuration\u2019s supportedVideoFormats to find one where videoHDRSupported is true , and set this format as the configuration\u2019s videoFormat before running the session. Thank you! I'll try it out! I just tried it, and it worked! Thanks :smile: This is a great addition to ARKit! A big thank you to the ARKit team for giving us access to those video options!","title":"Video feed is always overexposed using ARKit. Trying to enable HDR for ARSession doesn't seem to work. Setting videoHDRAllowed to true on ARWorldTrackingConfiguration does not change video rendering. Also when accessing the AVCaptureDevice with ARWorldTrackingConfiguration.configurableCaptureDeviceForPrimaryCamera, activeFormat.isVideoHDRSupported returns false (on iPhone 12 Pro Max) so I cannot set captureDevice.isVideoHDREnabled to true. Also when using setExposureModeCustom and setting iso to activeFormat.minISO, the image rendered by ARKit has always a way greater exposure than when running an AVCaptureSession. The use case is for using ARKit in a Basketball stadium: the pitch always appears totally white with ARKit so we cannot see any player while with AVCaptureSession (or just the iOS camera app) the pitch and players appear clearly thanks to HDR."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#does-arkit-or-realitykit-support-rigid-body-physics-defined-in-a-usd-file","text":"Hi Alexandre, ARKit doesn\u2019t support physics but rather detects the surrounding scene to allow RealityKit to handle virtual objects. RealityKit does support rigid body physics and a good place to start looking is at the physics APIs here: https://developer.apple.com/documentation/arkit/usdz_schemas_for_ar/simulated_physical_interaction/preliminary_physicsrigidbodyapi Not supported yet by RKit, but FYI: https://graphics.pixar.com/usd/release/api/usd_physics_page_front.html and https://graphics.pixar.com/usd/release/wp_rigid_body_physics.html This request is specific to the interaction between USD files and RealityKit, today we are focusing on ARKit question in our lounge. I encourage you to file a feature request through the feedback assistant. Feedback Assistant: https://feedbackassistant.apple.com","title":"Does ARkit or RealityKit support rigid body physics defined in a USD file?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-and-thanks-for-your-work-id-like-to-ask-what-might-be-the-possible-causes-for-the-arsessiondelegate-is-retaining-arframes-console-warning-i-use-the-sessiondidupdateframe-delegate-method-to-just-check-whether-the-anchorentityplane-im-looking-for-is-in-a-sufficient-distance-from-the-camera-thanks","text":"Hi Ioannis, we have a limited pool of resources for our ARFrames and in order to keep some available for ARKit to process, we recommend processing the frames as quickly as possible. If you need to perform longer computations, you can copy an ARFrame and release the ARFrame from the delegate method. Makes sense. Thank you!","title":"Hi, and thanks for your work! I\u2019d like to ask what might be the possible causes for the ARSessionDelegate is retaining ARFrames console warning. I use the session:didUpdateframe: delegate method to just check whether the AnchorEntity(plane:) I\u2019m looking for is in a sufficient distance from the camera. Thanks."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hello-i-would-like-to-know-if-its-possible-to-use-shareplay-with-a-arkit-app-when-i-try-there-is-no-video-on-the-facetime-call-if-the-back-camera-is-started-is-it-possible-to-have-both-camera-at-the-same-time-front-for-facetime-and-back-for-my-ar-app-thanks","text":"Hi Gil! ARKit configures the cameras according to the selected configuration. Capturing from another camera while an ARKit session is running is not supported.","title":"Hello, I would like to know if it's possible to use shareplay with a ARKit app? When i try there is no video on the facetime call if the back camera is started. Is it possible to have both camera at the same time (front for facetime and back for my AR app)? Thanks"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#what-can-you-share-about-this-report-fb9184883-is-it-marked-as-a-highest-priority-issue-for-those-wondering-a-post-on-the-forum-explains-it-httpsdeveloperapplecomforumsthread705860httpsdeveloperapplecomforumsthread705860","text":"Hi Gaetan, I did check with the request you filed and pinged the engineer assigned to it. I also encourage you to attach an ARKit recording (one in portrait, one in landscape) so we can rerun and reproduce your exact results. If you\u2019re not familiar with the process, here\u2019s the link to the documentation: https://developer.apple.com/documentation/arkit/arsession/recording_and_replaying_ar_session_data?language=objc Alright, thanks <@U03J7GC2A2C> !","title":"What can you share about this report FB9184883 ? Is it marked as a highest priority issue ? For those wondering, a post on the forum explains it : https://developer.apple.com/forums/thread/705860|https://developer.apple.com/forums/thread/705860"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-native-3d-processing-tool-coming-the-iosmacos-any-time-soon-we-needed-to-implement-usdz-cropping-on-ios-and-the-3rd-party-tools-dont-deliver-the-quality-we-want","text":"Hi Yurii, In this lounge we are focused on questions to ARKit, it looks like your request is specific to USDZ and/or other tools. As the related lounge is closed for WWDC, please use the developer forums or the feedback system: Feedback Assistant: https://feedbackassistant.apple.com Developer Forums: https://developer.apple.com/forums/","title":"Is there a native 3D processing tool coming the iOS/macOS any time soon? We needed to implement USDZ cropping on iOS, and the 3rd party tools don't deliver the quality we want."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-it-possible-to-do-perspective-correction-in-arkit-using-the-captured-depth-map-like-on-the-continuity-camera-desk-view-for-ex","text":"Glad you\u2019re also a fan of the new desk view feature. There are potentially two solutions to this: You might just try to to a single perspective projections for the whole image The second solution is to use a per-pixel correction like you suggested. Both come with their own benefits and drawbacks. Please check out our documentation for implementing the second approach: https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth","title":"Is it possible to do perspective correction in ArKit using the captured depth map? like on the continuity camera \"desk view\" for ex"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-any-plan-to-allow-built-in-hands-and-finger-detection-within-arkit-to-let-the-user-interact-with-an-object-directly-with-his-hands-and-not-only-though-touch-events-on-the-device-screen","text":"Hi Brian! ARKit has no built-in hand or finger detection, but you can use Vision to track hands or detect hand poses. Here is a developer sample illustrating this: https://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision . For ARKit feature requests, we encourage you to send us a report in Feedback Assistant: https://feedbackassistant.apple.com Hi Andreas! Thank you for your reply and for these informations.","title":"Is there any plan to allow built-in hands and finger detection within ARKit to let the user interact with an object directly with his hands and not only though touch events on the device screen?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#so-what-i-meant-is-if-you-know-the-four-corners-of-that-building-you-can-create-four-location-anchors-at-those-latlon-coordinates-if-they-are-tracked-in-arkit-you-get-the-local-coordinates-and-can-build-an-occlusioncollision-mesh-hi-christian-did-you-mean-i-should-build-an-occlusion-mesh-for-every-building-i-want-occlusioncollision-i-am-kind-of-new-to-arkit-but-i-saw-that-i-can-create-a-metal-renderer-to-work-with-arkit-can-i-get-depth-information-using-a-convolutional-neural-network-from-metal-sorry-if-this-is-off-topic","text":"Hey Abraham, throwing machine learning at it sounds like a super fun project, I would recommend to start a bit simpler. So as a small experiment you can take the four corners of a building from the Maps app and then create four Location Anchors based on these coordinates. As soon as those are tracked you can look at the local coordinates (in x,y,z) and then build a polygon based on it, you can extrude it towards the sky (y up) to get a nice collision/occlusion mesh. I will try Christian. I will have the code publicly hosted on GitHub https://github.com/Ansel7/Mobius ) and probably create some kind of tutorial on this journey. I have a lab scheduled for later today, I will present this solution, and ask for help on the implementation, Thank you ARKit team","title":"\"So what I meant is if you know the four corners of that building you can create four Location Anchors at those lat/lon coordinates. If they are tracked in ARKit you get the local coordinates and can build an occlusion/collision mesh.\"  Hi Christian, did you mean I should build an occlusion mesh for every building I want occlusion/collision. I am kind of new to ARKit but I saw that I can create a metal renderer to work with ARKit. can I get depth information using a convolutional neural network from metal. sorry if this is off topic."},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#using-additional-cameras-in-arkit-are-there-any-resources-to-show-how-this-iss-setup","text":"Hi Dan! ARKit allows streaming video from only one camera at a time. Which camera is used is determined by your configuration (e.g. ARFaceTrackingConfiguration will use the front facing camera, ARWorldTrackingConfiguration will use the back wide camera). You can, however, enable face anchors detected by the front camera in an ARWorldTrackingConfiguration . Vice versa, you can enable isWorldTrackingEnabled in an ARFaceTrackingConfiguration to benefit from 6DOF world tracking. You can learn more about these APIs here: https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3223422-userfacetrackingenabled https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/3175413-isworldtrackingenabled . Or check out this developer sample: https://developer.apple.com/documentation/arkit/content_anchors/combining_user_face-tracking_and_world_tracking Am I able to capture frames off additional feeds at the same time (not necessarily exactly synchronous). Eg main at session happening on rear facing camera, additional video frames captured occasionally from front facing. Maybe I've been fooled, see an ARKit 6 breakdown that says \u201cSimultaneous front and back cameras with AR\u201d - is this real? We introduced new API to capture frames in higher resolution than your configuration\u2019s video format: https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframe . Those frames are captured from the same camera. Setting up additional cameras is not supported. We encourage you to file a feature request in Feedback Assistant: http://feedbackassistant.apple.com/ The sentence you quoted refers to the API I mentioned above. Not all of the features listed in the lower part on https://developer.apple.com/augmented-reality/arkit/ are new additions in ARKit 6; this is meant more as highlighting ARKit\u2019s capabilities in general. Ok\u2026 that's ancient news. Thanks for finding the source of my misconception! Great work guys!","title":"Using additional cameras in ARKit - are there any resources to show how this iss setup?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#hi-were-working-on-an-ar-experience-that-allows-user-to-put-ar-objects-in-their-surroundings-and-replay-it-later-were-saving-the-data-on-an-arworldmap-and-archive-it-on-the-filesystem-to-be-retrieved-later-everything-works-great-on-smaller-areas-with-small-arworldmap-file-sizes-however-as-user-adds-more-stuff-the-arworldmap-file-gets-bigger-and-at-some-point-it-takes-so-long-or-even-impossible-to-relocalize-using-the-big-arworldmap-files-im-seeing-slower-relocalization-on-arworldmap-files-with-10-mb-size-question-is-there-like-a-known-cap-of-how-big-arworldmap-files-can-be-to-retain-effectivenes-of-relocalization-and-the-ar-experience-what-can-impact-performance-for-ar-relocalization-other-than-lighting-condition-and-the-object-textures-that-were-rendering-maybe-area-size-camera-motion-features-in-the-area-since-were-seeing-frame-drops-on-bigger-arworldmap-files-thanks","text":"ARWorldMaps are optimized for room sized scenarios. If you exceed that limit then relocalization will stop working in certain areas as the map is not big enough any more to cover the whole area. The frame drops sound related to the amount of content being displayed though. For that, feel free to provide more details through Feedback Assistant: https://feedbackassistant.apple.com","title":"Hi! We're working on an AR experience that allows user to put AR objects in their surroundings and replay it later. We're saving the data on an ARWorldMap and archive it on the filesystem to be retrieved later. Everything works great on smaller areas with small ARWorldMap file sizes. However as user adds more stuff, the ARWorldMap file gets bigger and at some point, it takes so long or even impossible to relocalize using the big ARWorldMap files. I'm seeing slower relocalization on ARWorldMap files with 10 mb size.   Question: Is there like a known cap of how big ARWorldMap files can be to retain effectivenes of relocalization and the AR experience?  What can impact performance for AR relocalization other than lighting condition and the object textures that we're rendering (maybe area size? camera motion? features in the area?) since we're seeing frame drops on bigger ARWorldMap files.  Thanks!"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#whats-the-current-upper-limit-on-sknode-objects-say-you-wanted-to-create-several-rows-of-image-spritesemoji-for-eg-a-virtual-interactive-bookshelf-layout-also-if-you-wanted-to-animate-these-sprites-would-that-be-feasible","text":"Hi <@U03J97X79CL>, In this lounge we are focused on questions to ARKit, it seems like your request is specific to SpriteKit. You might be able to find some answers in the SpriteKit performance documentation: https://developer.apple.com/documentation/spritekit/nodes_for_scene_building/maximizing_node_drawing_performance Otherwise, please use the developer forums or the feedback system to get connected to an engineer from SpriteKit.","title":"What\u2019s the current upper limit on SKNode objects? Say you wanted to create several rows of image sprites/emoji for eg a virtual interactive bookshelf layout.  Also if you wanted to animate these sprites would that be feasible?"},{"location":"wwdc22/arkit-realitykit-usdz-lounge.html#is-there-a-way-to-force-arworldmap-to-relocalize-on-our-position-instead-of-inferring-from-the-features-around-us-for-example-since-arworldmap-has-its-own-root-anchor-can-we-do-something-like-load-this-arworldmap-using-my-current-positiontransform-in-the-real-world-as-the-root-anchor-from-my-understanding-we-can-do-this-with-a-singlemultiple-arobjects-but-havent-found-any-resources-about-collections-of-aranchors-in-an-arworldmap","text":"This is not supported out of the box. What you could do is compute the offset between your current location (before relocalization) and after relocalization and apply that accordingly.","title":"Is there a way to force ARWorldMap to relocalize on our position instead of inferring from the features around us? For example, since ARWorldMap has its own root anchor, can we do something like \"load this ARWorldMap using my current position/transform in the real world as the root anchor\"?  From my understanding we can do this with a single/multiple ARObjects but haven't found any resources about collections of ARAnchors in an ARWorldMap"},{"location":"wwdc22/audio-and-video-lounge.html","text":"audio-and-video-lounge QAs by FeeTiki When would we be able to see an option for playback speed/pitch in Apple Music? This is something I'd love to see. Hello <@U03JNAGPDS6>! You can already change the playback speed with MusicKit by setting the https://developer.apple.com/documentation/musickit/musicplayer/state-swift.class/playbackrate|playbackRate property on the https://developer.apple.com/documentation/musickit/musicplayer|MusicPlayer \u2019s https://developer.apple.com/documentation/musickit/musicplayer/state-swift.class|state . As for changing pitch, we don\u2019t currently support this, but feel free to file an enhancement request in https://feedbackassistant.apple.com|Feedback Assistant . How is the Music API related to the Podcast API? Hey <@U03K8HA5U2U>! The Apple Music API is a separate product from any other metadata APIs. We currently don\u2019t have a dedicated Podcasts API, but if you need podcast metadata you can leverage the pre-existing iTunes Search API - https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html Hello! Thanks for your amazing work, to begin with. I wanted to ask about songs lyrics, there's a hasLyrics boolean variable inside the Song object, but I see no way to access those lyrics, am I missing something? I'd need to have timed lyrics for an app idea I have. Thank you! Hi Cristina, unfortunately there's no current way to access the lyrics of a Song object in either MusicKit or Apple Music API. Please consider filing an enhancement request in https://feedbackassistant.apple.com|Feedback Assistant for a request for this feature! Thank you for all the new structures in MusicKit this year!! All my wishlist items ticked. :white_check_mark: I wonder how to work with the MusicCatalogTopLevelResourceRequesting protocol and the new init() of MusicCatalogResourceRequest? Hello <@U03HMDM1J23>! I\u2019m afraid we made a small mistake in seed 1, and forgot to include that https://developer.apple.com/documentation/musickit/genre|Genre conforms to https://developer.apple.com/documentation/musickit/musiccatalogtoplevelresourcerequesting|MusicCatalogTopLevelResourceRequesting . This will be addressed in an upcoming seed. Sure, no worries! I assume that it will help me fetch all the top genres from this https://api.music.apple.com/v1/catalog/us/genres|Apple Music API ? That\u2019s exactly right. We think we need this to tie into the new https://developer.apple.com/documentation/musickit/musiccatalogchartsrequest|MusicCatalogChartsRequest , which can be https://developer.apple.com/documentation/musickit/musiccatalogchartsrequest/init(genre:kinds:types:)|initialized for a specific Genre (optionally). What are some great tips on creating immersive audio experiences in our apps? I really like Spatial Audio and Spatialise Stereo. Hi Cristian-Mihai, Currently https://developer.apple.com/documentation/musickit/audiovariant|MusicKit and https://developer.apple.com/documentation/applemusicapi/albums/attributes|Apple Music API expose which audio variants are available for a given music item like a Song or Album . You can also see the currently playing audio variant when using the MusicKit player. As for creating your own immersive audio experience, I suggest signing up for the CoreAudio lab or PHASE Audio engine lab. Thank you very much! Is it possible to determine the location (country, etc.) of origin of an album, track, or artist? Hey Mark - we don\u2019t expose this metadata via the Apple Music API at this time. But feel free to file an enhancement request in https://feedbackassistant.apple.com/|Feedback Assistant . I didn't see anything about Apple Classical / Primephonic so far. Will MusicKit and Apple Music API support it? Hi <@U03JEKFMT4Z>, thank you for your patience. We are working on an amazing new classical music experience from Apple. Thank you Betim. So I guess there was no official news I missed for WWDC That\u2019s correct, we currently don\u2019t have any updates on this yet Like the new recently played structure in MusicKit, is there a way to access the recently added items in the user's library? Hi Rudrank, Unfortunately, there currently isn't a way to access the recently added items in the user's library like the recently played structure. However, you could get this information from https://developer.apple.com/documentation/applemusicapi/get_recently_added_resources|Apple Music API . Please consider filing an enhancement request in https://feedbackassistant.apple.com|Feedback Assistant for a request for this feature! Like the new recently played structure in MusicKit, is there a way to access the heavy rotation content (like the one mentioned in Apple Music API) from the user's library? If not, do you recommend using MusicDataRequest for the URL https://api.music.apple.com/v1/me/history/heavy-rotation,|https://api.music.apple.com/v1/me/history/heavy-rotation, and decoding it as a music item collection of RecentlyPlayedMusicItem? Hi Rudrank, Unfortunately, there currently isn't a way to access heavy rotation content from the user's library. Using MusicDataRequest to fetch this would work but decoding the items as RecentlyPlayedMusicItem is not recommended. For recommendations based on listening, we encourage exploring https://developer.apple.com/documentation/musickit/musicpersonalrecommendationsrequest|MusicPersonalRecommendationsRequest . Does Spatial Audio only work with MusicKit? Or does it also work with MediaPlayer as well? Hi Tyson, Spatial Audio playback will be handled automatically, however, there is no metadata from MediaPlayer that would let you know which https://developer.apple.com/documentation/musickit/audiovariant|Audio Variant is either available or currently playing. Thank you! Is the ISRC given by the Music API reliable to identify a song outside of the Music API? In the past, I've found that there are differences between Apple Music ISRC and other platforms ISRC. The other platforms seem to be correct, but I thought ISRC is supposed to be universal. Hey <@U03JE7H2DM4>! Yes, the ISRC value is intended to be a way to identify songs outside of Apple Music. If you have examples of ISRC values not matching what you expected, please consider sharing them with us using the Feedback Assistant. Will there be an option to be able to playback Apple Music directly via AVPlayer? Hello <@U03JE7H2DM4>! Apple Music content is typically protected, and requires specialized playback logic. That\u2019s why it\u2019s not possible to play Apple Music content with AVPlayer. Instead, you should use one of https://developer.apple.com/documentation/musickit|MusicKit \u2019s players, like: \u2022 https://developer.apple.com/documentation/musickit/applicationmusicplayer|ApplicationMusicPlayer \u2022 https://developer.apple.com/documentation/musickit/systemmusicplayer|SystemMusicPlayer Thanks! Will those players ever be able to play audio directly from a url like AVPlayer? Those players do not currently allow playing audio directly from a URL like AVPlayer . What use-case do you have for that? And more generally, what are you trying to achieve? Essentially, a single player that can play the protected Apple Music songs and freely available content (url) in the same queue. I know it's possible using a mix of AVPlayer and System/ApplicationMusicPlayer but it would be really cool if we could do it in a single player. Ok, I see, thanks <@U03JE7H2DM4>. I think the best way forward would be to capture this information about your use-case in a new ticket on https://feedbackassistant.apple.com|Feedback Assistant . This will help us in our planning process for future enhancements to MusicKit. Thank you! Apple Music API - How to work with Catalog Activity? https://developer.apple.com/documentation/applemusicapi/get_a_catalog_activity|https://developer.apple.com/documentation/applemusicapi/get_a_catalog_activity The given example throws \"Resource with requested id was not found\" error, and I am not sure where to find the activity to understand it Hi <@U03HMDM1J23>, Activities could sometimes be found using the search feature. Many activities have been converted into https://developer.apple.com/documentation/applemusicapi/get_a_catalog_apple_curator|Apple Curators and can also be found in search or the related curator for a playlist. This year's enhancements to MusicKit for Apple platforms have also made it easier to work with curators. You can watch the session on those updates <https://developer.apple.com/videos/play/wwdc2022/110347/ |here>. Thank you for reporting the sample ID in the documentation is not working, that will need to be updated. Is this the appropriate place to ask about the new control center on the lock screen? They showed an audio example in the keynote which included fullscreen album art. But what if you have video running? Would you actually see the video on the lock screen? Hello <@U03J20WEYV8>! In the new lock screen in iOS 16, when a video is playing, the lock screen media controls will only show a still image provided by the app. To do this, the app needs to publish its https://developer.apple.com/documentation/mediaplayer/mpnowplayinginfocenter/1615903-nowplayinginfo|nowPlayingInfo dictionary through https://developer.apple.com/documentation/mediaplayer/mpnowplayinginfocenter|MPNowPlayingInfoCenter including the image with the key https://developer.apple.com/documentation/mediaplayer/mpmediaitempropertyartwork|MPMediaItemPropertyArtwork . One more thing <@U03J20WEYV8>, we\u2019ll have more information on a new way to publish the now playing information in a new session titled https://developer.apple.com/wwdc22/110338|Explore media metadata publishing and playback interactions , which will be released on Friday. Like MusicLibraryRequest, does MusicLibrarySearchRequest not do a network call, and searches from the local copy stored on device? Hi Rudrank, Yes, like the MusicLibraryRequest on iOS, MusicLibrarySearchRequest doesn't make a network call for searching but instead searches your local library. One wish (I will file feedback for it), the possibility of using MusicKit in the simulator? Hi <@U03HMDM1J23>! Getting MusicKit to work in the simulator is very challenging on a technical level, but we feel the pain of the lack of this support just like you do. We keep investigating ways we could enable using some aspects of MusicKit in the simulator, and we\u2019ll make sure to communicate around it if and when we have any specific updates. I'm working on migrating form MediaPlayer to MusicKit. Does MusicKit work to play back iTunes content even if the user does not have an AppleMusic subscription? Hi Tyson, Great to hear that you're migrating! If you were using MediaPlayer to play back iTunes content without a subscription, you can accomplish the same thing using MusicKit in iOS 16! Great, thank you! Now that we have artwork for artist (YAY), do we assume that licensing issues have been sorted? https://developer.apple.com/forums/thread/688012|https://developer.apple.com/forums/thread/688012 Hi Rudrank! Yes, we have worked hard with business to make sure it is compliant with licensing agreement for use in your app. For more details please check out Meet Apple Music API and MusicKit - WWDC22 https://developer.apple.com/wwdc22/10148 . Yes! Been wanting that for a long time! Hi Rudrank, Tyson and Everyone, Please reference https://developer.apple.com/app-store/review/guidelines/ section 4.5.2 for guidance of how artist images can be used in your app. `(ii) Using the MusicKit APIs is not a replacement for securing the licenses you might need for a deeper or more complex music integration. For example, if you want your app to play a specific song at a particular moment, or to create audio or video files that can be shared to social media, you'll need to contact rights-holders directly to get their permission (e.g. synchronization or adaptation rights) and assets. Cover art and other metadata may only be used in connection with music playback or playlists (including App Store screenshots displaying your app's functionality), and should not be used in any marketing or advertising without getting specific authorization from rights-holders. Make sure to follow the Apple Music Identity Guidelines when integrating Apple Music services in your app. ` Got it, thanks! Right, that\u2019s great because we used to have run all our AppStore screenshots though photoshop to change all the album artwork to public domain images. So we really appreciate this! Thank you! My app uses AVAudioEngine, and I am very disappointed that the \"TapOnBus()\" command has such a slow responsiveness. At an audio sample rate of 44,100 fps, it can not respond any faster than giving me 4,400 samples every 0.1 seconds. Is there any way to speed this up? Hi Keith, Thanks for the question! I think your question is best suited for a scheduled Core Audio Lab . <@U03J1SQJYRW> Could you please help me select a proper lab for my question? I just visited the Core Audio lab, but unfortunately, I feel like it wasn\u2019t a proper lab to address technical issues with AVAudioRecord . My question relates to this Apple\u2019s Q&A - https://developer.apple.com/library/archive/qa/qa1872/_index.html For iOS MultiCam capture, Apple provides AVCaptureMultiCamSession to help with capturing from multiple inputs and feeding to multiple outputs. For Mac, what is the best way to achieve the same result. Running multiple AVCaptureSessions is definitely not the way I want to go, do I have to figure out a way of adding multiple inputs and mapping them to multiple outputs as I will be using AVCaptureVideoDataOutputDelegate and is that okay to do Hello! Thank you for the question. This would be best suited for tomorrow\u2019s Q&A: AVFoundation (between 1pm and 3pm PT). Does SharePlay work inside Messages, specifically inside an iMessage App Extension? We do not support SharePlay in extensions (IE: iMessage App Extensions), but we love hearing the feedback so please do file the feedback! Is there existing API to share a URL of a document in iCloud Drive and for the invitees to be automatically given shared access to it for the duration of the SharePlay session, with iCloud dealing with keeping everyone up to date? If not, how do apps like Pages do this? Ugh sorry wrong lounge :man-facepalming: No worries! I can still answer this a little bit, we do not currently have any API for this in SharePlay, but one idea you could do is add the share URL to your GroupActivity ! You can then use the share URL to gain access on the other devices by accepting the share. Alternatively, we definitely encourage you to check out the new collaboration features in Messages offered this year! This way, people on a FaceTime call can easily get the option to share the CKShare with the group they\u2019re FaceTime\u2019ing with from the share sheet! Thanks. Not sure how that integrates with iCloud documents (wasn't so much the wrong place given SharePlay, just feels odd in the AV lounge!) It seems there is no recommended solution for doing such collab with iCloud documents which is puzzling This is the relevant API for the latter (people on FaceTime getting the option to collaborate with the group) - https://developer.apple.com/documentation/foundation/nsitemprovider/3951995-registerckshare . You\u2019ll want to register the CKShare on the item provider you pass to the Share Sheet. You\u2019re in the right place for all things SharePlay :slightly_smiling_face: not just limited to audio/video! Thanks SharePlay: Has the SharePlay message size been extended? or publicly documented? The message payload for the GroupSessionMessenger has been increased to 256KB! Feel free to checkout our \"What's new in SharePlay\" talk where we talk about that! https://developer.apple.com/videos/play/wwdc2022/10140/ :+1: SharePlay related: FB9991061 GroupActivitySharingController is unavailable in Mac Catalyst despite the documentation: https://developer.apple.com/documentation/groupactivities/groupactivitysharingcontroller|https://developer.apple.com/documentation/groupactivities/groupactivitysharingcontroller Thank you for bringing this to our attention. We will look into this. Hello all, hope everyone is doing well, i would like anyone's input on streaming audio through MQTT messages. I have created a working class that uses AudioQueue to grab packets of a certain size and listen to these bytes after applying an algorithm to it. I've done this for talking and listening using AudioQueue only. However, sometimes there could be some instability when listening and talking. Does anyone have any improvements or things i can look out for when handling live streamed audio data with AudioQueue? Or is this the right service for streaming audio? Just for context, I've tried AVAudioEngine but a few issues arose when doing so. Thanks! Hi <@U03J9T1R5M4> , Thanks for your question, we don\u2019t have someone from the CoreAudio team here today, but would love to go in to more detail with you in our Core Audio Lab. Please sign-up using the Developer App or at http://developer.apple.com|developer.apple.com Hey Gavin, thank you would love to discuss more. SharePlay related: Are there any updates to user management such as linking SharePlay and FaceTime participants? Apps can associate GroupSession participants with accounts in your app, but the framework does not provide access to the FaceTime participant information directly. Thank you I have a SharePlay game on the App Store. In the game you answer questions out loud. Starting with iOS 16 my game is no longer able to access the microphone during a FaceTime call. Is this an expected change? Hi <@U03J7G8UX50>! Thanks for bringing this to our attention! If you file a feedback assistant we\u2019ll be happy to dig into this and figure out what\u2019s going on. Great, thanks I've have Adam on 2X to see the SharePlay updates: so appologies: With the new Messages and SharePlay integration, does FaceTime remain as the transport layer for the stable connection and Messages therefore is a new way/UI to initiate a SharePlay/FaceTime session? 2X speed Adam is always fun! SharePlay in Messages provides the same guarantees you're used to as a developer! You'll still be leveraging the Apple infrastructure for messaging through the GroupSessionMessenger with end-to-end encryption, regardless of whether SharePlay was initiated from FaceTime or Messages! > just to clarify... our app is a presentation manager enabling file sharing and group editing (aka Multiuser Quick Look) and also supporting a realtime SharePlay canvas for drawing... so we support deep FaceTime programmatic access beyond the 15.4 GroupActivitySharingController... so we currently associate file editing with who is FaceTime/SharePlay/iCloud user... so just to be clear... is FaceTime the transport for SharePlay or do we need to also manager Messages identity? ... because Messages and FaceTime can be different email/phone numbers Hi Zaid, Your implementation of SharePlay is really interesting, we would love to dive in to the details with you in a 1-1 lab. Could you please sign up to a SharePlay 1-1 lab using the Developer App, or at http://developer.apple.com|developer.apple.com I shall and much thanks it seems that I already submitted a request for: June 8, 2022, between 8:00 p.m. and 10:00 p.m. EDT (UTC-04:00) When an arbitrary UIView is overlaid on top of an AVPlayer containing HDR content (think subtitles, mute toggle, etc.), the overlay appears much dimmer and can be hard to read. Is there a way to disable this behavior and render the overlay with the same brightness? > Hi Chris, > Subtitles and closed caption playback using AVPlayerLayer adapt to the corresponding HDR video. If you are seeing cases where this is a concern, please file use Feedback Assistant to describe the situation you are seeing. A sample app and especially a pointer to the asset will be most helpful. Please include the platform(s) you are using. > If you would like to discuss in more detail we have an AVFoundation playback lab tomorrow. Thanks! Please see FB9875960 -- this is on iOS What's the best way of determining audio output latency, esp with wireless devices like Airpods? My app synchronizes on-screen graphics (not video) with sound, and getting accurate latency info with Airpods has been challenging. Hi John. Thanks for your question. This is an interesting problem. 1. It is difficult to get correct latency values for AirPods right now. Would you please file an enhancement request for this ( https://developer.apple.com/bug-reporting/ )? 2. Can you elaborate how you are playing audio in your app? Is it by any chance using AVPlayer? Hi Moritz, thanks! I filed FB8961041 about 6 months ago. There's more detail and a sample project there if you want to check it out. So far I've been looking at outputLatency and ioBufferDuration from AVAudioSession but they only add up to around 20ms whereas the actual latency is somewhere around 110ms. I have read (I think from Jeff Moore) that some or all of the remainder comes from \"safety offsets\" but I don't see that those are exposed anywhere. Basically this has been holding up a couple of music-related apps that I'm working. I simplified the problem space somewhat for the sake of brevity, but input latency plays into it too. I will note that standard videos on iOS sync to Airpods sound quite reliably so I know this is doable :slightly_smiling_face: Thank you very much for the FB number. Note that AVFoundation playback objects like AVPlayer or AVSampleBufferAudioRenderer handle latency for you. So, if you can use one of those for rendering your audio, you can rely on the AVPlayerItem timebase or AVSampleBufferRenderSynchronizer currentTime to synchronize your graphics. Hi! I have a fairly technical question that might have been a better fit for a lab, but I missed the deadline on Monday to apply for it. :pensive: I am using a custom Core Image compositing pipeline (via the AVVideoCompositing protocol) that is using CIFilters for adjusting a foreground and background video and combine them via alpha blending/masking. The custom compositor has some parameters stored in an instance conforming to the AVVideoCompositionInstructionProtocol. The goal is for the user to interactively adjust these parameters, and have the composited video (played back continuously) reflect these changes with as little latency as possible. In my current implementation there is a significant lag between slider adjustments and the video image responding. Presumably this is because the video player has already processed and buffered frames using the previous parameter values. I am hoping to find an approach that would allow smooth and responsive adjustments of the CIFilter parameter(s) while the video is playing. Maybe there is some instruction that would flush the player's cache (without interrupting playback)? Amongst other things I tried setting the AVPlayerItem\u2019s preferredForwardBufferDuration to a very low value (such as 0.01), but this seemed to have no effect on the responsiveness of adjustments. Here is a simplified example project that only has a single slider for controlling foreground exposure : https://www.dropbox.com/s/bk3srki6ys455sr/VideoCompositing.zip?dl=1|https://www.dropbox.com/s/bk3srki6ys455sr/VideoCompositing.zip?dl=1 G'day, Hendrik. Thanks for your question. It's cool to hear about your advanced use of AVVideoCompositing. I want to point out that it isn't too late yet to request a timeslot in the Thursday AVFoundation Playback lab. Regarding your request to reduce the latency for rendering during user interaction: would you please file an enhancement request via the feedback assistant, and include the sample app that you have already built? Thank you. I didn\u2019t consider the Playback lab, but actually my question fits into that pretty well, so I will apply for a lab appointment tomorrow. I have a custom document format that embeds video files. Is there a way to get an AVPlayer to play that data without first writing the video data out to a tmp file so that I have a file URL to pass to AVPlayer? G'day, Daniel. Check out AVAssetResourceLoader. Through it, you can provide a delegate that will be called to interpret a custom URL scheme and deliver data that you will read from your custom document format. There is a new API property in iOS 16 and macOS 13 Ventura called entireLengthAvailableOnDemand which optimizes the case where the data is already available locally, as in your app. Can I disable the control center option to spatialize audio if the user is using AirPods? Setting AVPlayerItem.allowedAudioSpatializationFormats to .none doesn't seem to do the trick. Hi Martin, can you describe your use case a little more? Why would you like to disable the Control Center option? If this capability is important to your app, we'd love to hear more. Could you also file a request ( https://developer.apple.com/bug-reporting/ ) and describe your use case? Is there a way to get presentation and decoding data from AVPlayer such as the PTS (presentation time stamp), DTS (display time stamp), or PCR (program clock reference)? Hi Neil. In AVPlayerItemOutput.h, there is some sample code above the declaration of AVPlayerItemVideoOutput0 . Using AVPlayerItemVideoOutput 's copyPixelBufferForItemTime:itemTimeForDisplay: , you you should be able to grab what I believe comes out as the PTS. If you want to work more on this with us, you could come back and ask us this question to either the AVFoundation Playback lab on Thursday or the HTTP Live Streaming lab on Friday :slightly_smiling_face: Thanks. I'm still trying to find the code you're talking about (had to create an obj-c project to view the h file). I think I found it. Thanks Would these calls have any impact on the performance of the video playback? Our interest is in sussing out the source of an audio/video desynchronisation issue we're having. The only way to know for sure is to try it out and profile, but I would not think that using AVPlayerItemVideoOutput would have a major impact on playback performance. If you can determine that your content is not at fault, could you Feedback Assistant report? At this point we're not really sure. We have a feedback ticket open, but we have not come up with a cause yet. There is a desire from some of the people on our team to know the PTS etc. In our case, we have AAC-LC that throws decoding errors and drifting out of sync only when DRM (fairplay) is enabled. If the stream is clear, everything is fine. If we switch to HE-AAC, we're not seeing these issues and it's not clear to me why this would be or why DRM would make it happen/worse Could you share the FB ID of your feedback ticket? I should note that AVPlayerItemVideoOutput will not work with FairPlay. FB9995441 Ah ok, good to know In case we run out of time here, I highly encourage you to come to the HTTP Live Streaming Lab on Friday to discuss this further. I have submitted a request for that lab. ...and the AVFoundation playback lab Excellent. We have multiple pipelines...one for live content, and one for vod content....we only experience this on live content, but the pipelines are different vendors etc...so I would think it's some kind of configuration issue...but the whole DRM vs Clear throws that into doubt Did you make any progress on debugging the audio decode errors? I have not, no. It was suggested that it was an encoding issue, but I don't see why it would be ok when playing clear streams. Also, my understanding is that HE-AAC is just a more feature rich version of AAC-LC, so I don't know why that change would make a difference Perhaps there is an issue with the audio encoder that is only hit with the configuration of AAC-LC+FairPlay? We are definitively seeing bad data fed into our audio decoder, but I cannot tell you why. And so is the audio data the same whether or not DRM is applied, or are they structurally different? My assumption was just that DRM would need to be decrypted first and then it's the same data as clear Do you have two audio streams of what you consider to be byte-for-byte identical, one clear-text and one encrypted? Identical aside from their encryption, that is. In our lab we have 2 channels side by side...one clear and one Fairplay. I believe i noted the exact URLs in the FB but they should be identical in every way except drm I'm taking a look at the media segments. But I suspect we'll run out of time. Thanks...I appreciate any info we discover in the time allotted... The HTTP Live Streaming lab would be the best venue to continue this conversation :slightly_smiling_face: The initial question I have on my request for that one is different than what we're discussing today. Will that matter? This particular issue I have requested for the playback lab If possible, ask this question again in a new question, so we can appropriately prepare. ok, sounds good...I'll create a new request thanks Thanks! Have a nice rest of your day. It doesn't seem to let me request more than one appointment? do i cancel and create new? Yeah, that will work. ok, thanks How can we use multiple AVPlayerLayer's with one AVPlayer? This leads to frozen video in simulators. See: https://developer.apple.com/forums/thread/688766|https://developer.apple.com/forums/thread/688766 Hello Kai, it looks like you've submitted FB9566529. Thanks for the report. We are investigating this, but rest assured that multiple AVPlayerLayers connected to a single AVPlayer is a supported use case. It works on devices in production, but it's limiting our simulator productivity for debugging so any fixes or workarounds would be very helpful My iPad app has a camera view (streaming / recording). Is it possible to use the camera while in multitasking mode / stage manager? I'd like to adopt all the new apis, but currently I'm stuck requiring my app to stay in full screen. Yes. iOS 16 adds new Camera multitasking APIs that allow developers to use the camera while multitasking. Please refer to the session: https://developer.apple.com/videos/play/wwdc2022/110429/ And the Developer Publications Article: https://developer.apple.com/documentation/avkit/accessing_the_camera_while_multitasking?language=objc If you have additional questions, please sign up for the Camera Capture Lab on Friday 9am to noon PDT. WOW Thank you! You're very welcome. Have a great rest of WWDC! Are there any plans for native integration of CMCD in AVFoundation.... Good Afternoon! We would love to hear more information about your app and use case of CMCD, if you haven\u2019t already could you please file an enhancement request at https://developer.apple.com/bug-reporting/ This is about AVCaptureDevice. If the device is a telephoto camera, I suggest a direct way to query it (or query its active format) as to its lens power. That is, does it have 2.0x optical power, or 2.5x or 3.0x? This would be useful for any camera app where the user manually selects the lens to use (no virtual device). Thanks! Hi Eric, Thanks for your question, unfortunately we don\u2019t have someone from the AVCaptureDevice team with us today. But we have a Q&A: Camera Capture digital lounge tomorrow or a Camera Capture lab on Friday. How do you turn an array of Float s (representing audio) into an AVAudioPCMBuffer (to pass to AVAudioFile 's write(to:) method)? This seems to mangle the output: var floats: [Float] = ... // this comes from somewhere else try withUnsafeMutablePointer(to: &amp;floats) { bytes in let audioBuffer = AudioBuffer(mNumberChannels: 1, mDataByteSize: UInt32(bytes.pointee.count * MemoryLayout&lt;Float.size), mData: bytes) var bufferList = AudioBufferList(mNumberBuffers: 1, mBuffers: audioBuffer) let outputAudioBuffer = AVAudioPCMBuffer(pcmFormat: buffer.format, bufferListNoCopy: &amp;bufferList)! try self.renderedAudioFile?.write(from: outputAudioBuffer) } ... and the version without the withUnsafeMutablePointer { gives me a warning ( Cannot use inout expression here; argument 'mData' must be a pointer that outlives the call to 'init(mNumberChannels:mDataByteSize:mData:)' ) when you try and dereference the float array directly like: var floats: [Float] = ... // this comes from somewhere else let audioBuffer = AudioBuffer(mNumberChannels: 1, mDataByteSize: UInt32(floats.count * MemoryLayout&lt;Float.size), mData: &amp;floats) // ... Hi Robert, We would love to chat to you in more detail 1-1 on this. Please book an appointment in the Core Audio lab on Thursday 10am Pacific or Friday 2pm Pacific I\u2019d love to, but I\u2019m in CET+1 and the Thursday ones seem to be booked out and the Friday ones are 2am for me. I\u2019ve replicated the question in the dev forums: https://developer.apple.com/forums/thread/707690 I am trying to compress video files before uploading to a server. I am using an AVAssetExportSession to do this, however, i have not been able to get it to work. After running, the session's status is always \"failed\" and I get a very generic error with the ID 17508. Any insight as to what might be going wrong? This is the error: Asset Writer Error: Error Domain=AVFoundationErrorDomain Code=-11800 \"The operation could not be completed\" UserInfo={NSLocalizedFailureReason=An unknown error occurred (-17508), NSLocalizedDescription=The operation could not be completed, NSUnderlyingError=0x280314d80 {Error Domain=NSOSStatusErrorDomain Code=-17508 \"(null) G'day, Mitchell. I am curious about the destination URL that you're providing to AVAssetExportSession. It looks to me like this error indicates that our frameworks were unable to access that URL. Can you check that it's a URL that your app has access to -- for example, if you need a temporary filename, are you using NSTemporaryDirectory() to generate it? Yes! So I spent about three hours yesterday messing with this as with research thought this was the error too. I have tried every combination of different URLs and all had the same problem. I will paste what i\u2019ve tried below. However, to test if this was the problem, I tried copying the data I already had to one of these url\u2019s and reading it back and it worked. So I thought that the problem was not the output URL that I was writing too. Here is everything I tried: // let compressedURL = NSURL.fileURL(withPath: NSTemporaryDirectory() + NSUUID().uuidString + \u201c.mov\u201d) // let compressedURL = URL(string: NSTemporaryDirectory())?.appendingPathComponent(\u201cLibrary/Caches/compressedVideo101.mov\u201d) // let exportPath = NSTemporaryDirectory().appendingFormat(\u201cvideo.mov\u201d) // let compressedURL = URL(fileURLWithPath: exportPath) // let documentDirectoryURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first! as NSURL // let compressedURL = documentDirectoryURL.appendingPathComponent(\u201ccompressionVideos/compressedvideo2.mov\u201d)! as URL // let docPath: String = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true)[0]; // let compressedURL: URL = URL(fileURLWithPath: docPath).appendingPathComponent(\u201cvideo_output.mov\u201d); // let temporaryDirectoryUrl: URL = FileManager.default.temporaryDirectory // let temporaryFilename = ProcessInfo().globallyUniqueString // let compressedURL = temporaryDirectoryUrl.appendingPathComponent(\u201c(temporaryFilename).mov\u201d) *let* documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first *let* compressedURL = documentsPath!.appendingPathComponent(\"compressedvideo4\", isDirectory: *false*).appendingPathExtension(\"mov\") The last one, the uncommented one, is the one I am currently using because i was able to successfully copy the data to it and read it back properly which makes me think assume it works. Can you give an example of the URLs that these are generating for you? Could there possibly be a permissions problem with my app that is preventing the export session from writing to it? And yes! I\u2019ll provide a URL. Is the app ID generated in the URL sensitive information that I shouldn\u2019t provide to the somewhat public setting of the WWDC lounge? Need to keep certain aspects of the app confidential. Feel free to XXXX out whole words of the URL that might leak anything. Great! Here are two different URLs, one for the temporary directory and one for the documents directory. This is for the tmp directory and was created using this code: let compressedURL = NSURL.fileURL(withPath: NSTemporaryDirectory() + NSUUID().uuidString + \".mov\") sorry one moment send pressed too early For the tmp directory created using the code above: file:///private/var/mobile/Containers/Data/Application/XXXXXXXX/tmp/XXXXXXX.mov This is for the docs directory created using this code: *let* documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first *let* compressedURL = documentsPath!.appendingPathComponent(\"compressedvideo4\", isDirectory: *false*).appendingPathExtension(\"mov\") URL: file:///var/mobile/Containers/Data/Application/XXXXXXXXXXXX/Documents/compressedvideo4.mov Is that the information you wanted to see? Still producing this error If I understand correctly, you have a bunch of attempts to generate a temporary URL which aren't working, but one last one that does work, but seems like not the right place to put the temporary file? Sorry, it looks like I misread you. The last one doesn't work but you were able to confirm that your app can write there <@U03HRD4PZLN> I know I can write to the last one because of the copy test, but all of them produce the error. Whether a temporary URL or not, all of them produce the error message I pasted above Yes exactly! OK. A puzzle! I have not yet done a test with the temporary URLs but I can if that helps, I assume most of them would work also Maybe the destination isn't the problematic variable, then. What can you tell about the source asset? The asset is a video, taken from the user\u2019s photo library using PHPickerViewController which writes it to a URL on the system which I know I am able to read from because I am able to get raw data from it. The video is about 11 seconds long and about 20MB URL of the source: file:///private/var/mobile/Containers/Data/Application/XXXXXXX/tmp/.com.apple.Foundation.NSItemProvider.lobgL0/IMG_0061.mov We seem to be stumped too -- and we are running out of time in this lounge. Can I ask for you to file a feedback assistant report including a sysdiagnose, and if possible, a sample project that reproduces this? Yes I can get those things together, thank you <@U03HRD4PZLN>! Thanks for your patience and persistence, <@U03JRSDS7JQ>! Beyond that, you could sign up for a slot in the AVFoundation Playback lab on Thursday. The developer forums are also a resource for getting help with issues like this. <@U03HRD4PZLN> Is this question appropriate in the playback lab? Yes. absolutely. Please mention that this is follow-up for a digital lounge question. Sounds good, thank you so much! I have an app for iOS and macOS for which I am using AVFoundation to capture video. I have a specific question when it comes to capturing data simultaneously from multiple AVCaptureDevice objects. On iOS we have AVCaptureMultiCamSession. But I am more interested in macOS. Since that API is not available for macOS, what is the best way to achieve the same result i.e. mapping multiple simultaneous input devices to multiple outputs. Would I have to run a single AVCaptureSession and do my own management of adding multiple inputs and mapping each input to its own distinct output device ? Hoping to discuss the best method to do this. Thanks Thank you for the question, unfortunately we don\u2019t have someone from the AVCaptureDevice team with us today. But we have a Q&A: Camera Capture digital lounge tomorrow and a Camera Capture lab on Friday. we have some devices which provide audio and video samples which are synced with one another but not with the host clock. So the audio sample rate is not e.g. 48kHz, but something close to it. Microphones usually provide a feedback pipe so that the host can tell the device to djust its output sample rate to match the host clock rate; these devices cannot do that. Can we just measure the incoming sample rate and tell CoreAudio that it is (e.g.) 48012 Hz for some seconds and 48006 Hz for some seconds, etc, or can it not to do rate matching? G'day, Stuart. Our traditional strategy underlying A/V sync is to synchronize video to audio, wherever possible, because adjusting video timing is generally simpler and lower-impact than adjusting audio. In this approach, we allow the sequence of audio samples to define the timeline of the recorded movie, and the timestamps of video frames are recorded relative to that. It's not clear to me from your question whether you're asking about a scenario of capture, or playback, or something real-time? I think he might be asking what strategy AVFoundation or CoreAudio uses to re-sync the samples. Does it drop a slice and interpolate the waveform? Does it play the sample at a lower rate, or perform a full retime / resample? In a formal, professional production environment, this simply doesn't happen because pro gear is synced together with clocks to eliminate jitter and ensure accurate sampling rates. Consumer gear doesn't have such a stringent requirement. If you're saying the audio is inviolate and sacred, then video is expected to drop frames. But he has a point; if some piece of audio equipment records at the wrong sample rate, what's the solution? Speaking generically: yes, in those cases I'd expect that someone would need to resample it. it is real time playback and capture of incoming video and audio from AV capture hardware. Realtime playback never seems to be a problem, but we have problems with A/V desync, particularly if we are synchronizing another audio source <@U03JDS776JH>: if this is realtime in a studio environment, you might want to invest in something like a \"master clock\" or \"time code generator.\" because that second audio source is often a microphone, and a microphone usually delivers exactly the host\u2019s idea of the requested sample rate. So I guess what I\u2019m asking is can I expect Core Audio to smoothly and seamlessly rate match if I mix two samples with slightly different sample rates (e.g. 48 and 48.003 kHz), where the not-quite-48kHz sample rate may vary from minute to minute as we measure? Is it a directly-attached USB microphone like a Blue Yeti, or something attached with an XLR cable to a preamp, and if so, is that preamp USB or Thunderbolt? (the underlying issue is probably the hardware introducing clock drift, which should not happen) second audio channel is usually an external USB microphone. Primary audio channel is synced to the primary video. The recording sampling rate will be determined by the project definitions (Logic Pro, Avid, etc) and it's reasonable to assume that you are recording all devices simultaneously within the tracking session. Therefore, all hardware is expected to sample at the same rate . If this drift is only happening with one external device, then it's the device, not you. even if the capture source\u2019s audio did not drift, we can never guarantee that its output rate matches the computer\u2019s. Even if the capture source\u2019s audio rate were drift-free, the host clock can drift. With USB microphones, a feedback pipe requests a few more or a few less samples from the hardware to match the host\u2019s expectations, and the USB microphone adjusts its PLL accordingly. This isn\u2019t possible if the audio sample rate is dictated by another piece of hardware. I think I'm going to recommend that you sign up for a slot in the Core Audio lab on Thursday, <@U03JDS776JH>. Though I was not entirely clear which interface you're integrating at: e.g., an audio device, a CoreMediaIO DAL plugin, an app calling AVCapture, an app calling CoreAudio? Sounds like you have two pieces of hardware fighting for the role of master clock in this situation. In the Audio MIDI Setup app, it shows the 'default' clock source for each device. Your recording app should be able to determine which one plays the role of master clock. Unfortunately that's not possible from the AMS app. All you can really do is set the preferred sample rate. An app using CoreAudio. The question is can Core Audio resample without glitches if we continually update the sample rate. The audio channel from our device - our app generates audio and video samples from it, the audio channels don\u2019t appear in AMS. So we can describe the samples as having a rate of 48003 per second if we want to. The question is whether core audio can deal with on-the-fly changes of the order of 0 to 30ppm happening on a minute-by-minute basis, or does it cause discontinuities in the output? That's definitely a question for the labs. :lab_coat: :test_tube: :loud_sound: super I will try to book one I'm a Mac sysadmin who happens to have a degree in audio engineering and has supported post production environments & entertainment companies. But I don't know the code level stuff. I wonder if it's possible to have an AVAudioEngine running and connect the inputNode later and only request permission for the microphone when actual recording is going on. Most apps I've see have the recording indicator in the status bar immediately up even if recording will only happen later. Users might be spooked by the rec. indicator if it's always on, it gives the impression the app is always listening, even it's just generating audio. Is there a way to optimize this experience? Hi Daniel, Thank you for the question, unfortunately we don\u2019t have someone from the audio team with us today. But we have Core Audio lab tomorrow, and you could sign up by 6pm today. Did you get an answer to this? Curious about if this is possible too I'm trying to customize the tvOS native AV Player UI. Is there a way to toggle the LIVE badge on and off? Working on live DVR, and I would like to be able to use that Live badge to indicate if the user is at the live edge or in the archive. Hey Joseph, there is currently no way to toggle the live badge on and off when you\u2019re at the live edge. We\u2019re interested in understanding more about your particular need here. Please file an enhancement request through Feedback Assistant and we will take a look. Thanks. I will do! When defining an HLS manifest with multiple audio groups and and audio tracks that use the same NAME parameter such as this example: #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio-ac3\",LANGUAGE=\"en\",NAME=\"English\",AUTOSELECT=YES,CHANNELS=\"6\",URI=\"playlist-ac3.m3u8\" #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio-aac\",LANGUAGE=\"en\",NAME=\"English\",AUTOSELECT=YES,CHANNELS=\"2\",URI=\"playlist-aac.m3u8\" AVFoundation seems to filter out all but one of the audio tracks. Is there documentation somewhere on how the filters are applied (such as on device X with os Y you get the multichannel rendition vs the 2 channel rendition)? Or should we always be using separate NAME values across groups to keep the audio tracks available when asking AVPlayer for the tracks? Thanks for your question regarding the HLS manifest entries. 1, You should definitely keep using NAME=\"English\". HLS is dynamic and will automatically choose the most appropriate alternate selection based on hardware capabilities, user preferences, system settings, network conditions, and other factors. Could you please file a feedback assistant report to provide more information about your use case for getting the tracks? What are you trying to do with the tracks? Please state that the report is for the AVFoundation for HLS streaming and we'll take a look at it. So all video bitrates are associated with both the AAC group and the AC-3 group. When getting the audio tracks from AVPlayer, it only returns a single track for English (so whichever the system chose). Now if we wanted to present the user with the option of selecting the Stereo or Surround renditions from a menu, we'd need both to be returned when asking for those tracks. If I change the name of the surround one to English Surround, then I get both tracks. Am I using it correctly, or should the various audio tracks all be in the same group with different NAMEs? And if they do have separate names, do we lose on the system auto selecting the best audio track for playback? Sorry that we're out of time. I recommend that you sign up for the HTTP Live Streaming lab on Friday to get more help. In general, I don't recommend changing the NAME to simply split the media selection options. That will mess up the fallback behavior. We should find out more information about exactly what they are trying to do, though. As Sam pointed out, this might be a good topic for a lab. I have 2 questions for the HTTP lab....how can i best manager that, i can't request 2 appointments You could describe both of your questions when requesting the one appointment. Is it possible to configure an AVAudioRecorder to do echo cancellation on macOS/iOS? e.g. so that recording while a FaceTime call/Zoom/whatever is going on, the recorder only records the voices of people in the same room as the computer, not the sound coming out of the computer? Hi William, we would love to chat to you in more detail 1-1 on this. Please book an appointment in the Core Audio lab on Thursday 10am Pacific or Friday 2pm Pacific What are the constraints on code running in an AVAudioNode.installTap ( https://developer.apple.com/documentation/avfaudio/avaudionode/1387122-installtap)|https://developer.apple.com/documentation/avfaudio/avaudionode/1387122-installtap) block? Is that running in the audio thread or observing real-time rules, or can we make allocations/dispatch to other queues/write Swift? I've seen conflicting answers on this around the web Hi again William, looks like you have a few questions that would be well-suited for a lab appointment. We have Core Audio lab tomorrow, and you could sign up by 6pm today. Thank you! We're trying to run AVAudioRecord to use it for Speech Recognition in our iOS keyboard extension for an unique dictation experience, but we keep receiving an error, which is described in the following Apple's Q&A that it is impossible to start record in any app extension: com.apple.coreaudio.avfaudio error 561145187 My question is why it was limited since iOS 8 and is it still a case in iOS 16, so that maybe we can get an access to it? URL for a feedback: https://developer.apple.com/library/archive/qa/qa1872/_index.html|https://developer.apple.com/library/archive/qa/qa1872/_index.html Hi Roman, thanks for the question. Unfortunately we don\u2019t have someone from the audio team with us today. But we have a Core Audio lab tomorrow, and you could sign up by 6pm Pacific today. Hey <@U03HC3P9VS7>, I\u2019ve been at the Core Audio lab today, but unfortunately, no one knows anything about how it works in app extensions, since it\u2019s a very rare case. Hmm, I wonder whether there are app extension-specific labs. That's probably where I would try next. Next\u2026 year\u2026 :smile: But that would be a great feedback for WWDC team - having separate labs for app extensions :smile: If you haven't already, I think it is worth filing a report using the feedback assistant. And share the FB id here Sure, here you go. FB10060598 I'm building a Python app that runs on Raspberry Pi to use Apple Music. There are already bindings for the API (which will probably need updating due to the new features announced yesterday). My question is how does playback occur when I play a song through the API? Does it need to be handled by a native player, or is it delivered by the server through the API? Hi Matthew, Thank you for the question, unfortunately we don\u2019t have the folks that could answer your question with us right now. But we have MusicKit lab on Thursday, and you could register by 6pm today. Ah. My mistake. On the Mac, can multiple apps access the camera at the same time? I am considering developing an app that would track the user's posture (and poke them to sit up straight when necessary). It would likely just need to grab a frame once a second or so. Would it be able to do that without interfering with other apps using the camera (such as FaceTime or Zoom)? Thank you for the question, unfortunately we don\u2019t have someone from the AVCaptureDevice team with us today. But we have a Q&A: Camera Capture digital lounge tomorrow or a Camera Capture lab on Friday. Short answer, yes, multiple apps can access camera at the same time. on Mac Do you have any code snipets showing how to load a stereo audio file into MLMultiArray object? Hi Eddie, unfortunately we don\u2019t have any code snippets for interacting with MLMultiArray in AVFoundation. If you are looking for info for how to use MLMultiArray, there are Machine Learning & Computer Vision labs on Thursday and Friday that you might like to check out. For the audio itself, what form is it in (.mov, .mp4, AudioFile supported formats such as CAF, etc.) and are you reading it all at once or processing it over time? If one user has ads and the other doesn't and interstitialTimeRangesForPlayerItem is implemented along with having the no ads user wait, is it possible to display a count down for that user indicating when the main content will resume? Hello Peter, You could share the ad schedules across the group when playback starts using the GroupSessionMessenger. And when a participant without ads is waiting for the others to finish, you can show a countdown timer based on the schedules shared. :thumbsup: ty <@U03HC3P2J8K> What is the minimum value that users expect when they want to edit a document with someone using SharePlay? What edge cases should we handle, like connection being temporarily lost, trying to edit/delete the same selection of the text, etc.? And if we expect that there will be only one copy of the document, stored on the hosts' side, should we close the document for participants when the session is over, or should we let them see the content, being able to copy it, save, and so on? <@U03JER2C7MX>, love that you\u2019re thinking through all the edge cases! I think it\u2019s a great idea to consider cases where the connection can be temporarily lost, we\u2019ve commonly come across this in scenarios like people riding on subways, going into elevators, etc. People connecting through FaceTime can be in very dynamic situations where the network can go in and out, so it is important to think of an architecture that can support these cases. One topic that we recommend looking into is CRDTs , they can be a little intimidating at first, but they basic idea is to structure your data in a way that is resilient to being sent redundantly and can handle cases like trying to edit the same selection of text at the same time. We\u2019re always happy to chat in more technical detail about these concepts and edge cases in our labs as well! If you haven\u2019t already, feel free to register for the SharePlay lab on Friday and we\u2019ll make sure you get a slot! Awesome, will do so :innocent: CRDTs for the win! :sunglasses: If we wait for a coordinated playback session to resume when displaying stitched interstitials, is it possible to cap the maximum amount of waiting in a time period before we might revert to a skip behaviour? Thinking in particular of a scenario where two users might have ad-breaks that aren't quite simultaneous. It would be very frustrating to wait for Alice to complete her advert, both videos to resume for two seconds, then Bob to view his. Ideally we'd want to force these to play simultanously? Hello Nathan, You should be able to do this by removing the \"playingInterstitial\" suspension from AVPlaybackCoordinator.suspensionReasonsThatTriggerWaiting , and issuing a play command to the previously waiting participant. But we would strongly recommend to synchronize the ad timings across participants. You could also try issuing AVPlayer.playImmediatelyAtRate: at the waiting participant to force them to play. Like 'func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator)` there is no callback methods to track info-pannel visibility. Whats the best way to identify when info-pannel appears and disappears? Hi <@U03JXLMAJBE>, Thanks for your question, tell us more about the experience you would like to build based on the info panel visibility and the platform(s) you are targeting? <@U03JXLMAJBE> There is currently no way to track the visibility of the transport controls. That\u2019s great feedback though. As Gavin mentioned, we\u2019re interested in understanding your use case better. Could you give some more details about it? Hi <@U03HQNTQK33> and <@U03HC3KUDGB>. I have a pretty simple use-case. On tvOS platform, I want to show a custom overlay view over the top of AVPlayer view when playback controls and info panel are hidden. When the playback controls or info-panel appear I want to hide the custom overlay view and show it again when they hide. Im currently doing through didUpdateFocus by comparing the nextFocused and previousFocused views, but its not really full proof. The new player updates introduced with tvOS 15, didnt work with my previously build logic which works for tvOS 13 and 14. Now I have two separate logic for different players. I wish there was a callback like \u2018func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator)\u2019. It will just make it so simple <@U03HQNTQK33> ^^ <@U03HC3KUDGB> Hey Aalim, so sorry for the late reply! Just to clarify, you\u2019re looking for a way to know when the tvOS player in tvOS 13/14 will show/hide the info panel that pulls down from the top. Is that correct? With an API like func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator) ? Yes. thats correct and not just for tvOS 13/14 but also for tvOS 15 <@U03HQNTQK33> Ah I see. So, unfortunately, we can\u2019t update tvOS 13 or 14 with any new features (since it\u2019s already shipped and on peoples devices). For tvOS 15.0 and later though we\u2019ve removed the info panel that drops down from the top in AVPlayerViewController - we now have an info tab integrated into the transport bar. You should be able to use func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator) when the transport bar is shown/hidden though No I have looked at this method even on tvOS 15. It just works for playback controls not for info-panel. <@U03HQNTQK33> So in the new player, your looking to be able to know when the new info tab is opened? correct. If the transport bar is hidden the info tab is also hidden. On tvOS 15.0 and later The delegate method willTransitionToVisibilityOfTransportBar will be called when the transport controls visibility changes. The info tab will only ever be shown while the transport bars visibility is YES. So the other would look like this willTransitionToVisibilityOfTransportBar: YES // transport bar is about to appear // User clicks info tab, it appears // User exits info tab, it disappears willTransitionToVisibilityOfTransportBar: NO // transport bar is about to hide You should be able to use the delegate callback willTransitionToVisibilityOfTransportBar to determine if the info panel is not. If the transport bar is hidden the info tab is also hidden. In tvOS 15, There are two ways to go to info panel, 1: swipe down when playback controls are visible 2: swipe down when playback controls are not visible. In the first case, when you swipe down from transport bar willTransitionToVisibilityOfTransportBar gets called but the value of visible is false since, transport bar hides when info panel appears. In the second case, the willTransitionToVisibilityOfTransportBar doesn\u2019t even gets called. <@U03HQNTQK33> That\u2019s correct, with the current behavior of willTransitionToVisibilityOfTransportBar you can determine whether or not the entire transport bar is hidden or visible. Unfortunately you can\u2019t determine if any of our provided customInfoViewControllers are hidden or shown. However, in tvOS 15.0 and later with an app linking against tvOS 15.0 and later, if willTransitionToVisibilityOfTransportBar has recently returned false then the info tab is guaranteed to be hidden. The info tab is only ever shown when the transport bar is visible because it is part of the transport bar UI at the bottom of the screen. If the behavior I just described above is not working in the new tvOS 16.0 beta or if you need more granular callbacks for when individual customInfoViewControllers (like the info tab) are selected then I would definitely file a request in Feedback Assistant - that\u2019s valuable feedback! I have a question regarding SharePlay experience in a documents app that doesn't provide an features like collaborative editing or sharing links to the document yet. iOS 15 case: Let's say the user starts SharePlay session with another person, which joins to edit this document as a guest. Which means that whenever the season is over, the guest won't be able to suggest edits to the other user, so that we finish editing experience and give him an option to copy text or create his own copy of the doc. My question is how can we improve that user experience on iOS 15, improving the flow itself? And what are we capable of in iOS 16 - what would be the desired flow for editor's users and should we use collaboration api instead/combined? Thanks for the detailed question, <@U03JER2C7MX>! There\u2019s some more relevant details on this too in response to <@U03HC3KUDGB>\u2019s question above, but it sounds like a prime example for adopting the new Messages collaboration APIs! In this case, the user doesn\u2019t need to have an existing account in your app (e.g. they can still be a guest), but sharing the document over a Messages group will let your app get an identity for this user based on the real Messages identity, so you can ensure they can still access the document even after a SharePlay session has ended. Is it currently possible to add my view/interface like an OSD aside/on top of the video content I share with others? Hi <@U03J1TN6WBD> If you\u2019re interested in customising interface elements with AVKit we have a lounge tomorrow with the AVKit folks. But tell us more about the experience you would like to build and we can see what parts SharePlay can help with GroupActivities allows you to let your creativity go wild and create any shared experience! If you wanted to create a coordinated experience for one of your app's views then you can do so with the GroupSessionMessenger . Checkout our WWDC21 talk \"Build custom experiences with Group Activities\" where we do that and build a DrawTogether app! https://developer.apple.com/videos/play/wwdc2021/10187/ f.e I thought about showing live stats if i'm sharing a video of a sport game with friends IRL Thanks Lior, I will see if we can get someone from the AVKit team tomorrow to help you with the view, in the meantime check out the session Adam posted above so you can see ways to coordinate interface elements using the Group Sessions Messenger API Hello. Is it possible to adjust video quality in the player? Hey <@U03HVCK66P8>, could you give us some more detail about your use case? We\u2019d like to understand your application better. Some streams have multiple quality variants and based on the current state of the system and network we may choose higher or lower quality streams. For example, the Apple Developer App. Cannot do fullscreen / adjust quality <@U03HQNTQK33> Becoz the video always loads in low quality and then becomes high after a period. I tried both apps (iOS/macOS/web), even the new player cannot change too. Thus, I can only download the video, and it just takes a few seconds. Is it safe to update the GroupActivity conforming object after the session begins? I\u2019ve used this method to update some global info about the session without using messages to make sure everyone including late-joiners receive the new data. But would this have any unintended side-effects/should I refactor to only use messages? Great question, <@U03JLRZJHQR>! The GroupActivity on the session is safe, and even encouraged, to be updated after the session begins! We\u2019ve done work under the hood to make sure after one person updates it everyone ends up in the right state. It\u2019s important to note that you don\u2019t want to update it too frequently, where too many people could stomp on each other\u2019s toes \u2014 we use a form of conflict resolution known as last writer wins. This means that the last person to update the activity will win, and all others will see that latest activity. So if two people update it at the same time, you\u2019ll only see one. However, everyone will be guaranteed to be in the same state so you don\u2019t have to worry about that! This can be a great way to infrequently update the \u201cstate\u201d of the group, e.g. for a Music playing app you can update it to whatever song is playing so everyone transitions to the correct song. Great, thanks for the detailed reply! I am building a Music Visualizer (for macOS and iOS as well as any future Apple platforms that would support immersive Metal powered visualization) I have been wondering about how to properly support Apple Music instead of merely internet streams and purchased iTunes music. There is wonderful support for integrating with Apple Music to allow the user to select tracks and playlists etc. I have not found a way to actually tap into the audio buffers to do the analysis (e.g. FFT) that I need. Common sense tells me there could be fully understandable legal reasons for this, but I still want to ask for your advice on how to best support Apple Music, now that I have this brief window of time. Perhaps there is a way to get access to \"reduced information\" about the audio signal somehow without my app actually accessing the raw buffers? A workaround using the microphone incurs a lag in responsiveness and results in very poor audio with headphones. (I use AudioKit, but this seems to be a general issue even when activating the mic in System Preferences). Thank you! Hello <@U03HZ462YF7>! Indeed. This is a commonly requested feature, and one that is very hard for us to deliver for many reasons, some of which you alluded to. Could you please file this as an enhancement request in the https://feedbackassistant.apple.com|Feedback Assistant tool, and describe the types of use cases you would use the audio buffer for and we will include it in our investigations. Thank you very much in advance! Thank you <@U03H96G1FB7>. You have spared me days of futile investigation. If/when I know more about what I end up needing I can provide some feedback. I guess the right topic in Feedback Assistant would be \u201cWeb Services & SDKs\u201d? Or either \u201cmacOS\u201d or \u201ciOS & iPadOS\u201d? I think you should actually use either \u201cmacOS\u201d or \u201ciOS & iPad OS\u201d. Thank you! I think users would expect an app like this to work great with Apple Music and it would be so incredibly cool. Something I've been wondering. If I'm expecting a ShazamKit experience to be used in a noisy environment, do I need to mix in some simulated noise into source audio when creating a catalog? There\u2019s no need to mix in noise at any stage in the creation of custom catalogs, clean audio is always preferred. Shazam is noise tolerant (you\u2019ve used the app in a noisy setting and still received a match I\u2019m sure). So have a clean audio reference signature, and try and make sure that the query signature is as clean as possible, but it can tolerate a certain amount of noise. Please in Apple Music API add finally endpoint to remove a song from playlist! This is very tedious job if I want to have curated playlist that I share to other people. We wait for such simple feature :) Hello <@U03K04WLQGL>! Thank you very much for your feedback! We understand the strong desire to be able to delete a song from a playlist. While we don\u2019t have any update on this front for Apple Music API, I\u2019m happy to report that this functionality is now available on Apple platforms through the MusicKit framework. That being said, you cannot remove items from any playlist, but only playlists users have created in your specific app. Please refer to https://developer.apple.com/forums/thread/707759|this thread in the Apple Developer Forums for more information. If you would like to see this functionality exposed in Apple Music API so it can also benefit other platforms like the Web and Android, feel free to file an enhancement request on https://feedbackassistant.apple.com|Feedback Assistant , including some information about your use case. Thanks! Will MusicKitJS be updated? Currently AppleMusic API is upgraded to 1.2 however JS is still 1.0 with a bug preventing playback after 15 minutes. At the same time at https://js-cdn.music.apple.com/musickit/v3/docs/index.html?path=/docs/introduction--page|https://js-cdn.music.apple.com/musickit/v3/docs/index.html?path=/docs/introduction--page documentation for JS v3 exists but there is no ETA for that - is that abandoned project? Hello <@U03K04WLQGL>! Let me relay some information in this regard from my colleagues working on MusicKit on the Web. The future of MusicKit on the Web is indeed the v3 one you found, which is documented https://developer.apple.com/musickit/web/|here . However, it\u2019s currently in extended beta to make sure we incorporate developer feedback completely before removing the beta label. I hope this helps. Thank you. in v3 beta there is not possible to do setQueue for playlist, should I report bug normal way by Feedback Assistant? Also, understanding v3 is the future will 1.0 be updated in the meantime with fix for stop playing after 15 mminutes? I wish my colleagues working on MusicKit for the Web were here to help answer your questions more directly. My recommendation is to report any issues via https://feedbackassistant.apple.com|Feedback Assistant , for sure. But also, feel free to keep the conversation going in the Apple Developer Forums, by starting new threads there with the tag https://developer.apple.com/forums/tags/musickit-js|MusicKit JS . Are both devices able to gracefully end? Yes, either side can end gracefully by canceling the NWConnection. Thank you :slightly_smiling_face: Is this available for every Watch series? Every watch that supports watchOS 9 is supported Thank you :slightly_smiling_face: Is there a limit in what information we want to pass from the watch to the tv, or can we send images, video or sound? You can write whatever data you want into the nw_connection! You can use it just like you would communicating with a server over the internet. Thank you so much Eliot :slightly_smiling_face: This session made my day! :partying_face: Thank you for all the questions! Its always awesome to see people enthusiastic about the things we build here ^_^ Can only one device connect to the TV at one time? DeviceDiscoveryUI only supports having one device connected to the Apple TV at a time. Sorry google search didn\u2019t reveal the docs for some reason. I\u2019ve found them. Is the app icon that shows on the TV something that I can configure as part of the connection? Will it be the same as my iPhone app's icon? Good question! The app icon that shows on the TV is the icon of your tvOS app that's presenting the picker. Thanks, Eric. It was a silly question -- google search didn\u2019t reveal the docs for some reason. I\u2019ve found them and I understand now :sweat_smile: No worries! Hopefully they've helped clear up any questions you have, and if you have any further, don't hesitate to ask ^_^ All good! Feel free to post on the Forums too after WWDC, if there are any other questions. We're happy to help! Is there a max limit of time for the connection to exists between tv and watch or how is this regulated in case the user forgets to terminate the connection? The connection is allowed to exist for long as the application is running on both the tv and the selected device. Once the application has been suspended, the connection will be terminated. If your app has been granted extended runtime, say for example through the use of a workout session, then that connection will persist while the device has its screen off. Awesome! Thank you Elliot :slightly_smiling_face: There would be great opportunities with being able to connect multiple users to the Apple TV for my use case. I will submit a Feedback outlining that as a suggestion, but what is the best current best alternative to DeviceDiscoveryUI for implementing something like a multi-person quiz where the main interface is on the Apple TV, and multiple people in the room are submitting their input on their own devices? Hey Duncan, we hear you loud and clear! Right now, DeviceDiscoveryUI supports a connection between a TV and a single device at a time. If the other devices are running iOS and iPadOS, it is possible to use a <https://developer.apple.com/documentation/network/nwbrowser/descriptor/bonjour_type_domain |Bonjour browse descriptor> as shown in the iOS/iPadOS version of the <https://developer.apple.com/documentation/network/building_a_custom_peer-to-peer_protocol |TicTacToe sample app>. It'd be great if you could submit a Feedback for us so we can take this into consideration for future releases. That would be amazing! To have collaborative sessions between multiple devices with a single Apple TV, it would make the Apple TV a video game console (Make sure you add an amazing GPU in the next versions too :heart:)! And will make the Apple TV more competitive agains the most used ones like Roku and Chrome Cast and Amazon Fire. I watched the \"Create a great video playback experience\" session and tried to implement the external metadata discussed there to show a video description. The chevron shows up, but when you tap the title the description doesn't show up. Will this be addressed in a future beta? Hey Erick, This is a known issue in the iOS seed 1 builds I'm not sure if this is intended or just a bug in the Developer iOS app, but I was using the Developer app on iOS 16 and there's no full screen button for the videos. Is the new AVPlayerViewController not have a full screen button? Hey Erick, this is a known issue in the iOS and macOS seed 1 builds ok, good to know. Thanks. I love the AVPlayerViewController changes BTW So glad to hear it! Yeh we think people are really going to love it. The new gestures are :fire: (Re: previous question\u2013 my use case would be displaying a grid of /many/ videos at once on-screen on iPhone. I could whip up a prototype that displays a ton of AVPlayer visuals to see how it goes, but \u2026was wondering if there's a more concrete answer here, rather than a \"throw stuff at the wall & see what sticks\" approach :sweat_smile:) Hey Michael, we\u2019d like to understand more about your use case. Would all of the videos need to play at once, or just one at a time or a collection of a few? :wave::skin-tone-3: Multiple playing at once. What I have in mind is a full-screen grid, 3-4 columns wide, with rows of squares of videos playing. (Only those on-screen would need to be playing simultaneously; audio doesn\u2019t matter here \u2013 but the idea would be to have it appear, when scrolling, as though all of the videos are all playing at the same time\u2013 as in, they begin playing when they come on-screen) (3 columns * roughly 7 rows => 21 videos playing simultaneously give-or-take / 4 columns * roughly 9 rows => 36) Wondering mainly about the feasibility; in terms of smooth playback & not eating battery\u2013 as in, does this sound within the realm of the APIs + the hardware\u2019s capability on iPhone, or would it likely involve falling back to software decoding [or just struggling] behind the scenes [Oh, more context: I\u2019m envisioning H.264/HEVC videos\u2013 whichever makes most sense performance-wise] If the answer to my question isn\u2019t at all clear cut, and is more of a \u201cas you increase the number of simultaneous playback instances, they\u2019ll play smoothly \u2026until they don\u2019t\u201d, then no worries Playing multiple videos at once can be challenging, both from a user perspective and from a performance perspective. Try to limit the number of videos so that your users don\u2019t get overwhelmed. To get the best performance, we recommend that you limit how many videos you load at a time. Let\u2019s say you load 2-4 videos at once, only start loading the next one after a previous one has become readyToPlay (AVPlayerItem status property). Also, try to re-use AVPlayers as much as possible. So instead of creating new players when a new video scrolls into view, re-use an old player of a video that has scrolled out of view (if possible). (edited) Okay, thanks. (Re users + overwhelming: these videos would be short, simple, looping clips of people\u2019s headshots\u2013 if you know of the Harry Potter franchise; imagine the newspapers in their universe, and how they have \u2018moving photos\u2019 on them :relaxed:) Does/Can our app get notifications for Visual Intelligence events? Hey Jaime, what kinds of visual intelligence events would you be looking for? We\u2019re curious to better understand your use case. Maybe this is a bit out of the scope here, but do you have intentions of supporting video playback from mkv containers? Or have you some tips how to add custom support for such container formats efficiently and with as little work as possible? Hey Alexander, we do not support playing videos using the mkv container format. We are interested in understanding your use case though, could you give us some more details about it? For example, what platform would you like support for this on? We\u2019d also recommend putting in a request for this in Feedback Assistant my use case is similar, I use AVPlayer to load audio files, and my users have asked for ogg files support, which is not available out of the box (sorry for hijacking the thread) Hey <@U03KC9BKC9E>, thanks for the info. I would definitely file a request in Feedback Assistant requesting support for this. Sounds like a very useful feature!","title":"audio and video"},{"location":"wwdc22/audio-and-video-lounge.html#audio-and-video-lounge-qas","text":"","title":"audio-and-video-lounge QAs"},{"location":"wwdc22/audio-and-video-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/audio-and-video-lounge.html#when-would-we-be-able-to-see-an-option-for-playback-speedpitch-in-apple-music-this-is-something-id-love-to-see","text":"Hello <@U03JNAGPDS6>! You can already change the playback speed with MusicKit by setting the https://developer.apple.com/documentation/musickit/musicplayer/state-swift.class/playbackrate|playbackRate property on the https://developer.apple.com/documentation/musickit/musicplayer|MusicPlayer \u2019s https://developer.apple.com/documentation/musickit/musicplayer/state-swift.class|state . As for changing pitch, we don\u2019t currently support this, but feel free to file an enhancement request in https://feedbackassistant.apple.com|Feedback Assistant .","title":"When would we be able to see an option for playback speed/pitch in Apple Music? This is something I'd love to see."},{"location":"wwdc22/audio-and-video-lounge.html#how-is-the-music-api-related-to-the-podcast-api","text":"Hey <@U03K8HA5U2U>! The Apple Music API is a separate product from any other metadata APIs. We currently don\u2019t have a dedicated Podcasts API, but if you need podcast metadata you can leverage the pre-existing iTunes Search API - https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html","title":"How is the Music API related to the Podcast API?"},{"location":"wwdc22/audio-and-video-lounge.html#hello-thanks-for-your-amazing-work-to-begin-with-i-wanted-to-ask-about-songs-lyrics-theres-a-haslyrics-boolean-variable-inside-the-song-object-but-i-see-no-way-to-access-those-lyrics-am-i-missing-something-id-need-to-have-timed-lyrics-for-an-app-idea-i-have-thank-you","text":"Hi Cristina, unfortunately there's no current way to access the lyrics of a Song object in either MusicKit or Apple Music API. Please consider filing an enhancement request in https://feedbackassistant.apple.com|Feedback Assistant for a request for this feature!","title":"Hello! Thanks for your amazing work, to begin with. I wanted to ask about songs lyrics, there's a hasLyrics boolean variable inside the Song object, but I see no way to access those lyrics, am I missing something? I'd need to have timed lyrics for an app idea I have. Thank you!"},{"location":"wwdc22/audio-and-video-lounge.html#thank-you-for-all-the-new-structures-in-musickit-this-year-all-my-wishlist-items-ticked-white_check_mark-i-wonder-how-to-work-with-the-musiccatalogtoplevelresourcerequesting-protocol-and-the-new-init-of-musiccatalogresourcerequest","text":"Hello <@U03HMDM1J23>! I\u2019m afraid we made a small mistake in seed 1, and forgot to include that https://developer.apple.com/documentation/musickit/genre|Genre conforms to https://developer.apple.com/documentation/musickit/musiccatalogtoplevelresourcerequesting|MusicCatalogTopLevelResourceRequesting . This will be addressed in an upcoming seed. Sure, no worries! I assume that it will help me fetch all the top genres from this https://api.music.apple.com/v1/catalog/us/genres|Apple Music API ? That\u2019s exactly right. We think we need this to tie into the new https://developer.apple.com/documentation/musickit/musiccatalogchartsrequest|MusicCatalogChartsRequest , which can be https://developer.apple.com/documentation/musickit/musiccatalogchartsrequest/init(genre:kinds:types:)|initialized for a specific Genre (optionally).","title":"Thank you for all the new structures in MusicKit this year!! All my wishlist items ticked. :white_check_mark:  I wonder how to work with the MusicCatalogTopLevelResourceRequesting protocol and the new init() of MusicCatalogResourceRequest?"},{"location":"wwdc22/audio-and-video-lounge.html#what-are-some-great-tips-on-creating-immersive-audio-experiences-in-our-apps-i-really-like-spatial-audio-and-spatialise-stereo","text":"Hi Cristian-Mihai, Currently https://developer.apple.com/documentation/musickit/audiovariant|MusicKit and https://developer.apple.com/documentation/applemusicapi/albums/attributes|Apple Music API expose which audio variants are available for a given music item like a Song or Album . You can also see the currently playing audio variant when using the MusicKit player. As for creating your own immersive audio experience, I suggest signing up for the CoreAudio lab or PHASE Audio engine lab. Thank you very much!","title":"What are some great tips on creating immersive audio experiences in our apps?  I really like Spatial Audio and Spatialise Stereo."},{"location":"wwdc22/audio-and-video-lounge.html#is-it-possible-to-determine-the-location-country-etc-of-origin-of-an-album-track-or-artist","text":"Hey Mark - we don\u2019t expose this metadata via the Apple Music API at this time. But feel free to file an enhancement request in https://feedbackassistant.apple.com/|Feedback Assistant .","title":"Is it possible to determine the location (country, etc.) of origin of an album, track, or artist?"},{"location":"wwdc22/audio-and-video-lounge.html#i-didnt-see-anything-about-apple-classical-primephonic-so-far-will-musickit-and-apple-music-api-support-it","text":"Hi <@U03JEKFMT4Z>, thank you for your patience. We are working on an amazing new classical music experience from Apple. Thank you Betim. So I guess there was no official news I missed for WWDC That\u2019s correct, we currently don\u2019t have any updates on this yet","title":"I didn't see anything about Apple Classical / Primephonic so far. Will MusicKit and Apple Music API support it?"},{"location":"wwdc22/audio-and-video-lounge.html#like-the-new-recently-played-structure-in-musickit-is-there-a-way-to-access-the-recently-added-items-in-the-users-library","text":"Hi Rudrank, Unfortunately, there currently isn't a way to access the recently added items in the user's library like the recently played structure. However, you could get this information from https://developer.apple.com/documentation/applemusicapi/get_recently_added_resources|Apple Music API . Please consider filing an enhancement request in https://feedbackassistant.apple.com|Feedback Assistant for a request for this feature!","title":"Like the new recently played structure in MusicKit, is there a way to access the recently added items in the user's library?"},{"location":"wwdc22/audio-and-video-lounge.html#like-the-new-recently-played-structure-in-musickit-is-there-a-way-to-access-the-heavy-rotation-content-like-the-one-mentioned-in-apple-music-api-from-the-users-library-if-not-do-you-recommend-using-musicdatarequest-for-the-url-httpsapimusicapplecomv1mehistoryheavy-rotationhttpsapimusicapplecomv1mehistoryheavy-rotation-and-decoding-it-as-a-music-item-collection-of-recentlyplayedmusicitem","text":"Hi Rudrank, Unfortunately, there currently isn't a way to access heavy rotation content from the user's library. Using MusicDataRequest to fetch this would work but decoding the items as RecentlyPlayedMusicItem is not recommended. For recommendations based on listening, we encourage exploring https://developer.apple.com/documentation/musickit/musicpersonalrecommendationsrequest|MusicPersonalRecommendationsRequest .","title":"Like the new recently played structure in MusicKit, is there a way to access the heavy rotation content (like the one mentioned in Apple Music API) from the user's library?  If not, do you recommend using MusicDataRequest for the URL https://api.music.apple.com/v1/me/history/heavy-rotation,|https://api.music.apple.com/v1/me/history/heavy-rotation, and decoding it as a music item collection of RecentlyPlayedMusicItem?"},{"location":"wwdc22/audio-and-video-lounge.html#does-spatial-audio-only-work-with-musickit-or-does-it-also-work-with-mediaplayer-as-well","text":"Hi Tyson, Spatial Audio playback will be handled automatically, however, there is no metadata from MediaPlayer that would let you know which https://developer.apple.com/documentation/musickit/audiovariant|Audio Variant is either available or currently playing. Thank you!","title":"Does Spatial Audio only work with MusicKit? Or does it also work with MediaPlayer as well?"},{"location":"wwdc22/audio-and-video-lounge.html#is-the-isrc-given-by-the-music-api-reliable-to-identify-a-song-outside-of-the-music-api-in-the-past-ive-found-that-there-are-differences-between-apple-music-isrc-and-other-platforms-isrc-the-other-platforms-seem-to-be-correct-but-i-thought-isrc-is-supposed-to-be-universal","text":"Hey <@U03JE7H2DM4>! Yes, the ISRC value is intended to be a way to identify songs outside of Apple Music. If you have examples of ISRC values not matching what you expected, please consider sharing them with us using the Feedback Assistant.","title":"Is the ISRC given by the Music API reliable to identify a song outside of the Music API? In the past, I've found that there are differences between Apple Music ISRC and other platforms ISRC. The other platforms seem to be correct, but I thought ISRC is supposed to be universal."},{"location":"wwdc22/audio-and-video-lounge.html#will-there-be-an-option-to-be-able-to-playback-apple-music-directly-via-avplayer","text":"Hello <@U03JE7H2DM4>! Apple Music content is typically protected, and requires specialized playback logic. That\u2019s why it\u2019s not possible to play Apple Music content with AVPlayer. Instead, you should use one of https://developer.apple.com/documentation/musickit|MusicKit \u2019s players, like: \u2022 https://developer.apple.com/documentation/musickit/applicationmusicplayer|ApplicationMusicPlayer \u2022 https://developer.apple.com/documentation/musickit/systemmusicplayer|SystemMusicPlayer Thanks! Will those players ever be able to play audio directly from a url like AVPlayer? Those players do not currently allow playing audio directly from a URL like AVPlayer . What use-case do you have for that? And more generally, what are you trying to achieve? Essentially, a single player that can play the protected Apple Music songs and freely available content (url) in the same queue. I know it's possible using a mix of AVPlayer and System/ApplicationMusicPlayer but it would be really cool if we could do it in a single player. Ok, I see, thanks <@U03JE7H2DM4>. I think the best way forward would be to capture this information about your use-case in a new ticket on https://feedbackassistant.apple.com|Feedback Assistant . This will help us in our planning process for future enhancements to MusicKit. Thank you!","title":"Will there be an option to be able to playback Apple Music directly via AVPlayer?"},{"location":"wwdc22/audio-and-video-lounge.html#apple-music-api-how-to-work-with-catalog-activity-httpsdeveloperapplecomdocumentationapplemusicapiget_a_catalog_activityhttpsdeveloperapplecomdocumentationapplemusicapiget_a_catalog_activity-the-given-example-throws-resource-with-requested-id-was-not-found-error-and-i-am-not-sure-where-to-find-the-activity-to-understand-it","text":"Hi <@U03HMDM1J23>, Activities could sometimes be found using the search feature. Many activities have been converted into https://developer.apple.com/documentation/applemusicapi/get_a_catalog_apple_curator|Apple Curators and can also be found in search or the related curator for a playlist. This year's enhancements to MusicKit for Apple platforms have also made it easier to work with curators. You can watch the session on those updates <https://developer.apple.com/videos/play/wwdc2022/110347/ |here>. Thank you for reporting the sample ID in the documentation is not working, that will need to be updated.","title":"Apple Music API - How to work with Catalog Activity? https://developer.apple.com/documentation/applemusicapi/get_a_catalog_activity|https://developer.apple.com/documentation/applemusicapi/get_a_catalog_activity  The given example throws \"Resource with requested id was not found\" error, and I am not sure where to find the activity to understand it"},{"location":"wwdc22/audio-and-video-lounge.html#is-this-the-appropriate-place-to-ask-about-the-new-control-center-on-the-lock-screen-they-showed-an-audio-example-in-the-keynote-which-included-fullscreen-album-art-but-what-if-you-have-video-running-would-you-actually-see-the-video-on-the-lock-screen","text":"Hello <@U03J20WEYV8>! In the new lock screen in iOS 16, when a video is playing, the lock screen media controls will only show a still image provided by the app. To do this, the app needs to publish its https://developer.apple.com/documentation/mediaplayer/mpnowplayinginfocenter/1615903-nowplayinginfo|nowPlayingInfo dictionary through https://developer.apple.com/documentation/mediaplayer/mpnowplayinginfocenter|MPNowPlayingInfoCenter including the image with the key https://developer.apple.com/documentation/mediaplayer/mpmediaitempropertyartwork|MPMediaItemPropertyArtwork . One more thing <@U03J20WEYV8>, we\u2019ll have more information on a new way to publish the now playing information in a new session titled https://developer.apple.com/wwdc22/110338|Explore media metadata publishing and playback interactions , which will be released on Friday.","title":"Is this the appropriate place to ask about the new control center on the lock screen?  They showed an audio example in the keynote which included fullscreen album art.  But what if you have video running?  Would you actually see the video on the lock screen?"},{"location":"wwdc22/audio-and-video-lounge.html#like-musiclibraryrequest-does-musiclibrarysearchrequest-not-do-a-network-call-and-searches-from-the-local-copy-stored-on-device","text":"Hi Rudrank, Yes, like the MusicLibraryRequest on iOS, MusicLibrarySearchRequest doesn't make a network call for searching but instead searches your local library.","title":"Like MusicLibraryRequest, does MusicLibrarySearchRequest not do a network call, and searches from the local copy stored on device?"},{"location":"wwdc22/audio-and-video-lounge.html#one-wish-i-will-file-feedback-for-it-the-possibility-of-using-musickit-in-the-simulator","text":"Hi <@U03HMDM1J23>! Getting MusicKit to work in the simulator is very challenging on a technical level, but we feel the pain of the lack of this support just like you do. We keep investigating ways we could enable using some aspects of MusicKit in the simulator, and we\u2019ll make sure to communicate around it if and when we have any specific updates.","title":"One wish (I will file feedback for it), the possibility of using MusicKit in the simulator?"},{"location":"wwdc22/audio-and-video-lounge.html#im-working-on-migrating-form-mediaplayer-to-musickit-does-musickit-work-to-play-back-itunes-content-even-if-the-user-does-not-have-an-applemusic-subscription","text":"Hi Tyson, Great to hear that you're migrating! If you were using MediaPlayer to play back iTunes content without a subscription, you can accomplish the same thing using MusicKit in iOS 16! Great, thank you!","title":"I'm working on migrating form MediaPlayer to MusicKit. Does MusicKit work to play back iTunes content even if the user does not have an AppleMusic subscription?"},{"location":"wwdc22/audio-and-video-lounge.html#now-that-we-have-artwork-for-artist-yay-do-we-assume-that-licensing-issues-have-been-sorted-httpsdeveloperapplecomforumsthread688012httpsdeveloperapplecomforumsthread688012","text":"Hi Rudrank! Yes, we have worked hard with business to make sure it is compliant with licensing agreement for use in your app. For more details please check out Meet Apple Music API and MusicKit - WWDC22 https://developer.apple.com/wwdc22/10148 . Yes! Been wanting that for a long time! Hi Rudrank, Tyson and Everyone, Please reference https://developer.apple.com/app-store/review/guidelines/ section 4.5.2 for guidance of how artist images can be used in your app. `(ii) Using the MusicKit APIs is not a replacement for securing the licenses you might need for a deeper or more complex music integration. For example, if you want your app to play a specific song at a particular moment, or to create audio or video files that can be shared to social media, you'll need to contact rights-holders directly to get their permission (e.g. synchronization or adaptation rights) and assets. Cover art and other metadata may only be used in connection with music playback or playlists (including App Store screenshots displaying your app's functionality), and should not be used in any marketing or advertising without getting specific authorization from rights-holders. Make sure to follow the Apple Music Identity Guidelines when integrating Apple Music services in your app. ` Got it, thanks! Right, that\u2019s great because we used to have run all our AppStore screenshots though photoshop to change all the album artwork to public domain images. So we really appreciate this! Thank you!","title":"Now that we have artwork for artist (YAY), do we assume that licensing issues have been sorted?   https://developer.apple.com/forums/thread/688012|https://developer.apple.com/forums/thread/688012"},{"location":"wwdc22/audio-and-video-lounge.html#my-app-uses-avaudioengine-and-i-am-very-disappointed-that-the-taponbus-command-has-such-a-slow-responsiveness-at-an-audio-sample-rate-of-44100-fps-it-can-not-respond-any-faster-than-giving-me-4400-samples-every-01-seconds-is-there-any-way-to-speed-this-up","text":"Hi Keith, Thanks for the question! I think your question is best suited for a scheduled Core Audio Lab . <@U03J1SQJYRW> Could you please help me select a proper lab for my question? I just visited the Core Audio lab, but unfortunately, I feel like it wasn\u2019t a proper lab to address technical issues with AVAudioRecord . My question relates to this Apple\u2019s Q&A - https://developer.apple.com/library/archive/qa/qa1872/_index.html","title":"My app uses AVAudioEngine, and I am very disappointed that the \"TapOnBus()\" command has such a slow responsiveness.  At an audio sample rate of 44,100 fps, it can not respond any faster than giving me 4,400 samples every 0.1 seconds.  Is there any way to speed this up?"},{"location":"wwdc22/audio-and-video-lounge.html#for-ios-multicam-capture-apple-provides-avcapturemulticamsession-to-help-with-capturing-from-multiple-inputs-and-feeding-to-multiple-outputs-for-mac-what-is-the-best-way-to-achieve-the-same-result-running-multiple-avcapturesessions-is-definitely-not-the-way-i-want-to-go-do-i-have-to-figure-out-a-way-of-adding-multiple-inputs-and-mapping-them-to-multiple-outputs-as-i-will-be-using-avcapturevideodataoutputdelegate-and-is-that-okay-to-do","text":"Hello! Thank you for the question. This would be best suited for tomorrow\u2019s Q&A: AVFoundation (between 1pm and 3pm PT).","title":"For iOS MultiCam capture, Apple provides AVCaptureMultiCamSession to help with capturing from multiple inputs and feeding to multiple outputs.  For Mac, what is the best way to achieve the same result. Running multiple AVCaptureSessions is definitely not the way I want to go, do I have to figure out a way of adding multiple inputs and mapping them to multiple outputs as I will be using AVCaptureVideoDataOutputDelegate and is that okay to do"},{"location":"wwdc22/audio-and-video-lounge.html#does-shareplay-work-inside-messages-specifically-inside-an-imessage-app-extension","text":"We do not support SharePlay in extensions (IE: iMessage App Extensions), but we love hearing the feedback so please do file the feedback!","title":"Does SharePlay work inside Messages, specifically inside an iMessage App Extension?"},{"location":"wwdc22/audio-and-video-lounge.html#is-there-existing-api-to-share-a-url-of-a-document-in-icloud-drive-and-for-the-invitees-to-be-automatically-given-shared-access-to-it-for-the-duration-of-the-shareplay-session-with-icloud-dealing-with-keeping-everyone-up-to-date-if-not-how-do-apps-like-pages-do-this","text":"Ugh sorry wrong lounge :man-facepalming: No worries! I can still answer this a little bit, we do not currently have any API for this in SharePlay, but one idea you could do is add the share URL to your GroupActivity ! You can then use the share URL to gain access on the other devices by accepting the share. Alternatively, we definitely encourage you to check out the new collaboration features in Messages offered this year! This way, people on a FaceTime call can easily get the option to share the CKShare with the group they\u2019re FaceTime\u2019ing with from the share sheet! Thanks. Not sure how that integrates with iCloud documents (wasn't so much the wrong place given SharePlay, just feels odd in the AV lounge!) It seems there is no recommended solution for doing such collab with iCloud documents which is puzzling This is the relevant API for the latter (people on FaceTime getting the option to collaborate with the group) - https://developer.apple.com/documentation/foundation/nsitemprovider/3951995-registerckshare . You\u2019ll want to register the CKShare on the item provider you pass to the Share Sheet. You\u2019re in the right place for all things SharePlay :slightly_smiling_face: not just limited to audio/video! Thanks","title":"Is there existing API to share a URL of a document in iCloud Drive and for the invitees to be automatically given shared access to it for the duration of the SharePlay session, with iCloud dealing with keeping everyone up to date? If not, how do apps like Pages do this?"},{"location":"wwdc22/audio-and-video-lounge.html#shareplay-has-the-shareplay-message-size-been-extended-or-publicly-documented","text":"The message payload for the GroupSessionMessenger has been increased to 256KB! Feel free to checkout our \"What's new in SharePlay\" talk where we talk about that! https://developer.apple.com/videos/play/wwdc2022/10140/ :+1:","title":"SharePlay: Has the SharePlay message size been extended? or publicly documented?"},{"location":"wwdc22/audio-and-video-lounge.html#shareplay-related-fb9991061-groupactivitysharingcontroller-is-unavailable-in-mac-catalyst-despite-the-documentation-httpsdeveloperapplecomdocumentationgroupactivitiesgroupactivitysharingcontrollerhttpsdeveloperapplecomdocumentationgroupactivitiesgroupactivitysharingcontroller","text":"Thank you for bringing this to our attention. We will look into this.","title":"SharePlay related:  FB9991061  GroupActivitySharingController is unavailable in Mac Catalyst despite the documentation: https://developer.apple.com/documentation/groupactivities/groupactivitysharingcontroller|https://developer.apple.com/documentation/groupactivities/groupactivitysharingcontroller"},{"location":"wwdc22/audio-and-video-lounge.html#hello-all-hope-everyone-is-doing-well-i-would-like-anyones-input-on-streaming-audio-through-mqtt-messages-i-have-created-a-working-class-that-uses-audioqueue-to-grab-packets-of-a-certain-size-and-listen-to-these-bytes-after-applying-an-algorithm-to-it-ive-done-this-for-talking-and-listening-using-audioqueue-only-however-sometimes-there-could-be-some-instability-when-listening-and-talking-does-anyone-have-any-improvements-or-things-i-can-look-out-for-when-handling-live-streamed-audio-data-with-audioqueue-or-is-this-the-right-service-for-streaming-audio-just-for-context-ive-tried-avaudioengine-but-a-few-issues-arose-when-doing-so-thanks","text":"Hi <@U03J9T1R5M4> , Thanks for your question, we don\u2019t have someone from the CoreAudio team here today, but would love to go in to more detail with you in our Core Audio Lab. Please sign-up using the Developer App or at http://developer.apple.com|developer.apple.com Hey Gavin, thank you would love to discuss more.","title":"Hello all, hope everyone is doing well, i would like anyone's input on streaming audio through MQTT messages. I have created a working class that uses AudioQueue to grab packets of a certain size and listen to these bytes after applying an algorithm to it. I've done this for talking and listening using AudioQueue only. However, sometimes there could be some instability when listening and talking. Does anyone have any improvements or things i can look out for when handling live streamed audio data with AudioQueue? Or is this the right service for streaming audio? Just for context, I've tried AVAudioEngine but a few issues arose when doing so. Thanks!"},{"location":"wwdc22/audio-and-video-lounge.html#shareplay-related-are-there-any-updates-to-user-management-such-as-linking-shareplay-and-facetime-participants","text":"Apps can associate GroupSession participants with accounts in your app, but the framework does not provide access to the FaceTime participant information directly. Thank you","title":"SharePlay related:  Are there any updates to user management such as linking SharePlay and FaceTime participants?"},{"location":"wwdc22/audio-and-video-lounge.html#i-have-a-shareplay-game-on-the-app-store-in-the-game-you-answer-questions-out-loud-starting-with-ios-16-my-game-is-no-longer-able-to-access-the-microphone-during-a-facetime-call-is-this-an-expected-change","text":"Hi <@U03J7G8UX50>! Thanks for bringing this to our attention! If you file a feedback assistant we\u2019ll be happy to dig into this and figure out what\u2019s going on. Great, thanks","title":"I have a SharePlay game on the App Store. In the game you answer questions out loud. Starting with iOS 16 my game is no longer able to access the microphone during a FaceTime call. Is this an expected change?"},{"location":"wwdc22/audio-and-video-lounge.html#ive-have-adam-on-2x-to-see-the-shareplay-updates-so-appologies-with-the-new-messages-and-shareplay-integration-does-facetime-remain-as-the-transport-layer-for-the-stable-connection-and-messages-therefore-is-a-new-wayui-to-initiate-a-shareplayfacetime-session","text":"2X speed Adam is always fun! SharePlay in Messages provides the same guarantees you're used to as a developer! You'll still be leveraging the Apple infrastructure for messaging through the GroupSessionMessenger with end-to-end encryption, regardless of whether SharePlay was initiated from FaceTime or Messages! > just to clarify... our app is a presentation manager enabling file sharing and group editing (aka Multiuser Quick Look) and also supporting a realtime SharePlay canvas for drawing... so we support deep FaceTime programmatic access beyond the 15.4 GroupActivitySharingController... so we currently associate file editing with who is FaceTime/SharePlay/iCloud user... so just to be clear... is FaceTime the transport for SharePlay or do we need to also manager Messages identity? ... because Messages and FaceTime can be different email/phone numbers Hi Zaid, Your implementation of SharePlay is really interesting, we would love to dive in to the details with you in a 1-1 lab. Could you please sign up to a SharePlay 1-1 lab using the Developer App, or at http://developer.apple.com|developer.apple.com I shall and much thanks it seems that I already submitted a request for: June 8, 2022, between 8:00 p.m. and 10:00 p.m. EDT (UTC-04:00)","title":"I've have Adam on 2X to see the SharePlay updates: so appologies:  With the new Messages and SharePlay integration, does FaceTime remain as the transport layer for the stable connection and Messages therefore is a new way/UI to initiate a SharePlay/FaceTime session?"},{"location":"wwdc22/audio-and-video-lounge.html#when-an-arbitrary-uiview-is-overlaid-on-top-of-an-avplayer-containing-hdr-content-think-subtitles-mute-toggle-etc-the-overlay-appears-much-dimmer-and-can-be-hard-to-read-is-there-a-way-to-disable-this-behavior-and-render-the-overlay-with-the-same-brightness","text":"> Hi Chris, > Subtitles and closed caption playback using AVPlayerLayer adapt to the corresponding HDR video. If you are seeing cases where this is a concern, please file use Feedback Assistant to describe the situation you are seeing. A sample app and especially a pointer to the asset will be most helpful. Please include the platform(s) you are using. > If you would like to discuss in more detail we have an AVFoundation playback lab tomorrow. Thanks! Please see FB9875960 -- this is on iOS","title":"When an arbitrary UIView is overlaid on top of an AVPlayer containing HDR content (think subtitles, mute toggle, etc.), the overlay appears much dimmer and can be hard to read. Is there a way to disable this behavior and render the overlay with the same brightness?"},{"location":"wwdc22/audio-and-video-lounge.html#whats-the-best-way-of-determining-audio-output-latency-esp-with-wireless-devices-like-airpods-my-app-synchronizes-on-screen-graphics-not-video-with-sound-and-getting-accurate-latency-info-with-airpods-has-been-challenging","text":"Hi John. Thanks for your question. This is an interesting problem. 1. It is difficult to get correct latency values for AirPods right now. Would you please file an enhancement request for this ( https://developer.apple.com/bug-reporting/ )? 2. Can you elaborate how you are playing audio in your app? Is it by any chance using AVPlayer? Hi Moritz, thanks! I filed FB8961041 about 6 months ago. There's more detail and a sample project there if you want to check it out. So far I've been looking at outputLatency and ioBufferDuration from AVAudioSession but they only add up to around 20ms whereas the actual latency is somewhere around 110ms. I have read (I think from Jeff Moore) that some or all of the remainder comes from \"safety offsets\" but I don't see that those are exposed anywhere. Basically this has been holding up a couple of music-related apps that I'm working. I simplified the problem space somewhat for the sake of brevity, but input latency plays into it too. I will note that standard videos on iOS sync to Airpods sound quite reliably so I know this is doable :slightly_smiling_face: Thank you very much for the FB number. Note that AVFoundation playback objects like AVPlayer or AVSampleBufferAudioRenderer handle latency for you. So, if you can use one of those for rendering your audio, you can rely on the AVPlayerItem timebase or AVSampleBufferRenderSynchronizer currentTime to synchronize your graphics.","title":"What's the best way of determining audio output latency, esp with wireless devices like Airpods?  My app synchronizes on-screen graphics (not video) with sound, and getting accurate latency info with Airpods has been challenging."},{"location":"wwdc22/audio-and-video-lounge.html#hi-i-have-a-fairly-technical-question-that-might-have-been-a-better-fit-for-a-lab-but-i-missed-the-deadline-on-monday-to-apply-for-it-pensive-i-am-using-a-custom-core-image-compositing-pipeline-via-the-avvideocompositing-protocol-that-is-using-cifilters-for-adjusting-a-foreground-and-background-video-and-combine-them-via-alpha-blendingmasking-the-custom-compositor-has-some-parameters-stored-in-an-instance-conforming-to-the-avvideocompositioninstructionprotocol-the-goal-is-for-the-user-to-interactively-adjust-these-parameters-and-have-the-composited-video-played-back-continuously-reflect-these-changes-with-as-little-latency-as-possible-in-my-current-implementation-there-is-a-significant-lag-between-slider-adjustments-and-the-video-image-responding-presumably-this-is-because-the-video-player-has-already-processed-and-buffered-frames-using-the-previous-parameter-values-i-am-hoping-to-find-an-approach-that-would-allow-smooth-and-responsive-adjustments-of-the-cifilter-parameters-while-the-video-is-playing-maybe-there-is-some-instruction-that-would-flush-the-players-cache-without-interrupting-playback-amongst-other-things-i-tried-setting-the-avplayeritems-preferredforwardbufferduration-to-a-very-low-value-such-as-001-but-this-seemed-to-have-no-effect-on-the-responsiveness-of-adjustments-here-is-a-simplified-example-project-that-only-has-a-single-slider-for-controlling-foreground-exposure-httpswwwdropboxcomsbk3srki6ys455srvideocompositingzipdl1httpswwwdropboxcomsbk3srki6ys455srvideocompositingzipdl1","text":"G'day, Hendrik. Thanks for your question. It's cool to hear about your advanced use of AVVideoCompositing. I want to point out that it isn't too late yet to request a timeslot in the Thursday AVFoundation Playback lab. Regarding your request to reduce the latency for rendering during user interaction: would you please file an enhancement request via the feedback assistant, and include the sample app that you have already built? Thank you. I didn\u2019t consider the Playback lab, but actually my question fits into that pretty well, so I will apply for a lab appointment tomorrow.","title":"Hi! I have a fairly technical question that might have been a better fit for a lab, but I missed the deadline on Monday to apply for it. :pensive:  I am using a custom Core Image compositing pipeline (via the AVVideoCompositing protocol) that is using CIFilters for adjusting a foreground and background video and combine them via alpha blending/masking.  The custom compositor has some parameters stored in an instance conforming to the AVVideoCompositionInstructionProtocol.  The goal is for the user to interactively adjust these parameters, and have the composited video (played back continuously) reflect these changes with as little latency as possible.  In my current implementation there is a significant lag between slider adjustments and the video image responding. Presumably this is because the video player has already processed and buffered frames using the previous parameter values.  I am hoping to find an approach that would allow smooth and responsive adjustments of the CIFilter parameter(s) while the video is playing. Maybe there is some instruction that would flush the player's cache (without interrupting playback)?  Amongst other things I tried setting the AVPlayerItem\u2019s preferredForwardBufferDuration to a very low value (such as 0.01), but this seemed to have no effect on the responsiveness of adjustments.   Here is a simplified example project that only has a single slider for controlling foreground exposure : https://www.dropbox.com/s/bk3srki6ys455sr/VideoCompositing.zip?dl=1|https://www.dropbox.com/s/bk3srki6ys455sr/VideoCompositing.zip?dl=1"},{"location":"wwdc22/audio-and-video-lounge.html#i-have-a-custom-document-format-that-embeds-video-files-is-there-a-way-to-get-an-avplayer-to-play-that-data-without-first-writing-the-video-data-out-to-a-tmp-file-so-that-i-have-a-file-url-to-pass-to-avplayer","text":"G'day, Daniel. Check out AVAssetResourceLoader. Through it, you can provide a delegate that will be called to interpret a custom URL scheme and deliver data that you will read from your custom document format. There is a new API property in iOS 16 and macOS 13 Ventura called entireLengthAvailableOnDemand which optimizes the case where the data is already available locally, as in your app.","title":"I have a custom document format that embeds video files. Is there a way to get an AVPlayer to play that data without first writing the video data out to a tmp file so that I have a file URL to pass to AVPlayer?"},{"location":"wwdc22/audio-and-video-lounge.html#can-i-disable-the-control-center-option-to-spatialize-audio-if-the-user-is-using-airpods-setting-avplayeritemallowedaudiospatializationformats-to-none-doesnt-seem-to-do-the-trick","text":"Hi Martin, can you describe your use case a little more? Why would you like to disable the Control Center option? If this capability is important to your app, we'd love to hear more. Could you also file a request ( https://developer.apple.com/bug-reporting/ ) and describe your use case?","title":"Can I disable the control center option to spatialize audio if the user is using AirPods? Setting AVPlayerItem.allowedAudioSpatializationFormats to .none doesn't seem to do the trick."},{"location":"wwdc22/audio-and-video-lounge.html#is-there-a-way-to-get-presentation-and-decoding-data-from-avplayer-such-as-the-pts-presentation-time-stamp-dts-display-time-stamp-or-pcr-program-clock-reference","text":"Hi Neil. In AVPlayerItemOutput.h, there is some sample code above the declaration of AVPlayerItemVideoOutput0 . Using AVPlayerItemVideoOutput 's copyPixelBufferForItemTime:itemTimeForDisplay: , you you should be able to grab what I believe comes out as the PTS. If you want to work more on this with us, you could come back and ask us this question to either the AVFoundation Playback lab on Thursday or the HTTP Live Streaming lab on Friday :slightly_smiling_face: Thanks. I'm still trying to find the code you're talking about (had to create an obj-c project to view the h file). I think I found it. Thanks Would these calls have any impact on the performance of the video playback? Our interest is in sussing out the source of an audio/video desynchronisation issue we're having. The only way to know for sure is to try it out and profile, but I would not think that using AVPlayerItemVideoOutput would have a major impact on playback performance. If you can determine that your content is not at fault, could you Feedback Assistant report? At this point we're not really sure. We have a feedback ticket open, but we have not come up with a cause yet. There is a desire from some of the people on our team to know the PTS etc. In our case, we have AAC-LC that throws decoding errors and drifting out of sync only when DRM (fairplay) is enabled. If the stream is clear, everything is fine. If we switch to HE-AAC, we're not seeing these issues and it's not clear to me why this would be or why DRM would make it happen/worse Could you share the FB ID of your feedback ticket? I should note that AVPlayerItemVideoOutput will not work with FairPlay. FB9995441 Ah ok, good to know In case we run out of time here, I highly encourage you to come to the HTTP Live Streaming Lab on Friday to discuss this further. I have submitted a request for that lab. ...and the AVFoundation playback lab Excellent. We have multiple pipelines...one for live content, and one for vod content....we only experience this on live content, but the pipelines are different vendors etc...so I would think it's some kind of configuration issue...but the whole DRM vs Clear throws that into doubt Did you make any progress on debugging the audio decode errors? I have not, no. It was suggested that it was an encoding issue, but I don't see why it would be ok when playing clear streams. Also, my understanding is that HE-AAC is just a more feature rich version of AAC-LC, so I don't know why that change would make a difference Perhaps there is an issue with the audio encoder that is only hit with the configuration of AAC-LC+FairPlay? We are definitively seeing bad data fed into our audio decoder, but I cannot tell you why. And so is the audio data the same whether or not DRM is applied, or are they structurally different? My assumption was just that DRM would need to be decrypted first and then it's the same data as clear Do you have two audio streams of what you consider to be byte-for-byte identical, one clear-text and one encrypted? Identical aside from their encryption, that is. In our lab we have 2 channels side by side...one clear and one Fairplay. I believe i noted the exact URLs in the FB but they should be identical in every way except drm I'm taking a look at the media segments. But I suspect we'll run out of time. Thanks...I appreciate any info we discover in the time allotted... The HTTP Live Streaming lab would be the best venue to continue this conversation :slightly_smiling_face: The initial question I have on my request for that one is different than what we're discussing today. Will that matter? This particular issue I have requested for the playback lab If possible, ask this question again in a new question, so we can appropriately prepare. ok, sounds good...I'll create a new request thanks Thanks! Have a nice rest of your day. It doesn't seem to let me request more than one appointment? do i cancel and create new? Yeah, that will work. ok, thanks","title":"Is there a way to get presentation and decoding data from AVPlayer such as the PTS (presentation time stamp), DTS (display time stamp), or PCR (program clock reference)?"},{"location":"wwdc22/audio-and-video-lounge.html#how-can-we-use-multiple-avplayerlayers-with-one-avplayer-this-leads-to-frozen-video-in-simulators-see-httpsdeveloperapplecomforumsthread688766httpsdeveloperapplecomforumsthread688766","text":"Hello Kai, it looks like you've submitted FB9566529. Thanks for the report. We are investigating this, but rest assured that multiple AVPlayerLayers connected to a single AVPlayer is a supported use case. It works on devices in production, but it's limiting our simulator productivity for debugging so any fixes or workarounds would be very helpful","title":"How can we use multiple AVPlayerLayer's with one AVPlayer? This leads to frozen video in simulators. See: https://developer.apple.com/forums/thread/688766|https://developer.apple.com/forums/thread/688766"},{"location":"wwdc22/audio-and-video-lounge.html#my-ipad-app-has-a-camera-view-streaming-recording-is-it-possible-to-use-the-camera-while-in-multitasking-mode-stage-manager-id-like-to-adopt-all-the-new-apis-but-currently-im-stuck-requiring-my-app-to-stay-in-full-screen","text":"Yes. iOS 16 adds new Camera multitasking APIs that allow developers to use the camera while multitasking. Please refer to the session: https://developer.apple.com/videos/play/wwdc2022/110429/ And the Developer Publications Article: https://developer.apple.com/documentation/avkit/accessing_the_camera_while_multitasking?language=objc If you have additional questions, please sign up for the Camera Capture Lab on Friday 9am to noon PDT. WOW Thank you! You're very welcome. Have a great rest of WWDC!","title":"My iPad app has a camera view (streaming / recording).  Is it possible to use the camera while in multitasking mode / stage manager? I'd like to adopt all the new apis, but currently I'm stuck requiring my app to stay in full screen."},{"location":"wwdc22/audio-and-video-lounge.html#are-there-any-plans-for-native-integration-of-cmcd-in-avfoundation","text":"Good Afternoon! We would love to hear more information about your app and use case of CMCD, if you haven\u2019t already could you please file an enhancement request at https://developer.apple.com/bug-reporting/","title":"Are there any plans for native integration of CMCD in AVFoundation...."},{"location":"wwdc22/audio-and-video-lounge.html#this-is-about-avcapturedevice-if-the-device-is-a-telephoto-camera-i-suggest-a-direct-way-to-query-it-or-query-its-active-format-as-to-its-lens-power-that-is-does-it-have-20x-optical-power-or-25x-or-30x-this-would-be-useful-for-any-camera-app-where-the-user-manually-selects-the-lens-to-use-no-virtual-device-thanks","text":"Hi Eric, Thanks for your question, unfortunately we don\u2019t have someone from the AVCaptureDevice team with us today. But we have a Q&A: Camera Capture digital lounge tomorrow or a Camera Capture lab on Friday.","title":"This is about AVCaptureDevice.  If the device is a telephoto camera, I suggest a direct way to query it (or query its active format) as to its lens power.  That is, does it have 2.0x optical power, or 2.5x or 3.0x?  This would be useful for any camera app where the user manually selects the lens to use (no virtual device).  Thanks!"},{"location":"wwdc22/audio-and-video-lounge.html#how-do-you-turn-an-array-of-floats-representing-audio-into-an-avaudiopcmbuffer-to-pass-to-avaudiofiles-writeto-method-this-seems-to-mangle-the-output-var-floats-float-this-comes-from-somewhere-else-try-withunsafemutablepointerto-ampfloats-bytes-in-let-audiobuffer-audiobuffermnumberchannels-1-mdatabytesize-uint32bytespointeecount-memorylayoutltfloatsize-mdata-bytes-var-bufferlist-audiobufferlistmnumberbuffers-1-mbuffers-audiobuffer-let-outputaudiobuffer-avaudiopcmbufferpcmformat-bufferformat-bufferlistnocopy-ampbufferlist-try-selfrenderedaudiofilewritefrom-outputaudiobuffer-and-the-version-without-the-withunsafemutablepointer-gives-me-a-warning-cannot-use-inout-expression-here-argument-mdata-must-be-a-pointer-that-outlives-the-call-to-initmnumberchannelsmdatabytesizemdata-when-you-try-and-dereference-the-float-array-directly-like-var-floats-float-this-comes-from-somewhere-else-let-audiobuffer-audiobuffermnumberchannels-1-mdatabytesize-uint32floatscount-memorylayoutltfloatsize-mdata-ampfloats","text":"Hi Robert, We would love to chat to you in more detail 1-1 on this. Please book an appointment in the Core Audio lab on Thursday 10am Pacific or Friday 2pm Pacific I\u2019d love to, but I\u2019m in CET+1 and the Thursday ones seem to be booked out and the Friday ones are 2am for me. I\u2019ve replicated the question in the dev forums: https://developer.apple.com/forums/thread/707690","title":"How do you turn an array of Floats (representing audio) into an AVAudioPCMBuffer (to pass to AVAudioFile's write(to:) method)?  This seems to mangle the output: var floats: [Float] = ... // this comes from somewhere else try withUnsafeMutablePointer(to: &amp;amp;floats) { bytes in     let audioBuffer = AudioBuffer(mNumberChannels: 1, mDataByteSize: UInt32(bytes.pointee.count * MemoryLayout&amp;lt;Float.size), mData: bytes)     var bufferList = AudioBufferList(mNumberBuffers: 1, mBuffers: audioBuffer)     let outputAudioBuffer = AVAudioPCMBuffer(pcmFormat: buffer.format, bufferListNoCopy: &amp;amp;bufferList)!     try self.renderedAudioFile?.write(from: outputAudioBuffer) }  ... and the version without the withUnsafeMutablePointer { gives me a warning (Cannot use inout expression here; argument 'mData' must be a pointer that outlives the call to 'init(mNumberChannels:mDataByteSize:mData:)') when you try and dereference the float array directly like: var floats: [Float] = ... // this comes from somewhere else let audioBuffer = AudioBuffer(mNumberChannels: 1, mDataByteSize: UInt32(floats.count * MemoryLayout&amp;lt;Float.size), mData: &amp;amp;floats) // ..."},{"location":"wwdc22/audio-and-video-lounge.html#i-am-trying-to-compress-video-files-before-uploading-to-a-server-i-am-using-an-avassetexportsession-to-do-this-however-i-have-not-been-able-to-get-it-to-work-after-running-the-sessions-status-is-always-failed-and-i-get-a-very-generic-error-with-the-id-17508-any-insight-as-to-what-might-be-going-wrong-this-is-the-error-asset-writer-error-error-domainavfoundationerrordomain-code-11800-the-operation-could-not-be-completed-userinfonslocalizedfailurereasonan-unknown-error-occurred-17508-nslocalizeddescriptionthe-operation-could-not-be-completed-nsunderlyingerror0x280314d80-error-domainnsosstatuserrordomain-code-17508-null","text":"G'day, Mitchell. I am curious about the destination URL that you're providing to AVAssetExportSession. It looks to me like this error indicates that our frameworks were unable to access that URL. Can you check that it's a URL that your app has access to -- for example, if you need a temporary filename, are you using NSTemporaryDirectory() to generate it? Yes! So I spent about three hours yesterday messing with this as with research thought this was the error too. I have tried every combination of different URLs and all had the same problem. I will paste what i\u2019ve tried below. However, to test if this was the problem, I tried copying the data I already had to one of these url\u2019s and reading it back and it worked. So I thought that the problem was not the output URL that I was writing too. Here is everything I tried: // let compressedURL = NSURL.fileURL(withPath: NSTemporaryDirectory() + NSUUID().uuidString + \u201c.mov\u201d) // let compressedURL = URL(string: NSTemporaryDirectory())?.appendingPathComponent(\u201cLibrary/Caches/compressedVideo101.mov\u201d) // let exportPath = NSTemporaryDirectory().appendingFormat(\u201cvideo.mov\u201d) // let compressedURL = URL(fileURLWithPath: exportPath) // let documentDirectoryURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first! as NSURL // let compressedURL = documentDirectoryURL.appendingPathComponent(\u201ccompressionVideos/compressedvideo2.mov\u201d)! as URL // let docPath: String = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true)[0]; // let compressedURL: URL = URL(fileURLWithPath: docPath).appendingPathComponent(\u201cvideo_output.mov\u201d); // let temporaryDirectoryUrl: URL = FileManager.default.temporaryDirectory // let temporaryFilename = ProcessInfo().globallyUniqueString // let compressedURL = temporaryDirectoryUrl.appendingPathComponent(\u201c(temporaryFilename).mov\u201d) *let* documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first *let* compressedURL = documentsPath!.appendingPathComponent(\"compressedvideo4\", isDirectory: *false*).appendingPathExtension(\"mov\") The last one, the uncommented one, is the one I am currently using because i was able to successfully copy the data to it and read it back properly which makes me think assume it works. Can you give an example of the URLs that these are generating for you? Could there possibly be a permissions problem with my app that is preventing the export session from writing to it? And yes! I\u2019ll provide a URL. Is the app ID generated in the URL sensitive information that I shouldn\u2019t provide to the somewhat public setting of the WWDC lounge? Need to keep certain aspects of the app confidential. Feel free to XXXX out whole words of the URL that might leak anything. Great! Here are two different URLs, one for the temporary directory and one for the documents directory. This is for the tmp directory and was created using this code: let compressedURL = NSURL.fileURL(withPath: NSTemporaryDirectory() + NSUUID().uuidString + \".mov\") sorry one moment send pressed too early For the tmp directory created using the code above: file:///private/var/mobile/Containers/Data/Application/XXXXXXXX/tmp/XXXXXXX.mov This is for the docs directory created using this code: *let* documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first *let* compressedURL = documentsPath!.appendingPathComponent(\"compressedvideo4\", isDirectory: *false*).appendingPathExtension(\"mov\") URL: file:///var/mobile/Containers/Data/Application/XXXXXXXXXXXX/Documents/compressedvideo4.mov Is that the information you wanted to see? Still producing this error If I understand correctly, you have a bunch of attempts to generate a temporary URL which aren't working, but one last one that does work, but seems like not the right place to put the temporary file? Sorry, it looks like I misread you. The last one doesn't work but you were able to confirm that your app can write there <@U03HRD4PZLN> I know I can write to the last one because of the copy test, but all of them produce the error. Whether a temporary URL or not, all of them produce the error message I pasted above Yes exactly! OK. A puzzle! I have not yet done a test with the temporary URLs but I can if that helps, I assume most of them would work also Maybe the destination isn't the problematic variable, then. What can you tell about the source asset? The asset is a video, taken from the user\u2019s photo library using PHPickerViewController which writes it to a URL on the system which I know I am able to read from because I am able to get raw data from it. The video is about 11 seconds long and about 20MB URL of the source: file:///private/var/mobile/Containers/Data/Application/XXXXXXX/tmp/.com.apple.Foundation.NSItemProvider.lobgL0/IMG_0061.mov We seem to be stumped too -- and we are running out of time in this lounge. Can I ask for you to file a feedback assistant report including a sysdiagnose, and if possible, a sample project that reproduces this? Yes I can get those things together, thank you <@U03HRD4PZLN>! Thanks for your patience and persistence, <@U03JRSDS7JQ>! Beyond that, you could sign up for a slot in the AVFoundation Playback lab on Thursday. The developer forums are also a resource for getting help with issues like this. <@U03HRD4PZLN> Is this question appropriate in the playback lab? Yes. absolutely. Please mention that this is follow-up for a digital lounge question. Sounds good, thank you so much!","title":"I am trying to compress video files before uploading to a server. I am using an AVAssetExportSession to do this, however, i have not been able to get it to work. After running, the session's status is always \"failed\" and I get a very generic error with the ID 17508. Any insight as to what might be going wrong?   This is the error:  Asset Writer Error: Error Domain=AVFoundationErrorDomain Code=-11800 \"The operation could not be completed\" UserInfo={NSLocalizedFailureReason=An unknown error occurred (-17508), NSLocalizedDescription=The operation could not be completed, NSUnderlyingError=0x280314d80 {Error Domain=NSOSStatusErrorDomain Code=-17508 \"(null)"},{"location":"wwdc22/audio-and-video-lounge.html#i-have-an-app-for-ios-and-macos-for-which-i-am-using-avfoundation-to-capture-video-i-have-a-specific-question-when-it-comes-to-capturing-data-simultaneously-from-multiple-avcapturedevice-objects-on-ios-we-have-avcapturemulticamsession-but-i-am-more-interested-in-macos-since-that-api-is-not-available-for-macos-what-is-the-best-way-to-achieve-the-same-result-ie-mapping-multiple-simultaneous-input-devices-to-multiple-outputs-would-i-have-to-run-a-single-avcapturesession-and-do-my-own-management-of-adding-multiple-inputs-and-mapping-each-input-to-its-own-distinct-output-device-hoping-to-discuss-the-best-method-to-do-this-thanks","text":"Thank you for the question, unfortunately we don\u2019t have someone from the AVCaptureDevice team with us today. But we have a Q&A: Camera Capture digital lounge tomorrow and a Camera Capture lab on Friday.","title":"I have an app for iOS and macOS for which I am using AVFoundation to capture video.  I have a specific question when it comes to capturing data simultaneously from multiple AVCaptureDevice objects. On iOS we have AVCaptureMultiCamSession.  But I am more interested in macOS. Since that API is not available for macOS, what is the best way to achieve the same result i.e. mapping multiple simultaneous input devices to multiple outputs. Would I have to run a single AVCaptureSession and do my own management of adding multiple inputs and mapping each input to its own distinct output device ? Hoping to discuss the best method to do this. Thanks"},{"location":"wwdc22/audio-and-video-lounge.html#we-have-some-devices-which-provide-audio-and-video-samples-which-are-synced-with-one-another-but-not-with-the-host-clock-so-the-audio-sample-rate-is-not-eg-48khz-but-something-close-to-it-microphones-usually-provide-a-feedback-pipe-so-that-the-host-can-tell-the-device-to-djust-its-output-sample-rate-to-match-the-host-clock-rate-these-devices-cannot-do-that-can-we-just-measure-the-incoming-sample-rate-and-tell-coreaudio-that-it-is-eg-48012-hz-for-some-seconds-and-48006-hz-for-some-seconds-etc-or-can-it-not-to-do-rate-matching","text":"G'day, Stuart. Our traditional strategy underlying A/V sync is to synchronize video to audio, wherever possible, because adjusting video timing is generally simpler and lower-impact than adjusting audio. In this approach, we allow the sequence of audio samples to define the timeline of the recorded movie, and the timestamps of video frames are recorded relative to that. It's not clear to me from your question whether you're asking about a scenario of capture, or playback, or something real-time? I think he might be asking what strategy AVFoundation or CoreAudio uses to re-sync the samples. Does it drop a slice and interpolate the waveform? Does it play the sample at a lower rate, or perform a full retime / resample? In a formal, professional production environment, this simply doesn't happen because pro gear is synced together with clocks to eliminate jitter and ensure accurate sampling rates. Consumer gear doesn't have such a stringent requirement. If you're saying the audio is inviolate and sacred, then video is expected to drop frames. But he has a point; if some piece of audio equipment records at the wrong sample rate, what's the solution? Speaking generically: yes, in those cases I'd expect that someone would need to resample it. it is real time playback and capture of incoming video and audio from AV capture hardware. Realtime playback never seems to be a problem, but we have problems with A/V desync, particularly if we are synchronizing another audio source <@U03JDS776JH>: if this is realtime in a studio environment, you might want to invest in something like a \"master clock\" or \"time code generator.\" because that second audio source is often a microphone, and a microphone usually delivers exactly the host\u2019s idea of the requested sample rate. So I guess what I\u2019m asking is can I expect Core Audio to smoothly and seamlessly rate match if I mix two samples with slightly different sample rates (e.g. 48 and 48.003 kHz), where the not-quite-48kHz sample rate may vary from minute to minute as we measure? Is it a directly-attached USB microphone like a Blue Yeti, or something attached with an XLR cable to a preamp, and if so, is that preamp USB or Thunderbolt? (the underlying issue is probably the hardware introducing clock drift, which should not happen) second audio channel is usually an external USB microphone. Primary audio channel is synced to the primary video. The recording sampling rate will be determined by the project definitions (Logic Pro, Avid, etc) and it's reasonable to assume that you are recording all devices simultaneously within the tracking session. Therefore, all hardware is expected to sample at the same rate . If this drift is only happening with one external device, then it's the device, not you. even if the capture source\u2019s audio did not drift, we can never guarantee that its output rate matches the computer\u2019s. Even if the capture source\u2019s audio rate were drift-free, the host clock can drift. With USB microphones, a feedback pipe requests a few more or a few less samples from the hardware to match the host\u2019s expectations, and the USB microphone adjusts its PLL accordingly. This isn\u2019t possible if the audio sample rate is dictated by another piece of hardware. I think I'm going to recommend that you sign up for a slot in the Core Audio lab on Thursday, <@U03JDS776JH>. Though I was not entirely clear which interface you're integrating at: e.g., an audio device, a CoreMediaIO DAL plugin, an app calling AVCapture, an app calling CoreAudio? Sounds like you have two pieces of hardware fighting for the role of master clock in this situation. In the Audio MIDI Setup app, it shows the 'default' clock source for each device. Your recording app should be able to determine which one plays the role of master clock. Unfortunately that's not possible from the AMS app. All you can really do is set the preferred sample rate. An app using CoreAudio. The question is can Core Audio resample without glitches if we continually update the sample rate. The audio channel from our device - our app generates audio and video samples from it, the audio channels don\u2019t appear in AMS. So we can describe the samples as having a rate of 48003 per second if we want to. The question is whether core audio can deal with on-the-fly changes of the order of 0 to 30ppm happening on a minute-by-minute basis, or does it cause discontinuities in the output? That's definitely a question for the labs. :lab_coat: :test_tube: :loud_sound: super I will try to book one I'm a Mac sysadmin who happens to have a degree in audio engineering and has supported post production environments & entertainment companies. But I don't know the code level stuff.","title":"we have some devices which provide audio and video samples which are synced with one another but not with the host clock. So the audio sample rate is not e.g. 48kHz, but something close to it. Microphones usually provide a feedback pipe so that the host can tell the device to djust its output sample rate to match the host clock rate; these devices cannot do that. Can we just measure the incoming sample rate and tell CoreAudio that it is (e.g.) 48012 Hz for some seconds and 48006 Hz for some seconds, etc, or can it not to do rate matching?"},{"location":"wwdc22/audio-and-video-lounge.html#i-wonder-if-its-possible-to-have-an-avaudioengine-running-and-connect-the-inputnode-later-and-only-request-permission-for-the-microphone-when-actual-recording-is-going-on-most-apps-ive-see-have-the-recording-indicator-in-the-status-bar-immediately-up-even-if-recording-will-only-happen-later-users-might-be-spooked-by-the-rec-indicator-if-its-always-on-it-gives-the-impression-the-app-is-always-listening-even-its-just-generating-audio-is-there-a-way-to-optimize-this-experience","text":"Hi Daniel, Thank you for the question, unfortunately we don\u2019t have someone from the audio team with us today. But we have Core Audio lab tomorrow, and you could sign up by 6pm today. Did you get an answer to this? Curious about if this is possible too","title":"I wonder if it's possible to have an AVAudioEngine running and connect the inputNode later and only request permission for the microphone when actual recording is going on. Most apps I've see have the recording indicator in the status bar immediately up even if recording will only happen later. Users might be spooked by the rec. indicator if it's always on, it gives the impression the app is always listening, even it's just generating audio. Is there a way to optimize this experience?"},{"location":"wwdc22/audio-and-video-lounge.html#im-trying-to-customize-the-tvos-native-av-player-ui-is-there-a-way-to-toggle-the-live-badge-on-and-off-working-on-live-dvr-and-i-would-like-to-be-able-to-use-that-live-badge-to-indicate-if-the-user-is-at-the-live-edge-or-in-the-archive","text":"Hey Joseph, there is currently no way to toggle the live badge on and off when you\u2019re at the live edge. We\u2019re interested in understanding more about your particular need here. Please file an enhancement request through Feedback Assistant and we will take a look. Thanks. I will do!","title":"I'm trying to customize the tvOS native AV Player UI.  Is there a way to toggle the LIVE badge on and off?  Working on live DVR, and I would like to be able to use that Live badge to indicate if the user is at the live edge or in the archive."},{"location":"wwdc22/audio-and-video-lounge.html#when-defining-an-hls-manifest-with-multiple-audio-groups-and-and-audio-tracks-that-use-the-same-name-parameter-such-as-this-example-ext-x-mediatypeaudiogroup-idaudio-ac3languageennameenglishautoselectyeschannels6uriplaylist-ac3m3u8-ext-x-mediatypeaudiogroup-idaudio-aaclanguageennameenglishautoselectyeschannels2uriplaylist-aacm3u8-avfoundation-seems-to-filter-out-all-but-one-of-the-audio-tracks-is-there-documentation-somewhere-on-how-the-filters-are-applied-such-as-on-device-x-with-os-y-you-get-the-multichannel-rendition-vs-the-2-channel-rendition-or-should-we-always-be-using-separate-name-values-across-groups-to-keep-the-audio-tracks-available-when-asking-avplayer-for-the-tracks","text":"Thanks for your question regarding the HLS manifest entries. 1, You should definitely keep using NAME=\"English\". HLS is dynamic and will automatically choose the most appropriate alternate selection based on hardware capabilities, user preferences, system settings, network conditions, and other factors. Could you please file a feedback assistant report to provide more information about your use case for getting the tracks? What are you trying to do with the tracks? Please state that the report is for the AVFoundation for HLS streaming and we'll take a look at it. So all video bitrates are associated with both the AAC group and the AC-3 group. When getting the audio tracks from AVPlayer, it only returns a single track for English (so whichever the system chose). Now if we wanted to present the user with the option of selecting the Stereo or Surround renditions from a menu, we'd need both to be returned when asking for those tracks. If I change the name of the surround one to English Surround, then I get both tracks. Am I using it correctly, or should the various audio tracks all be in the same group with different NAMEs? And if they do have separate names, do we lose on the system auto selecting the best audio track for playback? Sorry that we're out of time. I recommend that you sign up for the HTTP Live Streaming lab on Friday to get more help. In general, I don't recommend changing the NAME to simply split the media selection options. That will mess up the fallback behavior. We should find out more information about exactly what they are trying to do, though. As Sam pointed out, this might be a good topic for a lab. I have 2 questions for the HTTP lab....how can i best manager that, i can't request 2 appointments You could describe both of your questions when requesting the one appointment.","title":"When defining an HLS manifest with multiple audio groups and and audio tracks that use the same NAME parameter such as this example:  #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio-ac3\",LANGUAGE=\"en\",NAME=\"English\",AUTOSELECT=YES,CHANNELS=\"6\",URI=\"playlist-ac3.m3u8\"  #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio-aac\",LANGUAGE=\"en\",NAME=\"English\",AUTOSELECT=YES,CHANNELS=\"2\",URI=\"playlist-aac.m3u8\"  AVFoundation seems to filter out all but one of the audio tracks. Is there documentation somewhere on how the filters are applied (such as on device X with os Y you get the multichannel rendition vs the 2 channel rendition)?  Or should we always be using separate NAME values across groups to keep the audio tracks available when asking AVPlayer for the tracks?"},{"location":"wwdc22/audio-and-video-lounge.html#is-it-possible-to-configure-an-avaudiorecorder-to-do-echo-cancellation-on-macosios-eg-so-that-recording-while-a-facetime-callzoomwhatever-is-going-on-the-recorder-only-records-the-voices-of-people-in-the-same-room-as-the-computer-not-the-sound-coming-out-of-the-computer","text":"Hi William, we would love to chat to you in more detail 1-1 on this. Please book an appointment in the Core Audio lab on Thursday 10am Pacific or Friday 2pm Pacific","title":"Is it possible to configure an AVAudioRecorder to do echo cancellation on macOS/iOS? e.g. so that recording while a FaceTime call/Zoom/whatever is going on, the recorder only records the voices of people in the same room as the computer, not the sound coming out of the computer?"},{"location":"wwdc22/audio-and-video-lounge.html#what-are-the-constraints-on-code-running-in-an-avaudionodeinstalltap-httpsdeveloperapplecomdocumentationavfaudioavaudionode1387122-installtaphttpsdeveloperapplecomdocumentationavfaudioavaudionode1387122-installtap-block-is-that-running-in-the-audio-thread-or-observing-real-time-rules-or-can-we-make-allocationsdispatch-to-other-queueswrite-swift-ive-seen-conflicting-answers-on-this-around-the-web","text":"Hi again William, looks like you have a few questions that would be well-suited for a lab appointment. We have Core Audio lab tomorrow, and you could sign up by 6pm today. Thank you!","title":"What are the constraints on code running in an AVAudioNode.installTap (https://developer.apple.com/documentation/avfaudio/avaudionode/1387122-installtap)|https://developer.apple.com/documentation/avfaudio/avaudionode/1387122-installtap) block? Is that running in the audio thread or observing real-time rules, or can we make allocations/dispatch to other queues/write Swift? I've seen conflicting answers on this around the web"},{"location":"wwdc22/audio-and-video-lounge.html#were-trying-to-run-avaudiorecord-to-use-it-for-speech-recognition-in-our-ios-keyboard-extension-for-an-unique-dictation-experience-but-we-keep-receiving-an-error-which-is-described-in-the-following-apples-qa-that-it-is-impossible-to-start-record-in-any-app-extension-comapplecoreaudioavfaudio-error-561145187-my-question-is-why-it-was-limited-since-ios-8-and-is-it-still-a-case-in-ios-16-so-that-maybe-we-can-get-an-access-to-it-url-for-a-feedback-httpsdeveloperapplecomlibraryarchiveqaqa1872_indexhtmlhttpsdeveloperapplecomlibraryarchiveqaqa1872_indexhtml","text":"Hi Roman, thanks for the question. Unfortunately we don\u2019t have someone from the audio team with us today. But we have a Core Audio lab tomorrow, and you could sign up by 6pm Pacific today. Hey <@U03HC3P9VS7>, I\u2019ve been at the Core Audio lab today, but unfortunately, no one knows anything about how it works in app extensions, since it\u2019s a very rare case. Hmm, I wonder whether there are app extension-specific labs. That's probably where I would try next. Next\u2026 year\u2026 :smile: But that would be a great feedback for WWDC team - having separate labs for app extensions :smile: If you haven't already, I think it is worth filing a report using the feedback assistant. And share the FB id here Sure, here you go. FB10060598","title":"We're trying to run AVAudioRecord to use it for Speech Recognition in our iOS keyboard extension for an unique dictation experience, but we keep receiving an error, which is described in the following Apple's Q&amp;A that it is impossible to start record in any app extension: com.apple.coreaudio.avfaudio error 561145187  My question is why it was limited since iOS 8 and is it still a case in iOS 16, so that maybe we can get an access to it?  URL for a feedback: https://developer.apple.com/library/archive/qa/qa1872/_index.html|https://developer.apple.com/library/archive/qa/qa1872/_index.html"},{"location":"wwdc22/audio-and-video-lounge.html#im-building-a-python-app-that-runs-on-raspberry-pi-to-use-apple-music-there-are-already-bindings-for-the-api-which-will-probably-need-updating-due-to-the-new-features-announced-yesterday-my-question-is-how-does-playback-occur-when-i-play-a-song-through-the-api-does-it-need-to-be-handled-by-a-native-player-or-is-it-delivered-by-the-server-through-the-api","text":"Hi Matthew, Thank you for the question, unfortunately we don\u2019t have the folks that could answer your question with us right now. But we have MusicKit lab on Thursday, and you could register by 6pm today. Ah. My mistake.","title":"I'm building a Python app that runs on Raspberry Pi to use Apple Music. There are already bindings for the API (which will probably need updating due to the new features announced yesterday).  My question is how does playback occur when I play a song through the API?  Does it need to be handled by a native player, or is it delivered by the server through the API?"},{"location":"wwdc22/audio-and-video-lounge.html#on-the-mac-can-multiple-apps-access-the-camera-at-the-same-time-i-am-considering-developing-an-app-that-would-track-the-users-posture-and-poke-them-to-sit-up-straight-when-necessary-it-would-likely-just-need-to-grab-a-frame-once-a-second-or-so-would-it-be-able-to-do-that-without-interfering-with-other-apps-using-the-camera-such-as-facetime-or-zoom","text":"Thank you for the question, unfortunately we don\u2019t have someone from the AVCaptureDevice team with us today. But we have a Q&A: Camera Capture digital lounge tomorrow or a Camera Capture lab on Friday. Short answer, yes, multiple apps can access camera at the same time. on Mac","title":"On the Mac, can multiple apps access the camera at the same time? I am considering developing an app that would track the user's posture (and poke them to sit up straight when necessary). It would likely just need to grab a frame once a second or so. Would it be able to do that without interfering with other apps using the camera (such as FaceTime or Zoom)?"},{"location":"wwdc22/audio-and-video-lounge.html#do-you-have-any-code-snipets-showing-how-to-load-a-stereo-audio-file-into-mlmultiarray-object","text":"Hi Eddie, unfortunately we don\u2019t have any code snippets for interacting with MLMultiArray in AVFoundation. If you are looking for info for how to use MLMultiArray, there are Machine Learning & Computer Vision labs on Thursday and Friday that you might like to check out. For the audio itself, what form is it in (.mov, .mp4, AudioFile supported formats such as CAF, etc.) and are you reading it all at once or processing it over time?","title":"Do you have any code snipets showing how to load a stereo audio file into MLMultiArray object?"},{"location":"wwdc22/audio-and-video-lounge.html#if-one-user-has-ads-and-the-other-doesnt-and-interstitialtimerangesforplayeritem-is-implemented-along-with-having-the-no-ads-user-wait-is-it-possible-to-display-a-count-down-for-that-user-indicating-when-the-main-content-will-resume","text":"Hello Peter, You could share the ad schedules across the group when playback starts using the GroupSessionMessenger. And when a participant without ads is waiting for the others to finish, you can show a countdown timer based on the schedules shared. :thumbsup: ty <@U03HC3P2J8K>","title":"If one user has ads and the other doesn't and interstitialTimeRangesForPlayerItem is implemented along with having the no ads user wait, is it possible to display a count down for that user indicating when the main content will resume?"},{"location":"wwdc22/audio-and-video-lounge.html#what-is-the-minimum-value-that-users-expect-when-they-want-to-edit-a-document-with-someone-using-shareplay-what-edge-cases-should-we-handle-like-connection-being-temporarily-lost-trying-to-editdelete-the-same-selection-of-the-text-etc-and-if-we-expect-that-there-will-be-only-one-copy-of-the-document-stored-on-the-hosts-side-should-we-close-the-document-for-participants-when-the-session-is-over-or-should-we-let-them-see-the-content-being-able-to-copy-it-save-and-so-on","text":"<@U03JER2C7MX>, love that you\u2019re thinking through all the edge cases! I think it\u2019s a great idea to consider cases where the connection can be temporarily lost, we\u2019ve commonly come across this in scenarios like people riding on subways, going into elevators, etc. People connecting through FaceTime can be in very dynamic situations where the network can go in and out, so it is important to think of an architecture that can support these cases. One topic that we recommend looking into is CRDTs , they can be a little intimidating at first, but they basic idea is to structure your data in a way that is resilient to being sent redundantly and can handle cases like trying to edit the same selection of text at the same time. We\u2019re always happy to chat in more technical detail about these concepts and edge cases in our labs as well! If you haven\u2019t already, feel free to register for the SharePlay lab on Friday and we\u2019ll make sure you get a slot! Awesome, will do so :innocent: CRDTs for the win! :sunglasses:","title":"What is the minimum value that users expect when they want to edit a document with someone using SharePlay? What edge cases should we handle, like connection being temporarily lost, trying to edit/delete the same selection of the text, etc.?  And if we expect that there will be only one copy of the document, stored on the hosts' side, should we close the document for participants when the session is over, or should we let them see the content, being able to copy it, save, and so on?"},{"location":"wwdc22/audio-and-video-lounge.html#if-we-wait-for-a-coordinated-playback-session-to-resume-when-displaying-stitched-interstitials-is-it-possible-to-cap-the-maximum-amount-of-waiting-in-a-time-period-before-we-might-revert-to-a-skip-behaviour-thinking-in-particular-of-a-scenario-where-two-users-might-have-ad-breaks-that-arent-quite-simultaneous-it-would-be-very-frustrating-to-wait-for-alice-to-complete-her-advert-both-videos-to-resume-for-two-seconds-then-bob-to-view-his-ideally-wed-want-to-force-these-to-play-simultanously","text":"Hello Nathan, You should be able to do this by removing the \"playingInterstitial\" suspension from AVPlaybackCoordinator.suspensionReasonsThatTriggerWaiting , and issuing a play command to the previously waiting participant. But we would strongly recommend to synchronize the ad timings across participants. You could also try issuing AVPlayer.playImmediatelyAtRate: at the waiting participant to force them to play.","title":"If we wait for a coordinated playback session to resume when displaying stitched interstitials, is it possible to cap the maximum amount of waiting in a time period before we might revert to a skip behaviour? Thinking in particular of a scenario where two users might have ad-breaks that aren't quite simultaneous. It would be very frustrating to wait for Alice to complete her advert, both videos to resume for two seconds, then Bob to view his. Ideally we'd want to force these to play simultanously?"},{"location":"wwdc22/audio-and-video-lounge.html#like-func-playerviewcontroller_-playerviewcontroller-avplayerviewcontroller-willtransitiontovisibilityoftransportbar-visible-bool-with-coordinator-avplayerviewcontrolleranimationcoordinator-there-is-no-callback-methods-to-track-info-pannel-visibility-whats-the-best-way-to-identify-when-info-pannel-appears-and-disappears","text":"Hi <@U03JXLMAJBE>, Thanks for your question, tell us more about the experience you would like to build based on the info panel visibility and the platform(s) you are targeting? <@U03JXLMAJBE> There is currently no way to track the visibility of the transport controls. That\u2019s great feedback though. As Gavin mentioned, we\u2019re interested in understanding your use case better. Could you give some more details about it? Hi <@U03HQNTQK33> and <@U03HC3KUDGB>. I have a pretty simple use-case. On tvOS platform, I want to show a custom overlay view over the top of AVPlayer view when playback controls and info panel are hidden. When the playback controls or info-panel appear I want to hide the custom overlay view and show it again when they hide. Im currently doing through didUpdateFocus by comparing the nextFocused and previousFocused views, but its not really full proof. The new player updates introduced with tvOS 15, didnt work with my previously build logic which works for tvOS 13 and 14. Now I have two separate logic for different players. I wish there was a callback like \u2018func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator)\u2019. It will just make it so simple <@U03HQNTQK33> ^^ <@U03HC3KUDGB> Hey Aalim, so sorry for the late reply! Just to clarify, you\u2019re looking for a way to know when the tvOS player in tvOS 13/14 will show/hide the info panel that pulls down from the top. Is that correct? With an API like func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator) ? Yes. thats correct and not just for tvOS 13/14 but also for tvOS 15 <@U03HQNTQK33> Ah I see. So, unfortunately, we can\u2019t update tvOS 13 or 14 with any new features (since it\u2019s already shipped and on peoples devices). For tvOS 15.0 and later though we\u2019ve removed the info panel that drops down from the top in AVPlayerViewController - we now have an info tab integrated into the transport bar. You should be able to use func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator) when the transport bar is shown/hidden though No I have looked at this method even on tvOS 15. It just works for playback controls not for info-panel. <@U03HQNTQK33> So in the new player, your looking to be able to know when the new info tab is opened? correct. If the transport bar is hidden the info tab is also hidden. On tvOS 15.0 and later The delegate method willTransitionToVisibilityOfTransportBar will be called when the transport controls visibility changes. The info tab will only ever be shown while the transport bars visibility is YES. So the other would look like this willTransitionToVisibilityOfTransportBar: YES // transport bar is about to appear // User clicks info tab, it appears // User exits info tab, it disappears willTransitionToVisibilityOfTransportBar: NO // transport bar is about to hide You should be able to use the delegate callback willTransitionToVisibilityOfTransportBar to determine if the info panel is not. If the transport bar is hidden the info tab is also hidden. In tvOS 15, There are two ways to go to info panel, 1: swipe down when playback controls are visible 2: swipe down when playback controls are not visible. In the first case, when you swipe down from transport bar willTransitionToVisibilityOfTransportBar gets called but the value of visible is false since, transport bar hides when info panel appears. In the second case, the willTransitionToVisibilityOfTransportBar doesn\u2019t even gets called. <@U03HQNTQK33> That\u2019s correct, with the current behavior of willTransitionToVisibilityOfTransportBar you can determine whether or not the entire transport bar is hidden or visible. Unfortunately you can\u2019t determine if any of our provided customInfoViewControllers are hidden or shown. However, in tvOS 15.0 and later with an app linking against tvOS 15.0 and later, if willTransitionToVisibilityOfTransportBar has recently returned false then the info tab is guaranteed to be hidden. The info tab is only ever shown when the transport bar is visible because it is part of the transport bar UI at the bottom of the screen. If the behavior I just described above is not working in the new tvOS 16.0 beta or if you need more granular callbacks for when individual customInfoViewControllers (like the info tab) are selected then I would definitely file a request in Feedback Assistant - that\u2019s valuable feedback!","title":"Like 'func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator)` there is no callback methods to track info-pannel visibility. Whats the best way to identify when info-pannel appears and disappears?"},{"location":"wwdc22/audio-and-video-lounge.html#i-have-a-question-regarding-shareplay-experience-in-a-documents-app-that-doesnt-provide-an-features-like-collaborative-editing-or-sharing-links-to-the-document-yet-ios-15-case-lets-say-the-user-starts-shareplay-session-with-another-person-which-joins-to-edit-this-document-as-a-guest-which-means-that-whenever-the-season-is-over-the-guest-wont-be-able-to-suggest-edits-to-the-other-user-so-that-we-finish-editing-experience-and-give-him-an-option-to-copy-text-or-create-his-own-copy-of-the-doc-my-question-is-how-can-we-improve-that-user-experience-on-ios-15-improving-the-flow-itself-and-what-are-we-capable-of-in-ios-16-what-would-be-the-desired-flow-for-editors-users-and-should-we-use-collaboration-api-insteadcombined","text":"Thanks for the detailed question, <@U03JER2C7MX>! There\u2019s some more relevant details on this too in response to <@U03HC3KUDGB>\u2019s question above, but it sounds like a prime example for adopting the new Messages collaboration APIs! In this case, the user doesn\u2019t need to have an existing account in your app (e.g. they can still be a guest), but sharing the document over a Messages group will let your app get an identity for this user based on the real Messages identity, so you can ensure they can still access the document even after a SharePlay session has ended.","title":"I have a question regarding SharePlay experience in a documents app that doesn't provide an features like collaborative editing or sharing links to the document yet.  iOS 15 case: Let's say the user starts SharePlay session with another person, which joins to edit this document as a guest. Which means that whenever the season is over, the guest won't be able to suggest edits to the other user, so that we finish editing experience and give him an option to copy text or create his own copy of the doc.  My question is how can we improve that user experience on iOS 15, improving the flow itself? And what are we capable of in iOS 16 - what would be the desired flow for editor's users and should we use collaboration api instead/combined?"},{"location":"wwdc22/audio-and-video-lounge.html#is-it-currently-possible-to-add-my-viewinterface-like-an-osd-asideon-top-of-the-video-content-i-share-with-others","text":"Hi <@U03J1TN6WBD> If you\u2019re interested in customising interface elements with AVKit we have a lounge tomorrow with the AVKit folks. But tell us more about the experience you would like to build and we can see what parts SharePlay can help with GroupActivities allows you to let your creativity go wild and create any shared experience! If you wanted to create a coordinated experience for one of your app's views then you can do so with the GroupSessionMessenger . Checkout our WWDC21 talk \"Build custom experiences with Group Activities\" where we do that and build a DrawTogether app! https://developer.apple.com/videos/play/wwdc2021/10187/ f.e I thought about showing live stats if i'm sharing a video of a sport game with friends IRL Thanks Lior, I will see if we can get someone from the AVKit team tomorrow to help you with the view, in the meantime check out the session Adam posted above so you can see ways to coordinate interface elements using the Group Sessions Messenger API","title":"Is it currently possible to add my view/interface like an OSD aside/on top of the video content I share with others?"},{"location":"wwdc22/audio-and-video-lounge.html#hello-is-it-possible-to-adjust-video-quality-in-the-player","text":"Hey <@U03HVCK66P8>, could you give us some more detail about your use case? We\u2019d like to understand your application better. Some streams have multiple quality variants and based on the current state of the system and network we may choose higher or lower quality streams. For example, the Apple Developer App. Cannot do fullscreen / adjust quality <@U03HQNTQK33> Becoz the video always loads in low quality and then becomes high after a period. I tried both apps (iOS/macOS/web), even the new player cannot change too. Thus, I can only download the video, and it just takes a few seconds.","title":"Hello. Is it possible to adjust video quality in the player?"},{"location":"wwdc22/audio-and-video-lounge.html#is-it-safe-to-update-the-groupactivity-conforming-object-after-the-session-begins-ive-used-this-method-to-update-some-global-info-about-the-session-without-using-messages-to-make-sure-everyone-including-late-joiners-receive-the-new-data-but-would-this-have-any-unintended-side-effectsshould-i-refactor-to-only-use-messages","text":"Great question, <@U03JLRZJHQR>! The GroupActivity on the session is safe, and even encouraged, to be updated after the session begins! We\u2019ve done work under the hood to make sure after one person updates it everyone ends up in the right state. It\u2019s important to note that you don\u2019t want to update it too frequently, where too many people could stomp on each other\u2019s toes \u2014 we use a form of conflict resolution known as last writer wins. This means that the last person to update the activity will win, and all others will see that latest activity. So if two people update it at the same time, you\u2019ll only see one. However, everyone will be guaranteed to be in the same state so you don\u2019t have to worry about that! This can be a great way to infrequently update the \u201cstate\u201d of the group, e.g. for a Music playing app you can update it to whatever song is playing so everyone transitions to the correct song. Great, thanks for the detailed reply!","title":"Is it safe to update the GroupActivity conforming object after the session begins?  I\u2019ve used this method to update some global info about the session without using messages to make sure everyone including late-joiners receive the new data. But would this have any unintended side-effects/should I refactor to only use messages?"},{"location":"wwdc22/audio-and-video-lounge.html#i-am-building-a-music-visualizer-for-macos-and-ios-as-well-as-any-future-apple-platforms-that-would-support-immersive-metal-powered-visualization-i-have-been-wondering-about-how-to-properly-support-apple-music-instead-of-merely-internet-streams-and-purchased-itunes-music-there-is-wonderful-support-for-integrating-with-apple-music-to-allow-the-user-to-select-tracks-and-playlists-etc-i-have-not-found-a-way-to-actually-tap-into-the-audio-buffers-to-do-the-analysis-eg-fft-that-i-need-common-sense-tells-me-there-could-be-fully-understandable-legal-reasons-for-this-but-i-still-want-to-ask-for-your-advice-on-how-to-best-support-apple-music-now-that-i-have-this-brief-window-of-time-perhaps-there-is-a-way-to-get-access-to-reduced-information-about-the-audio-signal-somehow-without-my-app-actually-accessing-the-raw-buffers-a-workaround-using-the-microphone-incurs-a-lag-in-responsiveness-and-results-in-very-poor-audio-with-headphones-i-use-audiokit-but-this-seems-to-be-a-general-issue-even-when-activating-the-mic-in-system-preferences-thank-you","text":"Hello <@U03HZ462YF7>! Indeed. This is a commonly requested feature, and one that is very hard for us to deliver for many reasons, some of which you alluded to. Could you please file this as an enhancement request in the https://feedbackassistant.apple.com|Feedback Assistant tool, and describe the types of use cases you would use the audio buffer for and we will include it in our investigations. Thank you very much in advance! Thank you <@U03H96G1FB7>. You have spared me days of futile investigation. If/when I know more about what I end up needing I can provide some feedback. I guess the right topic in Feedback Assistant would be \u201cWeb Services & SDKs\u201d? Or either \u201cmacOS\u201d or \u201ciOS & iPadOS\u201d? I think you should actually use either \u201cmacOS\u201d or \u201ciOS & iPad OS\u201d. Thank you! I think users would expect an app like this to work great with Apple Music and it would be so incredibly cool.","title":"I am building a Music Visualizer (for macOS and iOS as well as any future Apple platforms that would support immersive Metal powered visualization)   I have been wondering about how to properly support Apple Music instead of merely internet streams and purchased iTunes music. There is wonderful support for integrating with Apple Music to allow the user to select tracks and playlists etc. I have not found a way to actually tap into the audio buffers to do the analysis (e.g. FFT) that I need. Common sense tells me there could be fully understandable legal reasons for this, but I still want to ask for your advice on how to best support Apple Music, now that I have this brief window of time. Perhaps there is a way to get access to \"reduced information\" about the audio signal somehow without my app actually accessing the raw buffers?  A workaround using the microphone incurs a lag in responsiveness and results in very poor audio with headphones. (I use AudioKit, but this seems to be a general issue even when activating the mic in System Preferences).  Thank you!"},{"location":"wwdc22/audio-and-video-lounge.html#something-ive-been-wondering-if-im-expecting-a-shazamkit-experience-to-be-used-in-a-noisy-environment-do-i-need-to-mix-in-some-simulated-noise-into-source-audio-when-creating-a-catalog","text":"There\u2019s no need to mix in noise at any stage in the creation of custom catalogs, clean audio is always preferred. Shazam is noise tolerant (you\u2019ve used the app in a noisy setting and still received a match I\u2019m sure). So have a clean audio reference signature, and try and make sure that the query signature is as clean as possible, but it can tolerate a certain amount of noise.","title":"Something I've been wondering. If I'm expecting a ShazamKit experience to be used in a noisy environment, do I need to mix in some simulated noise into source audio when creating a catalog?"},{"location":"wwdc22/audio-and-video-lounge.html#please-in-apple-music-api-add-finally-endpoint-to-remove-a-song-from-playlist-this-is-very-tedious-job-if-i-want-to-have-curated-playlist-that-i-share-to-other-people-we-wait-for-such-simple-feature","text":"Hello <@U03K04WLQGL>! Thank you very much for your feedback! We understand the strong desire to be able to delete a song from a playlist. While we don\u2019t have any update on this front for Apple Music API, I\u2019m happy to report that this functionality is now available on Apple platforms through the MusicKit framework. That being said, you cannot remove items from any playlist, but only playlists users have created in your specific app. Please refer to https://developer.apple.com/forums/thread/707759|this thread in the Apple Developer Forums for more information. If you would like to see this functionality exposed in Apple Music API so it can also benefit other platforms like the Web and Android, feel free to file an enhancement request on https://feedbackassistant.apple.com|Feedback Assistant , including some information about your use case. Thanks!","title":"Please in Apple Music API add finally endpoint to remove a song from playlist! This is very tedious job if I want to have curated playlist that I share to other people. We wait for such simple feature :)"},{"location":"wwdc22/audio-and-video-lounge.html#will-musickitjs-be-updated-currently-applemusic-api-is-upgraded-to-12-however-js-is-still-10-with-a-bug-preventing-playback-after-15-minutes-at-the-same-time-at-httpsjs-cdnmusicapplecommusickitv3docsindexhtmlpathdocsintroduction-pagehttpsjs-cdnmusicapplecommusickitv3docsindexhtmlpathdocsintroduction-page-documentation-for-js-v3-exists-but-there-is-no-eta-for-that-is-that-abandoned-project","text":"Hello <@U03K04WLQGL>! Let me relay some information in this regard from my colleagues working on MusicKit on the Web. The future of MusicKit on the Web is indeed the v3 one you found, which is documented https://developer.apple.com/musickit/web/|here . However, it\u2019s currently in extended beta to make sure we incorporate developer feedback completely before removing the beta label. I hope this helps. Thank you. in v3 beta there is not possible to do setQueue for playlist, should I report bug normal way by Feedback Assistant? Also, understanding v3 is the future will 1.0 be updated in the meantime with fix for stop playing after 15 mminutes? I wish my colleagues working on MusicKit for the Web were here to help answer your questions more directly. My recommendation is to report any issues via https://feedbackassistant.apple.com|Feedback Assistant , for sure. But also, feel free to keep the conversation going in the Apple Developer Forums, by starting new threads there with the tag https://developer.apple.com/forums/tags/musickit-js|MusicKit JS .","title":"Will MusicKitJS be updated? Currently AppleMusic API is upgraded to 1.2 however JS is still 1.0 with a bug preventing playback after 15 minutes. At the same time at https://js-cdn.music.apple.com/musickit/v3/docs/index.html?path=/docs/introduction--page|https://js-cdn.music.apple.com/musickit/v3/docs/index.html?path=/docs/introduction--page documentation for JS v3 exists but there is no ETA for that - is that abandoned project?"},{"location":"wwdc22/audio-and-video-lounge.html#are-both-devices-able-to-gracefully-end","text":"Yes, either side can end gracefully by canceling the NWConnection. Thank you :slightly_smiling_face:","title":"Are both devices able to gracefully end?"},{"location":"wwdc22/audio-and-video-lounge.html#is-this-available-for-every-watch-series","text":"Every watch that supports watchOS 9 is supported Thank you :slightly_smiling_face:","title":"Is this available for every Watch series?"},{"location":"wwdc22/audio-and-video-lounge.html#is-there-a-limit-in-what-information-we-want-to-pass-from-the-watch-to-the-tv-or-can-we-send-images-video-or-sound","text":"You can write whatever data you want into the nw_connection! You can use it just like you would communicating with a server over the internet. Thank you so much Eliot :slightly_smiling_face: This session made my day! :partying_face: Thank you for all the questions! Its always awesome to see people enthusiastic about the things we build here ^_^","title":"Is there a limit in what information we want to pass from the watch to the tv, or can we send images, video or sound?"},{"location":"wwdc22/audio-and-video-lounge.html#can-only-one-device-connect-to-the-tv-at-one-time","text":"DeviceDiscoveryUI only supports having one device connected to the Apple TV at a time. Sorry google search didn\u2019t reveal the docs for some reason. I\u2019ve found them.","title":"Can only one device connect to the TV at one time?"},{"location":"wwdc22/audio-and-video-lounge.html#is-the-app-icon-that-shows-on-the-tv-something-that-i-can-configure-as-part-of-the-connection-will-it-be-the-same-as-my-iphone-apps-icon","text":"Good question! The app icon that shows on the TV is the icon of your tvOS app that's presenting the picker. Thanks, Eric. It was a silly question -- google search didn\u2019t reveal the docs for some reason. I\u2019ve found them and I understand now :sweat_smile: No worries! Hopefully they've helped clear up any questions you have, and if you have any further, don't hesitate to ask ^_^ All good! Feel free to post on the Forums too after WWDC, if there are any other questions. We're happy to help!","title":"Is the app icon that shows on the TV something that I can configure as part of the connection? Will it be the same as my iPhone app's icon?"},{"location":"wwdc22/audio-and-video-lounge.html#is-there-a-max-limit-of-time-for-the-connection-to-exists-between-tv-and-watch-or-how-is-this-regulated-in-case-the-user-forgets-to-terminate-the-connection","text":"The connection is allowed to exist for long as the application is running on both the tv and the selected device. Once the application has been suspended, the connection will be terminated. If your app has been granted extended runtime, say for example through the use of a workout session, then that connection will persist while the device has its screen off. Awesome! Thank you Elliot :slightly_smiling_face:","title":"Is there a max limit of time for the connection to exists between tv and watch or how is this regulated in case the user forgets to terminate the connection?"},{"location":"wwdc22/audio-and-video-lounge.html#there-would-be-great-opportunities-with-being-able-to-connect-multiple-users-to-the-apple-tv-for-my-use-case-i-will-submit-a-feedback-outlining-that-as-a-suggestion-but-what-is-the-best-current-best-alternative-to-devicediscoveryui-for-implementing-something-like-a-multi-person-quiz-where-the-main-interface-is-on-the-apple-tv-and-multiple-people-in-the-room-are-submitting-their-input-on-their-own-devices","text":"Hey Duncan, we hear you loud and clear! Right now, DeviceDiscoveryUI supports a connection between a TV and a single device at a time. If the other devices are running iOS and iPadOS, it is possible to use a <https://developer.apple.com/documentation/network/nwbrowser/descriptor/bonjour_type_domain |Bonjour browse descriptor> as shown in the iOS/iPadOS version of the <https://developer.apple.com/documentation/network/building_a_custom_peer-to-peer_protocol |TicTacToe sample app>. It'd be great if you could submit a Feedback for us so we can take this into consideration for future releases. That would be amazing! To have collaborative sessions between multiple devices with a single Apple TV, it would make the Apple TV a video game console (Make sure you add an amazing GPU in the next versions too :heart:)! And will make the Apple TV more competitive agains the most used ones like Roku and Chrome Cast and Amazon Fire.","title":"There would be great opportunities with being able to connect multiple users to the Apple TV for my use case. I will submit a Feedback outlining that as a suggestion, but what is the best current best alternative to DeviceDiscoveryUI for implementing something like a multi-person quiz where the main interface is on the Apple TV, and multiple people in the room are submitting their input on their own devices?"},{"location":"wwdc22/audio-and-video-lounge.html#i-watched-the-create-a-great-video-playback-experience-session-and-tried-to-implement-the-external-metadata-discussed-there-to-show-a-video-description-the-chevron-shows-up-but-when-you-tap-the-title-the-description-doesnt-show-up-will-this-be-addressed-in-a-future-beta","text":"Hey Erick, This is a known issue in the iOS seed 1 builds","title":"I watched the \"Create a great video playback experience\" session and tried to implement the external metadata discussed there to show a video description. The chevron shows up, but when you tap the title the description doesn't show up. Will this be addressed in a future beta?"},{"location":"wwdc22/audio-and-video-lounge.html#im-not-sure-if-this-is-intended-or-just-a-bug-in-the-developer-ios-app-but-i-was-using-the-developer-app-on-ios-16-and-theres-no-full-screen-button-for-the-videos-is-the-new-avplayerviewcontroller-not-have-a-full-screen-button","text":"Hey Erick, this is a known issue in the iOS and macOS seed 1 builds ok, good to know. Thanks. I love the AVPlayerViewController changes BTW So glad to hear it! Yeh we think people are really going to love it. The new gestures are :fire:","title":"I'm not sure if this is intended or just a bug in the Developer iOS app, but I was using the Developer app on iOS 16 and there's no full screen button for the videos. Is the new AVPlayerViewController not have a full screen button?"},{"location":"wwdc22/audio-and-video-lounge.html#re-previous-question-my-use-case-would-be-displaying-a-grid-of-many-videos-at-once-on-screen-on-iphone-i-could-whip-up-a-prototype-that-displays-a-ton-of-avplayer-visuals-to-see-how-it-goes-but-was-wondering-if-theres-a-more-concrete-answer-here-rather-than-a-throw-stuff-at-the-wall-see-what-sticks-approach-sweat_smile","text":"Hey Michael, we\u2019d like to understand more about your use case. Would all of the videos need to play at once, or just one at a time or a collection of a few? :wave::skin-tone-3: Multiple playing at once. What I have in mind is a full-screen grid, 3-4 columns wide, with rows of squares of videos playing. (Only those on-screen would need to be playing simultaneously; audio doesn\u2019t matter here \u2013 but the idea would be to have it appear, when scrolling, as though all of the videos are all playing at the same time\u2013 as in, they begin playing when they come on-screen) (3 columns * roughly 7 rows => 21 videos playing simultaneously give-or-take / 4 columns * roughly 9 rows => 36) Wondering mainly about the feasibility; in terms of smooth playback & not eating battery\u2013 as in, does this sound within the realm of the APIs + the hardware\u2019s capability on iPhone, or would it likely involve falling back to software decoding [or just struggling] behind the scenes [Oh, more context: I\u2019m envisioning H.264/HEVC videos\u2013 whichever makes most sense performance-wise] If the answer to my question isn\u2019t at all clear cut, and is more of a \u201cas you increase the number of simultaneous playback instances, they\u2019ll play smoothly \u2026until they don\u2019t\u201d, then no worries Playing multiple videos at once can be challenging, both from a user perspective and from a performance perspective. Try to limit the number of videos so that your users don\u2019t get overwhelmed. To get the best performance, we recommend that you limit how many videos you load at a time. Let\u2019s say you load 2-4 videos at once, only start loading the next one after a previous one has become readyToPlay (AVPlayerItem status property). Also, try to re-use AVPlayers as much as possible. So instead of creating new players when a new video scrolls into view, re-use an old player of a video that has scrolled out of view (if possible). (edited) Okay, thanks. (Re users + overwhelming: these videos would be short, simple, looping clips of people\u2019s headshots\u2013 if you know of the Harry Potter franchise; imagine the newspapers in their universe, and how they have \u2018moving photos\u2019 on them :relaxed:)","title":"(Re: previous question\u2013 my use case would be displaying a grid of /many/ videos at once on-screen on iPhone. I could whip up a prototype that displays a ton of AVPlayer visuals to see how it goes, but \u2026was wondering if there's a more concrete answer here, rather than a \"throw stuff at the wall &amp; see what sticks\" approach :sweat_smile:)"},{"location":"wwdc22/audio-and-video-lounge.html#doescan-our-app-get-notifications-for-visual-intelligence-events","text":"Hey Jaime, what kinds of visual intelligence events would you be looking for? We\u2019re curious to better understand your use case.","title":"Does/Can our app get notifications for Visual Intelligence events?"},{"location":"wwdc22/audio-and-video-lounge.html#maybe-this-is-a-bit-out-of-the-scope-here-but-do-you-have-intentions-of-supporting-video-playback-from-mkv-containers-or-have-you-some-tips-how-to-add-custom-support-for-such-container-formats-efficiently-and-with-as-little-work-as-possible","text":"Hey Alexander, we do not support playing videos using the mkv container format. We are interested in understanding your use case though, could you give us some more details about it? For example, what platform would you like support for this on? We\u2019d also recommend putting in a request for this in Feedback Assistant my use case is similar, I use AVPlayer to load audio files, and my users have asked for ogg files support, which is not available out of the box (sorry for hijacking the thread) Hey <@U03KC9BKC9E>, thanks for the info. I would definitely file a request in Feedback Assistant requesting support for this. Sounds like a very useful feature!","title":"Maybe this is a bit out of the scope here, but do you have intentions of supporting video playback from mkv containers? Or have you some tips how to add custom support for such container formats efficiently and with as little work as possible?"},{"location":"wwdc22/design-lounge.html","text":"design-lounge QAs by FeeTiki what are the next challenges you're working on tackling for SF Symbols? What design program do you prefer for creating the symbol paths...Sketch, Illustrator or something else? For custom symbols, I prefer using Illustrator. But Sketch works well too Illustrator\u2019s blending tools are helpfull For variable templates that will use interpolation, I personally like to nudge points around in a font editor before going back to Illustrator, this way I get more info on the number of points and the point order Illustrator and Glyphs 3 <@U03JEKW67KJ> Interesting. I\u2019ve never used a font editor, do you have a recommendation? Getting points correct for interpolation an issue I ran into with Sketch. I\u2019ve used Glyphs3 (Glyphs App) same as Lance mentioned, but other popular font editors include RoboFont and FontLab. All have fairly good documentation (and forums) so you should be able to get up and running quickly When would we be able to see variable fonts come to the SF Font Family? It would be awesome to have this along with SF Symbols. The SF Pro and SF Compact fonts that are installed with the SF Symbols app are variable! BTW, SF Symbols 4 and the updated fonts should be going live this morning. They are :slightly_smiling_face: Out of curiosity, the new SF Symbol variable colors for values from zero-to-one (like volume and signal strength symbols)... are those implemented as a variable font axis? Variable color is not available in the fonts, unfortunately. For showing different rendering modes and variable color in design comps, we recommend using the Edit &gt; Copy Image Thanks, Paul! Fonts are live! After creating a new custom symbol, how do we add it to our app? Is it like any other SF Symbol? After you've made and annotated your symbol in the app, choose File > Export Symbol... and bring the resulting SVG into an Xcode asset catalog. Then, you can access it like other embedded images. Thank you very much! Hi! Is there any limit of layers a symbol can have? There's not a hard-set limit in the app. There isn\u2019t a limit for practical purposes in the template either. Is there a way to transfer your own custom symbols with all the settings between different Mac's or pass it to other co-designers? The file created when you choose File > Export Symbol... can be shared with other people. It can be imported back in to the SF Symbols app and should preserve all of the annotations, too! Oh nice, so it\u2019s basically an SVG that\u2019s on steroids. :smile: Or is it a different file format? Yep, File > Export Symbol will give you an SVG. If you plan on sharing it with other designers it\u2019s best to export the latest version (4.0) so that new features like monochrome layers + variable color are included Awesome. Thanks a lot for! :pray::slightly_smiling_face: We don't recommend editing the SVGs that come out of the File &gt; Export Symbol... option, however, they're just intended for data delivery. The recommended workflow is to use File &gt; Export Template... if you want to edit the shape of the symbol, bring that template back into the SF Symbols app, do your annotation within the app, and then use File &gt; Export Symbol... for delivery. Any tips for creating SVG files that will work well with this? E.g. should we avoid internal groups, etc... There are some great tips in the Human Interface Guidlines: https://developer.apple.com/design/human-interface-guidelines/sf-symbols/overview/ Check out the section titled \"Creating Custom Symbols\" will do, thank you! Is it possible to assign colors dynamically to SF Symbols rendering in multicolor mode? Multicolor rendering mode uses the colors assigned to the symbol by the designer. Use palette mode to use a dynamic set of colors. There are some parts of symbols in multicolor that are still controlled by colors you specify, however. For example, folder.fill.badge.plus in multicolor will keep the badge's colors locked to green and white, but you still have control over the color of the folder. Is it possible to dynamically assign multiple colors to the symbol rendering in palette mode? Yes. SwiftUI allows multiple foreground styles, and AppKit & UIKit allow an array of colors for palette mode Amazing! Thanks for answering I will do some reading on how to do it The API for AppKit/UIKit is configurationWithPaletteColors: which takes an array of colors And in SwiftUI what you want is the .foregroundStyle modifier: https://developer.apple.com/documentation/swiftui/view/foregroundstyle(_:) When creating new symbols, what criteria is followed? I\u2019ve wanted to make custom symbols before but want it to match with the rest of the rest of the SF Symbol catalog. Awesome, we have Thalia here to answer this. I\u2019m sure it might take a minute to compose her many thoughts\u2026 I would start with looking at the symbols in the context of other symbols. For example, if you are drawing a symbol about astronomy, you would want to looks at symbols that are lated like the moon, stars, etc. From there, I would start by defining the proportions, not too wide, not too tall, all related to the other symbols. And next I'll try to match the stroke thickness, roundness of the shapes, angles. It all depends on what you are trying to draw. Let's say you need a telescope. Then I'll probably try to match some of the angles of other symbols. It's about looking at the context and iterating to get the best results. It's also good to look at the symbol in small and big sizes, to craft the best drawing possible for all scenarios. Matching the line weight within SF Symbols is a good metric to follow. Also, referencing the scale and alignment of SF Symbols for shapes that are similar to what you are trying to make is a good place to start. For example, if your custom symbol is circular, scale it similar to a circle in SF Symbols. Are there any books you'd recommend reading regarding topics like typography, color theory, grid designs, and anything else regarding mobile UI design? Grid systems: Grid system - josef m\u00fcller brockmann The Elements of Typographic Style! The 1923 ATF Catalog and Specimen book! Interaction of Color if one of my fav books! How would you convince a product team to adopt SF Symbols that already has a wide array of their own custom icons? <@U03HLJY8FRA>? But do their symbols have variable color support?! A big advantage is that there are so many symbols to choose from so extending the library is easy. And also the way symbols are designed to work with text (font weight matching, small and large variants). Right to left support As I said in the talk last year, the kind of design that requires no additional work from me is the best kind of design! :stuck_out_tongue_closed_eyes: But really, the team has been working on SF Symbols as a way to help people make apps that look fantastic and blend seamlessly with the rest of Apple platforms. Even more importantly, SF Symbols are also designed with accessibility and inclusivity in mind. They react to things like Dynamic Type and Increased Contrast automatically, which is a big win for users of your app. Symbols offer many benefits: visual craft and integration with the system font, accessibility features including localized variants for many languages and scripts. Familiarity with users across the platforms. Smaller app footprint. I think it was last year the .square and .circle modifier were added for some symbols. Is there a chance that will become and option for all symbols including custom symbols? :grimacing: I ended up creating square variants for a lot of our custom symbols last year and it was super tedious. Hi Emile, we have this policy where we don\u2019t talk about future products. :slightly_smiling_face: It was worth a shot :rolling_on_the_floor_laughing: Maybe a seed was planted. Hi <@U03JDV4PQR0>, happy to see you\u2019re here!! Hello!!!! I hope you are well! For these sorts of things we always encourage capturing your use case + request in a bug via Feedback Assistant: https://feedbackassistant.apple.com/ Will do. Will there ever be a Clarus the DogCow SF Symbol? :D Moof? Moof. :dog::cow2: MOOF! Moof, definitely. ~ Moof ~ Moof Moof! Moof!!! Moof!!! Mooooooooof! Omg this is amazing! :sweat_smile::joy: See if you can spot Clarus: https://www.wallpaper.com/design/apple-park-behind-the-scenes-design-team-interview Moof? Found it! Love all the vintage Mac icons! Found it! So is there a clarus hidden within the SF Symbols app?!?! :face_with_monocle::face_with_monocle: The Great Moof Mystery of WWDC22 I\u2019m still looking for that <@U03HA9EC4T1>! I need glasses I think I only get this because I was randomly watching this the other day (linked to the relevant timestamp) https://youtu.be/2bewvHiWLYA?t=927 Can we use SF or SF Symbols in other, non-Apple related projects or are we limited to Apple platforms? Thanks :blush: Hi Sean, only for software running on Apple Platforms SF Symbols are for Apple platforms only. Great thanks! :grinning: SF Symbols as you have seen are very dynamic. This functionality is implemented by Apple\u2019s OSs. So besides the restriction, it isn\u2019t practical. Paul, what are some similarities between solving Rubik's Cubes and working on the SF Symbols app? <@U03HLJY8FRA> Got an answer here? <@U03J3FV12KB>? Anyone is welcome to answer this pressing question. I don\u2019t do either so my answer is that it seems like a lot of work. LOL Lots of colors, lots of moving pieces, but in the end it all comes together to make something completely unified! (how's THAT for deep) Wow Paul, real deep. :slightly_smiling_face: That sounds like a nice way to bring this study hall to an end\u2026 Not really related to the session, but are we allowed to export and use SF Symbols in a cross platform app (e.g. an app for Windows/Linux/Mac), or are the symbols only allowed for use on Apple platforms? SF Symbols are for Apple platforms only. Okay, thanks Hey! Thanks for the epic video! Is there anyway to use new symbols from iOS 16 in iOS 15 and below? Thanks :grinning: It's possible to export a new symbol using File &gt; Export Symbol... and bring that into your app to support lower deployment targets. Remember to adjust the symbol version accordingly, though! 3.0 for last year's releases and 2.0 for the year before that. Note that you can still use the latest template format (4.0) with custom symbols even when deploying to earlier deployment targets as long as you are using Xcode 14 to compile your app. Naturally, the new content features won't be present on those earlier releases, so you need to be careful about which template content features you use. Safest of course is to just export with the older template versions as Paul recommends. I\u2019d love to hear more about what it was like working on this massive update to SF Symbols. How do you go about figuring out which symbols to choose? Is there a lot of collaboration across teams within Apple for such projects? Also: amazing work! I think I can answer the second part of the question only :wink: but yes, there is definitely a lot of collaboration between teams to define what are the best symbols and design that fit best in the UIs. We want to make the symbols as good as possible, so there is a lot of iterations too! I can\u2019t imagine the amount of work it must take when the library is so huge and each symbol is so carefully crafted. Thanks for your answer! There's also an incredible amount of work that comes in from the frameworks and platform engineering sides to support all these new features like Variable Color and different rendering modes! For adding new symbols: it\u2019s a combination of supporting new platform features, adopting symbols for existing features and identifying concepts that have broad utility and application across a variety of apps and contexts. Thanks so much for your replies, everyone! I super appreciate all of your hard work! :heart: SF Symbols are underrated, they really make designing an effective UI a lot easier. Great work with the new updates! It would be awesome to see collections expanded to an in app feature to allow users to pick from a set (like in the Reminders app). Unless this already exists and I am unaware :grin: Thank you. We don't offer that feature, but thanks for feedback! Are there ways we can submit icons for consideration as additions to future updates of SF Symbols and/or is there a place we can share our custom icons with the wider developer community? Absolutely. Use Feedback Assistant. Requests get routed right to the team for consideration It\u2019s been a very helpful way to learn about what symbols developers most want. Great, thank you! I work on a lot of medical/health apps that would benefit from SF symbols. My Feedback Assistant request for a tennis ball from 2020 is still open, but I'm stoked to see new symbols for a tennis racket and a player serving! I think a differentiated experience from Feedback Assistant for SF Symbols feedback would be helpful. There would probably be a lot more involvement from the community. I may have missed this in the SF Symbols video, but I was wondering: how do these new rendering modes interact with WidgetKit rendering using the new lock screen widget layouts like .accessoryCircular etc? Great question. For consistency, we suggest using monochrome, hierarchical, or automatic, but WidgetKit may not actually end up enforcing that in the betas. We also welcome your feedback on WidgetKit and SF Symbols in general as you get a chance to use them together. We learn from you, too! Not a question, just a thanks for adding symbols that represent HomeKit objects. Was hoping for those since SF Symbols was introduced. YAY! :heart: Really means a lot to the HomeKit community as they mean most apps will end up using similar icons now. Do variable colors animate automatically when changed? Hi <@U03JBMMB10A> can you elaborate a bit? There isn't any animation when the variable color percentage changes and causes a layer to \"dim\" or \"un-dim.\" The change appears instantly. Okay, thank you. The path color will instantly change according to the corresponding percentage value. Using variable colors does not automatically animate across the visual states. However it is entirely possible to add animation to Symbol just as you would to other UI Images using the UI framework animation facilities of your choice. Right, thank y\u2019all! Thank you for the new HIG! Especially appreciate that it starts from UI elements and themes and then mentions platform considerations, instead of being branched off into different platforms only. Are there good examples that come to mind of intentionally breaking the HIG for good reason (after knowing it well)? Great question! I think that starts coming down to the customization you'd love to do for the app. For example there's no hard and fast rule that you have to use SF Symbols, or Apple Typefaces or a specific aesthetic for your app (and that's even more true for games). Think of the HIG as a great \"base\" for what you're doing, and tailor the guidance based on what makes the most sense for your users! <@U03HJ86L1BL>\u2019s also done a great job at wording the language of the HIG so we rarely ever say \"don't\" or \"never\" unless that's really true. What are the key changes in the advice being given in the revised HIG, if any? Hey Duncan, a lot of the content in the HIG carries through from previous versions. But a lot of the older content was adjusted for clarity and we removed things that no longer seemed all that relevant. It\u2019s hard to go through all the specifics but pretty much every page needed to be edited. We also started adding in new guidance for iOS 16, macOS Ventura, and watchOS 9 (though there\u2019s still a lot more coming this summer One example I recall was the Buttons page. There was content in there about placard buttons and bezel buttons but those really aren\u2019t used these days We also removed some of the content about full color icons in toolbars, for example, since that\u2019s not something modern apps do all that much of. Another key change is in perspective: Rather than starting with guidance for an iOS experience, the revised HIG encourages designing the experience first and then adjusting it as needed to run great on each platform. We added a lot of new information in the Patterns section for things like Searching or Sharing. That kind of content was spread out in component pages previously and missed a lot of important information and context when it was tied to specific controls First off, congrats on the new HIG! I love how much more comprehensive it is. What are best practices when designing an app that is cross platform? Should the app design's adapt to the respective OS' design guidelines e.g. HIG on iOS, Material You on Android, and Fluent on Windows, or stick with one for a consistent user experience across different platforms (even though it might look slightly out of place depending on the OS)? The best approach is to follow the relevant platform conventions. Many of them exist because of how the hardware functions. So ignoring that leads to software that feels disconnected from the hardware. I also wonder to what extent it's applicable to the web, especially considering seeing more web apps coming from Apple! Apps that are designed once for all platforms will never feel right everywhere It does apply, there are differences in conventions between web and apps. Some of the difference are pretty easy to address. Like not using \u201cClick this button\u201d in an app that only runs on touch screen devices ^ huge pet peeve! Also Android and iOS differ a lot in how \"back\" functionality works for example. So making sure that the design patterns follow system conventions helps quite a bit to not make your app feel like it's alien to the platform Thanks for the insight, <@U03HBJXV0TY> and <@U03DMQBFPH8>! They are really helpful. I'll definitely share them with my team who insists on having the same design across different platforms (hamburger menus and FABs everywhere!) Look I love a good FAB but only when I've designed for Android in the past! Thanks for the question <@U03J20D4Q03> hope that helps because yes, you can be 80%-90% consistent where you can but those platform details matter I'd love to know more about the process of evolving the HIG. How do you identify things that feel out of date and less relevant, as well as spotting new design trends within Apple that you feel need documenting? This is a great question. Multiple things influence this. One is our work (in evangelism) with developers. Questions that we get during this work can often spark ideas for new areas of guidance, or new details to add to pages Another thing that influences the HIG is the work of our design and engineering teams, naturally. As the platforms evolve, there are needs to overhaul pages, or whole sections, to match the direction of the platform (or multiple platforms) +1 to what Doug said! We talk with so many designers throughout the year and for this particular redesign we reached out specifically to the community to get input on where we could improve the HIG. I am resisting the urge to point out the various sections that are in need of work now, although I\u2019d like to illustrate the point I\u2019m making by doing so What\u2019s stayed the same in the HIG over the past decades? Although many details have changed, I think it's the keen focus on keeping the user experience at the center of design that's at the heart of the HIG, from the earliest days to the present. Yes the earliest HIG (I believe from 1977???) continually emphasized the human nature of computers. How computer should conform to people and not the other way around. We are still very much firm believers of this, but with more nuance on what that means ergonomically, inclusively, as a holistic experience. Were there any topics/areas that were completely, or mostly rewritten? Any specific portions that you are most proud of that you'd really like to call out? I haven't had time to dig in as much as I'd like to yet, but at just the bit I have it looks really fantastic! Miss seeing you folks btw, sad I wasn't able to make it out this year :slightly_smiling_face: That's a great question! I want to say that almost every section was rewritten to take out old, missing, or revised guidance :sweat_smile: It was a huge haul. Two sections that I personally love: https://developer.apple.com/design/human-interface-guidelines/foundations/inclusion|Inclusion: We're SO happy this page is here because it speaks not only to good design guidance but our values https://developer.apple.com/design/human-interface-guidelines/platforms/overview|The new Platform pages: These are brand spanking new and give a nice overview of \"getting started with designing for [ios/mac/etc]\" that also speak to how many people use these devices and the considerations you need to have. > that were completely, or mostly rewritten I should clarify this, I guess what I'm wondering is if there are areas where in the rewrites the guidance has fully changed overall, rather than clarified or revised. The platform pages look great btw and I would imagine it's going to be super helpful for a lot of folks to have those as starting points :heart: Because we were working with content that ranged in age from months to more than a decade (in a few cases), I'm certain that there were places where in rewriting, we made significant changes to the guidance. But honestly, I'm not sure I can point to specific ones offhand! :face_with_monocle: Hi, nice work on the redesign, I've been going through it this morning! I have a question about color. On this page: https://developer.apple.com/design/human-interface-guidelines/foundations/dark-mode|https://developer.apple.com/design/human-interface-guidelines/foundations/dark-mode there's the advice \"At a minimum, make sure the contrast ratio between colors is no lower than 4.5:1. For custom foreground and background colors, strive for a contrast ratio of 7:1, especially in small text.\" I aim for high contrast as much as possible, but 4.5:1 is pretty limiting, and I don't think Apple always follows this rule themselves. Specifically I'm wondering about the systemColors such as systemOrange, systemMint etc, many of which have quite low contrast against white (i.e., in Light Mode). What is the intended purpose of these colours? Is it meant for text/SF Symbols? Because if so, systemMint comes in at 2.02:1 against white. Should we ever even use systemYellow? Or can we just rely on users turning on Increase Contrast if they need to? But maybe someone doesn't want to have to increase contrast for everything just so they can see that darn yellow text. Any insight on the intended use of these systemColors would be greatly appreciated. Thanks! Awesome questions Jessiah! I could take the next 45 minutes trying to answer. :slightly_smiling_face: In short, the rules about color contrast are a bit tricky, it\u2019s not black and white (forgive the pun) Higher contrast is always better but there are mitigating circumstances that make it OK to have lower contrast. For example, text size. The larger or bolder the font, the easier it is to read given the same color value But a color choice that works for large type can be too low contrast for smaller text. The same applies for icons or glyphs (symbols): Finer features are harder to naturally harder to discern so boosting contrast is useful As for system colors, they don\u2019t all work equally well in every circumstance. Orange or mint on a light background probably isn\u2019t the best choice. Definitely not as legible as blue or purple Yeah I was wondering about the Notes app, it looks like some darkening is being applied to the tint colour in this case, rather than using the default system color? to I\u2019ve worked on products where I had to incorporate colours that are notoriously difficult to have high contrast - like teal/green. In those instances i\u2019ve provided darker versions, that are perhaps not so aesthetically pleasing, but pass AA/AAA to assist visually impaired users once they enable accessible system settings. It has worked for me :heart: Yeah for our app which uses orange tint, we\u2019ve used our own custom orange for light mode, and only use the sytemOrange for dark mode My personal guide when designing is to make all elements that need to be legible and clear have strong contrast. Details like a divider, for example, can be lighter to convey less prominence. But text should always be readable and accessible. I also feel like I could spend the next hour speaking about this :sweat_smile: so GREAT question. As you might know, the 4.5:1 contrast ratio comes from WCAG 2.0 guidelines. While WCAG 3.0 is still not officially \"out\" they have an improved way of calculating contrast that I find incredibly interesting: https://www.w3.org/TR/wcag-3.0/#visual-contrast-of-text nice I\u2019ll take a look :thumbsup: I\u2019m imagining a future accessibility feature that allows you to increase contrast per colour :stuck_out_tongue: :wink: or colour range Hahahahah, yes <@U03J1EHL4KY> Providing high-contrast colors in the asset catalog is a good thing. :slightly_smiling_face: We do have some technical solutions after all. And of course the system colors provide high-contrast colors for free When filing feedback related to the HIG in Feedback Assistant, should it be under the \u201cdocumentation\u201d category or \u201cdesign resources\u201d? First of all, thank you SO much for filing feedback!! For the HIG, please use the \"documentation\" category; for issues or requests related to Apple Design Resources, please use the \"design resources\" category. How do ADA-winning apps balance the best practices and standards in the HIG with the novelty that makes them...well...ADA-winning? Well you hit on the two, sometimes opposing, forces that define ADA winners In my view, an ADA winner has to embody the spirit of the HIG, even if there are details that might not conform to the letter of the HIG But an equally important thing is the voice, or personality, of the app The categories that we introduced last year (2021) try to add a bit more substance and clarity around the qualities that define winning apps <@U03H3HNAGSK> Can you elaborate on the voice/personality please? Yeah, fair question I guess I just see those terms as representing two things \u2014 (1) a cohesiveness across the app \u2014 the app feels like a single experience, and is internally consistent, and (2) the app has a character or look or feel that is memorable Ohhh I think I gotcha, like how Carrot Weather uses satire and the robotic voice to strengthen the personality of the app\u2019s narrator? Yes there's being consistent with the platform experience, and then there's taking that to the next level by differentiating your app/game as a stellar example of one of our categories. That means you don't necessarily need a visually-heavy custom branded app but you offer something by way of your design/UX that sets you apart (I'm thinking Slopes here as a great example of this) Could you elaborate on any user research (quantitive or qualitative) processes the team uses to shape HIG guidelines? (These questions are good!) We meet with developers to ask them for feedback on existing content (qualitative). We also regularly talk with developers throughout the year about their apps and, in those conversations, common questions come up that we realize we haven\u2019t really answered in the HIG (or videos). So those turn into ideas about how we can improve the HIG. And the HIG is really a massive collective effort involving design and engineering teams throughout Apple. This HIG is the product of internal experiences where people had misconceptions or questions during the design process that we anticipate designer and developers will have once we ship new features. Does the team do any old school, Bruce Tognazzini-style quantitative experiments when designing new features? I\u2019m thinking of the equivalent of Fitts\u2019s Law, but for touch. <@U03JCS2C03Z> Depends on the team! We always nerded out about that and other things (catmull rom) on the Prototyping team, but other teams keep their process more intuitive/empathy-based vs. scientific Is the latest version of the HIG the first time I've seen Apple adopt sentence case for headings?! :smile: Ha! Nice catch :smile: We also use sentence-casing throughout the developer app Love the attention to detail! Also WWDC videos It\u2019s the future! I\u2019m sure this is in the HIG if so :grin: but figured it may be worth asking, are yall changing recommendations for when to use title-case vs sentence-case? :eyes: A web based HIG is great as documentation but right now\u2014wanting to read the entire revised HIG\u2014I want to reach for an ePUB that would keep track of where I\u2019m up to, and lead me linearly from start to finish, to ensure I read the whole thing. Is an ePUB available, or is there another solution for this use case? Thanks for the question! Could you submit this idea using http://feedbackassistant.apple.com ? Right now there isn't mostly because we've found folks stop by the HIG because they're looking for specific documentation vs. reading it like a whole book. Yes, I agree! I would love it if there were an ePUB or Apple Books version of the HIG, like the one for the Apple Style Guide. Great for offline and bedtime reading! A physical HIG book would be some nice swag! I know it's a living document though :grinning: HIG used to be available in iBooks! Back when it was separate for OS X vs iOS. Still have em in my Library. It\u2019s reallllly nice being able to use the search feature. And I added the original HIG from a PDF too :heart: I have some of the old HIG books in front of me! FB10108496: Provide a mechanism to keep track of progress in reading the whole HIG ^ in which I quickly argue for either progress indicators and \u201cNext\u201d links at the bottom of each page on the web version, or an ePub version, and do so quickly in the hopes of linking the FB number here before the lounge closes. :partying_face: <@U03HMCT187R> I included the superior search options in an ePub btw. Great point. Not a question but I LOVE the new HIG redesign!!! Not an answer but THANK YOU! - from all of us Hi there! Amazing work on the design guidelines. How can I measure contrast in my apps? Thank you so much! There are good tools online for measuring color contrast. Searching for \u201ccolor contrast calculator\u201d will return good results https://coolors.co/contrast-checker/112a46-acc8e5 https://colourcontrast.cc https://contrast-ratio.com Thank you very much, and again, amazing work on the HIG! You're listing the nice looking ones \u2013 I've always used this: :sweat_smile: https://webaim.org/resources/contrastchecker/ LOL Yes, it\u2019s true, I only picked the nice looking ones. <@U03DMQBFPH8> so funny, that\u2019s the one I use too It\u2019s about design after all:sweat_smile: If you use Sketch, the Stark plugin is a great in-app option Xcode has a built-in Color Contrast Calculator. To open it: 1. Open the Accessibility Inspector by choosing Xcode > Open Developer Tool > Accessibility Inspector. 2. In Accessibility Inspector, choose Window > Show Color Contrast Calculator Cluse is also a great one I love this one. It sits in the menu bar so you can use it anywhere. https://apps.apple.com/gb/app/contrast-color-accessibility/id1254981365?mt=12 <@U03J1EHL4KY> Same with XD I'm really hoping more tools start adding the LCH (lightness, chroma, hue) color space, it makes trying to keep similar contrast across different colors much easier since it (mostly) keeps the same perceptual contrast while just adjusting the hue. Good article on it if anyone is a color nerd like me and wants to dig in more: https://lea.verou.me/2020/04/lch-colors-in-css-what-why-and-how/ not a color nerd but often feel like i should be :joy: Hi, thank you for update for great HIG! Is there any design files or design system in Figma / Sketch? <@U03HBJXV0TY> <@U03H3LD2M2T> Will be released on this page? https://developer.apple.com/design/resources/ I hope to be also in Figma. Japanese designers love to use Figma now:pleading_face: The Sketch libraries are so cool - I love poking around in there to see how things are set up. For Figma, https://www.figma.com/@joey|Joey Banks has become the unofficial supplier of iOS UI Kits there :wink: So glad you shouted out Joey! He's doing great work :slightly_smiling_face: We currently don't support Figma at this time but updates to the Sketch and XD libraries and templates are coming later this summer Thank you Linda, I\u2019m waiting for release new ones! Hi all! The HIG for watchOS subscription paywall show buttons for T&C and Privacy Policy. But we cannot open these in a website like on iOS. What is the recommended way to show these, potentially very large, text elements on the Apple Watch? Hi <@U03JMMN8659>. This is a great question. The recommendation at this point is to display the T&C and Privacy Policy text in (likely very long) modal sheets or detail views on watchOS If you haven\u2019t already, can you file feedback for this? ok, thank you! :pray: will do How long has the team been working on this huge HIG update? The bulk of the work started late 2021 and ran to last Sunday or so! :sweat_smile: Seriously, this project has been our hearts for a long time and we're ecstatic to deliver this update. hahaha, yes we've been working on it consistently for about a year now! Firstly, congrats on such a great update to the HIG! :clap: Throughout the OS, there's often times when a temporary sheet appears at the bottom of the screen. For example, when connecting your AirPods to the iPhone - I've made my way through the guidelines and can't find any write ups about that view. Is this view actually just a \"Medium sheet\" that is styled differently? I'll thread a screenshot for context Hm\u2026 not sure if my reply came through. Posting again \u201cHi Ryan, yep, a medium height sheet is the closest system-provided element.\u201d Thanks <@U03HBJXV0TY> :ok_hand: Curious what your pov is on link buttons, specifically buttons embedded within text content on native. There are some cases where this type of treatment can make sense and then others where it feels like we\u2019re just pulling over from web. Do you have any formalized process around when to use and not use this treatment? Good question! My sense is that links inline commonly get overlooked because (a) not expected in apps and (b) the color distinction is often too subtle. In general they should be avoided. But there are cases where it can work. One that comes to mind are \u201cMore >\u201d links at the end of a paragraph of text where you navigate to a child view to see more. Or expand a partially revealed block of text. But it\u2019s almost always more clear to use a proper button or table row with a chevron. Probably the biggest issue with link style buttons, especially when presented in context of other text, is that the tap target size is too small (not 44pt tall) so tapping accuracy is compromised Thanks for the response. Right, we\u2019ve certainly argued from an accessibility perspective - tap targets and contrast. The challenge is (as component devs/designers) to come up with a general alternative. We\u2019ve seen push back that there is an efficiency to associating an action with a verb in a sentence. Pulling actions out to separate buttons can end up elevating them. T&C or legal section, for example. I wanted to say that I really love the unified experience of the supported platform the updated HIG adopts. I have recently come across a component that the native iOS Maps app uses to filter/refine searched results which I can't seem to find reference of in the HIG. I am not sure it if is a combination of a segmented controls and pull-down buttons. Is this filtering component something that will be added to the guidelines in the near future? This is the view I am referring to in iPadOS for Maps. Hi <@U03JL795TE2>. I think I know the component that you\u2019re talking about \u2026 It\u2019s the bar below the string? With the filter button, and the toggles? Hi <@U03H3HNAGSK>, yes thats correct. So the filter buttons. I had also noticed during the WWDC Keynote on Monday that the updated Home app on iOS adopts a similar component with different functionality. So my question expands to this as well. Ahh, I see. Toggle-able filters / buttons. Something like this may be the beginning of a pattern that is adopted across Apple apps, or these may be one-off solutions within particular apps. From a HIG POV, there is a bit of push-pull with this. If it turns into a pattern, and if that pattern is formalized as a specific element in a framework, that is typically the criteria for arrival in the HIG This is an interesting one, though. Thanks for raising it. Appreciate the feedback on this <@U03H3HNAGSK>. Thanks for asking, this is a fun one Makes me realize that, for a concrete control like this, the HIG is relatively conservative; something really needs to be a component that is defined in a framework to get guidance at this level. But for bigger picture, experiential things, the HIG is a bit more predictive? Or proactive? I've seen this type of control over the years in quite a few apps. Here is an example from Apple News on iOS 15 without the borders but works the same. Think most people understand how to use this control. Interestingly this has been changed in iOS 16 Beta 1 to a dropdown type control. Sorry correct screenshot. Any design guidelines for EDR color on Apple platforms? Mostly we just want people to add color profiles to their images. If we could do that, it\u2019d be a huge win. We don\u2019t really have guidance specifically about EDR in the HIG. Might be some good documentation elsewhere on http://developer.apple.com|developer.apple.com but I\u2019m not familiar with it. Color profile tagging is critical, esp for P3! AFAIK, images with EDR profiles don\u2019t show up in EDR across the OS, though videos do. Photos does do EDR for photos, though, so it\u2019s clearly something where thought has gone into it. What prototyping app would you recommend? Such a good question. There are lots of good methods and tools out there that there\u2019s really no one good answer. That one you're fastest in! It depends on what you like OK, that was a good answer Linda Currently using Sketch and XD, Figma is something I want to try :eyes: Principle, Figma, Sketch, XD, Keynote, paper and pencil,\u2026 Funny you say Keynote, that's actually what I began designing in :joy: Principle is underrated! They\u2019re really all good in their own ways. We use Sketch a lot at Apple. XD. Keynote <@U03HBJXV0TY> would the paper and pencil prototyping be used in the initial phase of the design process at Apple? Xcode and SwiftUI may also be great for prototyping. <@U03JL795TE2> you bet! Some people love paper sketches first (me) others go straight to SwiftUI because they're lightning fast in it In your guidance around colors, have you all started digging into color spaces like LCH and perceptual color contrast much yet? Maybe not something you can discuss yet but it's a super interesting area and trying to choose colors with a matching human perceptual contrast across the palette is super useful and something I'd love to see Apple push as well. I also think it's a great area to explore! I posted this in another question but the WCAG 3.0 perspective on color contrast is fascinating and something that addresses our issues with how color contrast is calculated now It really is fascinating stuff. And how it's looking to take into account type size, boldness, etc. into the calculation in a much more nuanced way than just large or small type. Would be awesome to see Apple push some things like this outside of WCAG as well. eg https://blog.datawrapper.de/color-contrast-check-data-vis-wcag-apca/ Do the HIG and Apple Style Guide influence each other? Yes! <@U03HJ86L1BL> will be here in a minute to say more. :wink: If you\u2019re interested in knowing more about writing for apps, we have this session coming up tomorrow: https://developer.apple.com/wwdc22/10037 LOL, thanks, <@U03HBJXV0TY>! Yes, definitely: Even though the use cases differ for these documents, both try to stay aligned with the other. It\u2019s a super fun detail that the Design page on http://developer.apple.com|developer.apple.com changes colour. How many different looks are there? I refreshed and counted 6 :laughing: Some different looks. Yep, six colors Ohhh\u2026 of course! :wink: They\u2019re meant to be read in order matching the old 6 colors logo. Tells a little bit of a story about making a button. Love it! Outline views are in the HIG as being macOS only but the need for a nested outline can still occur on iOS. The HIG just says not supported but that's from a control aspect. I've implemented them by using different list cell layout to provide a nesting but would welcome other suggestions. https://developer.apple.com/design/human-interface-guidelines/components/layout-and-organization/outline-views|https://developer.apple.com/design/human-interface-guidelines/components/layout-and-organization/outline-views Here's an example of these home-grown outlines (note there's no collapsing, which I'm mulling over). <@U03HJ7LRK43> Howdy! This is a great question \u2014 we're aware of the need for better guidance around outline views on platforms other than macOS, and it's on our radar for an update over the summer. I\u2019m a big believer in Human Interface Guidelines. However, I\u2019d like to have some sincere feedback. 1. The all-new version seems dominated by texts. The older version contains a lot of illustrations and animation along with the content, and maybe it would be better to add more images and animation in the new version. 2. I\u2019m so confused that even though this is an all-new version, there are still some images coming from macOS 10.x, with the design from Yosemite. Please update these images to the design after Big Sur. Hi <@U03HMCH9D5M> - appreciate you feedback and we full agree. We have a huge list of to-do items when it comes to making illustrations for the HIG. We\u2019re a small team and there\u2019s a lot of pages. But this is a top priority for us. As is fixing all those old screenshots! Yup we feel the same way! Luckily now that the redesign is out we can focus on these tasks and more exciting things we're planning! :slightly_smiling_face: It bothers us too. :slightly_smiling_face: Appreciate it !!! Do you follow an 8-point grid? For example, Table row padding would be 16 on either side, but 44 in height? I understand tap states need to be large enough, but why not 48? Hi <@U03JK302NRH> - Yes, an 8-point grid is used in many circumstances. However, there are some exceptions. For example, layout margins are 16pt in compact size class but 20pt in regular\u2026. 24pt would\u2019ve been too much. Sticking at 16 would look too thin. The grid is mostly about width, not height IIRC, the decision about 44pt goes back further than the use of an 8pt grid for layout. Thanks for the insight <@U03HBJXV0TY>! With dynamic type is it expect that all text based content on the interface will scale? Is there a time where that's not appropriate such as in button containers? Great question. It\u2019s really not possible for ALL text to scale in some cases. Tab bars, for instance, don\u2019t scale because text would truncate immediately which defeats the purpose. And there is an AX setting for showing the tab icon and label in a bezel on tap. The general approach is to make sure all text in the content area / scroll views scales. Awesome, thanks for the clarification Mike :thumbsup: Text in fixed height elements (like tab / toolbars) is tricker I often struggle with trying to minimize the amount of submenus, while not making the interface look too bloated (especially since I work on photo editing apps that get somewhat complex). Do you maybe have some thoughts on how to balance those things (how many controls there should be visible at once, how many submenus deep you can go, etc)? Hi <@U03J20RJQ2X>. Not sure whether a Slack answer will even scratch the surface of this question, but I have a couple of quick thoughts I share <@U03J20RJQ2X>'s pain, having a rich document editing experience within an iPhone interface (1) The person. Is this photo editing for \u2026 professionals? influences? some specific person? \u2026 I really would start here, even though it\u2019s the obvious designery thing to say, because I think it helps shape what level of detail and density is appropriate It gets especially difficult when you try to cater to both, making the app easy to use while also giving advanced users more tools. But that\u2019s for sure a good place to start, professionals are used to different kinds of UIs as well (2) The platform will dictate your heuristics around detail and density. Designing for Mac and iPad, I would push the density, and try to deliver sets of controls as completely as possible \u2014 i.e., i\u2019d try to avoid submenus, and where I had them, I would try to make them \u201csimilar\u201d or at least similarly labeled across controls How do I know when a small button is ok? :smile: Do I just need to ensure they have appropriate clearance of 44px all the way around to meet the guidelines? It\u2019s OK to have smaller button but you definitely want to have a tap area that extends to 44pts in height So, yes, padding can be a factor In general though, a physical background height of 44pt is a good idea. Even if the tap target is taller, users don\u2019t know that. So they\u2019ll naturally slow down to aim accurately Sparked by a previous thread, are yall changing any recommendations for when to use title-case vs sentence-case capitalization style? Great question! Although the guidance is to use title case in components like button labels and menu items, it also comes down to stylistic choices for the text content within your app. 100%, <@U03HJ86L1BL>! And when it comes to the content within your app, it \u2014 and your style guide \u2014 is going to evolve over time. As folks in this channel noted, we decided to move to sentence case in the HIG after many years. :slightly_smiling_face: The most important thing is to be consistent \u2014 if you\u2019re going to use title case for your content headers, make sure you\u2019re doing that everywhere in your app. If you change that style, make sure you\u2019re adopting it across the board. And final tip: Even if you largely follow the paradigms of a guide like AP or Chicago Style, it\u2019s super helpful to maintain a separate personal style guide for your content \u2014 that way, you have a great reference document. Is there still guidance on designing the navigation graph for an app? I cannot find it in the new layout. Hey <@U03JQEX9MGS> I\u2019m not totally sure I understood your question. Do you mean the diagrams we used to have? May not answer your question, but we have a video about Navigation this week you might want to check out. https://developer.apple.com/wwdc22/10001 I will watch that video! I was referring to the explanation of the different styles of navigation graphs (like event driven). That page helped a lot a couple of weeks ago when trying to explain why app navigation should be treated different from websites. More of an abstract question: Taking the Fitness app as an example, the calendar view tends to stick out as a completely separate design pattern. Are there any guidelines on when you would want to implement charting in a calendar as opposed to a tabbed interfaced that grouped by day/week/month etc.? That's a great question! The Fitness app is focused on a daily activity like closing your rings. Since this is a top-level goal of the app, the calendar view makes it easy to see your successes through time. The Health app mirrors the same information but in a longitudinal form, where you can see larger trends. Yup that makes sense! I guess what I'm trying to wrap my head around is finding the medium of digest-able information and too much but that's the joy of design! Each of these approaches has its advantages. The calendar view excels at showing streaks and achievements relative to the day of the week. Conversely, the charts in Health aggregate away those daily details into larger trends. Yeah, it\u2019s interesting to me that the calendar format is sort of perfect for the Activity Rings in that they can be displayed neatly in rows / columns on a calendar grid I would say the calendar is quick overview but the trends, such as in Health, is where the power of charts come in. Is it possible to decide the number of decimals if the numbers on one of the axes are really close to each other, e.g. a range from 0 to 1 with 5 lines A swift charts question I'm assuming! The framework can truncate labels for you to fit the space, but otherwise you could write in some logic to format the labels according to the space you have available. What are the best practices for using color in charts (such as considering accessibility for color blindness)? Great question! We talk about these best practices in the \u201cColor\u201d section of our companion talk, https://developer.apple.com/videos/play/wwdc2022/110340/|Design and effective chart At a high level, here are some key concepts to think about when using color in charts: \u2022 Use color to enhance \u2022 Consider associated meanings \u2022 Balance visual weight \u2022 Choose distinct colors \u2022 Respect system settings You can also find general best practices around color in the Human Interface Guidelines: https://developer.apple.com/design/human-interface-guidelines/foundations/color/ Please tell me if I'm missing important nuance here but would it be fair to boil the different scales down to: Macro scale = highlighting all regions of all marks Medium scale = highlighting same region across all marks Micro = highlighting a specific region of a specific mark That sounds right, as long as with Micro the highlight is about a single data point (since a chart mark could represent multiple data points). Are there any samples or repositories we can look at? for SwiftCharts* You bet! Here\u2019s sample code for implementations with Swift Charts https://developer.apple.com/documentation/charts/visualizing_your_app_s_data It\u2019s got our same beloved theme on pancakes :pancakes: OMG YESSSS THANK YOUUUU For apps that have multiple ways/formats to share, (say gif, video, images of selection, HTML) that have wildly different UI flows - how would you recommend exposing that in the UI? As far as I can tell the share button only really works for apps with single-format export Hi <@U03JB8S4SJ0> Innocent question: Is this about sharing a chart, or just sharing in general? Sharing in general. Sorry, are we only talking about charts right now? What considerations should be made when designing a time-based chart like Safari's inspector timeline? One of the most important considerations is that each mark has a start-time, and end-time and a duration. So it will be important to have a UI that reveals all of these pieces of data for each mark. Hello! This may be considered off topic, apologies in advance: can anyone recommend an alternative to \u201cflow charts\u201d for SwiftUI code? It was great to show non-programmers how the code execution should work. But since SwiftUI depends on state, this goes out the window. And I\u2019m not sure if UML is the way to go either. Any comments would be appreciated, thank you! Hi <@U03J21EKNSE>! This is a really good question, and it\u2019s not completely off topic. I would suggest that you bring it to the Swift UI lounge https://wwdc22.slack.com/archives/C03HX19UNCQ Hello Doug L! Thanks for letting me know; relief to know it\u2019s not a bad thing to ask. If you are referring to the Swift UI Lounge happening right now, I have posted it there too. No bites yet; still holding hope! Thanks again. Thanks, yeah. The folks in here are more focused on the design side of things, so you\u2019ll have better luck with the \u201chow to\u201d aspect of your question Copy, understood! Thanks for your time, and have a good afternoon! With Swift Charts, is it possible to reproduce the scrollable chart we find in Health App (where the chart is updated as we scroll the content)? I know we can use gestures on the chart to highlight some marks, but what about pan gesture? There was a similar question in <#C03HX19UNCQ|> and they called out that the views would work the same, just need to give proper widths. Hi Axel! Any Chart can be placed in a scroll view, but I'm guessing you are interested in the \"pinned\" Y axis behavior. You should be able to achieve this with a workaround, that is by ZStacking the Y axis on top of the scrolling chart. Please file a radar if this is a feature you want to get added to the API! Thanks <@U03H3193G3H>. Here is the result I want. Gottit. We'd love to make this easier for you, feature request this please! I'll thanks! you're welcome, thank you! Will there be some samples on building good-looking Watch Fitness apps like in the WWCD21 Session? I would love some samples showing off the design techniques used in this years Watch Workout app. Hi <@U03KJSLF04Q>. Very much agree that the new Workout app is quite inspiring. There isn\u2019t currently a code-along for it, but we will try to continue to update the HIG, Design Resources, and developer documentation with new patterns and techniques Would love that! What's the best way to design crown rotation behavior in my app? I found it hard to replicate the smooth flipping between different pages for example as the API only gives a float. Are there any samples on well done crown rotation integrations? hello again! I would bring this question to the Watch labs if possible Will do thanks! But to be clear, it sounds like you\u2019re trying to replicate the behavior of the in-workout UI for the Workouts app \u2026 Is your watch app made with SwiftUI? Or WatchKit? SwiftUI. Workouts is one example. Like selecting the currently highlighted metric (my implementation works but is not as \"satifying\" as the Workouts app), but also other views that might implement \"pages\" themselves. Is it possible to zoom in on a SwiftUI chart? I.e., pinch and zoom to see details closer up? Could you tap on a bar (or other mark) to open a new view, for example, showing a list of the items presented in that bar? Hi <@U03J1EHL4KY> great questions! (1) This might depend on what behavior you're looking for. There currently isn't a pinch + zoom API built directly into Swift Charts, but you could capture a the gesture in a chartOverlay and respond to it accordingly by manipulating the scale / data in the chart. (2) Yes! Absolutely. Using a chartOverlay and the provided ChartProxy to read the data point being tapped. There are some great examples of these in the Swift Charts sample app ( https://developer.apple.com/documentation/charts/visualizing_your_app_s_data ) Check out the \"Swift Charts: Raise the bar\" talk too! Donghao explains more there. https://developer.apple.com/videos/play/wwdc2022/10137/ okay awesome thank you! you're welcome! thank you for the question is there any sense in adding animations to Charts (w/o user interaction) or its a bad practice? When used properly, animation can certainly enhance your chart\u2026 even when not tied to user interaction. For example, to indicate that a chart has updated, animating a point or growing a bar can be a helpful and delightful addition. Note that when adding animation, it\u2019s also important to ensure they adapt as needed for a device\u2019s \u201cReduce Motion\u201d accessibility setting Is there documentation about all the option types for open ended requests? I was curious about Person in particular Hey <@U03J8GWEFU7> \u2026 It would help to know a bit more about what you\u2019re looking for related to Person Lynn mentioned that using these common options has Siri NLU benefits. So i was curious what data was backing this. Does Person referred to the customer\u2019s Contacts? Or something else Gotcha. Trying to find a reference for this. Is this section from the transcript relevant: > The App Intent framework does provide a set of common options for these open-ended requests, such as numerical values, dates, or time values. If your required parameter aligns with one of these, definitely select it. This will allow you to benefit from certain built-in dialog and visual patterns, as well as Siri\u2019s natural language understanding as it pertains to these types of information. If not, you can use a custom entity. I am also interested if there is anything Person related but can\u2019t find anything in the transcript re that being a supported common option type. here\u2019s the screen grab where I noticed it Here\u2019s the documentation on IntentPerson : https://developer.apple.com/documentation/appintents/intentperson <@U03HJ5M01M0> has entered the chat. Siri has built in, baseline NLU for many of these types. For example, for an integer, the user might say \u201cone dozen\u201d. Siri will parse this as 12 for you automatically Similarly, for IntentPerson, Siri has some baseline understanding of what is a first name, what is a last name, etc. Siri will attempt to construct an IntentPerson for you if you do an open ended value prompt for an IntentPerson type Is it still posable to have user inputs in initial Siri request for media requests? Good question <@U03JBMMB10A> \u2026 <@U03HJ506BGT> is on it Yes, you can do an open-ended request, disambiguation, or parameter confirmation before completing your intent. I'm wondering what the maximum (sensible) number of values is for a parameter. In my app, users attach notes to people. It'd be great to have a shortcut for \"New note for <Person Name\"... but an active user could easily have 1,000 or more people in the app that the note could be attached to... which it sounds like isn't the kind of \"fixed list\" you have in mind? So... the 20 top favorite people, or similar? Right, for lists of this size and for a list of contacts, you would want to use an open-ended request, which would also allow you to use a person entity. Ok right, thanks. It wouldn\u2019t be suitable for a dynamic parameter, because those contact names would become an array of strings, which limits the flexibility people can use to ask for them. Reading in the other thread about IntentPerson parameters. For an app that has access to CNContacts, is there a way (other than matching on name) to link an IntentPerson to the CNContact that presumably underlies the source of that information? <@U03HJ5M01M0> can you take this? right now you\u2019ll need to match on name. Please file a feedback request! We\u2019d like to hear about your use case and look into this more Could I create an iOS app that once downloaded from the App Store is never really meant to be opened by the user? Instead it would include App Intents, and primarily be accessed via Siri with just dialogs & custom snippets? It\u2019s an interesting question <@U03J8GWEFU7> \u2026 It\u2019s a paradigm we\u2019ve seen on Apple Watch with watch apps. Some watch apps are really just ways to deliver complications to the watch face, and rich notifications on the watch itself However, those watch apps also display the information that is delivered in the complication and notification. So anyway, the app matters :muscle: But technically yes, you can build an app that\u2019s focused on providing App Shortcuts, but when someone taps on your app icon you\u2019ll still have to have a viable experience that explains what it does. Also, if you support dynamic parameters, your app will need to be be opened for you to update the order. So I could make an app that\u2019s intended primarily as a container for Siri first App Intent interactions, and the app itself could just provide some context that explains the Siri functionality? You can build that, but if you want to manage your dynamic parameter and people are never opening your app (because it doesn\u2019t have functionality in itself), you won\u2019t be able to re-order or update your parameter values. Got it, that makes sense. I\u2019m just trying to get a mental model for what\u2019s the minimum viable app to create for Siri! This was so helpful How to create a timer shortcut with shorter phrase for our app if the system already have one, for example, Set a timer for 5 mins, how can we tell the system to use our app to setup the timer instead of the system clock app. Notably, App Shortcuts will need to include your app name, so that will help distinguish your phrases from standard Siri phrases. (i.e. \u201cstart a <app name> timer.\u201c) With a particularly strong brand, you may even be able to skip the word timer. (i.e. \u201cStart a Super Special Countdown\u201d, assuming \u201cSuper Special Countdown is the app name) A note though! You wouldn\u2019t want to use \u201cfor <time value>\u201d in your invocation phrase, because there are infinite possible replies. That\u2019s better as an open-ended request, for a Duration <@U03HJ5M1UV8> I want to check I understand this. Is the distinction here that \u201copen-ended requests\u201d are always by definition follow-up requests after the invocation phrase? I wasn\u2019t clear on this till now. There are a few different types of follow-up requests after the invocation phrase, and open-ended requests are one of them. Others are parameter confirmation, intent confirmation, and disambiguation. But these are distinct from the dynamic parameter that you provide as part of your initial invocation Working on the prototyping team seems like such an incredible thing to be a part of. Can you tell us about your journeys to becoming prototypers at Apple? And what do you look for in potential candidates for your team? A lot of us some mix of backgrounds that don\u2019t fit neatly into the typical design / engineering teams. Are there a lot of folks on the team who have a hard time identifying themselves as either designers or engineers, but actually both? I taught myself programming outside of school and dabbled in a number of open source projects and the like. Realized I enjoyed psychology / cognitive science to learn more about how people think and approach technology. Studied Cognitive Science, HCI, and Communication Design in school. Had some design jobs but eventually accidentally found this team where I got to use my programming skills too! That\u2019s amazing, thank you for sharing Julian :slightly_smiling_face: That\u2019s amazing! A teammate was at an HCI conference (TEI '17) in Japan that I happened to have work in. He passed on my name to the recruiter. I studied Design and HCI in school, and then I interviewed! That\u2019s awesome, Marisa! I studied computer science, spent some time in HCI academia, and worked for a few startups before joining Apple. For me making delightful interactive things has always been a single pursuit, and the boundaries between things like \u201cdesign\u201d and \u201cengineering\u201d never made too much sense to me. My teammates will have different answers but when I look at candidate portfolios, I like to see inspiration from fields beyond design & tech, a general sense of curiosity and playfulness\u2026 and just weird, unexpected, unique things! (even if they\u2019re not the most polished) I completely agree, Guillaume. And I super appreciate you all taking the time to answer. Thank you! I was a fresh graduated from ECAL in Lausanne Switzerland, where I studied interaction design. A few months after that, Jeff our current Manager saw my bachelor graduation project on a blog and then reached out. It was such a suprise! I was super excited and curious to know more about the team and I end up joining in 2018. It was great that even though I had not much professional experience, the team was interested in my portfolio. When we are looking at candidate, I think we are interested about ideas and how the candidate communicates them, even if you have short resume like I did back then. Thank you <@U03JGA79CQZ>! That\u2019s super inspiring to read! Hey team! Im curious, when first designing a prototype of a new product, what would you say the ratio is of looks to functionality? Are looks secondary or are they the focus from the get go? Thanks :) I think the main thing is to think about what questions you are trying to answer by building one (or several) prototypes. Are they questions of technical feasibility, product usefulness,\u2026? etc. Looks for the sake of looks are rarely worth spending lots of early prototyping time on, but sometimes different aesthetic directions or visual metaphors are definitely things you want to prototype! I\u2019d add that the key balance to strike in prototyping is to make the least amount you need to make to still learn something. Sometimes that means things have to look very real so that you don\u2019t distract from the part you\u2019re trying to learn. Interesting, forsure functionality and user experience is so important - although I do find myself holding my AirPod Max\u2019s and just staring at how beautiful they are more then I listen to music :rolling_on_the_floor_laughing: Do you guys have separate teams for functionality and then a team for looks, or is it all entwined? <@U03J52JBK0C> haha, sometimes when I\u2019m prototyping/evaluating a feature, I\u2019ll find some unrelated annoyances, and I HAVE TO FIX it. The Fake it Till You Make It Talk from WWDC 2014 was incredibly influential in me becoming a designer (also now have fond memories of toast). What has inspired you to join the Prototyping Team / what continues to motivate you? Those sessions were awesome!:round_pushpin::bread: IIRC from the presentation, they used Keynote as a prototyping tool. Is SwiftUI used by the team for prototyping? Glad you like the session! (back when I was on the team :sweat_smile:) Perhaps <@U03J52JBK0C> <@U03J0DULM0V> could add to your question You can get a set of templates for Keynote https://keynotopia.com/ or the ones on https://developer.apple.com/design/resources/ We\u2019ve used SwiftUI a little bit, yes. We still use Keynote a lot though! How does your team go about quickly prototyping advanced interactions? I have lots of ideas for interactions I want to test but I feel my momentum is burnt out by the time I get everything set up in, say, a SwiftUI project. Finding a way to fake it! We talk about this in most of our old WWDC sessions, but Prototyping for AR (2018) has some good examples of finding clever ways to prototype that don\u2019t involve code at all. There are ways to fake things with paper printouts, or with clever video capture. But often doing some simple Keynote animations can teach a lot too. What types of products do you prototype, what's the process of making a design, and how many rough drafts do you usually have before finalizing anything? Anything that Apple has shipped and might ship is the sort of thing we prototype! :slightly_smiling_face: I personally have worked a lot on cameras (iPhone, iPad) and \uf8ffPencil over the years, as well as \uf8ffWatch and Health related features more recently. The process of making a design is generally making something, showing it to people, and learning from their feedback, and doing it over and over again. We've given a few WWDC talks on that topic over the years! We don't really count how many \"drafts\" we make, but everything we work on undergoes many, many iterations. Are there differences between the final product hardware and software, or are the prototypes meant to test a specific feature and include some hardware and software features, but not all of it? aka. are you trying to test something with the mindset of a customer, or as a engineer? The mindset of a customer, or a person using the product or feature. We collaborate closely with engineering teams that figure out feasibility. Our prototypes help them understand which use cases need to be supported. When creating prototypes, how extensively do you test it? i.e. Is it used only within the direct team or do you take the prototype to broader teams to test as well? Definitely show it to broader teams. It's less about testing in a traditional, thorough sense, and more about getting lots of people from different backgrounds to try it and tell us what they think. Thanks <@U03HBKARTV4> :raised_hands: If I may ask a follow up question, what kind of feedback are you looking for when testing the prototype? We always build a prototype to answer questions, so what we're looking for will be about what that specific prototype is trying to answer. For example, when we were working on Scribble for iPad & Pencil, we wanted to really understand how people reacted to their handwriting being converted to digital text, what made that process more clear/understandable, what sorts of input would lead to confusion situations, and so on. Awesome, thanks for the insight :bulb: What are some of the tools you use for prototyping? As far as digital tools I personally use Sketch and Keynote a lot for static/2D interfaces, After Effects for animation, and Swift/UIKit/Core Animation/Metal when I need to make things a bit more real/custom or interface with hardware. But everyone on our team uses different tools and has workflows that work for them. The devs behind one of the ADA winners this year, Not Boring Habits, wrote a really interesting blog post ( https://www.andy.works/words/the-future-of-design-tools)|https://www.andy.works/words/the-future-of-design-tools) about the future of prototyping/design tools, arguing for a shift from flat tools like Figma to rich, 3D-first tools like Unity and SceneKit. Do you see those tools becoming first-class citizens in a prototypers toolkit in the future? Hi Sam! Making in the medium the design is meant to be in is important! For a prototype seeking to answer an interaction design question, interactive prototypes are essential. For the design of a graphic poster, print out poster iterations. If the end artifact is meant to be spatial, then making in 3D is essential too I'd say there's an underlying principle that the medium that one makes/prototypes in is also a very intentional choice, and should be chosen to best align with the desired end artifact Do you have a shrine to Bill Buxton? More seriously, do you start with paper prototypes and, if so, do you use any tooling to go from them to something digital? I love sketching on my iPad. Note taking and drawing in meetings is important to me. I've also found that Looom makes animation so fast I am able to make simple hand drawn animations describing interactions / motion in or immediately after meetings too (Looom won an apple design award a few years back. Highly recommend!) We love Looom!! Yes! Makes putting thoughts to motion so much faster! No more pushing pixels key frame by key frame out through molasses Thanks for the reminder - I didn't have an iPad capable of running it when first heard about it & had forgotten (my wife's 12\" Pro laptop-replacement turned out to be harder to prise from her grasp than I'd expected). Now have an iPad8 with Pencil so will try it out. Nice to see it's SVG-based for export too! Ah! I'm excited for you! How do you go about adding (magic || whimsey || delight) to a prototype? I feel a lot of my designs lack that extra :sparkles: to make a great experience. Give yourself time to not worry so much about solving the problem. \u201cWhat other ideas does this give us?\u201d can mean completely unrelated things. But if they seem interesting they\u2019re worth trying. Those weird-but-interesting ideas can inspire us to connect the weird / whimsical inspiration to something that actually does solve the problem. well said :smiling_face_with_tear: What tool would you suggest for rapid prototyping? Whatever you're most comfortable with will be what lets you try things rapidly! For some people that's code, others animations in a tool like After Effects... to get started it's really about what you're able to quickly be productive with. Thanks for the feedback <@U03HBKARTV4> :raised_hands: When testing with people, how often do you change the prototype in between sessions, or not at all? i.e picking up issues from the first session and adjusting it before the next session. Is this recommended or should we stick to one prototype throughout? We try to keep more than one direction open at a time. It might mean having multiple very different prototypes, or one prototype than has sliders / prefs and can be adjusted. If someone gives us good feedback, we incorporate it or try it out. If it\u2019s in conflict with the previous direction, we keep both around to let people compare. That\u2019s great - thanks Julian. It makes sense to let people interact with different prototypes and observe, if I may ask, how flexible do you make prototypes? Are people allowed to tap on anything interactive on the screen, or is it limited to one flow? It\u2019s usually very limited. We make it look like everything\u2019s there, but only certain parts will be functional. It\u2019s important that the rest look real because it\u2019s often in the whole context of a product that we learn the most. Thanks Julian. When you're prototyping new ideas that are likely to meet some resistance (touch bar or dropping headphone jack), what process, if any other than just gut, do you have to decide whether to ship it or kill it? Generally our team doesn't get to decide whether to ship or kill things. Our job is to explore (by making prototypes) + document all the possible directions that seem interesting or plausible for a specific topic, share with relevant teams, and see what the reactions are. It happens that something we're very excited about is not something that makes it out the door for some reason, in those cases we tend to keep bringing it up in future work that touches on that concept, and work with teams inside the company who are excited about it too to try to build momentum together. Very insightful thanks Cool A question about tools: I'm a UX/UI designer who mostly use Sketch and Figma to prototype apps mockup, but recently I have discovered SwiftUI as an alternative to make high fidelity prototypes, do you think it's worth learning to program just enough to prototype better? Which tools your team use? Alberto, you just described the founding philosophy of Meng To over at https://designcode.io/ <@U03HBKARTV4>? Haha, the question of \"should designers learn to code\" is an eternal one... long answer short, I think code should be seen as one more tool in your toolbox. If you feel like it might let you make things you couldn't make otherwise, and you're drawn to it, then go for it! SwiftUI is the easiest to learn it's ever been :slightly_smiling_face: And Meng's website is great! MagicMove in Keynote lets you prototype a lot of simple animations - do you have preferred animation prototyping tools? One of my gripes about most \"modern\" tools is they stop short of helping with animation handover or code generation, if they have any animation at all. AfterEffects has been my go to (light scripting, integration with C4D, able to combine with footage, I end up doing a lot of green screening) Recently got excited about a library that makes Unity's interface more like a traditional-timeline-animation-style ui I understand and commiserate with your gripes about modern animation tools though. I have to pull up Grapher and just fiddle with my equations (when transitioning to code) (so happy you mentioned magic move in keynote though! Early decks where we try to cover as many ideas as possible, going for breadth, there's plenty of keynote animations for our light sketches) I'm a big fan of https://createwithflow.com/ who generate code from visually-designed animations. Supernova Studio was another doing similar things but they pivoted off to design system management How long does it usually take from the first in hand prototype to the final polished version? Because we\u2019re often working with new hardware features: Years! The shortest turnaround we\u2019ve had on a project has been about two years. Some other projects we\u2019ve worked on have been on the order of seven years. <@U03J52JBK0C> Can I ask what that two year project was? :eyes: I believe our official answer is \"you may ask but Julian can't respond\" :stuck_out_tongue: Hi there! It's really nice to meet the prototyping team. Do you have any tips on prototyping? (best practices, stages of prototyping, and anything else you would consider important) Always remember what are you building a prototype for, what are you trying to answer. We can get caught up sometimes in trying to have a very polish prototype but it should always be about quickly and efficiently testing a panel of different ideas. Sometimes it helps to just use low tech tools, quickly sketch things on paper, get away from screen. Thank you! Our team like to repeat the process of: make things -> show to people -> get some feedback and repeat until it the experience feels good! Sounds like a great process to follow! Great advices <@U03JGA79CQZ> :raised_hands: thank you :pray: Yeah, thank you for this great advices! I really appreciate it. Having the opportunity to talk to Apple\u2019s Prototyping team is amazing! Do your prototypes sometimes lead to product ideas that Apple pursues, or is the context of your prototyping often within the confines of product ideas that already exist? It\u2019s often the opposite direction \u2014 we\u2019re typically prototyping new products or features that in turn teach us things we could make better about existing products! Ah, that\u2019s very interesting! And when prototyping new products, is that something your team decides to explore, or do you get \u201casked\u201d to explore a certain thing? Both. We pursue things that we think might be interesting, but we\u2019re also keeping tabs on the possibilities that interest partner teams that are asking for our help. What is the collaboration dynamic between the prototyping team and Design / Eng partners? We work closely with them! With engineering teams to understand the technical capabilities and limitations that we are working with, and with design teams to understand the roadmap they have for their products, and what problems they're trying to solve right now. The time horizon we work on (3-5 years) is a bit longer than the one most teams operate on, so communication is key for us to be aligned. If you're able to speak about it, I'd be interested to know if either of these roles ever initiate requests from the prototyping teams, or if novel / net new experiences are normally initiated by the prototyping team <@U03JENY84QH> Funny enough, this just got answered in the thread immediately above you! https://wwdc22.slack.com/archives/C03H77GM1NW/p1654795791726889 I often find that because there are so many considerations to make for a design to be truly great, that can hamper my creativity and almost block me from making progress. Do you deal with similar things, and if so, how do you manage to stay creative and keep going? Absolutely! I think any creative in any field suffers from the \"blank page\" problem... for me I find that spending time doing things not tech/design related (playing music, spending time outdoors, reading books about random obscure topics,...) will often spark unexpected connections & inspiration that somehow find their way into my work. Something a little more specific I like to do too is to dive in the history of a topic I'm working on - for example the history of human mark making & handwriting through various cultures was very inspirational when I worked on \uf8ffPencil. That\u2019s both reassuring and inspiring to read. Thanks <@U03HBKARTV4>! \"too many things to consider\" is totally relatable, and absolutely something present in the day to day. I have personally really enjoyed organizing and grouping ideas along a \"design space\", by trying to figure out what common axis the ideas might have. It helps me feel less overwhelmed, and also allows me to identify any areas I haven't considered. That\u2019s a really great idea, <@U03J0DULM0V>! I suppose it\u2019s also about sometimes just going with the flow and then reviewing and adapting the work to feedback and design principles afterwards. Are there any good WWDC sessions on prototyping past or present you\u2019d recommend? Absolutely! You can search for the \"prototyping\" keyword in WWDC sessions, but here are a couple we did: Life of a Button: https://developer.apple.com/videos/play/wwdc2018/804/ Prototyping for AR: https://developer.apple.com/videos/play/wwdc2018/808 Oh and this one too! Not specifically about prototyping but discoverability, something we care a lot about. Discoverable Design https://developer.apple.com/videos/play/wwdc2021/10126/ Nice, thank you! And some say the Fake It Til You Make It talk may still be on youtube if you search :wink: Okay, I\u2019ve seen a few mentions of this Fake it Til You Make It talk here, so I must find it! And gasp! Here it is again: https://developer.apple.com/wwdc14/223 cc <@U03J23RAFK4> Thanks! Added it to my bookmarks. :smile: how does your design process look like? Do you take part in problem define, before you start work on some ideas? Thanks! We try not to be too rigid. Often we are starting with a specific problem to solve, yes. But sometimes we make things just because they seem interesting, and then figure out why and what they can help solve. It\u2019s about giving ourselves space to figure out what feels great. We can fit both types of process into the cycle we describe in our WWDC sessions: Make things, Show them to people, Learn from their feedback. sounds great! thanks Thanks for this Q/A and congrats on all the work being released this week. Part of my day to day work is taking prototypes and being able to communicate findings to groups and teams that may not have an interest in them (yet..). If you're able to share high level insight - I'm curious how the findings from your prototypes are communicated (format, findings, risks) to external teams or even execs. To some extent the prototype itself carries this weight, but in cases where not and in person communication is infrequent - I'm curious how your team tackles this :) It\u2019s an interesting challenge, because different people respond to different communication styles. We do have to adapt how we communicate work depending on what people are looking for. The nice thing about a prototype that appears high-fidelity is that it lessens the amount of assumptions people are making. If it looks like a real app, but we fake one part of the interaction, we are helping to direct feedback toward the parts that seem new. Thank you - that's a nice answer and an excellent point on minimizing assumptions. How do you come up with an idea of a product? For example, do you do brainstorming sessions? And how do you transform an idea into a prototype? I think it's important to know what the biggest questions around an idea you have, because that's what you'll want to prototype first. Is it about testing the usefulness of a feature? About exploring how to best display the results from an algorithm or machine learning model? Or is it about seeing if a new gesture interaction you came up with is easy for people to understand and perform? The goal of prototypes is to answer questions before investing a lot of time into making things real (hence why it's important to keep your prototyping process light & nimble). As far as coming up with ideas... ask people questions, and listen to what they have to say! Thank you very much! I really appreciate your advice. How has your workflow & design reviews changed in a WFH world? What have you found works well for collaborating remotely? I've done much more green screening with remote work. (green screening with props and suits with OBS running the effects and compositing in realtime to a virtual camera I use in webex) We have a lot of collaborative Keynote decks too. Even for in-hand prototypes we build we\u2019re taking a lot more video of them that we then place in those Keynote decks. I'm not in a green suit day to day of course :sweat_smile:, but I was very happy the extensive setup came in use for a project in 2020-2021. I had thought to myself \"never thought I'd be grateful for webex\" I like how it gives more opportunity to make real life situation prototypes, because your setup is at home, and not in a studio where things are impersonal. It forces you to stay even more connected with your teammates, to make sure you share regularly about the work How do you prototype Siri experiences? Keynote animations with audio, in short! It\u2019s like anything else in that we\u2019re just faking the audio response portion. Thanks <@U03J52JBK0C>! Do you use human VO in place of Siri in prototypes? Or do you generate mock Siri audio? For our purposes it is better if it sounds like Siri. Mac\u2019s \u201cSpeak selection\u201d is very useful for this. Hey! Firstly, it\u2019s truly an honour to speak with this team and thank you for making products that impact each and every one of us each day :D As for my question, I would love to get better at design User Interfaces. What are the benefits of Sketch/Figma/etc. and would you recommend their usage? Thanks! Sketch and Figma are two popular tools for UI design and prototyping, but ultimately they're just tools - they won't give you magical results on their own. If you'd like to get better at designing UIs and you're starting from scratch I'd encourage you to focus more on things like layout, type, color, interaction patterns (the Apple HIG is a great place to get started on those topics as they specifically pertain to Apple platforms! https://developer.apple.com/design/human-interface-guidelines/ ) than focusing on a specific tool. I saw Julian mention the process of 'Make things, Show them to people, Learn from their feedback' and I was wondering if you've ever had a product that's had little to no changes after feedback? A product thats a hole in one? And how much does the product usually change after feedback? Never. If we\u2019re not getting feedback on something, we\u2019re just not showing it to the right people. We will eventually show it to someone who will have feedback about it (improvements or reasons why it won\u2019t work!) Things change a lot between our first conception of it and what makes it out the door. That\u2019s the fun part about working with a whole lot of people who are very talented at what they do. Thanks for this insight, and thanks for these amazing products! What are some philosophies that the team adheres to when designing and prototyping things? :D Make things, show them to people, learn from their feedback! That should be a tattoo at this point! Fake it until you make it <@U03DMQBFPH8> I\u2019m getting a WWDC tattoo in July! I\u2019ll get that phrase added for the prototyping team :laughing: > show them to people This piece is so, so important. It almost immediately pays off by helping break down assumptions and normalities established by devs and designers, and lets you see your project and tools from a new perspective. Being open and receptive to that feedback is an incredibly value experience :heart: Considering an interaction \"Before, During, and After\" (i.e. in the example of a button before people press a button, how do they know they can do it, and what it will do, during the press how do they know the action is registered and whether they need to continue pressing ( any visuals that encourage continued action would be 'feed-forward'). And afterwards, what is the feedback that ensures we know how the action went?) Julian;s 2018 WWDC talk covers this in more detail Always bring positive feedback when sharing the work. It should never be about personal judgement but how to make your app experience better. For example: \"I don't like this color\" vs \"I think blue instead of read would better communicate what the experience is about\" \"Always design a thing by considering it in its next larger context \u2013 a chair in a room, a room in a house, a house in an environment, an environment in a city plan.\" is a quote that I think also nicely captures the spirit of always being able to step back and not design something in isolation <@U03HVE4TV8E>!!! You will absolutely need to show this to us on Twitter when you do Favorite books? So many, I love books! We reference Tufte a lot in our studio. Some recent design books I read and enjoyed were Extra Bold https://papress.com/pages/extra-bold , and Politics of Design https://www.counter-print.co.uk/products/the-politics-of-design I always like The Humane Interface and The Design of Everyday Things. Been re-reading Interface Culture recently. Interesting How much individual work vs collaboration do you do? Do you work together within a document, or work independently and then present that work for feedback? Different people on the team have different preferences. But we all attend a weekly design review with the whole Prototyping team in which all our work is shared. It helps us build a joint understanding of what a project is, even if it\u2019s mostly one individual \u201cdoing the work\u201d, we\u2019re all helping with feedback, new ideas, and critique. The team\u2019s brain makes the work better. Some folks on the team prefer being in collaborative documents 100% of the time. Others will just bring their individual work to design review. Is there ever a time where you have to stop and refocus the vision or design? Like, if too many new ideas have been added and the original focus has been lost? Yes, but usually not because of too many new ideas! If there are too many new ideas it means we have a lot we can try out. Yes absolutely. When that happens we typically try to focus on what it was that people loved the most in that set of work. If you have dozens of things competing for your attention, focusing on the 2-3 that really seem to be winning hearts over is often a good way to move forward without getting bogged down. But sometimes you also have to accept that while you may have a bunch of kind of cool things, there's nothing in there that's a true winner. And that's okay! There's always way for those things you liked but which weren't clear winners to make their way into other work in the future. Yeah the latter part of Guillaume\u2019s answer was the second part of what I was going to say. If we pause a project it\u2019s because we feel we\u2019ve thoroughly explored a space and we just haven\u2019t hit on what feels great. You mentioned not being too rigid on your team and often times working on things in the span of several years. Do you have any rituals as a team to stay connected and get into a \"let\u2019s make some awesome things!\" mode? Maybe this is already somewhat answered in https://wwdc22.slack.com/archives/C03H77GM1NW/p1654797234259859?thread_ts=1654797121.173549&cid=C03H77GM1NW|this thread , but perhaps there\u2019s some additional thoughts around team rituals that you could reflect on here? Yes, as Julian said we have weekly team meetings and design reviews where we share very openly what we're working on. Those have happened every week as long as the team's been around, over 15 years now. I think being a small tight knit team is essential to our success. And in those design reviews we\u2019re very open (and excited) to see random things people made that aren\u2019t directly related to current/ongoing projects. This is so awesome and inspiring! Thanks so much for sharing! How do you scale a weekly meeting as the team / scope grows? We'll have smaller, more focused meetings for individual projects. The goal for the team meetings is for the whole team to see what's going on that week, but they're not exhaustive. Our team has never been bigger than ~12 people, which I think is a feature. I with you on keeping things small\u2026 let\u2019s just say that\u2019s a rarity in my experience :unicorn_face: Hello again, and thanks for your time! I am looking for general design guidelines on building out visuospatial metaphors in a 3D + AR environment. I have a project, http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat , that renders and visualizes an entire code base at once, but there's just so much to see, it gets cluttered and overwhelming! I would really appreciate some time to talk and think about what kinds of things can be presented, hidden, animated, and made otherwise visually appealing for users, such that the tool is less burdensome, and more useful. Cheers!! Hey Ivan! This is a great question to ask in a 1:1 Design lab! There's still slots for tomorrow let me hand you a link https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/upcoming?q=design%20lab here you go! Thank you once, twice, and thrice! I\u2019ll make sure I\u2019m signed up ASAP :smiley: cc <@U03HBHWCB4N> ^ What are some non-digital sources of inspiration for each of ya? Books, music, art, something that you aspire to perhaps. Ah. So many things, how does one dimension reduce to answer this? haha, everything from me spending too much time on a saturday morning brunch watching caustics, of light going through different glassware, to thursday night Exploratorium trips, adoring Tokuujin Yoshioka's installation work, Olafur Eliasson, Looking at the special effects work for movies (\"making of The Mandalorian\" a year or two ago with their realtime rendering was :fire:) (does this count as non-digital, it is half physical ahaha) See some fine art in museum is always inspiring! But yeah so many things. For me, just taking long walk randomly in San Francisco can help my thoughts wander in unexpected places. Those are great! I 100% love making of essays, vids, docs Thank you for sharing these :) To answer the last part, \"something you aspire to perhaps\" I aspire to make things as beautiful as poetry :pleading_face:. I like Amanda Gorman, Jack Gilbert, some good ol' TS Eliot, but ahah, thus far haven't personally been able to draw direct application for poetry. Oof poetry is a good one and not often an answer to such a question - maybe applied to UI copywriting ha? How accurately does the original design idea translate to the prototype? Are there many changes? I saw Jony Ive say that the best ideas come from conversation, does that continue through the prototype stage? The purpose of prototypes is to help advance the conversation. Instead of going in circles verbally about what might be, we start to talk about what\u2019s in front of us. Or what could be in front of us that we haven\u2019t made yet! Which is to say, we often find the things we think are interesting change dramatically once we have a few prototypes in front of us. And we have conversations about that! Do you differentiate between designing user interfaces and prototyping? Are they related or separate focuses? Yes they are definitely 2 different things, although for us they overlap a lot! I think prototyping in general refers to building artefacts quickly to answer questions that will then inform how to build something \"for real\". So Apple also has silicon prototypers working on the M series chips, for instance! For us, we prototype interactions and user interfaces, which means we try to explore what new modes of input or new capabilities (think for example something like FaceID/Depth Camera, or Apple Pencil) would bring to our products, which then informs whether the company should invest the resources in actually building that hardware/product. It depends on what the prototype is trying to answer and at what stage of the project we are. But generally a polished UI would help better communicate with other teams, make it look real. Yes to add to Myl\u00e8ne\u2019s answer, we often have to design user interfaces as a part of prototyping something! Thanks for the answers! So sometimes it involves building a UI, but much more and not just limited to a digital UI with prototyping. Though sometimes we can get away with re-using large parts of the existing designs. I\u2019m not sure we\u2019re necessarily doing more \u2014 the final user interface designs require a lot of work to be designed for all the different devices, screen sizes, edge cases, accessibility cases, error cases\u2026 we\u2019re fortunate to get to pass on that until it becomes a specific issue for our concept. We build on what\u2019s already there for those parts. Thanks for the clarity! Have you ever had to build a new tool in order to design a new experience? Do you ever use custom or internal prototyping tools that apple makes? Yes! And yes! The \u201ctools\u201d are a lot rougher than you might have in mind, but we\u2019re often cobbling together our own sets of code to be able to try things. We don\u2019t build it out into a generic reusable tool as much as we build the frameworks / scaffolding necessary to try out the very-specific thing we want to get into our hands. We build what\u2019s necessary to try the specific things we want to try. We try really hard to stay away from prematurely abstracting. It\u2019s too easy to accidentally become a framework engineer or tools engineer when we really were just trying to try something out. I noticed you said that a destructive action should be on the left and red text. I never knew that - where is this sort of thing in the HIG? Ooops, should\u2019ve responded with a reply. :wink: Hey Dan, we have writing guidance sprinkled around the HIG in various locations. This guidance is on the Alerts page: https://developer.apple.com/design/human-interface-guidelines/components/presentation/alerts Other than users and learners, any other suggestions for referring to the people using our apps? What is your audience? We often just use \u201cpeople\u201d but if you can be more specific, like \u201cfamily member\u201d or \u201csubscriber\u201d that\u2019s a good way to go. Think about what people using your app would call themselves. We try to avoid the word \u201cusers\u201d when we refer the people who use our apps, and prefer something that sounds more human, like \"artists\", \"collaborators\", \"participants\", etc. depending on the app and what people are trying to do. :slightly_smiling_face: Thank you! That is very helpful :relaxed: Are there any examples of combining different tonalities within one app? Or should one just stick to one? The voice of your app is something you want to remain consistent, so that folks get consistent messaging inside your app. But, as you point out, the tone can certainly modulate. When you welcome somebody, you might sound excited. But when that person has a credit card declined, you may want to sound more serious. Similar to how your voice is always you, but your tone changes depending on context and who you're talking to. In other words (no pun intended, hiyooo!), it's very common \u2014 if not important \u2014 to adjust your tone throughout your app. The tone should reflect the situation. As an example, coaching from the Activity app on Apple Watch is celebratory when you close a ring. But it's more matter-of-fact if your progress is lacking. Right, there is a clear distinction between voice and tone, thanks for answering both of you! And I loved your pun <@U03HJ5LRUEN> :joy: Hmmm - I guess I\u2019m still struggling with tone vs voice - Carrot weather has insults woven into weather forecasts. Is that voice? Maybe it doesn\u2019t have a tone. I\u2019ve never bought it because I didn\u2019t like whatever it was I don't think these rules apply to Carrot Weather App :joy: How do you see the different roles of the HIG's writing guidance, and the Apple Style Guide ( https://help.apple.com/applestyleguide/#/apsg1eef9171)?|https://help.apple.com/applestyleguide/#/apsg1eef9171)? Is the style guide more for contexts outside of apps and long form writing? Great question. One way to think about it, is that your product is a major part of your brand. With that in mind, your product may have more specific guidelines to follow so that you can make your interface feel familiar \u2014 and subsequently, intuitive. These kinds of patterns are outlined in the the Human Interface Guidelines. Your brand, however, may encompass more than just what appears in your product. Your website, emails, marketing collaterals, events, and even the App Store descriptions mentioned earlier, these are all examples of surfaces where your voice extends beyond your app. For us, that's where the Apple Style Guide kicks in, helping us to adhere to a wider set of grammar and syntax to ensure we have consistency at every customer touchpoint. Think about this balance for your app, too. How can your app have clarity and consistency throughout, and how can that narrative continue outside of your app to complement what you \u2014 and your users \u2014 are trying to do? HIG ASG :handshake: continuing the conversation I think the ASG should be more visible! For some reason it's hidden from search engines. Great session! So many useful tips I am 100% gonna apply to my apps. Just wondering though, would you recommend Onboarding for apps? I\u2019ve heard before it interrupts the user\u2019s usage of the app, but I\u2019ve also heard that it makes users feel more welcome. What do you think? Thanks :grinning: This is such a good question <@U03HVE4TV8E> I don't think there's a single answer to it, but <@U03HVEWEGAF> <@U03HWQ6F9A5> and <@U03HJ5LRUEN> have some thoughts It's going to vary based on the complexity of your app or how simple its premise is. We find what often helps is to incorporate onboarding elements or moments into the first-time experience for someone new. Also great question :fire: Maybe I'm saying the same thing, but the best onboarding doesn't feel like \"onboarding\" Thanks for answering :D so, I have an app that tracks deliveries, so should the be super simple along the lines of \u201cTrack your deliveries\u201d in large title text, maybe a little but of information, and then \u201cGet started\u201d? All in one view. Thanks :pray: There\u2019s no one answer to this! My favorite onboarding experiences are the ones you do right within the app itself. Like those early levels of a game that teach you how to play it. Or a chat app that uses the chat to get you set up. Alto's Adventure! You're just dropped into the game, and you're playing, and it's teaching you along the way <@U03H3HNAGSK> Ahh, I love that game! And yeah, the onboarding is really nice and seamless! Oooh off the cuff it's hard to give specific feedback over Slack. If you have time <@U03HVE4TV8E>, sign up for a slot in our UX Writing Lab tomorrow\u2014and add that question. https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/Y2Z4GC8D8Q/dashboard|Sign Up Here Ooooh I didn\u2019t know there was a lab happening tomorrow! Thanks :D Thanks for amazing advice! I wonder what the arrangement for options in an alarm means. For example putting major option on the left or top. Is it recommended to change the order, e.g. from major-on-left to major-on-right, for languages using different reading order? Hi Zhang - Yes, typically you\u2019d want to order more important things above or to the left of less important things. And for RTL languages, check out this page in the HIG: https://developer.apple.com/design/human-interface-guidelines/foundations/right-to-left Also this video: https://developer.apple.com/wwdc22/10034 Thank you for answering! Such fun designing for different cultures. For full screen modals with no interaction, is Apple's preferred dismiss control an X in a circle in the top right corner (as is referenced in the Explore Navigation Design for iOS talk) as opposed to a grab style control at the top middle (indicating to pull down to close)? Great question! I\u2019ve been wondering this myself recently :facepunch: Totally great question. we get it a lot. We have an answer for you in the HIG: https://developer.apple.com/design/human-interface-guidelines/components/presentation/sheets Position Done and Cancel buttons as people expect. Typically, a Done or Dismiss button belongs in a sheet\u2019s top-right corner (in a left-to-right layout) or top-left corner (in a right-to-left layout). The Cancel button belongs in a sheet\u2019s top-left (in a left-to-right layout) or top-right (in a right-to-left layout) corner. Good question <@U03JRQUJ27J>. This has been an interaction pattern in recent releases, moving away from the elevated sheet and toward the full-screen presentation with the close in the top right Omg I LOVE that this is in the HIG now!! I literally have sent in at least 100 bug reports pointing out where the Cancel button is incorrectly located at the right. I have like 100 screenshots collected I attach to each bug report showing places where Cancel is correctly located at the left. :heart: It is interesting however there seems to be a bit of conflict with close buttons moving to the right recently (Safari, sheets with X instead of Cancel) :thinking_face: If you add a new feature to an app, is it a good idea to showcase that new feature when the user does a certain action (i.e. completes a task etc.) or is that bad user experience as you\u2019re interrupting their session? Thanks :grin: This goes back to context. If that feature is relevant or helpful at that moment, then yes, go ahead and talk about the new feature. But if it\u2019s not relevant to that moment, think about where else it would feel more contextual. That\u2019s great, thanks :blush: Seems common for interface copy to be too verbose before it's edited down. Are there examples that come to mind where the UI was the opposite, too terse and needed more explanation? There are may examples of this, I'm sure. One that comes to mind is when people input personal information, like when ordering something for delivery to their home. Sure, you could just indicate that you received the information, but by restating what you received (so the customer can ensure it was entered correctly) and maybe even stating that the info won't be used for nefarious purposes (eg. it wont be sold to advertisers), you create a level of comfort that more economical language might not afford. Think about when you tell a server at a restaurant that you want a certain dish, but with extra pickles and hot sauce but no tomatoes and can you please switch the fries for a salad. Some servers won't write that down. And while that's impressive to see, it's also nice to hear when the server repeats it back to you so you know the meal will come as you asked. Love that metaphor <@U03HJ5LRUEN>. Another thing we do often is link out to support articles for people who want to learn more about a particular subject. That way the information is there if you need, but it's not in the way. What would be the best way to dismiss a full screen modal that has content? Would it be a button along the lines of \u2018Done\u2019 at the bottom, or is it still a cross in the top right as mentioned earlier? Hi Charlie, there\u2019s some good insights in the HIG on the Sheets and Modality pages. https://developer.apple.com/design/human-interface-guidelines/components/presentation/sheets Personally I would be reluctant to use a bottom corner in a Mac app, since you have to consider window movement and that it could be clipped by the Dock. https://developer.apple.com/design/human-interface-guidelines/patterns/modality Yes Austin, generally not a good idea to position controls at the bottom of mac windows. Also, check out: https://developer.apple.com/videos/play/wwdc2022/10001/ Any tips on how to position elements so as to help with functionality and design? In our session, we talked about information hierarchy. As with everything it\u2019s going to depend on the specifics, but in general, think about your headlines and buttons. Make them clear and aligned with each other. Keep other text brief and easy to read. Rather than a big block of text, can you break it up into bullet points or even multiple screens? Oh okay, that makes sense. I just have a lot of trouble getting my designs to be both intuitive and aesthetic. I\u2019ll keep that in mind, thank you! Should I still add an \u2018Add to Siri\u2019 button to my app with the new SiriKit/shortcuts functionality, or is it completely obsolete? Good question <@U03K1B3MQL8> ... You don't need to remove it, if you've got it. But on iOS 16 forward, you can begin to move away from it by creating and supplying App Shortcuts, rather than requiring people to set them up I haven't fully looked into the new SiriKit interactions\u2026 Does it just mean that if an Intent is available, iOS knows this and will automatically integrate it with Siri/shortcuts? I think you've essentially got it, but I would watch the App Intents video and the Design App Shortcuts video as well What's the average number of times you edit your Slack messages? Many times, sometimes. Too many times to count. :sweat_smile: I\u2019ve found a small mistake in the HIG, where should I report this? The Feedback assistant doesn\u2019t seem to have a appropriate category Would love the feedback! We are working on adding a HIG category. You can use the Apple Design Resources component for now. I\u2019ll make sure it gets to the right place. :wink: Reported, it\u2019s really small but wanted to mention it somewhere, FB10140417 Typos happen, but I don't think I'll ever in my lifetime see a HIG screenshot accidentally show a time other than 9:41 or 10:09. Not as long as I\u2019m here <@U03JELQLESV>! Have you ever accidentally shipped out a typo with an OS release? \"An Apple spokesperson declined to comment.\" :wink: It happened onec Apparently one of original iPhone's important demos to a cell carrier's boss accidentally contained a profanity as a localization string lookup key! Source: a Computer History Museum interview. Is the Apple Pencil mightier than the sword? Sorry for joining so late! I make a photo editing app and I often wonder how I should deal with technical terms. One example that comes to mind are blend modes (screen, overlay, multiply, etc.) as those things are foreign concepts to the average person. Do you recommend sticking to such technical terms and providing explanations via graphics, for example? Ooh good question. It really depends on your audience - are they professional photographers? Then they might understand more technical terms. You can also help define things with secondary text. Thanks for the reply! The app is not targeted at professionals, so I\u2019ve tried to find ways to better communicate what those tools do If these are terms of art, and it sounds like they are, it can be best to stick with them. Avoid jargon, or making up terms that might end up confusing everyone. That said, if your audience is less technical, maybe define the terms so people can learn them. +1 to what <@U03HWQ6F9A5> and <@U03HVEWEGAF> said here. We aim to democratize access to technology with much of our hardware, software, and services at Apple; so we aim to have our language be understood by as many people as possible. But with a few of our apps, we have a specific audience (eg our creator tools, like Apple Music for Artists, that focus on specific audiences), and therefore we want the language to be in the vernacular of the users\u2014otherwise it may feel like we've shipped a watered-down professional feature. Since it's not aimed at professionals, show it to friends (or strangers) and see if they understand what's written. And as Jen and Kaely pointed out in their talk, consider reading it out loud to yourself as well. Thanks a lot the replies! One area of iOS that just came to mind was the blur intensity adjustment in Portrait Mode. Even though it\u2019s a feature that\u2019s not necessarily aimed at pros, it\u2019s using f-stops as an analogy to real life photography. If I want to read the new HIG end-to-end, when should I plan to do that so everything will be in there? I know yall are working on more goodies. Hi <@U03HMCT187R> good question. The HIG is a living document, so it\u2019s always getting updated in big and small ways. Between now and the fall, the updates will (most likely) be small. In the fall, the bigger updates will land. Another way of answering your question is: You might as well get started with that read-through! It has been great to get feedback on the redesign, so don\u2019t be shy about that, either I think it would be great if it was clearer when there's more minor updates, like a technologies page gets added or something. Sometimes I'll discover on Twitter, but a more central diff would be nice! I can file feedback about this too. We're already planning for changelogs in the HIG coming some time this summer! The wonderful crusade against home tabs reminds me of the one you did for hamburger menus many years back. Are there other common app conventions that you see that you disagree with? Thanks for remembering that session. It was YEARS ago. We have a few, mostly they center on design conventions that come over from other platforms that could be adapted to match Apple platform conventions For example, drawing a back button as an arrow (with a tail) instead of a chevron ( < ) ^ that's an android thing Tab bars that have a button in the center that performs an action rather than navigating to a location in the app One that comes to mind that might've been mentioned in that session is avoiding the standard Apple platform icon for share sheet button. Both so common :pensive: Yep! That one gets my blood boiling. :slightly_smiling_face: My current pet peeve is floating buttons (i.e. on Twitter). Like, a plus in the corner that is just floating there :joy: What's the icon they use, like three dots or something? Also, kabob menus (three vertical stacked dots) instead of an ellipses symbol For me I personally dislike icons without labels. Unless you think your icon is universally understandable across all cultures \u2013 because it usually isn't :stuck_out_tongue: This thread really stirred the pot with food-related anti-patterns. Another one I just thought of: close buttons on the right instead of the left? That\u2019s the one that always gets me. Where does one place close buttons? In reference to not creating a \"home\" tab, what are your thoughts on having \"cards\" that are overviews of content from a different tab? Would you consider that a duplicate? Hey <@U03K1SMSPUZ>, great question. This is highly dependent of the context and content of the app. Be cautious of cards that display unrelated or disparate types of information and make an interface feel like a dashboard. Dashboards are similar to Home tabs, as the information is not directly related and often overloaded. This makes it difficult to glance and takes longer to understand the relationships and how to take action So, if you use cards or have a dashboard like view, try to keep the content relevant to the current tab? We\u2019ve seen some apps that have a wide array of Widgets that can be configured like a dashboard. It can be very powerful, especially on iPad. But these widgets would take you to the corresponding tab in the app. when do you recommend hiding the navigation bar when pushing a view? Ex: Apple News: When you open an article, the app pushes the article view and hides the main navigation. Curious about if there are situations when the tab bar should be hidden also. It\u2019s true that News doesn\u2019t follow the typical convention we recommend for tabbed apps. I can\u2019t really speak for the News team, but I believe the thinking was that people don\u2019t generally switch between tabs frequently or want to persist state in each tab since the content changes all the time People drill into articles, read, and then often leave. You could consider temporarily hide the Navigation Bar for immersive experiences, just be sure to allow the reverse interaction and show it when swiping down or tapping the screen. In that usage pattern, persisting the tab bar to allow for fast lateral navigation, or keeping it visible to help people stay oriented, is less of an imperative. This is pretty atypical for tabbed apps though got it! Yes I was thinking it can be also be possible to use a modal instead of a push with hidden nav bar I saw in Sarah M's session that there were Places cells with a ... button on their top right side. On what cases should we use such a button? I believe you\u2019re referring to the ellipses SF Symbol in the navigation bar? We would use this button for secondary actions that are related to the view, but perhaps not the primary actions someone would take \"...\" More menus are typically used to host a set of additional actions vs. navigation. You'll often see designs conflate the two. If an app must use more than 5 tabs, is an ellipsis the appropriate icon for the \u201cMore\u201d tab? <@U03J1EHL4KY> I believe that <@U03DMQBFPH8> answered this here: https://wwdc22.slack.com/archives/C03H77GM1NW/p1654813779763509?thread_ts=1654812948.356689&cid=C03H77GM1NW Are there resources we can share with more web-minded colleagues. It can be difficult to explain that apps and websites should not have the same navigation design. Absolutely! http://developer.apple.com/design|developer.apple.com/design Also this video :slightly_smiling_face: I\u2019m not sure we really speak explicitly to web designers per se, but this should be a helpful primer on Apple platform design patterns Also more specifically all the content inside \"Navigation and search\" in the HIG https://developer.apple.com/design/human-interface-guidelines/components/navigation-and-search/navigation-bars As well as modality: https://developer.apple.com/design/human-interface-guidelines/patterns/modality I saw in the session that there are list section headers that are bold and bigger than normal section headers. In which cases should we use these? Thanks They\u2019re a relative recent change to iOS (last year I think). It\u2019s a matter of taste and judgement I think. But they\u2019re particularly effective when the list is text only. It helps created needed differentiation (or grouping) between content items and category labels If the list items are visually distinctive, larger section headers might not be needed. But, personally, I think they\u2019re an improvement in all situations. \"Home\" is such a lazy name for the app's first tab! But the argument that's often given in its favour is that people understand it because that's how the first page is often referred to in web and app design and changing it would mean forcing people to change their already established habits. Would Spotify / Twitter doing away with the home tab be a risky UI move that could lose them followers or a much needed navigation cleanse? We can\u2019t speak about what other companies should do, but I don\u2019t consider risky moving to a more intuitive labeling convention, this way you ensure predictability and intention. Specificity is always a good thing. If there\u2019s a more descriptive word to use, it should be used. I was just thinking that. Maybe instead of \u201cHome\u201d it should just be \u201cTimeline\u201d for Twitter And even web is starting to move away from the \"home\" concept. I have a feeling it will end up in the same category as having a floppy disk represent \"save\" In the demo app shown, the Navigation Bar was hidden but the toolbar items was still showing. Is this new in iOS 16?! :blush: Hmm. Can you elaborate what example you\u2019re referring to? None of these examples share UI that isn\u2019t yet released :blush: Navigation bar appearances? https://developer.apple.com/documentation/uikit/uinavigationbar/3198027-scrolledgeappearance Yeah of course! If you notice how the image goes behind an invisible NavigationBar but the toolbar is still visible That\u2019s a standard navigation bar with the background blur removed The image goes behind the navigation bar and it\u2019s optional to remove its background, but for TAB BARS it\u2019s not recommended. Yes, the background translucency should not be changed or removed on the tab bar. On a navigation bar, it can be an aesthetic choice if it doesn\u2019t hinder the user experience I would love to hear the Design Teams thoughts on Kebab Menus and Floating Action Buttons! (I get a feeling they fall in the same boat as Hamburger Menus and Home Tab Bar Items :smile:) Funnily enough we were just talking about this here: https://wwdc22.slack.com/archives/C03H77GM1NW/p1654814044982489 \u201cKebab menu\u201d? Is that a thing? now I\u2019m hungry\u2026 Most of these components are problematic for other reasons. For example, FABS tend to cover elements of an interface which can be difficult to read or interact with the content behind the action. They also tend to be duplicated across tabs and the redundancy, shows that the action lacks context And yes to all of the above, a lot of those paradigms you'll see on Android and it instantly makes people feel as if the design wasn't intended for an Apple platform. Yes to above! I should add that FABs aren\u2019t verboten on iOS but the action should be VERY important and VERY frequently used. Otherwise they get in the way. Things and Notes are two good examples of where a FAB makes sense. Problem is a lot of other apps use FABs for an action that isn\u2019t all that common. After years of freelancing and working in startups, the most annoying thing was UI/UX concepts fighting against the (native) platform language - for the sake of brand experience, for \u201cstreamlining and build it faster\u201d (the opposite was true) or lack of knowledge (or empowerment) of the designers. It is really hard to find good mobile1st designers\u2026. still after over 10yrs We feel your pain. btw: I consider SwiftUI more a design than a code challenge\u2026 (for the same reasons) Agree Oliver. I'm not a designer but I've certainly tried to advocate for native look and feel when working with designers on iOS apps, <@U03HBJXV0TY>'s session Essential Design Principles from WWDC 2017 helped clarify my thinking around this. Wondering where \"featured\" tabs would fall\u2014are they just home tabs with a different name or could they actually be considered useful? Hi <@U03JRQAFUKA>, could you share what a feature tab would consist of? I guess maybe recommended articles for example? Hard to say a blanket \"yes or no\" because it depends on the content inside the tab. We do \"For You\" tabs where the content inside changes dynamically based on what we think is relevant to the user. That could be considered a \"featured\" section but its use case is specific enough from navigation that it can be its own page Where it gets iffy is when you're just repeating the same content as in your navigation put on a page. That causes confusion with whether people should use the tab bar or Featured page to access information Recommended & feature articles could be highlighted under the Articles tab, instead of having a dedicated tab. I\u2019d say, as <@U03DMQBFPH8>, that it would be case by case and depending of the level of personalization the app offers. Ah okay, that makes sense! Thank you! I hope it's still fine to ask an unrelated question. I missed the previous Q&As here, there is just so much going on :exploding_head: I would love to have the design templates available for Affinity Photo and Designer. Is this something you are considering? I guess I should probably file a feedback for that, just thought about it now :innocent: Hi <@U03J4CVE1U4> - We are continually considering what apps to support for the ADR. :wink: Feedback Assistant would be a good next move. Awesome, I'm glad to hear it! Actually just submitted one: FB10142299 :muscle: I saw a tweet showing batch editing in the iOS 16 Photos app and apparently it uses a progress indicator at the bottom that reminds me of an Android-style snackbar/toast. Do you have any guidance on this UI element? https://twitter.com/apollozac/status/1534903049373761537|https://twitter.com/apollozac/status/1534903049373761537 We have progress indicators on iOS currently https://developer.apple.com/design/human-interface-guidelines/components/status/progress-indicators We don't currently have published design guidance on that particular style of progress indicator (when it becomes more common the platform we'll consider). Gotcha! Thanks for your answers, <@U03HBHWCB4N> and <@U03DMQBFPH8>. I really enjoy reading through the conversations going on here. Although from prior experience I can say that what that toasts are tricky to work with. The one you're seeing in iOS 16 is nice because it's using a deliberate \"x\" close button so people can dismiss the UI and it knows the process takes long enough that the toast won't magically disappear in 1 sec making it impossible to read or interact with So it's not disappearing automatically after the batch edit has completed? I need to try it myself, I guess\u2026 It is disappearing automatically after the batch edit completes, but the reason it appears in the first place is that batch edits take time so it's appropriate for this context In the \"Explore navigation design for iOS\" session there is an example of a \"Profile\" Tab which I've seen several apps using, but others like f.e. the Appstore uses Profile as button in the top-right side. Does that depend on the importance of such a Profile screen ? in other words, What makes something to be a \"tab-worthy\" ? Great question, <@U03J1TN6WBD>. This is a judgement call based off of the importance of the profile. In many cases, the profile is not that important to the apps main functionality and it isn\u2019t needed as a tab bar item. When considering what should be a tab, think of your apps menu of options. When tabs are designed well, they tell a story at a glance about what your app can do and how people can use it. Profile tends to be less important in the scheme of things. And just because you have tabs to use, doesn\u2019t mean you have to use them all. Sometimes an app will feel more approachable if the tabbed navigation is simplified Agree with <@U03HBHWCB4N>. Also, if the user needs to access the Profile content regularly and you don\u2019t want to place a profile button in the nav bar of every tab, a profile tab may be a better option. Side note I wish the Updates tab were restored in App Store, I check for app updates multiple times a day and read the release notes. Def inefficient to open the profile screen. (Yes I might have a problem :laughing:) Another \"heated\" topic: where to put the X/close/cancel button - top left or top right? Are there really no guidelines about it? Confuses the hell out me (as a user and developer) <@U03JRNE4KJL> I totally understand how this could be confusing. You\u2019re right, top right accessories are used for the main actions. But in the case of Modals there might be other resources more suitable - like buttons - to convey important actions. So Close takes the top right place. I\u2019d say that prioritization of the actions on screen are key here, once you have a clear understanding - you could decide what\u2019s the best pattern to follow. <@U03JRNE4KJL> a bit late but your question is answered here: https://wwdc22.slack.com/archives/C03H77GM1NW/p1654807862947179 It struck me that tab bars and navigation bars are, at least in principle, identical to the original iPhone interaction paradigm. I\u2019m wondering what the biggest difference you\u2019ve seen in navigation is, since that first iPhone? I love this question. I wish I had the time to answer more thoroughly. Here are some thoughts about tab bars and navigation bars: these components and interaction patterns clearly resonated with people. Maybe that was luck, maybe it\u2019s a pattern that reflected our natural understanding of how things are organized and arranged already in the world around us. But they have evolved and modernized while staying true to what \u2018just worked\u2019 originally. As people\u2019s comfort with apps grow, new patterns emerge to support new complexity such as sidebar being a solution specific to a new device and form factor Thank you for such an amazing WWDC! I definitely learned a lot this week. One last question from me: what are some books/resources that you would recommend every designer to read for design guidance, creative inspiration, best practices, etc (besides Apple's HIG, of course)? For me: \u2022 https://www.goodreads.com/book/show/9386.Free_Play|Free Play: Improvisation in Life and Art \u2013 Not specific to UX but this has really helped me unblock myself creatively \u2022 https://www.goodreads.com/book/show/111113.Interaction_of_Color|Interaction of Color by Josef Albers \u2013 Great color theory resource \u2022 https://www.goodreads.com/book/show/44735.The_Elements_of_Typographic_Style?from_search=true&from_srp=true&qid=Q1DklR77Li&rank=1|The Elements of Typographic Style \u2013 Great typography resource \u2022 https://www.goodreads.com/book/show/34473.In_Praise_of_Shadows|In Praise of Shadows \u2013 Lovely essay on aesthetic I lost the description of Stickers and Sticker packs from HIG. Where can I find them? <@U03HVBUV0KY> This will be coming back soon! I have heavy use of swipe buttons on different rows of a \"tree control\". (TableView with lots of different cell types). I've not found any recent writing on swipe button discoverability but watching people, average users don't seem to explore to see if swipes are available on lists. Any suggestions on affordances or onboarding approaches? I've thought about an initial animation of sliding things over enough for a button to peek through. Since swipe actions are more for power users, we always recommend that they are also exposed in the detail view. For example, if there\u2019s a delete action on your table cell, make sure there\u2019s a delete action in a toolbar, navigation bar or in-line within the secondary view. Hi, I really enjoyed the session on the new iPad navigation bar for document-based apps. I just did a lab to explore this further - one of the suggestions was to adopt a layout/hierarchy similar to the Notes app. I wanted to ask if there is any API for the file explorer (lists and grid view) that the Notes app uses. I would like to adopt this in a sidebar as well, and move my document-based app away from a tab controller. Thanks. Hi <@U03JGS1CE2H>, thank you for the question, I\u2019m not sure about an API specifically (I\u2019m just a designer!). But here are two resources from the session side that may be helpful for you: 1. The session Designed for iPad, which dives deep into the design for Sidebar https://developer.apple.com/wwdc20/10206 This year\u2019s session about Desktop Class iPad apps which covers some tips for the new toolbar design https://developer.apple.com/wwdc22/10009 Just realized you referenced the second video in your message, it\u2019s been a long week :sweat_smile:","title":"design"},{"location":"wwdc22/design-lounge.html#design-lounge-qas","text":"","title":"design-lounge QAs"},{"location":"wwdc22/design-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/design-lounge.html#what-are-the-next-challenges-youre-working-on-tackling-for-sf-symbols","text":"","title":"what are the next challenges you're working on tackling for SF Symbols?"},{"location":"wwdc22/design-lounge.html#what-design-program-do-you-prefer-for-creating-the-symbol-pathssketch-illustrator-or-something-else","text":"For custom symbols, I prefer using Illustrator. But Sketch works well too Illustrator\u2019s blending tools are helpfull For variable templates that will use interpolation, I personally like to nudge points around in a font editor before going back to Illustrator, this way I get more info on the number of points and the point order Illustrator and Glyphs 3 <@U03JEKW67KJ> Interesting. I\u2019ve never used a font editor, do you have a recommendation? Getting points correct for interpolation an issue I ran into with Sketch. I\u2019ve used Glyphs3 (Glyphs App) same as Lance mentioned, but other popular font editors include RoboFont and FontLab. All have fairly good documentation (and forums) so you should be able to get up and running quickly","title":"What design program do you prefer for creating the symbol paths...Sketch, Illustrator or something else?"},{"location":"wwdc22/design-lounge.html#when-would-we-be-able-to-see-variable-fonts-come-to-the-sf-font-family-it-would-be-awesome-to-have-this-along-with-sf-symbols","text":"The SF Pro and SF Compact fonts that are installed with the SF Symbols app are variable! BTW, SF Symbols 4 and the updated fonts should be going live this morning. They are :slightly_smiling_face: Out of curiosity, the new SF Symbol variable colors for values from zero-to-one (like volume and signal strength symbols)... are those implemented as a variable font axis? Variable color is not available in the fonts, unfortunately. For showing different rendering modes and variable color in design comps, we recommend using the Edit &gt; Copy Image Thanks, Paul! Fonts are live!","title":"When would we be able to see variable fonts come to the SF Font Family? It would be awesome to have this along with SF Symbols."},{"location":"wwdc22/design-lounge.html#after-creating-a-new-custom-symbol-how-do-we-add-it-to-our-app-is-it-like-any-other-sf-symbol","text":"After you've made and annotated your symbol in the app, choose File > Export Symbol... and bring the resulting SVG into an Xcode asset catalog. Then, you can access it like other embedded images. Thank you very much!","title":"After creating a new custom symbol, how do we add it to our app? Is it like any other SF Symbol?"},{"location":"wwdc22/design-lounge.html#hi-is-there-any-limit-of-layers-a-symbol-can-have","text":"There's not a hard-set limit in the app. There isn\u2019t a limit for practical purposes in the template either.","title":"Hi! Is there any limit of layers a symbol can have?"},{"location":"wwdc22/design-lounge.html#is-there-a-way-to-transfer-your-own-custom-symbols-with-all-the-settings-between-different-macs-or-pass-it-to-other-co-designers","text":"The file created when you choose File > Export Symbol... can be shared with other people. It can be imported back in to the SF Symbols app and should preserve all of the annotations, too! Oh nice, so it\u2019s basically an SVG that\u2019s on steroids. :smile: Or is it a different file format? Yep, File > Export Symbol will give you an SVG. If you plan on sharing it with other designers it\u2019s best to export the latest version (4.0) so that new features like monochrome layers + variable color are included Awesome. Thanks a lot for! :pray::slightly_smiling_face: We don't recommend editing the SVGs that come out of the File &gt; Export Symbol... option, however, they're just intended for data delivery. The recommended workflow is to use File &gt; Export Template... if you want to edit the shape of the symbol, bring that template back into the SF Symbols app, do your annotation within the app, and then use File &gt; Export Symbol... for delivery.","title":"Is there a way to transfer your own custom symbols with all the settings between different Mac's or pass it to other co-designers?"},{"location":"wwdc22/design-lounge.html#any-tips-for-creating-svg-files-that-will-work-well-with-this-eg-should-we-avoid-internal-groups-etc","text":"There are some great tips in the Human Interface Guidlines: https://developer.apple.com/design/human-interface-guidelines/sf-symbols/overview/ Check out the section titled \"Creating Custom Symbols\" will do, thank you!","title":"Any tips for creating SVG files that will work well with this? E.g. should we avoid internal groups, etc..."},{"location":"wwdc22/design-lounge.html#is-it-possible-to-assign-colors-dynamically-to-sf-symbols-rendering-in-multicolor-mode","text":"Multicolor rendering mode uses the colors assigned to the symbol by the designer. Use palette mode to use a dynamic set of colors. There are some parts of symbols in multicolor that are still controlled by colors you specify, however. For example, folder.fill.badge.plus in multicolor will keep the badge's colors locked to green and white, but you still have control over the color of the folder. Is it possible to dynamically assign multiple colors to the symbol rendering in palette mode? Yes. SwiftUI allows multiple foreground styles, and AppKit & UIKit allow an array of colors for palette mode Amazing! Thanks for answering I will do some reading on how to do it The API for AppKit/UIKit is configurationWithPaletteColors: which takes an array of colors And in SwiftUI what you want is the .foregroundStyle modifier: https://developer.apple.com/documentation/swiftui/view/foregroundstyle(_:)","title":"Is it possible to assign colors dynamically to SF Symbols rendering in multicolor mode?"},{"location":"wwdc22/design-lounge.html#when-creating-new-symbols-what-criteria-is-followed-ive-wanted-to-make-custom-symbols-before-but-want-it-to-match-with-the-rest-of-the-rest-of-the-sf-symbol-catalog","text":"Awesome, we have Thalia here to answer this. I\u2019m sure it might take a minute to compose her many thoughts\u2026 I would start with looking at the symbols in the context of other symbols. For example, if you are drawing a symbol about astronomy, you would want to looks at symbols that are lated like the moon, stars, etc. From there, I would start by defining the proportions, not too wide, not too tall, all related to the other symbols. And next I'll try to match the stroke thickness, roundness of the shapes, angles. It all depends on what you are trying to draw. Let's say you need a telescope. Then I'll probably try to match some of the angles of other symbols. It's about looking at the context and iterating to get the best results. It's also good to look at the symbol in small and big sizes, to craft the best drawing possible for all scenarios. Matching the line weight within SF Symbols is a good metric to follow. Also, referencing the scale and alignment of SF Symbols for shapes that are similar to what you are trying to make is a good place to start. For example, if your custom symbol is circular, scale it similar to a circle in SF Symbols.","title":"When creating new symbols, what criteria is followed? I\u2019ve wanted to make custom symbols before but want it to match with the rest of the rest of the SF Symbol catalog."},{"location":"wwdc22/design-lounge.html#are-there-any-books-youd-recommend-reading-regarding-topics-like-typography-color-theory-grid-designs-and-anything-else-regarding-mobile-ui-design","text":"Grid systems: Grid system - josef m\u00fcller brockmann The Elements of Typographic Style! The 1923 ATF Catalog and Specimen book! Interaction of Color if one of my fav books!","title":"Are there any books you'd recommend reading regarding topics like typography, color theory, grid designs, and anything else regarding mobile UI design?"},{"location":"wwdc22/design-lounge.html#how-would-you-convince-a-product-team-to-adopt-sf-symbols-that-already-has-a-wide-array-of-their-own-custom-icons","text":"<@U03HLJY8FRA>? But do their symbols have variable color support?! A big advantage is that there are so many symbols to choose from so extending the library is easy. And also the way symbols are designed to work with text (font weight matching, small and large variants). Right to left support As I said in the talk last year, the kind of design that requires no additional work from me is the best kind of design! :stuck_out_tongue_closed_eyes: But really, the team has been working on SF Symbols as a way to help people make apps that look fantastic and blend seamlessly with the rest of Apple platforms. Even more importantly, SF Symbols are also designed with accessibility and inclusivity in mind. They react to things like Dynamic Type and Increased Contrast automatically, which is a big win for users of your app. Symbols offer many benefits: visual craft and integration with the system font, accessibility features including localized variants for many languages and scripts. Familiarity with users across the platforms. Smaller app footprint.","title":"How would you convince a product team to adopt SF Symbols that already has a wide array of their own custom icons?"},{"location":"wwdc22/design-lounge.html#i-think-it-was-last-year-the-square-and-circle-modifier-were-added-for-some-symbols-is-there-a-chance-that-will-become-and-option-for-all-symbols-including-custom-symbols-grimacing-i-ended-up-creating-square-variants-for-a-lot-of-our-custom-symbols-last-year-and-it-was-super-tedious","text":"Hi Emile, we have this policy where we don\u2019t talk about future products. :slightly_smiling_face: It was worth a shot :rolling_on_the_floor_laughing: Maybe a seed was planted. Hi <@U03JDV4PQR0>, happy to see you\u2019re here!! Hello!!!! I hope you are well! For these sorts of things we always encourage capturing your use case + request in a bug via Feedback Assistant: https://feedbackassistant.apple.com/ Will do.","title":"I think it was last year the .square and .circle modifier were added for some symbols. Is there a chance that will become and option for all symbols including custom symbols? :grimacing: I ended up creating square variants for a lot of our custom symbols last year and it was super tedious."},{"location":"wwdc22/design-lounge.html#will-there-ever-be-a-clarus-the-dogcow-sf-symbol-d","text":"Moof? Moof. :dog::cow2: MOOF! Moof, definitely. ~ Moof ~ Moof Moof! Moof!!! Moof!!! Mooooooooof! Omg this is amazing! :sweat_smile::joy: See if you can spot Clarus: https://www.wallpaper.com/design/apple-park-behind-the-scenes-design-team-interview Moof? Found it! Love all the vintage Mac icons! Found it! So is there a clarus hidden within the SF Symbols app?!?! :face_with_monocle::face_with_monocle: The Great Moof Mystery of WWDC22 I\u2019m still looking for that <@U03HA9EC4T1>! I need glasses I think I only get this because I was randomly watching this the other day (linked to the relevant timestamp) https://youtu.be/2bewvHiWLYA?t=927","title":"Will there ever be a Clarus the DogCow SF Symbol? :D"},{"location":"wwdc22/design-lounge.html#can-we-use-sf-or-sf-symbols-in-other-non-apple-related-projects-or-are-we-limited-to-apple-platforms-thanks-blush","text":"Hi Sean, only for software running on Apple Platforms SF Symbols are for Apple platforms only. Great thanks! :grinning: SF Symbols as you have seen are very dynamic. This functionality is implemented by Apple\u2019s OSs. So besides the restriction, it isn\u2019t practical.","title":"Can we use SF or SF Symbols in other, non-Apple related projects or are we limited to Apple platforms? Thanks :blush:"},{"location":"wwdc22/design-lounge.html#paul-what-are-some-similarities-between-solving-rubiks-cubes-and-working-on-the-sf-symbols-app","text":"<@U03HLJY8FRA> Got an answer here? <@U03J3FV12KB>? Anyone is welcome to answer this pressing question. I don\u2019t do either so my answer is that it seems like a lot of work. LOL Lots of colors, lots of moving pieces, but in the end it all comes together to make something completely unified! (how's THAT for deep) Wow Paul, real deep. :slightly_smiling_face: That sounds like a nice way to bring this study hall to an end\u2026","title":"Paul, what are some similarities between solving Rubik's Cubes and working on the SF Symbols app?"},{"location":"wwdc22/design-lounge.html#not-really-related-to-the-session-but-are-we-allowed-to-export-and-use-sf-symbols-in-a-cross-platform-app-eg-an-app-for-windowslinuxmac-or-are-the-symbols-only-allowed-for-use-on-apple-platforms","text":"SF Symbols are for Apple platforms only. Okay, thanks","title":"Not really related to the session, but are we allowed to export and use SF Symbols in a cross platform app (e.g. an app for Windows/Linux/Mac), or are the symbols only allowed for use on Apple platforms?"},{"location":"wwdc22/design-lounge.html#hey-thanks-for-the-epic-video-is-there-anyway-to-use-new-symbols-from-ios-16-in-ios-15-and-below-thanks-grinning","text":"It's possible to export a new symbol using File &gt; Export Symbol... and bring that into your app to support lower deployment targets. Remember to adjust the symbol version accordingly, though! 3.0 for last year's releases and 2.0 for the year before that. Note that you can still use the latest template format (4.0) with custom symbols even when deploying to earlier deployment targets as long as you are using Xcode 14 to compile your app. Naturally, the new content features won't be present on those earlier releases, so you need to be careful about which template content features you use. Safest of course is to just export with the older template versions as Paul recommends.","title":"Hey! Thanks for the epic video! Is there anyway to use new symbols from iOS 16 in iOS 15 and below? Thanks :grinning:"},{"location":"wwdc22/design-lounge.html#id-love-to-hear-more-about-what-it-was-like-working-on-this-massive-update-to-sf-symbols-how-do-you-go-about-figuring-out-which-symbols-to-choose-is-there-a-lot-of-collaboration-across-teams-within-apple-for-such-projects-also-amazing-work","text":"I think I can answer the second part of the question only :wink: but yes, there is definitely a lot of collaboration between teams to define what are the best symbols and design that fit best in the UIs. We want to make the symbols as good as possible, so there is a lot of iterations too! I can\u2019t imagine the amount of work it must take when the library is so huge and each symbol is so carefully crafted. Thanks for your answer! There's also an incredible amount of work that comes in from the frameworks and platform engineering sides to support all these new features like Variable Color and different rendering modes! For adding new symbols: it\u2019s a combination of supporting new platform features, adopting symbols for existing features and identifying concepts that have broad utility and application across a variety of apps and contexts. Thanks so much for your replies, everyone! I super appreciate all of your hard work! :heart:","title":"I\u2019d love to hear more about what it was like working on this massive update to SF Symbols. How do you go about figuring out which symbols to choose? Is there a lot of collaboration across teams within Apple for such projects? Also: amazing work!"},{"location":"wwdc22/design-lounge.html#sf-symbols-are-underrated-they-really-make-designing-an-effective-ui-a-lot-easier-great-work-with-the-new-updates-it-would-be-awesome-to-see-collections-expanded-to-an-in-app-feature-to-allow-users-to-pick-from-a-set-like-in-the-reminders-app-unless-this-already-exists-and-i-am-unaware-grin","text":"Thank you. We don't offer that feature, but thanks for feedback!","title":"SF Symbols are underrated, they really make designing an effective UI a lot easier. Great work with the new updates! It would be awesome to see collections expanded to an in app feature to allow users to pick from a set (like in the Reminders app). Unless this already exists and I am unaware :grin:"},{"location":"wwdc22/design-lounge.html#are-there-ways-we-can-submit-icons-for-consideration-as-additions-to-future-updates-of-sf-symbols-andor-is-there-a-place-we-can-share-our-custom-icons-with-the-wider-developer-community","text":"Absolutely. Use Feedback Assistant. Requests get routed right to the team for consideration It\u2019s been a very helpful way to learn about what symbols developers most want. Great, thank you! I work on a lot of medical/health apps that would benefit from SF symbols. My Feedback Assistant request for a tennis ball from 2020 is still open, but I'm stoked to see new symbols for a tennis racket and a player serving! I think a differentiated experience from Feedback Assistant for SF Symbols feedback would be helpful. There would probably be a lot more involvement from the community.","title":"Are there ways we can submit icons for consideration as additions to future updates of SF Symbols and/or is there a place we can share our custom icons with the wider developer community?"},{"location":"wwdc22/design-lounge.html#i-may-have-missed-this-in-the-sf-symbols-video-but-i-was-wondering-how-do-these-new-rendering-modes-interact-with-widgetkit-rendering-using-the-new-lock-screen-widget-layouts-like-accessorycircular-etc","text":"Great question. For consistency, we suggest using monochrome, hierarchical, or automatic, but WidgetKit may not actually end up enforcing that in the betas. We also welcome your feedback on WidgetKit and SF Symbols in general as you get a chance to use them together. We learn from you, too!","title":"I may have missed this in the SF Symbols video, but I was wondering: how do these new rendering modes interact with WidgetKit rendering using the new lock screen widget layouts like .accessoryCircular etc?"},{"location":"wwdc22/design-lounge.html#not-a-question-just-a-thanks-for-adding-symbols-that-represent-homekit-objects-was-hoping-for-those-since-sf-symbols-was-introduced","text":"YAY! :heart: Really means a lot to the HomeKit community as they mean most apps will end up using similar icons now.","title":"Not a question, just a thanks for adding symbols that represent HomeKit objects. Was hoping for those since SF Symbols was introduced."},{"location":"wwdc22/design-lounge.html#do-variable-colors-animate-automatically-when-changed","text":"Hi <@U03JBMMB10A> can you elaborate a bit? There isn't any animation when the variable color percentage changes and causes a layer to \"dim\" or \"un-dim.\" The change appears instantly. Okay, thank you. The path color will instantly change according to the corresponding percentage value. Using variable colors does not automatically animate across the visual states. However it is entirely possible to add animation to Symbol just as you would to other UI Images using the UI framework animation facilities of your choice. Right, thank y\u2019all!","title":"Do variable colors animate automatically when changed?"},{"location":"wwdc22/design-lounge.html#thank-you-for-the-new-hig-especially-appreciate-that-it-starts-from-ui-elements-and-themes-and-then-mentions-platform-considerations-instead-of-being-branched-off-into-different-platforms-only-are-there-good-examples-that-come-to-mind-of-intentionally-breaking-the-hig-for-good-reason-after-knowing-it-well","text":"Great question! I think that starts coming down to the customization you'd love to do for the app. For example there's no hard and fast rule that you have to use SF Symbols, or Apple Typefaces or a specific aesthetic for your app (and that's even more true for games). Think of the HIG as a great \"base\" for what you're doing, and tailor the guidance based on what makes the most sense for your users! <@U03HJ86L1BL>\u2019s also done a great job at wording the language of the HIG so we rarely ever say \"don't\" or \"never\" unless that's really true.","title":"Thank you for the new HIG! Especially appreciate that it starts from UI elements and themes and then mentions platform considerations, instead of being branched off into different platforms only.   Are there good examples that come to mind of intentionally breaking the HIG for good reason (after knowing it well)?"},{"location":"wwdc22/design-lounge.html#what-are-the-key-changes-in-the-advice-being-given-in-the-revised-hig-if-any","text":"Hey Duncan, a lot of the content in the HIG carries through from previous versions. But a lot of the older content was adjusted for clarity and we removed things that no longer seemed all that relevant. It\u2019s hard to go through all the specifics but pretty much every page needed to be edited. We also started adding in new guidance for iOS 16, macOS Ventura, and watchOS 9 (though there\u2019s still a lot more coming this summer One example I recall was the Buttons page. There was content in there about placard buttons and bezel buttons but those really aren\u2019t used these days We also removed some of the content about full color icons in toolbars, for example, since that\u2019s not something modern apps do all that much of. Another key change is in perspective: Rather than starting with guidance for an iOS experience, the revised HIG encourages designing the experience first and then adjusting it as needed to run great on each platform. We added a lot of new information in the Patterns section for things like Searching or Sharing. That kind of content was spread out in component pages previously and missed a lot of important information and context when it was tied to specific controls","title":"What are the key changes in the advice being given in the revised HIG, if any?"},{"location":"wwdc22/design-lounge.html#first-off-congrats-on-the-new-hig-i-love-how-much-more-comprehensive-it-is-what-are-best-practices-when-designing-an-app-that-is-cross-platform-should-the-app-designs-adapt-to-the-respective-os-design-guidelines-eg-hig-on-ios-material-you-on-android-and-fluent-on-windows-or-stick-with-one-for-a-consistent-user-experience-across-different-platforms-even-though-it-might-look-slightly-out-of-place-depending-on-the-os","text":"The best approach is to follow the relevant platform conventions. Many of them exist because of how the hardware functions. So ignoring that leads to software that feels disconnected from the hardware. I also wonder to what extent it's applicable to the web, especially considering seeing more web apps coming from Apple! Apps that are designed once for all platforms will never feel right everywhere It does apply, there are differences in conventions between web and apps. Some of the difference are pretty easy to address. Like not using \u201cClick this button\u201d in an app that only runs on touch screen devices ^ huge pet peeve! Also Android and iOS differ a lot in how \"back\" functionality works for example. So making sure that the design patterns follow system conventions helps quite a bit to not make your app feel like it's alien to the platform Thanks for the insight, <@U03HBJXV0TY> and <@U03DMQBFPH8>! They are really helpful. I'll definitely share them with my team who insists on having the same design across different platforms (hamburger menus and FABs everywhere!) Look I love a good FAB but only when I've designed for Android in the past! Thanks for the question <@U03J20D4Q03> hope that helps because yes, you can be 80%-90% consistent where you can but those platform details matter","title":"First off, congrats on the new HIG! I love how much more comprehensive it is. What are best practices when designing an app that is cross platform? Should the app design's adapt to the respective OS' design guidelines e.g. HIG on iOS, Material You on Android, and Fluent on Windows, or stick with one for a consistent user experience across different platforms (even though it might look slightly out of place depending on the OS)?"},{"location":"wwdc22/design-lounge.html#id-love-to-know-more-about-the-process-of-evolving-the-hig-how-do-you-identify-things-that-feel-out-of-date-and-less-relevant-as-well-as-spotting-new-design-trends-within-apple-that-you-feel-need-documenting","text":"This is a great question. Multiple things influence this. One is our work (in evangelism) with developers. Questions that we get during this work can often spark ideas for new areas of guidance, or new details to add to pages Another thing that influences the HIG is the work of our design and engineering teams, naturally. As the platforms evolve, there are needs to overhaul pages, or whole sections, to match the direction of the platform (or multiple platforms) +1 to what Doug said! We talk with so many designers throughout the year and for this particular redesign we reached out specifically to the community to get input on where we could improve the HIG. I am resisting the urge to point out the various sections that are in need of work now, although I\u2019d like to illustrate the point I\u2019m making by doing so","title":"I'd love to know more about the process of evolving the HIG.  How do you identify things that feel out of date and less relevant, as well as spotting new design trends within Apple that you feel need documenting?"},{"location":"wwdc22/design-lounge.html#whats-stayed-the-same-in-the-hig-over-the-past-decades","text":"Although many details have changed, I think it's the keen focus on keeping the user experience at the center of design that's at the heart of the HIG, from the earliest days to the present. Yes the earliest HIG (I believe from 1977???) continually emphasized the human nature of computers. How computer should conform to people and not the other way around. We are still very much firm believers of this, but with more nuance on what that means ergonomically, inclusively, as a holistic experience.","title":"What\u2019s stayed the same in the HIG over the past decades?"},{"location":"wwdc22/design-lounge.html#were-there-any-topicsareas-that-were-completely-or-mostly-rewritten-any-specific-portions-that-you-are-most-proud-of-that-youd-really-like-to-call-out","text":"I haven't had time to dig in as much as I'd like to yet, but at just the bit I have it looks really fantastic! Miss seeing you folks btw, sad I wasn't able to make it out this year :slightly_smiling_face: That's a great question! I want to say that almost every section was rewritten to take out old, missing, or revised guidance :sweat_smile: It was a huge haul. Two sections that I personally love: https://developer.apple.com/design/human-interface-guidelines/foundations/inclusion|Inclusion: We're SO happy this page is here because it speaks not only to good design guidance but our values https://developer.apple.com/design/human-interface-guidelines/platforms/overview|The new Platform pages: These are brand spanking new and give a nice overview of \"getting started with designing for [ios/mac/etc]\" that also speak to how many people use these devices and the considerations you need to have. > that were completely, or mostly rewritten I should clarify this, I guess what I'm wondering is if there are areas where in the rewrites the guidance has fully changed overall, rather than clarified or revised. The platform pages look great btw and I would imagine it's going to be super helpful for a lot of folks to have those as starting points :heart: Because we were working with content that ranged in age from months to more than a decade (in a few cases), I'm certain that there were places where in rewriting, we made significant changes to the guidance. But honestly, I'm not sure I can point to specific ones offhand! :face_with_monocle:","title":"Were there any topics/areas that were completely, or mostly rewritten? Any specific portions that you are most proud of that you'd really like to call out?"},{"location":"wwdc22/design-lounge.html#hi-nice-work-on-the-redesign-ive-been-going-through-it-this-morning-i-have-a-question-about-color-on-this-page-httpsdeveloperapplecomdesignhuman-interface-guidelinesfoundationsdark-modehttpsdeveloperapplecomdesignhuman-interface-guidelinesfoundationsdark-mode-theres-the-advice-at-a-minimum-make-sure-the-contrast-ratio-between-colors-is-no-lower-than-451-for-custom-foreground-and-background-colors-strive-for-a-contrast-ratio-of-71-especially-in-small-text-i-aim-for-high-contrast-as-much-as-possible-but-451-is-pretty-limiting-and-i-dont-think-apple-always-follows-this-rule-themselves-specifically-im-wondering-about-the-systemcolors-such-as-systemorange-systemmint-etc-many-of-which-have-quite-low-contrast-against-white-ie-in-light-mode-what-is-the-intended-purpose-of-these-colours-is-it-meant-for-textsf-symbols-because-if-so-systemmint-comes-in-at-2021-against-white-should-we-ever-even-use-systemyellow-or-can-we-just-rely-on-users-turning-on-increase-contrast-if-they-need-to-but-maybe-someone-doesnt-want-to-have-to-increase-contrast-for-everything-just-so-they-can-see-that-darn-yellow-text-any-insight-on-the-intended-use-of-these-systemcolors-would-be-greatly-appreciated-thanks","text":"Awesome questions Jessiah! I could take the next 45 minutes trying to answer. :slightly_smiling_face: In short, the rules about color contrast are a bit tricky, it\u2019s not black and white (forgive the pun) Higher contrast is always better but there are mitigating circumstances that make it OK to have lower contrast. For example, text size. The larger or bolder the font, the easier it is to read given the same color value But a color choice that works for large type can be too low contrast for smaller text. The same applies for icons or glyphs (symbols): Finer features are harder to naturally harder to discern so boosting contrast is useful As for system colors, they don\u2019t all work equally well in every circumstance. Orange or mint on a light background probably isn\u2019t the best choice. Definitely not as legible as blue or purple Yeah I was wondering about the Notes app, it looks like some darkening is being applied to the tint colour in this case, rather than using the default system color? to I\u2019ve worked on products where I had to incorporate colours that are notoriously difficult to have high contrast - like teal/green. In those instances i\u2019ve provided darker versions, that are perhaps not so aesthetically pleasing, but pass AA/AAA to assist visually impaired users once they enable accessible system settings. It has worked for me :heart: Yeah for our app which uses orange tint, we\u2019ve used our own custom orange for light mode, and only use the sytemOrange for dark mode My personal guide when designing is to make all elements that need to be legible and clear have strong contrast. Details like a divider, for example, can be lighter to convey less prominence. But text should always be readable and accessible. I also feel like I could spend the next hour speaking about this :sweat_smile: so GREAT question. As you might know, the 4.5:1 contrast ratio comes from WCAG 2.0 guidelines. While WCAG 3.0 is still not officially \"out\" they have an improved way of calculating contrast that I find incredibly interesting: https://www.w3.org/TR/wcag-3.0/#visual-contrast-of-text nice I\u2019ll take a look :thumbsup: I\u2019m imagining a future accessibility feature that allows you to increase contrast per colour :stuck_out_tongue: :wink: or colour range Hahahahah, yes <@U03J1EHL4KY> Providing high-contrast colors in the asset catalog is a good thing. :slightly_smiling_face: We do have some technical solutions after all. And of course the system colors provide high-contrast colors for free","title":"Hi, nice work on the redesign, I've been going through it this morning! I have a question about color.  On this page: https://developer.apple.com/design/human-interface-guidelines/foundations/dark-mode|https://developer.apple.com/design/human-interface-guidelines/foundations/dark-mode there's the advice \"At a minimum, make sure the contrast ratio between colors is no lower than 4.5:1. For custom foreground and background colors, strive for a contrast ratio of 7:1, especially in small text.\"  I aim for high contrast as much as possible, but 4.5:1 is pretty limiting, and I don't think Apple always follows this rule themselves. Specifically I'm wondering about the systemColors such as systemOrange, systemMint etc, many of which have quite low contrast against white (i.e., in Light Mode). What is the intended purpose of these colours? Is it meant for text/SF Symbols? Because if so, systemMint comes in at 2.02:1 against white. Should we ever even use systemYellow? Or can we just rely on users turning on Increase Contrast if they need to? But maybe someone doesn't want to have to increase contrast for everything just so they can see that darn yellow text.  Any insight on the intended use of these systemColors would be greatly appreciated. Thanks!"},{"location":"wwdc22/design-lounge.html#when-filing-feedback-related-to-the-hig-in-feedback-assistant-should-it-be-under-the-documentation-category-or-design-resources","text":"First of all, thank you SO much for filing feedback!! For the HIG, please use the \"documentation\" category; for issues or requests related to Apple Design Resources, please use the \"design resources\" category.","title":"When filing feedback related to the HIG in Feedback Assistant, should it be under the \u201cdocumentation\u201d category or \u201cdesign resources\u201d?"},{"location":"wwdc22/design-lounge.html#how-do-ada-winning-apps-balance-the-best-practices-and-standards-in-the-hig-with-the-novelty-that-makes-themwellada-winning","text":"Well you hit on the two, sometimes opposing, forces that define ADA winners In my view, an ADA winner has to embody the spirit of the HIG, even if there are details that might not conform to the letter of the HIG But an equally important thing is the voice, or personality, of the app The categories that we introduced last year (2021) try to add a bit more substance and clarity around the qualities that define winning apps <@U03H3HNAGSK> Can you elaborate on the voice/personality please? Yeah, fair question I guess I just see those terms as representing two things \u2014 (1) a cohesiveness across the app \u2014 the app feels like a single experience, and is internally consistent, and (2) the app has a character or look or feel that is memorable Ohhh I think I gotcha, like how Carrot Weather uses satire and the robotic voice to strengthen the personality of the app\u2019s narrator? Yes there's being consistent with the platform experience, and then there's taking that to the next level by differentiating your app/game as a stellar example of one of our categories. That means you don't necessarily need a visually-heavy custom branded app but you offer something by way of your design/UX that sets you apart (I'm thinking Slopes here as a great example of this)","title":"How do ADA-winning apps balance the best practices and standards in the HIG with the novelty that makes them...well...ADA-winning?"},{"location":"wwdc22/design-lounge.html#could-you-elaborate-on-any-user-research-quantitive-or-qualitative-processes-the-team-uses-to-shape-hig-guidelines","text":"(These questions are good!) We meet with developers to ask them for feedback on existing content (qualitative). We also regularly talk with developers throughout the year about their apps and, in those conversations, common questions come up that we realize we haven\u2019t really answered in the HIG (or videos). So those turn into ideas about how we can improve the HIG. And the HIG is really a massive collective effort involving design and engineering teams throughout Apple. This HIG is the product of internal experiences where people had misconceptions or questions during the design process that we anticipate designer and developers will have once we ship new features. Does the team do any old school, Bruce Tognazzini-style quantitative experiments when designing new features? I\u2019m thinking of the equivalent of Fitts\u2019s Law, but for touch. <@U03JCS2C03Z> Depends on the team! We always nerded out about that and other things (catmull rom) on the Prototyping team, but other teams keep their process more intuitive/empathy-based vs. scientific","title":"Could you elaborate on any user research (quantitive or qualitative) processes the team uses to shape HIG guidelines?"},{"location":"wwdc22/design-lounge.html#is-the-latest-version-of-the-hig-the-first-time-ive-seen-apple-adopt-sentence-case-for-headings-smile","text":"Ha! Nice catch :smile: We also use sentence-casing throughout the developer app Love the attention to detail! Also WWDC videos It\u2019s the future! I\u2019m sure this is in the HIG if so :grin: but figured it may be worth asking, are yall changing recommendations for when to use title-case vs sentence-case? :eyes:","title":"Is the latest version of the HIG the first time I've seen Apple adopt sentence case for headings?! :smile:"},{"location":"wwdc22/design-lounge.html#a-web-based-hig-is-great-as-documentation-but-right-nowwanting-to-read-the-entire-revised-higi-want-to-reach-for-an-epub-that-would-keep-track-of-where-im-up-to-and-lead-me-linearly-from-start-to-finish-to-ensure-i-read-the-whole-thing-is-an-epub-available-or-is-there-another-solution-for-this-use-case","text":"Thanks for the question! Could you submit this idea using http://feedbackassistant.apple.com ? Right now there isn't mostly because we've found folks stop by the HIG because they're looking for specific documentation vs. reading it like a whole book. Yes, I agree! I would love it if there were an ePUB or Apple Books version of the HIG, like the one for the Apple Style Guide. Great for offline and bedtime reading! A physical HIG book would be some nice swag! I know it's a living document though :grinning: HIG used to be available in iBooks! Back when it was separate for OS X vs iOS. Still have em in my Library. It\u2019s reallllly nice being able to use the search feature. And I added the original HIG from a PDF too :heart: I have some of the old HIG books in front of me! FB10108496: Provide a mechanism to keep track of progress in reading the whole HIG ^ in which I quickly argue for either progress indicators and \u201cNext\u201d links at the bottom of each page on the web version, or an ePub version, and do so quickly in the hopes of linking the FB number here before the lounge closes. :partying_face: <@U03HMCT187R> I included the superior search options in an ePub btw. Great point.","title":"A web based HIG is great as documentation but right now\u2014wanting to read the entire revised HIG\u2014I want to reach for an ePUB that would keep track of where I\u2019m up to, and lead me linearly from start to finish, to ensure I read the whole thing. Is an ePUB available, or is there another solution for this use case?"},{"location":"wwdc22/design-lounge.html#not-a-question-but-i-love-the-new-hig-redesign","text":"Not an answer but THANK YOU! - from all of us","title":"Not a question but I LOVE the new HIG redesign!!!"},{"location":"wwdc22/design-lounge.html#hi-there-amazing-work-on-the-design-guidelines-how-can-i-measure-contrast-in-my-apps","text":"Thank you so much! There are good tools online for measuring color contrast. Searching for \u201ccolor contrast calculator\u201d will return good results https://coolors.co/contrast-checker/112a46-acc8e5 https://colourcontrast.cc https://contrast-ratio.com Thank you very much, and again, amazing work on the HIG! You're listing the nice looking ones \u2013 I've always used this: :sweat_smile: https://webaim.org/resources/contrastchecker/ LOL Yes, it\u2019s true, I only picked the nice looking ones. <@U03DMQBFPH8> so funny, that\u2019s the one I use too It\u2019s about design after all:sweat_smile: If you use Sketch, the Stark plugin is a great in-app option Xcode has a built-in Color Contrast Calculator. To open it: 1. Open the Accessibility Inspector by choosing Xcode > Open Developer Tool > Accessibility Inspector. 2. In Accessibility Inspector, choose Window > Show Color Contrast Calculator Cluse is also a great one I love this one. It sits in the menu bar so you can use it anywhere. https://apps.apple.com/gb/app/contrast-color-accessibility/id1254981365?mt=12 <@U03J1EHL4KY> Same with XD I'm really hoping more tools start adding the LCH (lightness, chroma, hue) color space, it makes trying to keep similar contrast across different colors much easier since it (mostly) keeps the same perceptual contrast while just adjusting the hue. Good article on it if anyone is a color nerd like me and wants to dig in more: https://lea.verou.me/2020/04/lch-colors-in-css-what-why-and-how/ not a color nerd but often feel like i should be :joy:","title":"Hi there! Amazing work on the design guidelines. How can I measure contrast in my apps?"},{"location":"wwdc22/design-lounge.html#hi-thank-you-for-update-for-great-hig-is-there-any-design-files-or-design-system-in-figma-sketch","text":"<@U03HBJXV0TY> <@U03H3LD2M2T> Will be released on this page? https://developer.apple.com/design/resources/ I hope to be also in Figma. Japanese designers love to use Figma now:pleading_face: The Sketch libraries are so cool - I love poking around in there to see how things are set up. For Figma, https://www.figma.com/@joey|Joey Banks has become the unofficial supplier of iOS UI Kits there :wink: So glad you shouted out Joey! He's doing great work :slightly_smiling_face: We currently don't support Figma at this time but updates to the Sketch and XD libraries and templates are coming later this summer Thank you Linda, I\u2019m waiting for release new ones!","title":"Hi, thank you for update for great HIG! Is there any design files or design system in Figma / Sketch?"},{"location":"wwdc22/design-lounge.html#hi-all-the-hig-for-watchos-subscription-paywall-show-buttons-for-tc-and-privacy-policy-but-we-cannot-open-these-in-a-website-like-on-ios-what-is-the-recommended-way-to-show-these-potentially-very-large-text-elements-on-the-apple-watch","text":"Hi <@U03JMMN8659>. This is a great question. The recommendation at this point is to display the T&C and Privacy Policy text in (likely very long) modal sheets or detail views on watchOS If you haven\u2019t already, can you file feedback for this? ok, thank you! :pray: will do","title":"Hi all! The HIG for watchOS subscription paywall show buttons for T&amp;C and Privacy Policy. But we cannot open these in a website like on iOS. What is the recommended way to show these, potentially very large, text elements on the Apple Watch?"},{"location":"wwdc22/design-lounge.html#how-long-has-the-team-been-working-on-this-huge-hig-update","text":"The bulk of the work started late 2021 and ran to last Sunday or so! :sweat_smile: Seriously, this project has been our hearts for a long time and we're ecstatic to deliver this update. hahaha, yes we've been working on it consistently for about a year now!","title":"How long has the team been working on this huge HIG update?"},{"location":"wwdc22/design-lounge.html#firstly-congrats-on-such-a-great-update-to-the-hig-clap-throughout-the-os-theres-often-times-when-a-temporary-sheet-appears-at-the-bottom-of-the-screen-for-example-when-connecting-your-airpods-to-the-iphone-ive-made-my-way-through-the-guidelines-and-cant-find-any-write-ups-about-that-view-is-this-view-actually-just-a-medium-sheet-that-is-styled-differently-ill-thread-a-screenshot-for-context","text":"Hm\u2026 not sure if my reply came through. Posting again \u201cHi Ryan, yep, a medium height sheet is the closest system-provided element.\u201d Thanks <@U03HBJXV0TY> :ok_hand:","title":"Firstly, congrats on such a great update to the HIG! :clap: Throughout the OS, there's often times when a temporary sheet appears at the bottom of the screen. For example, when connecting your AirPods to the iPhone - I've made my way through the guidelines and can't find any write ups about that view. Is this view actually just a \"Medium sheet\" that is styled differently?  I'll thread a screenshot for context"},{"location":"wwdc22/design-lounge.html#curious-what-your-pov-is-on-link-buttons-specifically-buttons-embedded-within-text-content-on-native-there-are-some-cases-where-this-type-of-treatment-can-make-sense-and-then-others-where-it-feels-like-were-just-pulling-over-from-web-do-you-have-any-formalized-process-around-when-to-use-and-not-use-this-treatment","text":"Good question! My sense is that links inline commonly get overlooked because (a) not expected in apps and (b) the color distinction is often too subtle. In general they should be avoided. But there are cases where it can work. One that comes to mind are \u201cMore >\u201d links at the end of a paragraph of text where you navigate to a child view to see more. Or expand a partially revealed block of text. But it\u2019s almost always more clear to use a proper button or table row with a chevron. Probably the biggest issue with link style buttons, especially when presented in context of other text, is that the tap target size is too small (not 44pt tall) so tapping accuracy is compromised Thanks for the response. Right, we\u2019ve certainly argued from an accessibility perspective - tap targets and contrast. The challenge is (as component devs/designers) to come up with a general alternative. We\u2019ve seen push back that there is an efficiency to associating an action with a verb in a sentence. Pulling actions out to separate buttons can end up elevating them. T&C or legal section, for example.","title":"Curious what your pov is on link buttons, specifically buttons embedded within text content on native.  There are some cases where this type of treatment can make sense and then others where it feels like we\u2019re just pulling over from web.  Do you have any formalized process around when to use and not use this treatment?"},{"location":"wwdc22/design-lounge.html#i-wanted-to-say-that-i-really-love-the-unified-experience-of-the-supported-platform-the-updated-hig-adopts-i-have-recently-come-across-a-component-that-the-native-ios-maps-app-uses-to-filterrefine-searched-results-which-i-cant-seem-to-find-reference-of-in-the-hig-i-am-not-sure-it-if-is-a-combination-of-a-segmented-controls-and-pull-down-buttons-is-this-filtering-component-something-that-will-be-added-to-the-guidelines-in-the-near-future","text":"This is the view I am referring to in iPadOS for Maps. Hi <@U03JL795TE2>. I think I know the component that you\u2019re talking about \u2026 It\u2019s the bar below the string? With the filter button, and the toggles? Hi <@U03H3HNAGSK>, yes thats correct. So the filter buttons. I had also noticed during the WWDC Keynote on Monday that the updated Home app on iOS adopts a similar component with different functionality. So my question expands to this as well. Ahh, I see. Toggle-able filters / buttons. Something like this may be the beginning of a pattern that is adopted across Apple apps, or these may be one-off solutions within particular apps. From a HIG POV, there is a bit of push-pull with this. If it turns into a pattern, and if that pattern is formalized as a specific element in a framework, that is typically the criteria for arrival in the HIG This is an interesting one, though. Thanks for raising it. Appreciate the feedback on this <@U03H3HNAGSK>. Thanks for asking, this is a fun one Makes me realize that, for a concrete control like this, the HIG is relatively conservative; something really needs to be a component that is defined in a framework to get guidance at this level. But for bigger picture, experiential things, the HIG is a bit more predictive? Or proactive? I've seen this type of control over the years in quite a few apps. Here is an example from Apple News on iOS 15 without the borders but works the same. Think most people understand how to use this control. Interestingly this has been changed in iOS 16 Beta 1 to a dropdown type control. Sorry correct screenshot.","title":"I wanted to say that I really love the unified experience of the supported platform the updated HIG adopts.  I have recently come across a component that the native iOS Maps app uses to filter/refine searched results which I can't seem to find reference of in the HIG. I am not sure it if is a combination of a segmented controls and pull-down buttons.  Is this filtering component something that will be added to the guidelines in the near future?"},{"location":"wwdc22/design-lounge.html#any-design-guidelines-for-edr-color-on-apple-platforms","text":"Mostly we just want people to add color profiles to their images. If we could do that, it\u2019d be a huge win. We don\u2019t really have guidance specifically about EDR in the HIG. Might be some good documentation elsewhere on http://developer.apple.com|developer.apple.com but I\u2019m not familiar with it. Color profile tagging is critical, esp for P3! AFAIK, images with EDR profiles don\u2019t show up in EDR across the OS, though videos do. Photos does do EDR for photos, though, so it\u2019s clearly something where thought has gone into it.","title":"Any design guidelines for EDR color on Apple platforms?"},{"location":"wwdc22/design-lounge.html#what-prototyping-app-would-you-recommend","text":"Such a good question. There are lots of good methods and tools out there that there\u2019s really no one good answer. That one you're fastest in! It depends on what you like OK, that was a good answer Linda Currently using Sketch and XD, Figma is something I want to try :eyes: Principle, Figma, Sketch, XD, Keynote, paper and pencil,\u2026 Funny you say Keynote, that's actually what I began designing in :joy: Principle is underrated! They\u2019re really all good in their own ways. We use Sketch a lot at Apple. XD. Keynote <@U03HBJXV0TY> would the paper and pencil prototyping be used in the initial phase of the design process at Apple? Xcode and SwiftUI may also be great for prototyping. <@U03JL795TE2> you bet! Some people love paper sketches first (me) others go straight to SwiftUI because they're lightning fast in it","title":"What prototyping app would you recommend?"},{"location":"wwdc22/design-lounge.html#in-your-guidance-around-colors-have-you-all-started-digging-into-color-spaces-like-lch-and-perceptual-color-contrast-much-yet-maybe-not-something-you-can-discuss-yet-but-its-a-super-interesting-area-and-trying-to-choose-colors-with-a-matching-human-perceptual-contrast-across-the-palette-is-super-useful-and-something-id-love-to-see-apple-push-as-well","text":"I also think it's a great area to explore! I posted this in another question but the WCAG 3.0 perspective on color contrast is fascinating and something that addresses our issues with how color contrast is calculated now It really is fascinating stuff. And how it's looking to take into account type size, boldness, etc. into the calculation in a much more nuanced way than just large or small type. Would be awesome to see Apple push some things like this outside of WCAG as well. eg https://blog.datawrapper.de/color-contrast-check-data-vis-wcag-apca/","title":"In your guidance around colors, have you all started digging into color spaces like LCH and perceptual color contrast much yet? Maybe not something you can discuss yet but it's a super interesting area and trying to choose colors with a matching human perceptual contrast across the palette is super useful and something I'd love to see Apple push as well."},{"location":"wwdc22/design-lounge.html#do-the-hig-and-apple-style-guide-influence-each-other","text":"Yes! <@U03HJ86L1BL> will be here in a minute to say more. :wink: If you\u2019re interested in knowing more about writing for apps, we have this session coming up tomorrow: https://developer.apple.com/wwdc22/10037 LOL, thanks, <@U03HBJXV0TY>! Yes, definitely: Even though the use cases differ for these documents, both try to stay aligned with the other.","title":"Do the HIG and Apple Style Guide influence each other?"},{"location":"wwdc22/design-lounge.html#its-a-super-fun-detail-that-the-design-page-on-httpdeveloperapplecomdeveloperapplecom-changes-colour-how-many-different-looks-are-there","text":"I refreshed and counted 6 :laughing: Some different looks. Yep, six colors Ohhh\u2026 of course! :wink: They\u2019re meant to be read in order matching the old 6 colors logo. Tells a little bit of a story about making a button. Love it!","title":"It\u2019s a super fun detail that the Design page on http://developer.apple.com|developer.apple.com changes colour. How many different looks are there?"},{"location":"wwdc22/design-lounge.html#outline-views-are-in-the-hig-as-being-macos-only-but-the-need-for-a-nested-outline-can-still-occur-on-ios-the-hig-just-says-not-supported-but-thats-from-a-control-aspect-ive-implemented-them-by-using-different-list-cell-layout-to-provide-a-nesting-but-would-welcome-other-suggestions-httpsdeveloperapplecomdesignhuman-interface-guidelinescomponentslayout-and-organizationoutline-viewshttpsdeveloperapplecomdesignhuman-interface-guidelinescomponentslayout-and-organizationoutline-views","text":"Here's an example of these home-grown outlines (note there's no collapsing, which I'm mulling over). <@U03HJ7LRK43> Howdy! This is a great question \u2014 we're aware of the need for better guidance around outline views on platforms other than macOS, and it's on our radar for an update over the summer.","title":"Outline views are in the HIG as being macOS only but the need for a nested outline can still occur on iOS. The HIG just says not supported but that's from a control aspect. I've implemented them by using different list cell layout to provide a nesting but would welcome other suggestions.  https://developer.apple.com/design/human-interface-guidelines/components/layout-and-organization/outline-views|https://developer.apple.com/design/human-interface-guidelines/components/layout-and-organization/outline-views"},{"location":"wwdc22/design-lounge.html#im-a-big-believer-in-human-interface-guidelines-however-id-like-to-have-some-sincere-feedback-1-the-all-new-version-seems-dominated-by-texts-the-older-version-contains-a-lot-of-illustrations-and-animation-along-with-the-content-and-maybe-it-would-be-better-to-add-more-images-and-animation-in-the-new-version-2-im-so-confused-that-even-though-this-is-an-all-new-version-there-are-still-some-images-coming-from-macos-10x-with-the-design-from-yosemite-please-update-these-images-to-the-design-after-big-sur","text":"Hi <@U03HMCH9D5M> - appreciate you feedback and we full agree. We have a huge list of to-do items when it comes to making illustrations for the HIG. We\u2019re a small team and there\u2019s a lot of pages. But this is a top priority for us. As is fixing all those old screenshots! Yup we feel the same way! Luckily now that the redesign is out we can focus on these tasks and more exciting things we're planning! :slightly_smiling_face: It bothers us too. :slightly_smiling_face: Appreciate it !!!","title":"I\u2019m a big believer in Human Interface Guidelines. However, I\u2019d like to have some sincere feedback.   1. The all-new version seems dominated by texts. The older version contains a lot of illustrations and animation along with the content, and maybe it would be better to add more images and animation in the new version.   2. I\u2019m so confused that even though this is an all-new version, there are still some images coming from macOS 10.x, with the design from Yosemite. Please update these images to the design after Big Sur."},{"location":"wwdc22/design-lounge.html#do-you-follow-an-8-point-grid-for-example-table-row-padding-would-be-16-on-either-side-but-44-in-height-i-understand-tap-states-need-to-be-large-enough-but-why-not-48","text":"Hi <@U03JK302NRH> - Yes, an 8-point grid is used in many circumstances. However, there are some exceptions. For example, layout margins are 16pt in compact size class but 20pt in regular\u2026. 24pt would\u2019ve been too much. Sticking at 16 would look too thin. The grid is mostly about width, not height IIRC, the decision about 44pt goes back further than the use of an 8pt grid for layout. Thanks for the insight <@U03HBJXV0TY>!","title":"Do you follow an 8-point grid? For example, Table row padding would be 16 on either side, but 44 in height? I understand tap states need to be large enough, but why not 48?"},{"location":"wwdc22/design-lounge.html#with-dynamic-type-is-it-expect-that-all-text-based-content-on-the-interface-will-scale-is-there-a-time-where-thats-not-appropriate-such-as-in-button-containers","text":"Great question. It\u2019s really not possible for ALL text to scale in some cases. Tab bars, for instance, don\u2019t scale because text would truncate immediately which defeats the purpose. And there is an AX setting for showing the tab icon and label in a bezel on tap. The general approach is to make sure all text in the content area / scroll views scales. Awesome, thanks for the clarification Mike :thumbsup: Text in fixed height elements (like tab / toolbars) is tricker","title":"With dynamic type is it expect that all text based content on the interface will scale? Is there a time where that's not appropriate such as in button containers?"},{"location":"wwdc22/design-lounge.html#i-often-struggle-with-trying-to-minimize-the-amount-of-submenus-while-not-making-the-interface-look-too-bloated-especially-since-i-work-on-photo-editing-apps-that-get-somewhat-complex-do-you-maybe-have-some-thoughts-on-how-to-balance-those-things-how-many-controls-there-should-be-visible-at-once-how-many-submenus-deep-you-can-go-etc","text":"Hi <@U03J20RJQ2X>. Not sure whether a Slack answer will even scratch the surface of this question, but I have a couple of quick thoughts I share <@U03J20RJQ2X>'s pain, having a rich document editing experience within an iPhone interface (1) The person. Is this photo editing for \u2026 professionals? influences? some specific person? \u2026 I really would start here, even though it\u2019s the obvious designery thing to say, because I think it helps shape what level of detail and density is appropriate It gets especially difficult when you try to cater to both, making the app easy to use while also giving advanced users more tools. But that\u2019s for sure a good place to start, professionals are used to different kinds of UIs as well (2) The platform will dictate your heuristics around detail and density. Designing for Mac and iPad, I would push the density, and try to deliver sets of controls as completely as possible \u2014 i.e., i\u2019d try to avoid submenus, and where I had them, I would try to make them \u201csimilar\u201d or at least similarly labeled across controls","title":"I often struggle with trying to minimize the amount of submenus, while not making the interface look too bloated (especially since I work on photo editing apps that get somewhat complex). Do you maybe have some thoughts on how to balance those things (how many controls there should be visible at once, how many submenus deep you can go, etc)?"},{"location":"wwdc22/design-lounge.html#how-do-i-know-when-a-small-button-is-ok-smile-do-i-just-need-to-ensure-they-have-appropriate-clearance-of-44px-all-the-way-around-to-meet-the-guidelines","text":"It\u2019s OK to have smaller button but you definitely want to have a tap area that extends to 44pts in height So, yes, padding can be a factor In general though, a physical background height of 44pt is a good idea. Even if the tap target is taller, users don\u2019t know that. So they\u2019ll naturally slow down to aim accurately","title":"How do I know when a small button is ok? :smile: Do I just need to ensure they have appropriate clearance of 44px all the way around to meet the guidelines?"},{"location":"wwdc22/design-lounge.html#sparked-by-a-previous-thread-are-yall-changing-any-recommendations-for-when-to-use-title-case-vs-sentence-case-capitalization-style","text":"Great question! Although the guidance is to use title case in components like button labels and menu items, it also comes down to stylistic choices for the text content within your app. 100%, <@U03HJ86L1BL>! And when it comes to the content within your app, it \u2014 and your style guide \u2014 is going to evolve over time. As folks in this channel noted, we decided to move to sentence case in the HIG after many years. :slightly_smiling_face: The most important thing is to be consistent \u2014 if you\u2019re going to use title case for your content headers, make sure you\u2019re doing that everywhere in your app. If you change that style, make sure you\u2019re adopting it across the board. And final tip: Even if you largely follow the paradigms of a guide like AP or Chicago Style, it\u2019s super helpful to maintain a separate personal style guide for your content \u2014 that way, you have a great reference document.","title":"Sparked by a previous thread, are yall changing any recommendations for when to use title-case vs sentence-case capitalization style?"},{"location":"wwdc22/design-lounge.html#is-there-still-guidance-on-designing-the-navigation-graph-for-an-app-i-cannot-find-it-in-the-new-layout","text":"Hey <@U03JQEX9MGS> I\u2019m not totally sure I understood your question. Do you mean the diagrams we used to have? May not answer your question, but we have a video about Navigation this week you might want to check out. https://developer.apple.com/wwdc22/10001 I will watch that video! I was referring to the explanation of the different styles of navigation graphs (like event driven). That page helped a lot a couple of weeks ago when trying to explain why app navigation should be treated different from websites.","title":"Is there still guidance on designing the navigation graph for an app? I cannot find it in the new layout."},{"location":"wwdc22/design-lounge.html#more-of-an-abstract-question-taking-the-fitness-app-as-an-example-the-calendar-view-tends-to-stick-out-as-a-completely-separate-design-pattern-are-there-any-guidelines-on-when-you-would-want-to-implement-charting-in-a-calendar-as-opposed-to-a-tabbed-interfaced-that-grouped-by-dayweekmonth-etc","text":"That's a great question! The Fitness app is focused on a daily activity like closing your rings. Since this is a top-level goal of the app, the calendar view makes it easy to see your successes through time. The Health app mirrors the same information but in a longitudinal form, where you can see larger trends. Yup that makes sense! I guess what I'm trying to wrap my head around is finding the medium of digest-able information and too much but that's the joy of design! Each of these approaches has its advantages. The calendar view excels at showing streaks and achievements relative to the day of the week. Conversely, the charts in Health aggregate away those daily details into larger trends. Yeah, it\u2019s interesting to me that the calendar format is sort of perfect for the Activity Rings in that they can be displayed neatly in rows / columns on a calendar grid I would say the calendar is quick overview but the trends, such as in Health, is where the power of charts come in.","title":"More of an abstract question: Taking the Fitness app as an example, the calendar view tends to stick out as a completely separate design pattern. Are there any guidelines on when you would want to implement charting in a calendar as opposed to a tabbed interfaced that grouped by day/week/month etc.?"},{"location":"wwdc22/design-lounge.html#is-it-possible-to-decide-the-number-of-decimals-if-the-numbers-on-one-of-the-axes-are-really-close-to-each-other-eg-a-range-from-0-to-1-with-5-lines","text":"A swift charts question I'm assuming! The framework can truncate labels for you to fit the space, but otherwise you could write in some logic to format the labels according to the space you have available.","title":"Is it possible to decide the number of decimals if the numbers on one of the axes are really close to each other, e.g. a range from 0 to 1 with 5 lines"},{"location":"wwdc22/design-lounge.html#what-are-the-best-practices-for-using-color-in-charts-such-as-considering-accessibility-for-color-blindness","text":"Great question! We talk about these best practices in the \u201cColor\u201d section of our companion talk, https://developer.apple.com/videos/play/wwdc2022/110340/|Design and effective chart At a high level, here are some key concepts to think about when using color in charts: \u2022 Use color to enhance \u2022 Consider associated meanings \u2022 Balance visual weight \u2022 Choose distinct colors \u2022 Respect system settings You can also find general best practices around color in the Human Interface Guidelines: https://developer.apple.com/design/human-interface-guidelines/foundations/color/","title":"What are the best practices for using color in charts (such as considering accessibility for color blindness)?"},{"location":"wwdc22/design-lounge.html#please-tell-me-if-im-missing-important-nuance-here-but-would-it-be-fair-to-boil-the-different-scales-down-to-macro-scale-highlighting-all-regions-of-all-marks-medium-scale-highlighting-same-region-across-all-marks-micro-highlighting-a-specific-region-of-a-specific-mark","text":"That sounds right, as long as with Micro the highlight is about a single data point (since a chart mark could represent multiple data points).","title":"Please tell me if I'm missing important nuance here but would it be fair to boil the different scales down to: Macro scale = highlighting all regions of all marks Medium scale = highlighting same region across all marks Micro = highlighting a specific region of a specific mark"},{"location":"wwdc22/design-lounge.html#are-there-any-samples-or-repositories-we-can-look-at","text":"for SwiftCharts* You bet! Here\u2019s sample code for implementations with Swift Charts https://developer.apple.com/documentation/charts/visualizing_your_app_s_data It\u2019s got our same beloved theme on pancakes :pancakes: OMG YESSSS THANK YOUUUU","title":"Are there any samples or repositories we can look at?"},{"location":"wwdc22/design-lounge.html#for-apps-that-have-multiple-waysformats-to-share-say-gif-video-images-of-selection-html-that-have-wildly-different-ui-flows-how-would-you-recommend-exposing-that-in-the-ui-as-far-as-i-can-tell-the-share-button-only-really-works-for-apps-with-single-format-export","text":"Hi <@U03JB8S4SJ0> Innocent question: Is this about sharing a chart, or just sharing in general? Sharing in general. Sorry, are we only talking about charts right now?","title":"For apps that have multiple ways/formats to share, (say gif, video, images of selection, HTML) that have wildly different UI flows - how would you recommend exposing that in the UI? As far as I can tell the share button only really works for apps with single-format export"},{"location":"wwdc22/design-lounge.html#what-considerations-should-be-made-when-designing-a-time-based-chart-like-safaris-inspector-timeline","text":"One of the most important considerations is that each mark has a start-time, and end-time and a duration. So it will be important to have a UI that reveals all of these pieces of data for each mark.","title":"What considerations should be made when designing a time-based chart like Safari's inspector timeline?"},{"location":"wwdc22/design-lounge.html#hello-this-may-be-considered-off-topic-apologies-in-advance-can-anyone-recommend-an-alternative-to-flow-charts-for-swiftui-code-it-was-great-to-show-non-programmers-how-the-code-execution-should-work-but-since-swiftui-depends-on-state-this-goes-out-the-window-and-im-not-sure-if-uml-is-the-way-to-go-either-any-comments-would-be-appreciated-thank-you","text":"Hi <@U03J21EKNSE>! This is a really good question, and it\u2019s not completely off topic. I would suggest that you bring it to the Swift UI lounge https://wwdc22.slack.com/archives/C03HX19UNCQ Hello Doug L! Thanks for letting me know; relief to know it\u2019s not a bad thing to ask. If you are referring to the Swift UI Lounge happening right now, I have posted it there too. No bites yet; still holding hope! Thanks again. Thanks, yeah. The folks in here are more focused on the design side of things, so you\u2019ll have better luck with the \u201chow to\u201d aspect of your question Copy, understood! Thanks for your time, and have a good afternoon!","title":"Hello! This may be considered off topic, apologies in advance: can anyone recommend an alternative to \u201cflow charts\u201d for SwiftUI code? It was great to show non-programmers how the code execution should work. But since SwiftUI depends on state, this goes out the window. And I\u2019m not sure if UML is the way to go either. Any comments would be appreciated, thank you!"},{"location":"wwdc22/design-lounge.html#with-swift-charts-is-it-possible-to-reproduce-the-scrollable-chart-we-find-in-health-app-where-the-chart-is-updated-as-we-scroll-the-content-i-know-we-can-use-gestures-on-the-chart-to-highlight-some-marks-but-what-about-pan-gesture","text":"There was a similar question in <#C03HX19UNCQ|> and they called out that the views would work the same, just need to give proper widths. Hi Axel! Any Chart can be placed in a scroll view, but I'm guessing you are interested in the \"pinned\" Y axis behavior. You should be able to achieve this with a workaround, that is by ZStacking the Y axis on top of the scrolling chart. Please file a radar if this is a feature you want to get added to the API! Thanks <@U03H3193G3H>. Here is the result I want. Gottit. We'd love to make this easier for you, feature request this please! I'll thanks! you're welcome, thank you!","title":"With Swift Charts, is it possible to reproduce the scrollable chart we find in Health App (where the chart is updated as we scroll the content)? I know we can use gestures on the chart to highlight some marks, but what about pan gesture?"},{"location":"wwdc22/design-lounge.html#will-there-be-some-samples-on-building-good-looking-watch-fitness-apps-like-in-the-wwcd21-session-i-would-love-some-samples-showing-off-the-design-techniques-used-in-this-years-watch-workout-app","text":"Hi <@U03KJSLF04Q>. Very much agree that the new Workout app is quite inspiring. There isn\u2019t currently a code-along for it, but we will try to continue to update the HIG, Design Resources, and developer documentation with new patterns and techniques Would love that!","title":"Will there be some samples on building good-looking Watch Fitness apps like in the WWCD21 Session? I would love some samples showing off the design techniques used in this years Watch Workout app."},{"location":"wwdc22/design-lounge.html#whats-the-best-way-to-design-crown-rotation-behavior-in-my-app-i-found-it-hard-to-replicate-the-smooth-flipping-between-different-pages-for-example-as-the-api-only-gives-a-float-are-there-any-samples-on-well-done-crown-rotation-integrations","text":"hello again! I would bring this question to the Watch labs if possible Will do thanks! But to be clear, it sounds like you\u2019re trying to replicate the behavior of the in-workout UI for the Workouts app \u2026 Is your watch app made with SwiftUI? Or WatchKit? SwiftUI. Workouts is one example. Like selecting the currently highlighted metric (my implementation works but is not as \"satifying\" as the Workouts app), but also other views that might implement \"pages\" themselves.","title":"What's the best way to design crown rotation behavior in my app? I found it hard to replicate the smooth flipping between different pages for example as the API only gives a float. Are there any samples on well done crown rotation integrations?"},{"location":"wwdc22/design-lounge.html#is-it-possible-to-zoom-in-on-a-swiftui-chart-ie-pinch-and-zoom-to-see-details-closer-up-could-you-tap-on-a-bar-or-other-mark-to-open-a-new-view-for-example-showing-a-list-of-the-items-presented-in-that-bar","text":"Hi <@U03J1EHL4KY> great questions! (1) This might depend on what behavior you're looking for. There currently isn't a pinch + zoom API built directly into Swift Charts, but you could capture a the gesture in a chartOverlay and respond to it accordingly by manipulating the scale / data in the chart. (2) Yes! Absolutely. Using a chartOverlay and the provided ChartProxy to read the data point being tapped. There are some great examples of these in the Swift Charts sample app ( https://developer.apple.com/documentation/charts/visualizing_your_app_s_data ) Check out the \"Swift Charts: Raise the bar\" talk too! Donghao explains more there. https://developer.apple.com/videos/play/wwdc2022/10137/ okay awesome thank you! you're welcome! thank you for the question","title":"Is it possible to zoom in on a SwiftUI chart? I.e., pinch and zoom to see details closer up? Could you tap on a bar (or other mark) to open a new view, for example, showing a list of the items presented in that bar?"},{"location":"wwdc22/design-lounge.html#is-there-any-sense-in-adding-animations-to-charts-wo-user-interaction-or-its-a-bad-practice","text":"When used properly, animation can certainly enhance your chart\u2026 even when not tied to user interaction. For example, to indicate that a chart has updated, animating a point or growing a bar can be a helpful and delightful addition. Note that when adding animation, it\u2019s also important to ensure they adapt as needed for a device\u2019s \u201cReduce Motion\u201d accessibility setting","title":"is there any sense in adding animations to Charts (w/o user interaction) or its a bad practice?"},{"location":"wwdc22/design-lounge.html#is-there-documentation-about-all-the-option-types-for-open-ended-requests-i-was-curious-about-person-in-particular","text":"Hey <@U03J8GWEFU7> \u2026 It would help to know a bit more about what you\u2019re looking for related to Person Lynn mentioned that using these common options has Siri NLU benefits. So i was curious what data was backing this. Does Person referred to the customer\u2019s Contacts? Or something else Gotcha. Trying to find a reference for this. Is this section from the transcript relevant: > The App Intent framework does provide a set of common options for these open-ended requests, such as numerical values, dates, or time values. If your required parameter aligns with one of these, definitely select it. This will allow you to benefit from certain built-in dialog and visual patterns, as well as Siri\u2019s natural language understanding as it pertains to these types of information. If not, you can use a custom entity. I am also interested if there is anything Person related but can\u2019t find anything in the transcript re that being a supported common option type. here\u2019s the screen grab where I noticed it Here\u2019s the documentation on IntentPerson : https://developer.apple.com/documentation/appintents/intentperson <@U03HJ5M01M0> has entered the chat. Siri has built in, baseline NLU for many of these types. For example, for an integer, the user might say \u201cone dozen\u201d. Siri will parse this as 12 for you automatically Similarly, for IntentPerson, Siri has some baseline understanding of what is a first name, what is a last name, etc. Siri will attempt to construct an IntentPerson for you if you do an open ended value prompt for an IntentPerson type","title":"Is there documentation about all the option types for open ended requests?  I was curious about Person in particular"},{"location":"wwdc22/design-lounge.html#is-it-still-posable-to-have-user-inputs-in-initial-siri-request-for-media-requests","text":"Good question <@U03JBMMB10A> \u2026 <@U03HJ506BGT> is on it Yes, you can do an open-ended request, disambiguation, or parameter confirmation before completing your intent.","title":"Is it still posable to have user inputs in initial Siri request for media requests?"},{"location":"wwdc22/design-lounge.html#im-wondering-what-the-maximum-sensible-number-of-values-is-for-a-parameter-in-my-app-users-attach-notes-to-people-itd-be-great-to-have-a-shortcut-for-new-note-for-person-name-but-an-active-user-could-easily-have-1000-or-more-people-in-the-app-that-the-note-could-be-attached-to-which-it-sounds-like-isnt-the-kind-of-fixed-list-you-have-in-mind-so-the-20-top-favorite-people-or-similar","text":"Right, for lists of this size and for a list of contacts, you would want to use an open-ended request, which would also allow you to use a person entity. Ok right, thanks. It wouldn\u2019t be suitable for a dynamic parameter, because those contact names would become an array of strings, which limits the flexibility people can use to ask for them. Reading in the other thread about IntentPerson parameters. For an app that has access to CNContacts, is there a way (other than matching on name) to link an IntentPerson to the CNContact that presumably underlies the source of that information? <@U03HJ5M01M0> can you take this? right now you\u2019ll need to match on name. Please file a feedback request! We\u2019d like to hear about your use case and look into this more","title":"I'm wondering what the maximum (sensible) number of values is for a parameter. In my app, users attach notes to people. It'd be great to have a shortcut for \"New note for &lt;Person Name\"... but an active user could easily have 1,000 or more people in the app that the note could be attached to... which it sounds like isn't the kind of \"fixed list\" you have in mind?  So... the 20 top favorite people, or similar?"},{"location":"wwdc22/design-lounge.html#could-i-create-an-ios-app-that-once-downloaded-from-the-app-store-is-never-really-meant-to-be-opened-by-the-user-instead-it-would-include-app-intents-and-primarily-be-accessed-via-siri-with-just-dialogs-custom-snippets","text":"It\u2019s an interesting question <@U03J8GWEFU7> \u2026 It\u2019s a paradigm we\u2019ve seen on Apple Watch with watch apps. Some watch apps are really just ways to deliver complications to the watch face, and rich notifications on the watch itself However, those watch apps also display the information that is delivered in the complication and notification. So anyway, the app matters :muscle: But technically yes, you can build an app that\u2019s focused on providing App Shortcuts, but when someone taps on your app icon you\u2019ll still have to have a viable experience that explains what it does. Also, if you support dynamic parameters, your app will need to be be opened for you to update the order. So I could make an app that\u2019s intended primarily as a container for Siri first App Intent interactions, and the app itself could just provide some context that explains the Siri functionality? You can build that, but if you want to manage your dynamic parameter and people are never opening your app (because it doesn\u2019t have functionality in itself), you won\u2019t be able to re-order or update your parameter values. Got it, that makes sense. I\u2019m just trying to get a mental model for what\u2019s the minimum viable app to create for Siri! This was so helpful","title":"Could I create an iOS app that once downloaded from the App Store is never really meant to be opened by the user?  Instead it would include App Intents, and primarily be accessed via Siri with just dialogs &amp; custom snippets?"},{"location":"wwdc22/design-lounge.html#how-to-create-a-timer-shortcut-with-shorter-phrase-for-our-app-if-the-system-already-have-one-for-example-set-a-timer-for-5-mins-how-can-we-tell-the-system-to-use-our-app-to-setup-the-timer-instead-of-the-system-clock-app","text":"Notably, App Shortcuts will need to include your app name, so that will help distinguish your phrases from standard Siri phrases. (i.e. \u201cstart a <app name> timer.\u201c) With a particularly strong brand, you may even be able to skip the word timer. (i.e. \u201cStart a Super Special Countdown\u201d, assuming \u201cSuper Special Countdown is the app name) A note though! You wouldn\u2019t want to use \u201cfor <time value>\u201d in your invocation phrase, because there are infinite possible replies. That\u2019s better as an open-ended request, for a Duration <@U03HJ5M1UV8> I want to check I understand this. Is the distinction here that \u201copen-ended requests\u201d are always by definition follow-up requests after the invocation phrase? I wasn\u2019t clear on this till now. There are a few different types of follow-up requests after the invocation phrase, and open-ended requests are one of them. Others are parameter confirmation, intent confirmation, and disambiguation. But these are distinct from the dynamic parameter that you provide as part of your initial invocation","title":"How to create a timer shortcut with shorter phrase for our app if the system already have one, for example, Set a timer for 5 mins, how can we tell the system to use our app to setup the timer instead of the system clock app."},{"location":"wwdc22/design-lounge.html#working-on-the-prototyping-team-seems-like-such-an-incredible-thing-to-be-a-part-of-can-you-tell-us-about-your-journeys-to-becoming-prototypers-at-apple-and-what-do-you-look-for-in-potential-candidates-for-your-team","text":"A lot of us some mix of backgrounds that don\u2019t fit neatly into the typical design / engineering teams. Are there a lot of folks on the team who have a hard time identifying themselves as either designers or engineers, but actually both? I taught myself programming outside of school and dabbled in a number of open source projects and the like. Realized I enjoyed psychology / cognitive science to learn more about how people think and approach technology. Studied Cognitive Science, HCI, and Communication Design in school. Had some design jobs but eventually accidentally found this team where I got to use my programming skills too! That\u2019s amazing, thank you for sharing Julian :slightly_smiling_face: That\u2019s amazing! A teammate was at an HCI conference (TEI '17) in Japan that I happened to have work in. He passed on my name to the recruiter. I studied Design and HCI in school, and then I interviewed! That\u2019s awesome, Marisa! I studied computer science, spent some time in HCI academia, and worked for a few startups before joining Apple. For me making delightful interactive things has always been a single pursuit, and the boundaries between things like \u201cdesign\u201d and \u201cengineering\u201d never made too much sense to me. My teammates will have different answers but when I look at candidate portfolios, I like to see inspiration from fields beyond design & tech, a general sense of curiosity and playfulness\u2026 and just weird, unexpected, unique things! (even if they\u2019re not the most polished) I completely agree, Guillaume. And I super appreciate you all taking the time to answer. Thank you! I was a fresh graduated from ECAL in Lausanne Switzerland, where I studied interaction design. A few months after that, Jeff our current Manager saw my bachelor graduation project on a blog and then reached out. It was such a suprise! I was super excited and curious to know more about the team and I end up joining in 2018. It was great that even though I had not much professional experience, the team was interested in my portfolio. When we are looking at candidate, I think we are interested about ideas and how the candidate communicates them, even if you have short resume like I did back then. Thank you <@U03JGA79CQZ>! That\u2019s super inspiring to read!","title":"Working on the prototyping team seems like such an incredible thing to be a part of. Can you tell us about your journeys to becoming prototypers at Apple? And what do you look for in potential candidates for your team?"},{"location":"wwdc22/design-lounge.html#hey-team-im-curious-when-first-designing-a-prototype-of-a-new-product-what-would-you-say-the-ratio-is-of-looks-to-functionality-are-looks-secondary-or-are-they-the-focus-from-the-get-go-thanks","text":"I think the main thing is to think about what questions you are trying to answer by building one (or several) prototypes. Are they questions of technical feasibility, product usefulness,\u2026? etc. Looks for the sake of looks are rarely worth spending lots of early prototyping time on, but sometimes different aesthetic directions or visual metaphors are definitely things you want to prototype! I\u2019d add that the key balance to strike in prototyping is to make the least amount you need to make to still learn something. Sometimes that means things have to look very real so that you don\u2019t distract from the part you\u2019re trying to learn. Interesting, forsure functionality and user experience is so important - although I do find myself holding my AirPod Max\u2019s and just staring at how beautiful they are more then I listen to music :rolling_on_the_floor_laughing: Do you guys have separate teams for functionality and then a team for looks, or is it all entwined? <@U03J52JBK0C> haha, sometimes when I\u2019m prototyping/evaluating a feature, I\u2019ll find some unrelated annoyances, and I HAVE TO FIX it.","title":"Hey team! Im curious, when first designing a prototype of a new product, what would you say the ratio is of looks to functionality? Are looks secondary or are they the focus from the get go? Thanks :)"},{"location":"wwdc22/design-lounge.html#the-fake-it-till-you-make-it-talk-from-wwdc-2014-was-incredibly-influential-in-me-becoming-a-designer-also-now-have-fond-memories-of-toast-what-has-inspired-you-to-join-the-prototyping-team-what-continues-to-motivate-you","text":"Those sessions were awesome!:round_pushpin::bread: IIRC from the presentation, they used Keynote as a prototyping tool. Is SwiftUI used by the team for prototyping? Glad you like the session! (back when I was on the team :sweat_smile:) Perhaps <@U03J52JBK0C> <@U03J0DULM0V> could add to your question You can get a set of templates for Keynote https://keynotopia.com/ or the ones on https://developer.apple.com/design/resources/ We\u2019ve used SwiftUI a little bit, yes. We still use Keynote a lot though!","title":"The Fake it Till You Make It Talk from WWDC 2014 was incredibly influential in me becoming a designer (also now have fond memories of toast). What has inspired you to join the Prototyping Team / what continues to motivate you?"},{"location":"wwdc22/design-lounge.html#how-does-your-team-go-about-quickly-prototyping-advanced-interactions-i-have-lots-of-ideas-for-interactions-i-want-to-test-but-i-feel-my-momentum-is-burnt-out-by-the-time-i-get-everything-set-up-in-say-a-swiftui-project","text":"Finding a way to fake it! We talk about this in most of our old WWDC sessions, but Prototyping for AR (2018) has some good examples of finding clever ways to prototype that don\u2019t involve code at all. There are ways to fake things with paper printouts, or with clever video capture. But often doing some simple Keynote animations can teach a lot too.","title":"How does your team go about quickly prototyping advanced interactions? I have lots of ideas for interactions I want to test but I feel my momentum is burnt out by the time I get everything set up in, say, a SwiftUI project."},{"location":"wwdc22/design-lounge.html#what-types-of-products-do-you-prototype-whats-the-process-of-making-a-design-and-how-many-rough-drafts-do-you-usually-have-before-finalizing-anything","text":"Anything that Apple has shipped and might ship is the sort of thing we prototype! :slightly_smiling_face: I personally have worked a lot on cameras (iPhone, iPad) and \uf8ffPencil over the years, as well as \uf8ffWatch and Health related features more recently. The process of making a design is generally making something, showing it to people, and learning from their feedback, and doing it over and over again. We've given a few WWDC talks on that topic over the years! We don't really count how many \"drafts\" we make, but everything we work on undergoes many, many iterations. Are there differences between the final product hardware and software, or are the prototypes meant to test a specific feature and include some hardware and software features, but not all of it? aka. are you trying to test something with the mindset of a customer, or as a engineer? The mindset of a customer, or a person using the product or feature. We collaborate closely with engineering teams that figure out feasibility. Our prototypes help them understand which use cases need to be supported.","title":"What types of products do you prototype, what's the process of making a design, and how many rough drafts do you usually have before finalizing anything?"},{"location":"wwdc22/design-lounge.html#when-creating-prototypes-how-extensively-do-you-test-it-ie-is-it-used-only-within-the-direct-team-or-do-you-take-the-prototype-to-broader-teams-to-test-as-well","text":"Definitely show it to broader teams. It's less about testing in a traditional, thorough sense, and more about getting lots of people from different backgrounds to try it and tell us what they think. Thanks <@U03HBKARTV4> :raised_hands: If I may ask a follow up question, what kind of feedback are you looking for when testing the prototype? We always build a prototype to answer questions, so what we're looking for will be about what that specific prototype is trying to answer. For example, when we were working on Scribble for iPad & Pencil, we wanted to really understand how people reacted to their handwriting being converted to digital text, what made that process more clear/understandable, what sorts of input would lead to confusion situations, and so on. Awesome, thanks for the insight :bulb:","title":"When creating prototypes, how extensively do you test it? i.e. Is it used only within the direct team or do you take the prototype to broader teams to test as well?"},{"location":"wwdc22/design-lounge.html#what-are-some-of-the-tools-you-use-for-prototyping","text":"As far as digital tools I personally use Sketch and Keynote a lot for static/2D interfaces, After Effects for animation, and Swift/UIKit/Core Animation/Metal when I need to make things a bit more real/custom or interface with hardware. But everyone on our team uses different tools and has workflows that work for them.","title":"What are some of the tools you use for prototyping?"},{"location":"wwdc22/design-lounge.html#the-devs-behind-one-of-the-ada-winners-this-year-not-boring-habits-wrote-a-really-interesting-blog-post-httpswwwandyworkswordsthe-future-of-design-toolshttpswwwandyworkswordsthe-future-of-design-tools-about-the-future-of-prototypingdesign-tools-arguing-for-a-shift-from-flat-tools-like-figma-to-rich-3d-first-tools-like-unity-and-scenekit-do-you-see-those-tools-becoming-first-class-citizens-in-a-prototypers-toolkit-in-the-future","text":"Hi Sam! Making in the medium the design is meant to be in is important! For a prototype seeking to answer an interaction design question, interactive prototypes are essential. For the design of a graphic poster, print out poster iterations. If the end artifact is meant to be spatial, then making in 3D is essential too I'd say there's an underlying principle that the medium that one makes/prototypes in is also a very intentional choice, and should be chosen to best align with the desired end artifact","title":"The devs behind one of the ADA winners this year, Not Boring Habits, wrote a really interesting blog post (https://www.andy.works/words/the-future-of-design-tools)|https://www.andy.works/words/the-future-of-design-tools) about the future of prototyping/design tools, arguing for a shift from flat tools like Figma to rich, 3D-first tools like Unity and SceneKit. Do you see those tools becoming first-class citizens in a prototypers toolkit in the future?"},{"location":"wwdc22/design-lounge.html#do-you-have-a-shrine-to-bill-buxton-more-seriously-do-you-start-with-paper-prototypes-and-if-so-do-you-use-any-tooling-to-go-from-them-to-something-digital","text":"I love sketching on my iPad. Note taking and drawing in meetings is important to me. I've also found that Looom makes animation so fast I am able to make simple hand drawn animations describing interactions / motion in or immediately after meetings too (Looom won an apple design award a few years back. Highly recommend!) We love Looom!! Yes! Makes putting thoughts to motion so much faster! No more pushing pixels key frame by key frame out through molasses Thanks for the reminder - I didn't have an iPad capable of running it when first heard about it & had forgotten (my wife's 12\" Pro laptop-replacement turned out to be harder to prise from her grasp than I'd expected). Now have an iPad8 with Pencil so will try it out. Nice to see it's SVG-based for export too! Ah! I'm excited for you!","title":"Do you have a shrine to Bill Buxton? More seriously, do you start with paper prototypes and, if so, do you use any tooling to go from them to something digital?"},{"location":"wwdc22/design-lounge.html#how-do-you-go-about-adding-magic-whimsey-delight-to-a-prototype-i-feel-a-lot-of-my-designs-lack-that-extra-sparkles-to-make-a-great-experience","text":"Give yourself time to not worry so much about solving the problem. \u201cWhat other ideas does this give us?\u201d can mean completely unrelated things. But if they seem interesting they\u2019re worth trying. Those weird-but-interesting ideas can inspire us to connect the weird / whimsical inspiration to something that actually does solve the problem. well said :smiling_face_with_tear:","title":"How do you go about adding (magic || whimsey || delight) to a prototype? I feel a lot of my designs lack that extra :sparkles: to make a great experience."},{"location":"wwdc22/design-lounge.html#what-tool-would-you-suggest-for-rapid-prototyping","text":"Whatever you're most comfortable with will be what lets you try things rapidly! For some people that's code, others animations in a tool like After Effects... to get started it's really about what you're able to quickly be productive with. Thanks for the feedback <@U03HBKARTV4> :raised_hands:","title":"What tool would you suggest for rapid prototyping?"},{"location":"wwdc22/design-lounge.html#when-testing-with-people-how-often-do-you-change-the-prototype-in-between-sessions-or-not-at-all-ie-picking-up-issues-from-the-first-session-and-adjusting-it-before-the-next-session-is-this-recommended-or-should-we-stick-to-one-prototype-throughout","text":"We try to keep more than one direction open at a time. It might mean having multiple very different prototypes, or one prototype than has sliders / prefs and can be adjusted. If someone gives us good feedback, we incorporate it or try it out. If it\u2019s in conflict with the previous direction, we keep both around to let people compare. That\u2019s great - thanks Julian. It makes sense to let people interact with different prototypes and observe, if I may ask, how flexible do you make prototypes? Are people allowed to tap on anything interactive on the screen, or is it limited to one flow? It\u2019s usually very limited. We make it look like everything\u2019s there, but only certain parts will be functional. It\u2019s important that the rest look real because it\u2019s often in the whole context of a product that we learn the most. Thanks Julian.","title":"When testing with people, how often do you change the prototype in between sessions, or not at all? i.e picking up issues from the first session and adjusting it before the next session. Is this recommended or should we stick to one prototype throughout?"},{"location":"wwdc22/design-lounge.html#when-youre-prototyping-new-ideas-that-are-likely-to-meet-some-resistance-touch-bar-or-dropping-headphone-jack-what-process-if-any-other-than-just-gut-do-you-have-to-decide-whether-to-ship-it-or-kill-it","text":"Generally our team doesn't get to decide whether to ship or kill things. Our job is to explore (by making prototypes) + document all the possible directions that seem interesting or plausible for a specific topic, share with relevant teams, and see what the reactions are. It happens that something we're very excited about is not something that makes it out the door for some reason, in those cases we tend to keep bringing it up in future work that touches on that concept, and work with teams inside the company who are excited about it too to try to build momentum together. Very insightful thanks Cool","title":"When you're prototyping new ideas that are likely to meet some resistance (touch bar or dropping headphone jack), what process, if any other than just gut, do you have to decide whether to ship it or kill it?"},{"location":"wwdc22/design-lounge.html#a-question-about-tools-im-a-uxui-designer-who-mostly-use-sketch-and-figma-to-prototype-apps-mockup-but-recently-i-have-discovered-swiftui-as-an-alternative-to-make-high-fidelity-prototypes-do-you-think-its-worth-learning-to-program-just-enough-to-prototype-better-which-tools-your-team-use","text":"Alberto, you just described the founding philosophy of Meng To over at https://designcode.io/ <@U03HBKARTV4>? Haha, the question of \"should designers learn to code\" is an eternal one... long answer short, I think code should be seen as one more tool in your toolbox. If you feel like it might let you make things you couldn't make otherwise, and you're drawn to it, then go for it! SwiftUI is the easiest to learn it's ever been :slightly_smiling_face: And Meng's website is great!","title":"A question about tools: I'm a UX/UI designer who mostly use Sketch and Figma to prototype apps mockup, but recently I have discovered SwiftUI as an alternative to make high fidelity prototypes, do you think it's worth learning to program just enough to prototype better? Which tools your team use?"},{"location":"wwdc22/design-lounge.html#magicmove-in-keynote-lets-you-prototype-a-lot-of-simple-animations-do-you-have-preferred-animation-prototyping-tools-one-of-my-gripes-about-most-modern-tools-is-they-stop-short-of-helping-with-animation-handover-or-code-generation-if-they-have-any-animation-at-all","text":"AfterEffects has been my go to (light scripting, integration with C4D, able to combine with footage, I end up doing a lot of green screening) Recently got excited about a library that makes Unity's interface more like a traditional-timeline-animation-style ui I understand and commiserate with your gripes about modern animation tools though. I have to pull up Grapher and just fiddle with my equations (when transitioning to code) (so happy you mentioned magic move in keynote though! Early decks where we try to cover as many ideas as possible, going for breadth, there's plenty of keynote animations for our light sketches) I'm a big fan of https://createwithflow.com/ who generate code from visually-designed animations. Supernova Studio was another doing similar things but they pivoted off to design system management","title":"MagicMove in Keynote lets you prototype a lot of simple animations - do you have preferred animation prototyping tools?  One of my gripes about most \"modern\" tools is they stop short of helping with animation handover or code generation, if they have any animation at all."},{"location":"wwdc22/design-lounge.html#how-long-does-it-usually-take-from-the-first-in-hand-prototype-to-the-final-polished-version","text":"Because we\u2019re often working with new hardware features: Years! The shortest turnaround we\u2019ve had on a project has been about two years. Some other projects we\u2019ve worked on have been on the order of seven years. <@U03J52JBK0C> Can I ask what that two year project was? :eyes: I believe our official answer is \"you may ask but Julian can't respond\" :stuck_out_tongue:","title":"How long does it usually take from the first in hand prototype to the final polished version?"},{"location":"wwdc22/design-lounge.html#hi-there-its-really-nice-to-meet-the-prototyping-team-do-you-have-any-tips-on-prototyping-best-practices-stages-of-prototyping-and-anything-else-you-would-consider-important","text":"Always remember what are you building a prototype for, what are you trying to answer. We can get caught up sometimes in trying to have a very polish prototype but it should always be about quickly and efficiently testing a panel of different ideas. Sometimes it helps to just use low tech tools, quickly sketch things on paper, get away from screen. Thank you! Our team like to repeat the process of: make things -> show to people -> get some feedback and repeat until it the experience feels good! Sounds like a great process to follow! Great advices <@U03JGA79CQZ> :raised_hands: thank you :pray: Yeah, thank you for this great advices! I really appreciate it. Having the opportunity to talk to Apple\u2019s Prototyping team is amazing!","title":"Hi there! It's really nice to meet the prototyping team. Do you have any tips on prototyping? (best practices, stages of prototyping, and anything else you would consider important)"},{"location":"wwdc22/design-lounge.html#do-your-prototypes-sometimes-lead-to-product-ideas-that-apple-pursues-or-is-the-context-of-your-prototyping-often-within-the-confines-of-product-ideas-that-already-exist","text":"It\u2019s often the opposite direction \u2014 we\u2019re typically prototyping new products or features that in turn teach us things we could make better about existing products! Ah, that\u2019s very interesting! And when prototyping new products, is that something your team decides to explore, or do you get \u201casked\u201d to explore a certain thing? Both. We pursue things that we think might be interesting, but we\u2019re also keeping tabs on the possibilities that interest partner teams that are asking for our help.","title":"Do your prototypes sometimes lead to product ideas that Apple pursues, or is the context of your prototyping often within the confines of product ideas that already exist?"},{"location":"wwdc22/design-lounge.html#what-is-the-collaboration-dynamic-between-the-prototyping-team-and-design-eng-partners","text":"We work closely with them! With engineering teams to understand the technical capabilities and limitations that we are working with, and with design teams to understand the roadmap they have for their products, and what problems they're trying to solve right now. The time horizon we work on (3-5 years) is a bit longer than the one most teams operate on, so communication is key for us to be aligned. If you're able to speak about it, I'd be interested to know if either of these roles ever initiate requests from the prototyping teams, or if novel / net new experiences are normally initiated by the prototyping team <@U03JENY84QH> Funny enough, this just got answered in the thread immediately above you! https://wwdc22.slack.com/archives/C03H77GM1NW/p1654795791726889","title":"What is the collaboration dynamic between the prototyping team and Design / Eng partners?"},{"location":"wwdc22/design-lounge.html#i-often-find-that-because-there-are-so-many-considerations-to-make-for-a-design-to-be-truly-great-that-can-hamper-my-creativity-and-almost-block-me-from-making-progress-do-you-deal-with-similar-things-and-if-so-how-do-you-manage-to-stay-creative-and-keep-going","text":"Absolutely! I think any creative in any field suffers from the \"blank page\" problem... for me I find that spending time doing things not tech/design related (playing music, spending time outdoors, reading books about random obscure topics,...) will often spark unexpected connections & inspiration that somehow find their way into my work. Something a little more specific I like to do too is to dive in the history of a topic I'm working on - for example the history of human mark making & handwriting through various cultures was very inspirational when I worked on \uf8ffPencil. That\u2019s both reassuring and inspiring to read. Thanks <@U03HBKARTV4>! \"too many things to consider\" is totally relatable, and absolutely something present in the day to day. I have personally really enjoyed organizing and grouping ideas along a \"design space\", by trying to figure out what common axis the ideas might have. It helps me feel less overwhelmed, and also allows me to identify any areas I haven't considered. That\u2019s a really great idea, <@U03J0DULM0V>! I suppose it\u2019s also about sometimes just going with the flow and then reviewing and adapting the work to feedback and design principles afterwards.","title":"I often find that because there are so many considerations to make for a design to be truly great, that can hamper my creativity and almost block me from making progress. Do you deal with similar things, and if so, how do you manage to stay creative and keep going?"},{"location":"wwdc22/design-lounge.html#are-there-any-good-wwdc-sessions-on-prototyping-past-or-present-youd-recommend","text":"Absolutely! You can search for the \"prototyping\" keyword in WWDC sessions, but here are a couple we did: Life of a Button: https://developer.apple.com/videos/play/wwdc2018/804/ Prototyping for AR: https://developer.apple.com/videos/play/wwdc2018/808 Oh and this one too! Not specifically about prototyping but discoverability, something we care a lot about. Discoverable Design https://developer.apple.com/videos/play/wwdc2021/10126/ Nice, thank you! And some say the Fake It Til You Make It talk may still be on youtube if you search :wink: Okay, I\u2019ve seen a few mentions of this Fake it Til You Make It talk here, so I must find it! And gasp! Here it is again: https://developer.apple.com/wwdc14/223 cc <@U03J23RAFK4> Thanks! Added it to my bookmarks. :smile:","title":"Are there any good WWDC sessions on prototyping past or present you\u2019d recommend?"},{"location":"wwdc22/design-lounge.html#how-does-your-design-process-look-like-do-you-take-part-in-problem-define-before-you-start-work-on-some-ideas-thanks","text":"We try not to be too rigid. Often we are starting with a specific problem to solve, yes. But sometimes we make things just because they seem interesting, and then figure out why and what they can help solve. It\u2019s about giving ourselves space to figure out what feels great. We can fit both types of process into the cycle we describe in our WWDC sessions: Make things, Show them to people, Learn from their feedback. sounds great! thanks","title":"how does your design process look like? Do you take part in problem define, before you start work on some ideas? Thanks!"},{"location":"wwdc22/design-lounge.html#thanks-for-this-qa-and-congrats-on-all-the-work-being-released-this-week-part-of-my-day-to-day-work-is-taking-prototypes-and-being-able-to-communicate-findings-to-groups-and-teams-that-may-not-have-an-interest-in-them-yet-if-youre-able-to-share-high-level-insight-im-curious-how-the-findings-from-your-prototypes-are-communicated-format-findings-risks-to-external-teams-or-even-execs-to-some-extent-the-prototype-itself-carries-this-weight-but-in-cases-where-not-and-in-person-communication-is-infrequent-im-curious-how-your-team-tackles-this","text":"It\u2019s an interesting challenge, because different people respond to different communication styles. We do have to adapt how we communicate work depending on what people are looking for. The nice thing about a prototype that appears high-fidelity is that it lessens the amount of assumptions people are making. If it looks like a real app, but we fake one part of the interaction, we are helping to direct feedback toward the parts that seem new. Thank you - that's a nice answer and an excellent point on minimizing assumptions.","title":"Thanks for this Q/A and congrats on all the work being released this week. Part of my day to day work is taking prototypes and being able to communicate findings to groups and teams that may not have an interest in them (yet..). If you're able to share high level insight - I'm curious how the findings from your prototypes are communicated (format, findings, risks) to external teams or even execs. To some extent the prototype itself carries this weight, but in cases where not and in person communication is infrequent - I'm curious how your team tackles this :)"},{"location":"wwdc22/design-lounge.html#how-do-you-come-up-with-an-idea-of-a-product-for-example-do-you-do-brainstorming-sessions-and-how-do-you-transform-an-idea-into-a-prototype","text":"I think it's important to know what the biggest questions around an idea you have, because that's what you'll want to prototype first. Is it about testing the usefulness of a feature? About exploring how to best display the results from an algorithm or machine learning model? Or is it about seeing if a new gesture interaction you came up with is easy for people to understand and perform? The goal of prototypes is to answer questions before investing a lot of time into making things real (hence why it's important to keep your prototyping process light & nimble). As far as coming up with ideas... ask people questions, and listen to what they have to say! Thank you very much! I really appreciate your advice.","title":"How do you come up with an idea of a product? For example, do you do brainstorming sessions? And how do you transform an idea into a prototype?"},{"location":"wwdc22/design-lounge.html#how-has-your-workflow-design-reviews-changed-in-a-wfh-world-what-have-you-found-works-well-for-collaborating-remotely","text":"I've done much more green screening with remote work. (green screening with props and suits with OBS running the effects and compositing in realtime to a virtual camera I use in webex) We have a lot of collaborative Keynote decks too. Even for in-hand prototypes we build we\u2019re taking a lot more video of them that we then place in those Keynote decks. I'm not in a green suit day to day of course :sweat_smile:, but I was very happy the extensive setup came in use for a project in 2020-2021. I had thought to myself \"never thought I'd be grateful for webex\" I like how it gives more opportunity to make real life situation prototypes, because your setup is at home, and not in a studio where things are impersonal. It forces you to stay even more connected with your teammates, to make sure you share regularly about the work","title":"How has your workflow &amp; design reviews changed in a WFH world? What have you found works well for collaborating remotely?"},{"location":"wwdc22/design-lounge.html#how-do-you-prototype-siri-experiences","text":"Keynote animations with audio, in short! It\u2019s like anything else in that we\u2019re just faking the audio response portion. Thanks <@U03J52JBK0C>! Do you use human VO in place of Siri in prototypes? Or do you generate mock Siri audio? For our purposes it is better if it sounds like Siri. Mac\u2019s \u201cSpeak selection\u201d is very useful for this.","title":"How do you prototype Siri experiences?"},{"location":"wwdc22/design-lounge.html#hey-firstly-its-truly-an-honour-to-speak-with-this-team-and-thank-you-for-making-products-that-impact-each-and-every-one-of-us-each-day-d-as-for-my-question-i-would-love-to-get-better-at-design-user-interfaces-what-are-the-benefits-of-sketchfigmaetc-and-would-you-recommend-their-usage-thanks","text":"Sketch and Figma are two popular tools for UI design and prototyping, but ultimately they're just tools - they won't give you magical results on their own. If you'd like to get better at designing UIs and you're starting from scratch I'd encourage you to focus more on things like layout, type, color, interaction patterns (the Apple HIG is a great place to get started on those topics as they specifically pertain to Apple platforms! https://developer.apple.com/design/human-interface-guidelines/ ) than focusing on a specific tool.","title":"Hey! Firstly, it\u2019s truly an honour to speak with this team and thank you for making products that impact each and every one of us each day :D As for my question, I would love to get better at design User Interfaces. What are the benefits of Sketch/Figma/etc. and would you recommend their usage? Thanks!"},{"location":"wwdc22/design-lounge.html#i-saw-julian-mention-the-process-of-make-things-show-them-to-people-learn-from-their-feedback-and-i-was-wondering-if-youve-ever-had-a-product-thats-had-little-to-no-changes-after-feedback-a-product-thats-a-hole-in-one-and-how-much-does-the-product-usually-change-after-feedback","text":"Never. If we\u2019re not getting feedback on something, we\u2019re just not showing it to the right people. We will eventually show it to someone who will have feedback about it (improvements or reasons why it won\u2019t work!) Things change a lot between our first conception of it and what makes it out the door. That\u2019s the fun part about working with a whole lot of people who are very talented at what they do. Thanks for this insight, and thanks for these amazing products!","title":"I saw Julian mention the process of 'Make things, Show them to people, Learn from their feedback' and I was wondering if you've ever had a product that's had little to no changes after feedback? A product thats a hole in one? And how much does the product usually change after feedback?"},{"location":"wwdc22/design-lounge.html#what-are-some-philosophies-that-the-team-adheres-to-when-designing-and-prototyping-things-d","text":"Make things, show them to people, learn from their feedback! That should be a tattoo at this point! Fake it until you make it <@U03DMQBFPH8> I\u2019m getting a WWDC tattoo in July! I\u2019ll get that phrase added for the prototyping team :laughing: > show them to people This piece is so, so important. It almost immediately pays off by helping break down assumptions and normalities established by devs and designers, and lets you see your project and tools from a new perspective. Being open and receptive to that feedback is an incredibly value experience :heart: Considering an interaction \"Before, During, and After\" (i.e. in the example of a button before people press a button, how do they know they can do it, and what it will do, during the press how do they know the action is registered and whether they need to continue pressing ( any visuals that encourage continued action would be 'feed-forward'). And afterwards, what is the feedback that ensures we know how the action went?) Julian;s 2018 WWDC talk covers this in more detail Always bring positive feedback when sharing the work. It should never be about personal judgement but how to make your app experience better. For example: \"I don't like this color\" vs \"I think blue instead of read would better communicate what the experience is about\" \"Always design a thing by considering it in its next larger context \u2013 a chair in a room, a room in a house, a house in an environment, an environment in a city plan.\" is a quote that I think also nicely captures the spirit of always being able to step back and not design something in isolation <@U03HVE4TV8E>!!! You will absolutely need to show this to us on Twitter when you do","title":"What are some philosophies that the team adheres to when designing and prototyping things? :D"},{"location":"wwdc22/design-lounge.html#favorite-books","text":"So many, I love books! We reference Tufte a lot in our studio. Some recent design books I read and enjoyed were Extra Bold https://papress.com/pages/extra-bold , and Politics of Design https://www.counter-print.co.uk/products/the-politics-of-design I always like The Humane Interface and The Design of Everyday Things. Been re-reading Interface Culture recently. Interesting","title":"Favorite books?"},{"location":"wwdc22/design-lounge.html#how-much-individual-work-vs-collaboration-do-you-do-do-you-work-together-within-a-document-or-work-independently-and-then-present-that-work-for-feedback","text":"Different people on the team have different preferences. But we all attend a weekly design review with the whole Prototyping team in which all our work is shared. It helps us build a joint understanding of what a project is, even if it\u2019s mostly one individual \u201cdoing the work\u201d, we\u2019re all helping with feedback, new ideas, and critique. The team\u2019s brain makes the work better. Some folks on the team prefer being in collaborative documents 100% of the time. Others will just bring their individual work to design review.","title":"How much individual work vs collaboration do you do? Do you work together within a document, or work independently and then present that work for feedback?"},{"location":"wwdc22/design-lounge.html#is-there-ever-a-time-where-you-have-to-stop-and-refocus-the-vision-or-design-like-if-too-many-new-ideas-have-been-added-and-the-original-focus-has-been-lost","text":"Yes, but usually not because of too many new ideas! If there are too many new ideas it means we have a lot we can try out. Yes absolutely. When that happens we typically try to focus on what it was that people loved the most in that set of work. If you have dozens of things competing for your attention, focusing on the 2-3 that really seem to be winning hearts over is often a good way to move forward without getting bogged down. But sometimes you also have to accept that while you may have a bunch of kind of cool things, there's nothing in there that's a true winner. And that's okay! There's always way for those things you liked but which weren't clear winners to make their way into other work in the future. Yeah the latter part of Guillaume\u2019s answer was the second part of what I was going to say. If we pause a project it\u2019s because we feel we\u2019ve thoroughly explored a space and we just haven\u2019t hit on what feels great.","title":"Is there ever a time where you have to stop and refocus the vision or design? Like, if too many new ideas have been added and the original focus has been lost?"},{"location":"wwdc22/design-lounge.html#you-mentioned-not-being-too-rigid-on-your-team-and-often-times-working-on-things-in-the-span-of-several-years-do-you-have-any-rituals-as-a-team-to-stay-connected-and-get-into-a-lets-make-some-awesome-things-mode","text":"Maybe this is already somewhat answered in https://wwdc22.slack.com/archives/C03H77GM1NW/p1654797234259859?thread_ts=1654797121.173549&cid=C03H77GM1NW|this thread , but perhaps there\u2019s some additional thoughts around team rituals that you could reflect on here? Yes, as Julian said we have weekly team meetings and design reviews where we share very openly what we're working on. Those have happened every week as long as the team's been around, over 15 years now. I think being a small tight knit team is essential to our success. And in those design reviews we\u2019re very open (and excited) to see random things people made that aren\u2019t directly related to current/ongoing projects. This is so awesome and inspiring! Thanks so much for sharing! How do you scale a weekly meeting as the team / scope grows? We'll have smaller, more focused meetings for individual projects. The goal for the team meetings is for the whole team to see what's going on that week, but they're not exhaustive. Our team has never been bigger than ~12 people, which I think is a feature. I with you on keeping things small\u2026 let\u2019s just say that\u2019s a rarity in my experience :unicorn_face:","title":"You mentioned not being too rigid on your team and often times working on things in the span of several years. Do you have any rituals as a team to stay connected and get into a \"let\u2019s make some awesome things!\" mode?"},{"location":"wwdc22/design-lounge.html#hello-again-and-thanks-for-your-time-i-am-looking-for-general-design-guidelines-on-building-out-visuospatial-metaphors-in-a-3d-ar-environment-i-have-a-project-httpgithubcomtikimcfeelookatthatgithubcomtikimcfeelookatthat-that-renders-and-visualizes-an-entire-code-base-at-once-but-theres-just-so-much-to-see-it-gets-cluttered-and-overwhelming-i-would-really-appreciate-some-time-to-talk-and-think-about-what-kinds-of-things-can-be-presented-hidden-animated-and-made-otherwise-visually-appealing-for-users-such-that-the-tool-is-less-burdensome-and-more-useful-cheers","text":"Hey Ivan! This is a great question to ask in a 1:1 Design lab! There's still slots for tomorrow let me hand you a link https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/upcoming?q=design%20lab here you go! Thank you once, twice, and thrice! I\u2019ll make sure I\u2019m signed up ASAP :smiley: cc <@U03HBHWCB4N> ^","title":"Hello again, and thanks for your time! I am looking for general design guidelines on building out visuospatial metaphors in a 3D + AR environment. I have a project, http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat, that renders and visualizes an entire code base at once, but there's just so much to see, it gets cluttered and overwhelming! I would really appreciate some time to talk and think about what kinds of things can be presented, hidden, animated, and made otherwise visually appealing for users, such that the tool is less burdensome, and more useful. Cheers!!"},{"location":"wwdc22/design-lounge.html#what-are-some-non-digital-sources-of-inspiration-for-each-of-ya-books-music-art-something-that-you-aspire-to-perhaps","text":"Ah. So many things, how does one dimension reduce to answer this? haha, everything from me spending too much time on a saturday morning brunch watching caustics, of light going through different glassware, to thursday night Exploratorium trips, adoring Tokuujin Yoshioka's installation work, Olafur Eliasson, Looking at the special effects work for movies (\"making of The Mandalorian\" a year or two ago with their realtime rendering was :fire:) (does this count as non-digital, it is half physical ahaha) See some fine art in museum is always inspiring! But yeah so many things. For me, just taking long walk randomly in San Francisco can help my thoughts wander in unexpected places. Those are great! I 100% love making of essays, vids, docs Thank you for sharing these :) To answer the last part, \"something you aspire to perhaps\" I aspire to make things as beautiful as poetry :pleading_face:. I like Amanda Gorman, Jack Gilbert, some good ol' TS Eliot, but ahah, thus far haven't personally been able to draw direct application for poetry. Oof poetry is a good one and not often an answer to such a question - maybe applied to UI copywriting ha?","title":"What are some non-digital sources of inspiration for each of ya? Books, music, art, something that you aspire to perhaps."},{"location":"wwdc22/design-lounge.html#how-accurately-does-the-original-design-idea-translate-to-the-prototype-are-there-many-changes-i-saw-jony-ive-say-that-the-best-ideas-come-from-conversation-does-that-continue-through-the-prototype-stage","text":"The purpose of prototypes is to help advance the conversation. Instead of going in circles verbally about what might be, we start to talk about what\u2019s in front of us. Or what could be in front of us that we haven\u2019t made yet! Which is to say, we often find the things we think are interesting change dramatically once we have a few prototypes in front of us. And we have conversations about that!","title":"How accurately does the original design idea translate to the prototype? Are there many changes? I saw Jony Ive say that the best ideas come from conversation, does that continue through the prototype stage?"},{"location":"wwdc22/design-lounge.html#do-you-differentiate-between-designing-user-interfaces-and-prototyping-are-they-related-or-separate-focuses","text":"Yes they are definitely 2 different things, although for us they overlap a lot! I think prototyping in general refers to building artefacts quickly to answer questions that will then inform how to build something \"for real\". So Apple also has silicon prototypers working on the M series chips, for instance! For us, we prototype interactions and user interfaces, which means we try to explore what new modes of input or new capabilities (think for example something like FaceID/Depth Camera, or Apple Pencil) would bring to our products, which then informs whether the company should invest the resources in actually building that hardware/product. It depends on what the prototype is trying to answer and at what stage of the project we are. But generally a polished UI would help better communicate with other teams, make it look real. Yes to add to Myl\u00e8ne\u2019s answer, we often have to design user interfaces as a part of prototyping something! Thanks for the answers! So sometimes it involves building a UI, but much more and not just limited to a digital UI with prototyping. Though sometimes we can get away with re-using large parts of the existing designs. I\u2019m not sure we\u2019re necessarily doing more \u2014 the final user interface designs require a lot of work to be designed for all the different devices, screen sizes, edge cases, accessibility cases, error cases\u2026 we\u2019re fortunate to get to pass on that until it becomes a specific issue for our concept. We build on what\u2019s already there for those parts. Thanks for the clarity!","title":"Do you differentiate between designing user interfaces and prototyping? Are they related or separate focuses?"},{"location":"wwdc22/design-lounge.html#have-you-ever-had-to-build-a-new-tool-in-order-to-design-a-new-experience-do-you-ever-use-custom-or-internal-prototyping-tools-that-apple-makes","text":"Yes! And yes! The \u201ctools\u201d are a lot rougher than you might have in mind, but we\u2019re often cobbling together our own sets of code to be able to try things. We don\u2019t build it out into a generic reusable tool as much as we build the frameworks / scaffolding necessary to try out the very-specific thing we want to get into our hands. We build what\u2019s necessary to try the specific things we want to try. We try really hard to stay away from prematurely abstracting. It\u2019s too easy to accidentally become a framework engineer or tools engineer when we really were just trying to try something out.","title":"Have you ever had to build a new tool in order to design a new experience? Do you ever use custom or internal prototyping tools that apple makes?"},{"location":"wwdc22/design-lounge.html#i-noticed-you-said-that-a-destructive-action-should-be-on-the-left-and-red-text-i-never-knew-that-where-is-this-sort-of-thing-in-the-hig","text":"Ooops, should\u2019ve responded with a reply. :wink: Hey Dan, we have writing guidance sprinkled around the HIG in various locations. This guidance is on the Alerts page: https://developer.apple.com/design/human-interface-guidelines/components/presentation/alerts","title":"I noticed you said that a destructive action should be on the left and red text. I never knew that - where is this sort of thing in the HIG?"},{"location":"wwdc22/design-lounge.html#other-than-users-and-learners-any-other-suggestions-for-referring-to-the-people-using-our-apps","text":"What is your audience? We often just use \u201cpeople\u201d but if you can be more specific, like \u201cfamily member\u201d or \u201csubscriber\u201d that\u2019s a good way to go. Think about what people using your app would call themselves. We try to avoid the word \u201cusers\u201d when we refer the people who use our apps, and prefer something that sounds more human, like \"artists\", \"collaborators\", \"participants\", etc. depending on the app and what people are trying to do. :slightly_smiling_face: Thank you! That is very helpful :relaxed:","title":"Other than users and learners, any other suggestions for referring to the people using our apps?"},{"location":"wwdc22/design-lounge.html#are-there-any-examples-of-combining-different-tonalities-within-one-app-or-should-one-just-stick-to-one","text":"The voice of your app is something you want to remain consistent, so that folks get consistent messaging inside your app. But, as you point out, the tone can certainly modulate. When you welcome somebody, you might sound excited. But when that person has a credit card declined, you may want to sound more serious. Similar to how your voice is always you, but your tone changes depending on context and who you're talking to. In other words (no pun intended, hiyooo!), it's very common \u2014 if not important \u2014 to adjust your tone throughout your app. The tone should reflect the situation. As an example, coaching from the Activity app on Apple Watch is celebratory when you close a ring. But it's more matter-of-fact if your progress is lacking. Right, there is a clear distinction between voice and tone, thanks for answering both of you! And I loved your pun <@U03HJ5LRUEN> :joy: Hmmm - I guess I\u2019m still struggling with tone vs voice - Carrot weather has insults woven into weather forecasts. Is that voice? Maybe it doesn\u2019t have a tone. I\u2019ve never bought it because I didn\u2019t like whatever it was I don't think these rules apply to Carrot Weather App :joy:","title":"Are there any examples of combining different tonalities within one app? Or should one just stick to one?"},{"location":"wwdc22/design-lounge.html#how-do-you-see-the-different-roles-of-the-higs-writing-guidance-and-the-apple-style-guide-httpshelpapplecomapplestyleguideapsg1eef9171httpshelpapplecomapplestyleguideapsg1eef9171-is-the-style-guide-more-for-contexts-outside-of-apps-and-long-form-writing","text":"Great question. One way to think about it, is that your product is a major part of your brand. With that in mind, your product may have more specific guidelines to follow so that you can make your interface feel familiar \u2014 and subsequently, intuitive. These kinds of patterns are outlined in the the Human Interface Guidelines. Your brand, however, may encompass more than just what appears in your product. Your website, emails, marketing collaterals, events, and even the App Store descriptions mentioned earlier, these are all examples of surfaces where your voice extends beyond your app. For us, that's where the Apple Style Guide kicks in, helping us to adhere to a wider set of grammar and syntax to ensure we have consistency at every customer touchpoint. Think about this balance for your app, too. How can your app have clarity and consistency throughout, and how can that narrative continue outside of your app to complement what you \u2014 and your users \u2014 are trying to do? HIG ASG :handshake: continuing the conversation I think the ASG should be more visible! For some reason it's hidden from search engines.","title":"How do you see the different roles of the HIG's writing guidance, and the Apple Style Guide (https://help.apple.com/applestyleguide/#/apsg1eef9171)?|https://help.apple.com/applestyleguide/#/apsg1eef9171)? Is the style guide more for contexts outside of apps and long form writing?"},{"location":"wwdc22/design-lounge.html#great-session-so-many-useful-tips-i-am-100-gonna-apply-to-my-apps-just-wondering-though-would-you-recommend-onboarding-for-apps-ive-heard-before-it-interrupts-the-users-usage-of-the-app-but-ive-also-heard-that-it-makes-users-feel-more-welcome-what-do-you-think-thanks-grinning","text":"This is such a good question <@U03HVE4TV8E> I don't think there's a single answer to it, but <@U03HVEWEGAF> <@U03HWQ6F9A5> and <@U03HJ5LRUEN> have some thoughts It's going to vary based on the complexity of your app or how simple its premise is. We find what often helps is to incorporate onboarding elements or moments into the first-time experience for someone new. Also great question :fire: Maybe I'm saying the same thing, but the best onboarding doesn't feel like \"onboarding\" Thanks for answering :D so, I have an app that tracks deliveries, so should the be super simple along the lines of \u201cTrack your deliveries\u201d in large title text, maybe a little but of information, and then \u201cGet started\u201d? All in one view. Thanks :pray: There\u2019s no one answer to this! My favorite onboarding experiences are the ones you do right within the app itself. Like those early levels of a game that teach you how to play it. Or a chat app that uses the chat to get you set up. Alto's Adventure! You're just dropped into the game, and you're playing, and it's teaching you along the way <@U03H3HNAGSK> Ahh, I love that game! And yeah, the onboarding is really nice and seamless! Oooh off the cuff it's hard to give specific feedback over Slack. If you have time <@U03HVE4TV8E>, sign up for a slot in our UX Writing Lab tomorrow\u2014and add that question. https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/Y2Z4GC8D8Q/dashboard|Sign Up Here Ooooh I didn\u2019t know there was a lab happening tomorrow! Thanks :D","title":"Great session! So many useful tips I am 100% gonna apply to my apps. Just wondering though, would you recommend Onboarding for apps? I\u2019ve heard before it interrupts the user\u2019s usage of the app, but I\u2019ve also heard that it makes users feel more welcome. What do you think? Thanks :grinning:"},{"location":"wwdc22/design-lounge.html#thanks-for-amazing-advice-i-wonder-what-the-arrangement-for-options-in-an-alarm-means-for-example-putting-major-option-on-the-left-or-top-is-it-recommended-to-change-the-order-eg-from-major-on-left-to-major-on-right-for-languages-using-different-reading-order","text":"Hi Zhang - Yes, typically you\u2019d want to order more important things above or to the left of less important things. And for RTL languages, check out this page in the HIG: https://developer.apple.com/design/human-interface-guidelines/foundations/right-to-left Also this video: https://developer.apple.com/wwdc22/10034 Thank you for answering! Such fun designing for different cultures.","title":"Thanks for amazing advice! I wonder what the arrangement for options in an alarm means. For example putting major option on the left or top. Is it recommended to change the order, e.g. from major-on-left to major-on-right, for languages using different reading order?"},{"location":"wwdc22/design-lounge.html#for-full-screen-modals-with-no-interaction-is-apples-preferred-dismiss-control-an-x-in-a-circle-in-the-top-right-corner-as-is-referenced-in-the-explore-navigation-design-for-ios-talk-as-opposed-to-a-grab-style-control-at-the-top-middle-indicating-to-pull-down-to-close","text":"Great question! I\u2019ve been wondering this myself recently :facepunch: Totally great question. we get it a lot. We have an answer for you in the HIG: https://developer.apple.com/design/human-interface-guidelines/components/presentation/sheets Position Done and Cancel buttons as people expect. Typically, a Done or Dismiss button belongs in a sheet\u2019s top-right corner (in a left-to-right layout) or top-left corner (in a right-to-left layout). The Cancel button belongs in a sheet\u2019s top-left (in a left-to-right layout) or top-right (in a right-to-left layout) corner. Good question <@U03JRQUJ27J>. This has been an interaction pattern in recent releases, moving away from the elevated sheet and toward the full-screen presentation with the close in the top right Omg I LOVE that this is in the HIG now!! I literally have sent in at least 100 bug reports pointing out where the Cancel button is incorrectly located at the right. I have like 100 screenshots collected I attach to each bug report showing places where Cancel is correctly located at the left. :heart: It is interesting however there seems to be a bit of conflict with close buttons moving to the right recently (Safari, sheets with X instead of Cancel) :thinking_face:","title":"For full screen modals with no interaction, is Apple's preferred dismiss control an X in a circle in the top right corner (as is referenced in the Explore Navigation Design for iOS talk) as opposed to a grab style control at the top middle (indicating to pull down to close)?"},{"location":"wwdc22/design-lounge.html#if-you-add-a-new-feature-to-an-app-is-it-a-good-idea-to-showcase-that-new-feature-when-the-user-does-a-certain-action-ie-completes-a-task-etc-or-is-that-bad-user-experience-as-youre-interrupting-their-session-thanks-grin","text":"This goes back to context. If that feature is relevant or helpful at that moment, then yes, go ahead and talk about the new feature. But if it\u2019s not relevant to that moment, think about where else it would feel more contextual. That\u2019s great, thanks :blush:","title":"If you add a new feature to an app, is it a good idea to showcase that new feature when the user does a certain action (i.e. completes a task etc.) or is that bad user experience as you\u2019re interrupting their session? Thanks :grin:"},{"location":"wwdc22/design-lounge.html#seems-common-for-interface-copy-to-be-too-verbose-before-its-edited-down-are-there-examples-that-come-to-mind-where-the-ui-was-the-opposite-too-terse-and-needed-more-explanation","text":"There are may examples of this, I'm sure. One that comes to mind is when people input personal information, like when ordering something for delivery to their home. Sure, you could just indicate that you received the information, but by restating what you received (so the customer can ensure it was entered correctly) and maybe even stating that the info won't be used for nefarious purposes (eg. it wont be sold to advertisers), you create a level of comfort that more economical language might not afford. Think about when you tell a server at a restaurant that you want a certain dish, but with extra pickles and hot sauce but no tomatoes and can you please switch the fries for a salad. Some servers won't write that down. And while that's impressive to see, it's also nice to hear when the server repeats it back to you so you know the meal will come as you asked. Love that metaphor <@U03HJ5LRUEN>. Another thing we do often is link out to support articles for people who want to learn more about a particular subject. That way the information is there if you need, but it's not in the way.","title":"Seems common for interface copy to be too verbose before it's edited down. Are there examples that come to mind where the UI was the opposite, too terse and needed more explanation?"},{"location":"wwdc22/design-lounge.html#what-would-be-the-best-way-to-dismiss-a-full-screen-modal-that-has-content-would-it-be-a-button-along-the-lines-of-done-at-the-bottom-or-is-it-still-a-cross-in-the-top-right-as-mentioned-earlier","text":"Hi Charlie, there\u2019s some good insights in the HIG on the Sheets and Modality pages. https://developer.apple.com/design/human-interface-guidelines/components/presentation/sheets Personally I would be reluctant to use a bottom corner in a Mac app, since you have to consider window movement and that it could be clipped by the Dock. https://developer.apple.com/design/human-interface-guidelines/patterns/modality Yes Austin, generally not a good idea to position controls at the bottom of mac windows. Also, check out: https://developer.apple.com/videos/play/wwdc2022/10001/","title":"What would be the best way to dismiss a full screen modal that has content? Would it be a button along the lines of \u2018Done\u2019 at the bottom, or is it still a cross in the top right as mentioned earlier?"},{"location":"wwdc22/design-lounge.html#any-tips-on-how-to-position-elements-so-as-to-help-with-functionality-and-design","text":"In our session, we talked about information hierarchy. As with everything it\u2019s going to depend on the specifics, but in general, think about your headlines and buttons. Make them clear and aligned with each other. Keep other text brief and easy to read. Rather than a big block of text, can you break it up into bullet points or even multiple screens? Oh okay, that makes sense. I just have a lot of trouble getting my designs to be both intuitive and aesthetic. I\u2019ll keep that in mind, thank you!","title":"Any tips on how to position elements so as to help with functionality and design?"},{"location":"wwdc22/design-lounge.html#should-i-still-add-an-add-to-siri-button-to-my-app-with-the-new-sirikitshortcuts-functionality-or-is-it-completely-obsolete","text":"Good question <@U03K1B3MQL8> ... You don't need to remove it, if you've got it. But on iOS 16 forward, you can begin to move away from it by creating and supplying App Shortcuts, rather than requiring people to set them up I haven't fully looked into the new SiriKit interactions\u2026 Does it just mean that if an Intent is available, iOS knows this and will automatically integrate it with Siri/shortcuts? I think you've essentially got it, but I would watch the App Intents video and the Design App Shortcuts video as well","title":"Should I still add an \u2018Add to Siri\u2019 button to my app with the new SiriKit/shortcuts functionality, or is it completely obsolete?"},{"location":"wwdc22/design-lounge.html#whats-the-average-number-of-times-you-edit-your-slack-messages","text":"Many times, sometimes. Too many times to count. :sweat_smile:","title":"What's the average number of times you edit your Slack messages?"},{"location":"wwdc22/design-lounge.html#ive-found-a-small-mistake-in-the-hig-where-should-i-report-this-the-feedback-assistant-doesnt-seem-to-have-a-appropriate-category","text":"Would love the feedback! We are working on adding a HIG category. You can use the Apple Design Resources component for now. I\u2019ll make sure it gets to the right place. :wink: Reported, it\u2019s really small but wanted to mention it somewhere, FB10140417 Typos happen, but I don't think I'll ever in my lifetime see a HIG screenshot accidentally show a time other than 9:41 or 10:09. Not as long as I\u2019m here <@U03JELQLESV>!","title":"I\u2019ve found a small mistake in the HIG, where should I report this?  The Feedback assistant doesn\u2019t seem to have a appropriate category"},{"location":"wwdc22/design-lounge.html#have-you-ever-accidentally-shipped-out-a-typo-with-an-os-release","text":"\"An Apple spokesperson declined to comment.\" :wink: It happened onec Apparently one of original iPhone's important demos to a cell carrier's boss accidentally contained a profanity as a localization string lookup key! Source: a Computer History Museum interview.","title":"Have you ever accidentally shipped out a typo with an OS release?"},{"location":"wwdc22/design-lounge.html#is-the-apple-pencil-mightier-than-the-sword","text":"","title":"Is the Apple Pencil mightier than the sword?"},{"location":"wwdc22/design-lounge.html#sorry-for-joining-so-late-i-make-a-photo-editing-app-and-i-often-wonder-how-i-should-deal-with-technical-terms-one-example-that-comes-to-mind-are-blend-modes-screen-overlay-multiply-etc-as-those-things-are-foreign-concepts-to-the-average-person-do-you-recommend-sticking-to-such-technical-terms-and-providing-explanations-via-graphics-for-example","text":"Ooh good question. It really depends on your audience - are they professional photographers? Then they might understand more technical terms. You can also help define things with secondary text. Thanks for the reply! The app is not targeted at professionals, so I\u2019ve tried to find ways to better communicate what those tools do If these are terms of art, and it sounds like they are, it can be best to stick with them. Avoid jargon, or making up terms that might end up confusing everyone. That said, if your audience is less technical, maybe define the terms so people can learn them. +1 to what <@U03HWQ6F9A5> and <@U03HVEWEGAF> said here. We aim to democratize access to technology with much of our hardware, software, and services at Apple; so we aim to have our language be understood by as many people as possible. But with a few of our apps, we have a specific audience (eg our creator tools, like Apple Music for Artists, that focus on specific audiences), and therefore we want the language to be in the vernacular of the users\u2014otherwise it may feel like we've shipped a watered-down professional feature. Since it's not aimed at professionals, show it to friends (or strangers) and see if they understand what's written. And as Jen and Kaely pointed out in their talk, consider reading it out loud to yourself as well. Thanks a lot the replies! One area of iOS that just came to mind was the blur intensity adjustment in Portrait Mode. Even though it\u2019s a feature that\u2019s not necessarily aimed at pros, it\u2019s using f-stops as an analogy to real life photography.","title":"Sorry for joining so late! I make a photo editing app and I often wonder how I should deal with technical terms. One example that comes to mind are blend modes (screen, overlay, multiply, etc.) as those things are foreign concepts to the average person. Do you recommend sticking to such technical terms and providing explanations via graphics, for example?"},{"location":"wwdc22/design-lounge.html#if-i-want-to-read-the-new-hig-end-to-end-when-should-i-plan-to-do-that-so-everything-will-be-in-there-i-know-yall-are-working-on-more-goodies","text":"Hi <@U03HMCT187R> good question. The HIG is a living document, so it\u2019s always getting updated in big and small ways. Between now and the fall, the updates will (most likely) be small. In the fall, the bigger updates will land. Another way of answering your question is: You might as well get started with that read-through! It has been great to get feedback on the redesign, so don\u2019t be shy about that, either I think it would be great if it was clearer when there's more minor updates, like a technologies page gets added or something. Sometimes I'll discover on Twitter, but a more central diff would be nice! I can file feedback about this too. We're already planning for changelogs in the HIG coming some time this summer!","title":"If I want to read the new HIG end-to-end, when should I plan to do that so everything will be in there? I know yall are working on more goodies."},{"location":"wwdc22/design-lounge.html#the-wonderful-crusade-against-home-tabs-reminds-me-of-the-one-you-did-for-hamburger-menus-many-years-back-are-there-other-common-app-conventions-that-you-see-that-you-disagree-with","text":"Thanks for remembering that session. It was YEARS ago. We have a few, mostly they center on design conventions that come over from other platforms that could be adapted to match Apple platform conventions For example, drawing a back button as an arrow (with a tail) instead of a chevron ( < ) ^ that's an android thing Tab bars that have a button in the center that performs an action rather than navigating to a location in the app One that comes to mind that might've been mentioned in that session is avoiding the standard Apple platform icon for share sheet button. Both so common :pensive: Yep! That one gets my blood boiling. :slightly_smiling_face: My current pet peeve is floating buttons (i.e. on Twitter). Like, a plus in the corner that is just floating there :joy: What's the icon they use, like three dots or something? Also, kabob menus (three vertical stacked dots) instead of an ellipses symbol For me I personally dislike icons without labels. Unless you think your icon is universally understandable across all cultures \u2013 because it usually isn't :stuck_out_tongue: This thread really stirred the pot with food-related anti-patterns. Another one I just thought of: close buttons on the right instead of the left? That\u2019s the one that always gets me. Where does one place close buttons?","title":"The wonderful crusade against home tabs reminds me of the one you did for hamburger menus many years back. Are there other common app conventions that you see that you disagree with?"},{"location":"wwdc22/design-lounge.html#in-reference-to-not-creating-a-home-tab-what-are-your-thoughts-on-having-cards-that-are-overviews-of-content-from-a-different-tab-would-you-consider-that-a-duplicate","text":"Hey <@U03K1SMSPUZ>, great question. This is highly dependent of the context and content of the app. Be cautious of cards that display unrelated or disparate types of information and make an interface feel like a dashboard. Dashboards are similar to Home tabs, as the information is not directly related and often overloaded. This makes it difficult to glance and takes longer to understand the relationships and how to take action So, if you use cards or have a dashboard like view, try to keep the content relevant to the current tab? We\u2019ve seen some apps that have a wide array of Widgets that can be configured like a dashboard. It can be very powerful, especially on iPad. But these widgets would take you to the corresponding tab in the app.","title":"In reference to not creating a \"home\" tab, what are your thoughts on having \"cards\" that are overviews of content from a different tab? Would you consider that a duplicate?"},{"location":"wwdc22/design-lounge.html#when-do-you-recommend-hiding-the-navigation-bar-when-pushing-a-view-ex-apple-news-when-you-open-an-article-the-app-pushes-the-article-view-and-hides-the-main-navigation","text":"Curious about if there are situations when the tab bar should be hidden also. It\u2019s true that News doesn\u2019t follow the typical convention we recommend for tabbed apps. I can\u2019t really speak for the News team, but I believe the thinking was that people don\u2019t generally switch between tabs frequently or want to persist state in each tab since the content changes all the time People drill into articles, read, and then often leave. You could consider temporarily hide the Navigation Bar for immersive experiences, just be sure to allow the reverse interaction and show it when swiping down or tapping the screen. In that usage pattern, persisting the tab bar to allow for fast lateral navigation, or keeping it visible to help people stay oriented, is less of an imperative. This is pretty atypical for tabbed apps though got it! Yes I was thinking it can be also be possible to use a modal instead of a push with hidden nav bar","title":"when do you recommend hiding the navigation bar when pushing a view? Ex: Apple News: When you open an article, the app pushes the article view and hides the main navigation."},{"location":"wwdc22/design-lounge.html#i-saw-in-sarah-ms-session-that-there-were-places-cells-with-a-button-on-their-top-right-side-on-what-cases-should-we-use-such-a-button","text":"I believe you\u2019re referring to the ellipses SF Symbol in the navigation bar? We would use this button for secondary actions that are related to the view, but perhaps not the primary actions someone would take \"...\" More menus are typically used to host a set of additional actions vs. navigation. You'll often see designs conflate the two. If an app must use more than 5 tabs, is an ellipsis the appropriate icon for the \u201cMore\u201d tab? <@U03J1EHL4KY> I believe that <@U03DMQBFPH8> answered this here: https://wwdc22.slack.com/archives/C03H77GM1NW/p1654813779763509?thread_ts=1654812948.356689&cid=C03H77GM1NW","title":"I saw in Sarah M's session that there were Places cells with a ... button on their top right side. On what cases should we use such a button?"},{"location":"wwdc22/design-lounge.html#are-there-resources-we-can-share-with-more-web-minded-colleagues-it-can-be-difficult-to-explain-that-apps-and-websites-should-not-have-the-same-navigation-design","text":"Absolutely! http://developer.apple.com/design|developer.apple.com/design Also this video :slightly_smiling_face: I\u2019m not sure we really speak explicitly to web designers per se, but this should be a helpful primer on Apple platform design patterns Also more specifically all the content inside \"Navigation and search\" in the HIG https://developer.apple.com/design/human-interface-guidelines/components/navigation-and-search/navigation-bars As well as modality: https://developer.apple.com/design/human-interface-guidelines/patterns/modality","title":"Are there resources we can share with more web-minded colleagues. It can be difficult to explain that apps and websites should not have the same navigation design."},{"location":"wwdc22/design-lounge.html#i-saw-in-the-session-that-there-are-list-section-headers-that-are-bold-and-bigger-than-normal-section-headers-in-which-cases-should-we-use-these-thanks","text":"They\u2019re a relative recent change to iOS (last year I think). It\u2019s a matter of taste and judgement I think. But they\u2019re particularly effective when the list is text only. It helps created needed differentiation (or grouping) between content items and category labels If the list items are visually distinctive, larger section headers might not be needed. But, personally, I think they\u2019re an improvement in all situations.","title":"I saw in the session that there are list section headers that are bold and bigger than normal section headers. In which cases should we use these? Thanks"},{"location":"wwdc22/design-lounge.html#home-is-such-a-lazy-name-for-the-apps-first-tab-but-the-argument-thats-often-given-in-its-favour-is-that-people-understand-it-because-thats-how-the-first-page-is-often-referred-to-in-web-and-app-design-and-changing-it-would-mean-forcing-people-to-change-their-already-established-habits-would-spotify-twitter-doing-away-with-the-home-tab-be-a-risky-ui-move-that-could-lose-them-followers-or-a-much-needed-navigation-cleanse","text":"We can\u2019t speak about what other companies should do, but I don\u2019t consider risky moving to a more intuitive labeling convention, this way you ensure predictability and intention. Specificity is always a good thing. If there\u2019s a more descriptive word to use, it should be used. I was just thinking that. Maybe instead of \u201cHome\u201d it should just be \u201cTimeline\u201d for Twitter And even web is starting to move away from the \"home\" concept. I have a feeling it will end up in the same category as having a floppy disk represent \"save\"","title":"\"Home\" is such a lazy name for the app's first tab! But the argument that's often given in its favour is that people understand it because that's how the first page is often referred to in web and app design and changing it would mean forcing people to change their already established habits.  Would Spotify / Twitter doing away with the home tab be a risky UI move that could lose them followers or a much needed navigation cleanse?"},{"location":"wwdc22/design-lounge.html#in-the-demo-app-shown-the-navigation-bar-was-hidden-but-the-toolbar-items-was-still-showing-is-this-new-in-ios-16-blush","text":"Hmm. Can you elaborate what example you\u2019re referring to? None of these examples share UI that isn\u2019t yet released :blush: Navigation bar appearances? https://developer.apple.com/documentation/uikit/uinavigationbar/3198027-scrolledgeappearance Yeah of course! If you notice how the image goes behind an invisible NavigationBar but the toolbar is still visible That\u2019s a standard navigation bar with the background blur removed The image goes behind the navigation bar and it\u2019s optional to remove its background, but for TAB BARS it\u2019s not recommended. Yes, the background translucency should not be changed or removed on the tab bar. On a navigation bar, it can be an aesthetic choice if it doesn\u2019t hinder the user experience","title":"In the demo app shown, the Navigation Bar was hidden but the toolbar items was still showing. Is this new in iOS 16?! :blush:"},{"location":"wwdc22/design-lounge.html#i-would-love-to-hear-the-design-teams-thoughts-on-kebab-menus-and-floating-action-buttons-i-get-a-feeling-they-fall-in-the-same-boat-as-hamburger-menus-and-home-tab-bar-items-smile","text":"Funnily enough we were just talking about this here: https://wwdc22.slack.com/archives/C03H77GM1NW/p1654814044982489 \u201cKebab menu\u201d? Is that a thing? now I\u2019m hungry\u2026 Most of these components are problematic for other reasons. For example, FABS tend to cover elements of an interface which can be difficult to read or interact with the content behind the action. They also tend to be duplicated across tabs and the redundancy, shows that the action lacks context And yes to all of the above, a lot of those paradigms you'll see on Android and it instantly makes people feel as if the design wasn't intended for an Apple platform. Yes to above! I should add that FABs aren\u2019t verboten on iOS but the action should be VERY important and VERY frequently used. Otherwise they get in the way. Things and Notes are two good examples of where a FAB makes sense. Problem is a lot of other apps use FABs for an action that isn\u2019t all that common. After years of freelancing and working in startups, the most annoying thing was UI/UX concepts fighting against the (native) platform language - for the sake of brand experience, for \u201cstreamlining and build it faster\u201d (the opposite was true) or lack of knowledge (or empowerment) of the designers. It is really hard to find good mobile1st designers\u2026. still after over 10yrs We feel your pain. btw: I consider SwiftUI more a design than a code challenge\u2026 (for the same reasons) Agree Oliver. I'm not a designer but I've certainly tried to advocate for native look and feel when working with designers on iOS apps, <@U03HBJXV0TY>'s session Essential Design Principles from WWDC 2017 helped clarify my thinking around this.","title":"I would love to hear the Design Teams thoughts on Kebab Menus and Floating Action Buttons! (I get a feeling they fall in the same boat as Hamburger Menus and Home Tab Bar Items :smile:)"},{"location":"wwdc22/design-lounge.html#wondering-where-featured-tabs-would-fallare-they-just-home-tabs-with-a-different-name-or-could-they-actually-be-considered-useful","text":"Hi <@U03JRQAFUKA>, could you share what a feature tab would consist of? I guess maybe recommended articles for example? Hard to say a blanket \"yes or no\" because it depends on the content inside the tab. We do \"For You\" tabs where the content inside changes dynamically based on what we think is relevant to the user. That could be considered a \"featured\" section but its use case is specific enough from navigation that it can be its own page Where it gets iffy is when you're just repeating the same content as in your navigation put on a page. That causes confusion with whether people should use the tab bar or Featured page to access information Recommended & feature articles could be highlighted under the Articles tab, instead of having a dedicated tab. I\u2019d say, as <@U03DMQBFPH8>, that it would be case by case and depending of the level of personalization the app offers. Ah okay, that makes sense! Thank you!","title":"Wondering where \"featured\" tabs would fall\u2014are they just home tabs with a different name or could they actually be considered useful?"},{"location":"wwdc22/design-lounge.html#i-hope-its-still-fine-to-ask-an-unrelated-question-i-missed-the-previous-qas-here-there-is-just-so-much-going-on-exploding_head-i-would-love-to-have-the-design-templates-available-for-affinity-photo-and-designer-is-this-something-you-are-considering-i-guess-i-should-probably-file-a-feedback-for-that-just-thought-about-it-now-innocent","text":"Hi <@U03J4CVE1U4> - We are continually considering what apps to support for the ADR. :wink: Feedback Assistant would be a good next move. Awesome, I'm glad to hear it! Actually just submitted one: FB10142299 :muscle:","title":"I hope it's still fine to ask an unrelated question. I missed the previous Q&amp;As here, there is just so much going on :exploding_head:  I would love to have the design templates available for Affinity Photo and Designer. Is this something you are considering? I guess I should probably file a feedback for that, just thought about it now :innocent:"},{"location":"wwdc22/design-lounge.html#i-saw-a-tweet-showing-batch-editing-in-the-ios-16-photos-app-and-apparently-it-uses-a-progress-indicator-at-the-bottom-that-reminds-me-of-an-android-style-snackbartoast-do-you-have-any-guidance-on-this-ui-element-httpstwittercomapollozacstatus1534903049373761537httpstwittercomapollozacstatus1534903049373761537","text":"We have progress indicators on iOS currently https://developer.apple.com/design/human-interface-guidelines/components/status/progress-indicators We don't currently have published design guidance on that particular style of progress indicator (when it becomes more common the platform we'll consider). Gotcha! Thanks for your answers, <@U03HBHWCB4N> and <@U03DMQBFPH8>. I really enjoy reading through the conversations going on here. Although from prior experience I can say that what that toasts are tricky to work with. The one you're seeing in iOS 16 is nice because it's using a deliberate \"x\" close button so people can dismiss the UI and it knows the process takes long enough that the toast won't magically disappear in 1 sec making it impossible to read or interact with So it's not disappearing automatically after the batch edit has completed? I need to try it myself, I guess\u2026 It is disappearing automatically after the batch edit completes, but the reason it appears in the first place is that batch edits take time so it's appropriate for this context","title":"I saw a tweet showing batch editing in the iOS 16 Photos app and apparently it uses a progress indicator at the bottom that reminds me of an Android-style snackbar/toast. Do you have any guidance on this UI element? https://twitter.com/apollozac/status/1534903049373761537|https://twitter.com/apollozac/status/1534903049373761537"},{"location":"wwdc22/design-lounge.html#in-the-explore-navigation-design-for-ios-session-there-is-an-example-of-a-profile-tab-which-ive-seen-several-apps-using-but-others-like-fe-the-appstore-uses-profile-as-button-in-the-top-right-side-does-that-depend-on-the-importance-of-such-a-profile-screen-in-other-words-what-makes-something-to-be-a-tab-worthy","text":"Great question, <@U03J1TN6WBD>. This is a judgement call based off of the importance of the profile. In many cases, the profile is not that important to the apps main functionality and it isn\u2019t needed as a tab bar item. When considering what should be a tab, think of your apps menu of options. When tabs are designed well, they tell a story at a glance about what your app can do and how people can use it. Profile tends to be less important in the scheme of things. And just because you have tabs to use, doesn\u2019t mean you have to use them all. Sometimes an app will feel more approachable if the tabbed navigation is simplified Agree with <@U03HBHWCB4N>. Also, if the user needs to access the Profile content regularly and you don\u2019t want to place a profile button in the nav bar of every tab, a profile tab may be a better option. Side note I wish the Updates tab were restored in App Store, I check for app updates multiple times a day and read the release notes. Def inefficient to open the profile screen. (Yes I might have a problem :laughing:)","title":"In the \"Explore navigation design for iOS\" session there is an example of a \"Profile\" Tab which I've seen several apps using, but others like f.e. the Appstore uses Profile as button in the top-right side. Does that depend on the importance of such a Profile screen ? in other words, What makes something to be a \"tab-worthy\" ?"},{"location":"wwdc22/design-lounge.html#another-heated-topic-where-to-put-the-xclosecancel-button-top-left-or-top-right-are-there-really-no-guidelines-about-it-confuses-the-hell-out-me-as-a-user-and-developer","text":"<@U03JRNE4KJL> I totally understand how this could be confusing. You\u2019re right, top right accessories are used for the main actions. But in the case of Modals there might be other resources more suitable - like buttons - to convey important actions. So Close takes the top right place. I\u2019d say that prioritization of the actions on screen are key here, once you have a clear understanding - you could decide what\u2019s the best pattern to follow. <@U03JRNE4KJL> a bit late but your question is answered here: https://wwdc22.slack.com/archives/C03H77GM1NW/p1654807862947179","title":"Another \"heated\" topic: where to put the X/close/cancel button - top left or top right? Are there really no guidelines about it? Confuses the hell out me (as a user and developer)"},{"location":"wwdc22/design-lounge.html#it-struck-me-that-tab-bars-and-navigation-bars-are-at-least-in-principle-identical-to-the-original-iphone-interaction-paradigm-im-wondering-what-the-biggest-difference-youve-seen-in-navigation-is-since-that-first-iphone","text":"I love this question. I wish I had the time to answer more thoroughly. Here are some thoughts about tab bars and navigation bars: these components and interaction patterns clearly resonated with people. Maybe that was luck, maybe it\u2019s a pattern that reflected our natural understanding of how things are organized and arranged already in the world around us. But they have evolved and modernized while staying true to what \u2018just worked\u2019 originally. As people\u2019s comfort with apps grow, new patterns emerge to support new complexity such as sidebar being a solution specific to a new device and form factor","title":"It struck me that tab bars and navigation bars are, at least in principle, identical to the original iPhone interaction paradigm. I\u2019m wondering what the biggest difference you\u2019ve seen in navigation is, since that first iPhone?"},{"location":"wwdc22/design-lounge.html#thank-you-for-such-an-amazing-wwdc-i-definitely-learned-a-lot-this-week-one-last-question-from-me-what-are-some-booksresources-that-you-would-recommend-every-designer-to-read-for-design-guidance-creative-inspiration-best-practices-etc-besides-apples-hig-of-course","text":"For me: \u2022 https://www.goodreads.com/book/show/9386.Free_Play|Free Play: Improvisation in Life and Art \u2013 Not specific to UX but this has really helped me unblock myself creatively \u2022 https://www.goodreads.com/book/show/111113.Interaction_of_Color|Interaction of Color by Josef Albers \u2013 Great color theory resource \u2022 https://www.goodreads.com/book/show/44735.The_Elements_of_Typographic_Style?from_search=true&from_srp=true&qid=Q1DklR77Li&rank=1|The Elements of Typographic Style \u2013 Great typography resource \u2022 https://www.goodreads.com/book/show/34473.In_Praise_of_Shadows|In Praise of Shadows \u2013 Lovely essay on aesthetic","title":"Thank you for such an amazing WWDC! I definitely learned a lot this week. One last question from me: what are some books/resources that you would recommend every designer to read for design guidance, creative inspiration, best practices, etc (besides Apple's HIG, of course)?"},{"location":"wwdc22/design-lounge.html#i-lost-the-description-of-stickers-and-sticker-packs-from-hig-where-can-i-find-them","text":"<@U03HVBUV0KY> This will be coming back soon!","title":"I lost the description of Stickers and Sticker packs from HIG. Where can I find them?"},{"location":"wwdc22/design-lounge.html#i-have-heavy-use-of-swipe-buttons-on-different-rows-of-a-tree-control-tableview-with-lots-of-different-cell-types-ive-not-found-any-recent-writing-on-swipe-button-discoverability-but-watching-people-average-users-dont-seem-to-explore-to-see-if-swipes-are-available-on-lists-any-suggestions-on-affordances-or-onboarding-approaches-ive-thought-about-an-initial-animation-of-sliding-things-over-enough-for-a-button-to-peek-through","text":"Since swipe actions are more for power users, we always recommend that they are also exposed in the detail view. For example, if there\u2019s a delete action on your table cell, make sure there\u2019s a delete action in a toolbar, navigation bar or in-line within the secondary view.","title":"I have heavy use of swipe buttons on different rows of a \"tree control\". (TableView with lots of different cell types).  I've not found any recent writing on swipe button discoverability but watching people, average users don't seem to explore to see if swipes are available on lists.   Any suggestions on affordances or onboarding approaches? I've thought about an initial animation of sliding things over enough for a button to peek through."},{"location":"wwdc22/design-lounge.html#hi-i-really-enjoyed-the-session-on-the-new-ipad-navigation-bar-for-document-based-apps-i-just-did-a-lab-to-explore-this-further-one-of-the-suggestions-was-to-adopt-a-layouthierarchy-similar-to-the-notes-app-i-wanted-to-ask-if-there-is-any-api-for-the-file-explorer-lists-and-grid-view-that-the-notes-app-uses-i-would-like-to-adopt-this-in-a-sidebar-as-well-and-move-my-document-based-app-away-from-a-tab-controller-thanks","text":"Hi <@U03JGS1CE2H>, thank you for the question, I\u2019m not sure about an API specifically (I\u2019m just a designer!). But here are two resources from the session side that may be helpful for you: 1. The session Designed for iPad, which dives deep into the design for Sidebar https://developer.apple.com/wwdc20/10206 This year\u2019s session about Desktop Class iPad apps which covers some tips for the new toolbar design https://developer.apple.com/wwdc22/10009 Just realized you referenced the second video in your message, it\u2019s been a long week :sweat_smile:","title":"Hi, I really enjoyed the session on the new iPad navigation bar for document-based apps. I just did a lab to explore this further - one of the suggestions was to adopt a layout/hierarchy similar to the Notes app.   I wanted to ask if there is any API for the file explorer (lists and grid view) that the Notes app uses. I would like to adopt this in a sidebar as well, and move my document-based app away from a tab controller. Thanks."},{"location":"wwdc22/graphics-and-games-lounge.html","text":"graphics-and-games-lounge QAs by FeeTiki There any main tradeoffs between tessellation and mesh shading for vertex creation? Mesh Shading has a more flexible workflow and allows for dynamic scaling of your workload through the object shader stage. In terms of performance, Mesh Shading allows a more pipelined flow directly from the Mesh Shader stage to the rasterizer, so you have full mesh visibility during vertex creation. On the other hand - I would suggest using tessellation if you are doing actual tessellation of an existing geometry pipeline. For users who are doing procedural work that doesn't directly map to tessellation per-se, I would definitely suggest Mesh Shaders. The new https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders?language=objc|sample code we published shows how mesh shaders work and you can get an idea of the difference. The sample code generates Bezier patches and uses mesh shaders to control the level of detail. While the patches are generated on the CPU, you will see that it would not be a huge leap to do that in the object stage or mesh stage. Not much of a question because I'm just starting to move towards rendering but I'd like to say thanks to everyone involved with this. The documentation, tooling and APIs seems better than ever and the MetalFX Upscaling seems a game changer. I can see some foveated rendering hacks being used with this API and using different LOD with the fast resource. Cheers lads, awesome work. Thanks for the feedback. We are curious about your idea \"I can see some foveated rendering hacks being used with this API\". Can you elaborate more on it? Using the same concept that VR googles are starting now but on a flat screen. Render the main point of interest in 4k and moving away from it, create lower zones (almost like a gradient) where the borders can be rendered at like 1080p with high anti alias combining with lower LOD. Again, starting so not even sure if it\u2019s possible. For foveated rendering there\u2019s also a variable rasterization rate API in Metal to help with that https://developer.apple.com/documentation/metal/render_passes/rendering_at_different_rasterization_rates Thanks <@U03HHP77WUB>, there\u2019s not even the need for hacks then. Cheers. <@U03J76T9WG6> Just a quick note: MetalFX Upscaling is designed with fixed rasterization rate in mind. You can find more details in the MetalFX session tomorrow. Cheers. I\u2019ll keep an eye on it. Cheers <@U03HW8Y0RFB>. What are the package size considerations when shipping with offline compiled metal shaders? Do these need to be updated over time to keep up with any driver or software updates for a particular piece of hardware? Hi <@U03J9S1L38W>, 1. Regarding package size considerations, optimizing for your particular deployment platform and particular set of supported devices is what make most sense. 2. Please refer to the video \u201cTarget and optimize GPU binaries with Metal 3\u201d available tomorrow for information on this. How do I start creating anything in Metal 3? Can you please show me code for a \"Hello, World!\" type Metal 3; maybe a box with a shaded gradient of color inside. Yup! We have a list of examples with source code at https://developer.apple.com/metal/sample-code/ You can also create a new project in Xcode, select the Game template and pick Game Technology: Metal , which will create a new project that renders a cube in Metal Excellent! The sample code link will do well. Thanks And just created the Metal cube project in Xcode; this is a huge boost to getting me started using Metal 3! Awesome! :tada: If you have any questions throughout the week please let us know :smile: I also recommend checking out https://developer.apple.com/documentation/metal/debugging_tools to help you debug any issues you encounter Hi, are Metal 3 features exclusive to Apple Silicon or do they also work on other processors (e.g A-series on iOS, or the GPUs in Intel-based Macs)? hello! thanks for the question, Alessandro That information is in the video at approx 12m 45s Thank you very much! What's the best way to get started with Metal as a beginner in the graphics field? Hello, we have a bunch of great samples for getting started with Metal going from the very basics all the way to more complicated rendering. https://developer.apple.com/metal/sample-code/ The samples under \"Metal Fundamentals\" are a great place to get started. Thank you very much! Will look into it. I\u2019m learning graphics (coming from Swift) and am really interested in Ray tracing. What should I learn first to learn how to make a hybrid renderer in Metal? Hi Ethan! In addition to this year's Raytracing content, we also had a session fully dedicated to the topic of Hybrid Rendering in last year's WWDC. Hopefully you find it a good introduction to the topic - https://developer.apple.com/videos/play/wwdc2021/10150/ . In particular, the session shows how to do your \"primaries\" using rasterization, and then how to use that as the basis for RT Shadows, Ambient Occlusion, and Mirror-like Reflections. This sample is also great for beginners https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal (it's not in Swift though :sweat_smile: sorry!) Thanks! Metal makes Ray tracing really exciting since you can now pair it with MetalFX. Going to be interesting! Are there any memory and texture format considerations when using MetalFX? I'm guessing the historical frames need to be stored in some way, though the trade off would be smaller intermediate render targets. For more details on MetalFX Upscaling, please watch the session tomorrow. Thanks. Thanks! Will be sure to check it out. Also will be interesting if this could be used for some intermediate render targets like an AO pass. If you have any further questions, join us in the digital lounge for the session 1PM-2PM PST. We'd be happy to answer any further questions. Since the shaders are compiled at project build time and compilation speed is no longer as critical, is that extra headroom used to allow the compiler to try to optimize the shaders further? Or is does it produce identical machine code as when building it at runtime? Independent from offline compilation, we did introduce a new optimization level -Os (size) as an option, but there is not additional optimization that occurs during runtime. The produced code will be identical. Understood, thanks! Also, be sure to check out \u201cTarget and optimize GPU binaries with Metal 3\u201d available tomorrow for more information! Hi! Great wwdc session, question: do you have a hello world example for making shaders using Apple technologies? Thank you! Please see the article \" https://developer.apple.com/documentation/metal/developing_and_debugging_metal_shaders?language=objc|Developing and Debugging Metal Shaders \" that will introduce you to the Xcode's excellent shader debugger. Then you can try out the Game Template if you want to get a Metal app running. And you can check out our https://developer.apple.com/documentation/metal/metal_sample_code_library|sample code library that have many kinds of Metal samples (and shaders) you can learn from. Can you explain how to use triple buffering for complicated scene with 2d (overlay/ui) and 3d? What kind of data I should store in buffers? In Vulkan examples they use framebuffer, but metal dont have any similar entity. Could you give a bit more context? In general all interfacing with the compositing is done through the Drawable. (usually through a MTKView in an app). Are you referring to manually triple-buffering GPU resources for CPU synchronization? Yeah, indeed. I wanna fill some data on CPU side while frame rendering (inflight frames) and pass filled buffer to command queue. In apple examples I see how to use inflight frames to fill camera data to buffers, but camera data isn't complicated. Im newbie in Metal and trying to create my own render engine. One way to do this is to have a set of data for each frame in flight. You can then use a mutex (or other thread-safe structure) to keep track of what sets are in flight by: \u2022 marking them when you use them in a command bufferand \u2022 unmarking them in the completion handler of the command buffer. If you run out of free sets, you can simply wait on the mutex and halt rendering on CPU until GPU has caught up. Ok, I will try it! Thanks a lot. check synchronize https://developer.apple.com/documentation/metal/mtlblitcommandencoder/1400775-synchronize|link and the various ways of signalling and reading data between buffers on GPU and CPU https://developer.apple.com/documentation/metal/mtlbuffer|link Also, if you wanted a example of this pattern check out the game template in XCode by creating a new project in Xcode, selecting the Game template and picking Game Technology: Metal . In that template check how _inFlightSemaphore is used to multi buffer the uniforms buffer. From the Dynamic Terrain Sample code: // We allow up to three command buffers to be in flight on GPU before we wait static const NSUInteger kMaxBuffersInFlight = 3; _device = device; _commandQueue = [_device newCommandQueue]; _startTime = [NSDate date]; _inFlightSemaphore = dispatch_semaphore_create (kMaxBuffersInFlight); _frameAllocator = new AAPLAllocator (device, 1024 * 1024 * 16, kMaxBuffersInFlight); _uniforms_gpu = _frameAllocator-&gt;allocBuffer &lt;AAPLUniforms&gt; (1); You can see we explicitly limit our in-flight sets to 3 in this example. ( https://developer.apple.com/documentation/metal/buffers/rendering_terrain_dynamically_with_argument_buffers|link to sample code ) The main idea is that you shouldn\u2019t modify data with the CPU if the GPU might be reading it, so using that semaphore with a completion handler (per the examples above) allows you to know when at least one of your 3 copies of any data are able to be modified. If you are not using the CPU to modify textures/buffers on the fly then there isn\u2019t a need for this semaphore. -[CAMetalLayer nextDrawable] will automatically provide up to 3 drawables and will limit an app from getting too far ahead (if you render faster than the screen refresh). Thanks! I will watch linked examples asap. Didn't see it earlier, new to Slack: \"Not sure if this is the right place to ask, but I am still looking through the plugins. But would it be able to use Bluetooth or related tech to share say an item with another player that also has the game/app. Like if there is a Unity plugin from apple that does this. \" Unfortunately we don't currently offer a plug-in that exposes this functionality, but this is certainly an interesting idea. Can you tell me more about what you're trying to do. Are you looking for something that behaves like the share sheet? It's more of a thought experiment at this point, but I kind of want to create a fantasy/rpg game and want to incentivize working together in real life to defeat certain bosses or challenges. My goal is to not rely on having any servers if possible. I think having bluetooth access might be a way for people to create a party, like in a typical 4 player rpg game like final fantasy. For example, if you are a healer. You can only heal or buff other people most of the time. In this case to beat certain challenges you would need to meet up with other people to be able to progress. What's the earliest iOS you can use this with? The plug-ins are compatible as far back as iOS 13 as long as the underlying framework is also supported. For example, the PHASE framework was new in iOS 15, so is not supported prior to that version. Thanks. (My current game supports iPad 2 still, iOS 9\u2026 But for new projects, that\u2019s probably fine.) Keep in mind the plug-ins are open-source, so feel free to make modifications if needed to help with your specific backwards compatibility goals. If we're making feature requests: I really like the ability to email bug reports from my game with MFMailComposeViewController ! Thanks for the feedback. We're definitely tracking feature requests, so this input is super valuable. Though that might be really tricky with Unity \u2014 I use it in UIKit-based games. Yeah, we'll have to investigate how we might surface the functionality. It's certainly interesting - but there may be a more Unity-esque approach to solving your issue as well. I believe Unity has some integrated bug report metrics that you can leverage as a Unity developer. We did it in Kobold2D (Cocos2D) years ago, too. Email is nice for indies because you don\u2019t need to worry about a back end. A question on the Accessability plugin for Unity. Is it only available for Apple devices or can it also be used for Windows, Xbox etc. Reason for asking is that it would be great to see a cross platform plugin to increase the reach of accessability (making it easier to prioritize for stakeholders) Hi Denny, thanks for the perspective. Currently this plug-in is only available for Apple platforms as it ties into foundational Apple accessibility technologies. Ok! I did miss to say it in the question--it's really great to see the accessibility plugin :-) What is the recommended framework for connecting to another player stably for a multiplayer match? Not the old turn-based games, but a modern approach to having a 5-10 minute connection for PvP. (Would ARKit, RealityKit be better today than multipeer connectivity, which can be flaky with frequent disconnects?) Hi - GameKit currently provides real time multiplayer support and is currently used in a number of games. Coincidentally, we've just shared this functionality as part of the Unity plug-ins made available today. Besides that, GameKit provides a few way for our users to play with each other: \u2022 Auto matching. \u2022 Inviting Game Center friends via http://Messages.app|Messages.app or push notifications. \u2022 Or play with nearby friends. You will be able to see some games in Apple Arcade utilizing the multiplayer functionality provided by gamekit. Hi! It looks like the documentation for the unity plugins has not been submitted to the GitHub repo -- all the documentation links are broken. Thanks for the report! We\u2019ll look into this. Hi, thanks for pointing out! Documentation links should now work again. I've had a quick browse of the Accessibility Unity plugin and noticed that it's only supporting iOS and tvOS. Is macOS planned as well? There are also some performance concerns with the code that I'd love to see resolved, will GitHub PR's be considered? We don't currently support macOS, but you can always file a bug through feedback assistant for us to consider these requests. Hi Alex, this is very appreciated. Unfortunately we don't currently accept external PRs - but we do really take your feedback to heart, so keep the feedback coming and we'll do our best to address in the mean time. Great to hear. Just based on a cursory scan through the codebase, the most serious performance issues are allocating in the mono heap with calls like GetComponentsInChildren<Renderer>() (there are allocation-free versions of these available), and serializing rects and points to native code via strings. Fantastic. Thanks for already helping to make these plug-ins better. Would love to get some more detail on how the element ordering is working too. It looks like the plugin determines an element ordering based on screen-space positioning which seems quite complex; however my project already knows the logical layout and grouping of every UI element; is there a mechanism for using this actual logical ordering? That is a great feedback! Currently no, but we will add this to our feedback list to allow honoring element ordering provided by developers:thumbsup: I haven\u2019t looked at the Accessibility plugin yet but it\u2019s the one I\u2019m most excited about. My games are playable via VoiceOver, and I definitely override -accessibilityElements to do things like give a consistent navigation order when the visuals are mirrored. I look into unity plugins and have a relative question: Why you using swift instead of objc in AppleCore module? Why Swift with all Unmanaged things better, than ObjC? But thanks for that bridging! I grab information how to use _cdecl :) Thanks for asking Vladislav! We used Swift because it is the best language for Apple platforms. Glad you learned something new too! Yey! That is very great answer for me! Thanks Brett Is wheel support only on Mac? The API seems to be available on all platforms except Apple Watch, but I\u2019ve only seen it mentioned with regard to Mac. Am I misreading something? Yes, racing wheels are only supported on macOS at this time. If you\u2019re interested in supporting racing wheels in your game on other platforms, please file feedback to let us know! Could mesh shaders be used for procedural terrain generation with continuous LOD? Currently I use a tessellation compute shader (which also does some basic culling) followed by a vertex/fragment shader. You can definitely use Mesh Shaders to do this! - The great thing about Mesh Shaders is that you are not limited to the fixed tessellation layout; you can kick off an Object Shader to calculate what tiles to populate (and which ones to leave out for custom tiles), and then dynamically schedule Mesh Shader grids to create the individual tile geometry. :) This way, you could effectively generate and cull your entire terrain on the GPU Is there any specific issues you are worried/thinking about? Fantastic! Are there any more details/docs on mesh shaders? I\u2019m having trouble finding much about it. One of the problems I have at the moment is culling. I pass in some quads to the tessellator, then do frustum transforms to try to figure out whether the quad would be onscreen or not (using -1 for the tessellation factor if I want to cull it). Then my vertex shader displaces vertices with the procedural function and does the transforms again, and it all feels a bit clunky. I know there\u2019s a session tomorrow on mesh shaders, but I\u2019m having trouble understanding the basic behavior of them atm. There will be a full session on Mesh Shaders tomorrow! And you can always drop back into the next Q&A on friday if you have any follow-up. My suggestion for doing these kinds of things, is to assume each \"tile\" has a AABB of the min/max displacement within the segment (pre-calc if needed), then use the Object Shader (which is a full featured compute shader) to generate work items for tiles of various sizes etc. Then run a Mesh Shader Grid with a TG per tile to generate. You can simply cull your AABB in the OS and you should never have any overhead aside from the AABB check :slightly_smiling_face: You can effectively have the OS TG fill the payload buffer with an array of TileDesc structs (or something like that), then have each MS TG pick up a TileDesc to expand into a mesh and send it straight to the rasterizer Tomorrow's Mesh Shader session will be here: https://developer.apple.com/videos/play/wwdc2022/10162/ Documentation will be updated in the near future, and I believe a sample project will be available along with the session. OK, this sounds good! Is there any integration with the existing tessellation pipeline, or would I need to somehow implement something like it in the OS or MS? I believe you have to do a manual tessellation in your MS. If I can make a suggestion, a doubling straight forward quad tessellation scheme while you fade in height dat through manual mip levels might be a very good and quick way of getting a simple-but-smooth solution. The nice thing about OS/MS is that you can nicely debug both the tile scheduling and the mesh generation separately, as they are both basically compute kernels you can run individually. Hmm I see. It definitely sounds like something to try out. I love the built-in tessellator because it distinguishes between internal and edge tessellation which lets me avoid cracks in the terrain. I guess I would just need to do a bit more work in my MS to make that work. I wonder if the current fixed tessellator could be exposed as a function we could call from our MS\u2026 I think the main issue with the classic tessellation scheme is that it tends to generate noisy silhouettes when tessellation smoothly increases. (you get this wave-effect as the vertices move across the displacement samples). There should be a way to integrate an external \"classic tessellation function\" into the MS kernel to generate a list of 2D triples for a given tessellation. I wouldn't be surprised if there are examples of this online. Yes the swimming vertices are an issue. Although it\u2019s an issue in my case anyway as I\u2019m using a single square mesh to render an entire spherical terrain from ground to space by \u201cwrapping\u201d it over the sphere depending on distance and what is potentially visible. My solution is to buy better GPUs and increase the vertex count to minimise the sample aliasing.. :smile: Thanks for your help Jaap, looking forward to the session tomorrow and I\u2019ll probably be back for more Q&A following it! If you wanna have more of an in-depth discussion, we still have a few Lab openings to sign up! https://developer.apple.com/wwdc22/labs/ These are in-depth 1:1 discussions :) How should we approach issues like Shader Debugger crashes and GPU faults? We don't know where to look for hints The best thing to do for debugger crashes is to create a feedback assistant with steps to reproduce. As for GPU faults, I would encourage you to try enabling shader validation for your application. We already have all validation turned on (no results), and we use FA all the time, but what do we do about these \"game over\" issues in the meantime? I would encourage you to try the new betas with your use case, we improved a lot of areas there If you are still having troubles, maybe you could create new FBA tickets, attach a gputrace and post the numbers here? One example of GPU Fault FB9968899 Is the GPU fault happening at application runtime or while you are working with the gputrace? The fault is a separate issue, but I mention both because there are no diagnostics we can understand But this is a common occurrence for us, something fails catastrophically with no hints and we don't know how to proceed Does this happen on the latest OSs? I haven't tried the fault on Ventura because it can lock things up if not quickly resolved, may result in a loginwindow watchdog timeout (force reboot) Are you using raytracing or ICBs? No raytracing, I assume ICB is indirect command buffer, also no Which GPU architecture is this Steven? Same app worked fine on Intel (AMD GPU) fails badly on M1 for no apparent reason But I'm really asking about methodology, because we want to keep making forward progress without DTS all the time The crash log on M1 appears to indicate GPU firmware detected lockup, meaning that from the software perspective the GPU is hung for some time hence it is rebooted. Do you by chance implement a concurrent producer/consumer pattern via atomics on M1? It uses atomics, but I wouldn't call it producer/consumer, it's just an atomic allocator to a buffer We found entirely by chance that increasing the number of allocators from \"1\" fixed it But have no idea why or how, etc Are you ever in a situation where you rely on forward progress guarantees to access the allocator? Not sure what that means from a GPU standpoint E.g.: would threads spin until memory is available or do you just assume there\u2019s always enough memory? My mention of \"forward progress\" above just means development progress, not getting stuck on crashes there is no spinning, just atomics though we check the allocated value hasn't gone off the end of the array, we discard in that case there is nothing \"consuming\" Oh coincidentally you also used the term \u201cforward progress\u201d, although I meant the property of a program to make progress no matter what There is no concurrent producer/consumer I see, then it\u2019s likely not the problem I thought, thanks for clarifying Have you tried running without shader validation enabled, just the API validation? Also, one of the ways GPU Faults happen when there\u2019s basically unpaged memory reads/writes, basically out of bounds reads/writes or access to a memory that\u2019s not resident - meaning the underlying memory of a resource that wasn\u2019t made resident using useResource or useHeap I don't think that explains how increasing the allocators fixed it 1 = crash, 4 = ok I realize there may be more parallelism on TBDR, but some kind of hint would help us out List of FB for reference FB10020742, FB10020198, FB10014465, FB9968922, FB9968899 Had a quick look at the shader code in the Metal library contained in the app you attached to FB9968899 and I noticed a function called transparency_populate that loops on a linked list. Does this function get called in the workload that hangs? That's in the second \"resolve\" pass, which is called but shouldn't be directly affected by the number of allocators or their values transparency_add , transparency_add_counter , transparency_write , etc are the ones affected If you remove entirely the second \u201cresolve\u201d pass, do you still see a GPU fault though? I could try that, but it seems unlikely if you're looking for sample code, check the attachment to FB9968922 The reason I\u2019m asking is that that transparency_populate relies on the data structure to be well formed to complete, otherwise it might spin forever That's not an issue we see It's an A-Buffer implementation, a 2D buffer of head pointers (inited to -1) then a linked list Yes, I can see the A-buffer structure for sure. At first glance the code looks sane to me, so I guess we need to repro locally to understand what\u2019s going on FYI, just triggered in Ventura, will be attaching the reports Looks like some diagnostics are missing from the messages? 2022-06-08 13:18:09.498390-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang) 2022-06-08 13:18:09.498450-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang) 2022-06-08 13:18:09.498713-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim) 2022-06-08 13:18:09.498734-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim) 2022-06-08 13:18:09.498867-0400 CADApp[8724:128585] GPU Soft Fault count: 1 2022-06-08 13:18:09.559102-0400 CADApp[8724:128580] [Metal Diagnostics] __message__: MTLCommandBuffer \"Main Loop\" execution failed: The commands associated with an encoder caused an error __:::__ __delegate_identifier__: GPUToolsDiagnostics 2022-06-08 13:18:09.559156-0400 CADApp[8724:128580] [Metal Diagnostics] __message__: MTLCommandBuffer \"Callout 3\" execution failed: The commands associated with the encoder were affected by an error, which may or may not have been caused by the commands themselves, and failed to execute in full __:::__ __delegate_identifier__: GPUToolsDiagnostics Thanks for the update. What message do you think is missing though? Seems like __:::__ __delegate_identifier__: is missing the identifier? I see what you mean, could you please add this message to the original FB? also, I managed to reproduce the lock up locally, so it\u2019s easy enough for somebody to bottom that out I updated FB9968899 at 1:22PM EDT GPU Soft Fault count: 1 kinda identifies that theres something either accessing out of bounds or at 0 <@U03HLFDFS20> I guess I would expect some kind of diagnostic for that rather than an unknown issue. Also wouldn't that affect AMD equally? There\u2019s a possibility that amd reads out of bounds, but doesn\u2019t page fault What I\u2019m seeing locally is definitely a hang, not an out-of-bounds though I guess the current answer is going to be that we would need to investigate these feedback assistant reports That's fine, though my original question was a little different And to answer your initial question, the fastest way to reach us is through developer forums ok I will start using that more Unfortunately there might not be a single solution to progress when you are faced with these issues Thanks for the feedback Definitely thanks for the feedback. Somebody will be investigating the GPU hang soon hopefully Can we somehow cancel work committed to GPU? For example we commit\u0435ed some neural net processing (example of huge work) and we received memory warning from the system, can we somehow stop execution? Hi Pavel, unfortunately no facility exists to cancel committed GPU work. You can use Indirect Command Buffers and NOP out the commands before execution though, could that be a work around? (Also we'd appreciate it if you can file a feedback request with this scenario for us to evaluate.) What is the best way to denoise the Ray traced frame in metal? Is MPSSVGFDenoiser recommended for this purpose? MPSSVGFDenoiser will work for denoising the full application frame. Typically you want a denoiser targeted at each ray traced effect, focusing on the visibility signal for shadows and ambient occlusion, and a separate denoiser for reflections aware of how to reproject reflected content. thanks for the response :thumbsup: Denoising is a rapidly moving field and there's several open source options that you could look at adding to your application! Those are new topics for me, so sorry if it is a silly question, but what is the latest best way to organize global illumination passes with metal ray tracing then? In my inexperienced understanding it was like: accumulate the light from ~3 bounces and apply denoising to the whole result Not a silly question at all! Accumulating light from multiple bounces will result in an approximation to global illumination and will likely be noisy depending on how you define the rays for each bounce. Typically for later bounces you could accept something more diffuse for your global illumination representation - there's more diffuse representations such as https://jcgt.org/published/0008/02/01/|Dynamic Diffuse Global Illumination Does MFXSpatialScalingEffect support PQ encoded HDR color? Specifically, pixel format MTLPixelFormatBGR10A2Unorm and colorspace kCGColorSpaceITUR_2100_PQ ? If so, which colorProcessingMode should be used? HDR colorProcessingMode should be used in that case. Are there any demos using the MTLParallelRenderCommandEncoder ? Is there a possibility of supporting a parallel compute encoder in the future? Hi, thanks for your question. Currently, we do not have a MTLParallelRenderCommandEncoder sample. When you said a parallel compute encoder, can you tell us more about your use case? We do support a compute encoder that executes dispatches in parallel on GPU but not encoding compute dispatches in parallel on CPU. I suppose I don't have a use case in mind :sweat_smile: but since Metal compute seems to becoming more popular (GPU-driven pipelines, ICBs, new machine learning stuff) I was just curious if a parallel encoder would be available. I suppose though it might not actually be necessary if compute encoding is not a bottleneck Typically what we've seen of GPU compute is that encoding is quite light weight while the GPU work is heavy. Encoding is just feeding pointers to GPU shader cores of where all the data lives and then GPU go on a massive parallel data processing/computing spree. :smile: Is there a limit to the scale factor (outputWidth / inputWidth) that MFXSpatialScalingEffect can apply? We recommend using a scale factor that's between > 1x && <= 2x. Just an FYI, we have a digital lounge session dedicated to MetalFX from 1PM-2PM PST today. Cheers. Alright, thanks. I'll try to check in later. :slightly_smiling_face: What's the most performant way to copy a CALayer into a MTLTexture on iOS? Can you elaborate on your use case? What kind of layer are you trying to copy, and does your app provide the original content of this layer? We\u2019re trying to render Lottie animations into a Metal texture interactively in real-time. Our goal is for a user to be able to drag a Lottie animation around a canvas as it plays and have it composited with other Metal content. The Lottie library we\u2019re using renders to a CALayer and we need to copy it into a MTLTexture. Do you know what kind of layer they\u2019re rendering into? It matters significantly whether the library is also using a CAMetalLayer or if it is using the traditional CALayer drawing APIs. I think it\u2019s not a CAMetalLayer, which is the problem. I think it\u2019s using CAShapeLayer, CATextLayer, etc. (which are much simpler than implementing text rendering for instance). But it seems to be tricky to then get the result from the CALayer into Metal. On macOS CARenderer seems to permit this but not in iOS Most generally, the only way to capture the composited appearance of a layer is via CARenderer. You will have difficulty syncing the framerate of this renderer with the display. CARenderer is available on iOS, can you elaborate on your difficulty with it? Hmm.. I could have sworn that CARenderer::rendererWithMTLTexture was not available on iOS.. but the docs says it has been there since iOS 11. Well, I think that\u2019s the way to do it, right? One difference I can think of between macOS and iOS is that iOS makes much heavier use of implicit CATransactions. If you try to render the layer tree while the main queue\u2019s implicit transaction is still open, you might not capture the latest drawing. On iOS, to guarantee correct drawing, use UIGraphicsImageRenderer in conjunction with -[UIView drawViewHierarchyInRect:afterScreenUpdates:YES] . You can then use the resulting UIImage to populate an MTLTexture. This will not be as fast as using CARenderer directly but it is more likely to be resilient to the complexities of how iOS apps use Core Animation. But do try CARenderer first. :slightly_smiling_face: Thanks, I\u2019ll take another look! Good luck! If neither approach works for you, please file Feedback so the teams involved can understand more about your use case. The new EDR support in Metal for iOS looks really exciting, but on what current devices can we test this on? Devices with an XDR display currently support EDR as long as the device brightness isn't set too high (among other factors) which would clamp those EDR values. Please see https://developer.apple.com/documentation/metal/hdr_content/determining_support_for_edr_values?language=objc for more details. On macOS, you can read the maximumPotentialExtendedDynamicRangeColorComponentValue property on an NSScreen object for that display. And on iOS, you can use UIScreen.potentialEDRHeadroom . Is the iPhone 13 Pro included in this? I haven't tested it, but I think you should get a large EDR headroom on iPhone 13 pro. We also recommend to poll the value of UIScreen.currentEDRHeadroom (for iOS) or NSScreen.maximumExtendedDynamicRangeColorComponentValue (for macOS) to accommodate for possible display brightness changes that might affect available headroom. Those can be caused for ex. by the user interaction - changing the screen brightness or by the system if auto-brightness setting is enabled or as a result of thermal event when device has to throttle. More out of curiosity, what are the potential use cases of quadgroup functions? I've only seen one video mentioning them (\"Discovering advances in Metal for A15 Bionic\") where they're used to reduce the number of texture reads per thread. Are there any other use \"common\" use cases? It can be used for any workload where you want to spread orthogonal work across a group; for instance, you can have each thread in a quad cull a part of a light list, then use quadgroup functions to peek at each others' lists and sum lights. The gist of quadgroup and simdgroup functions are to perform data exchange in a more efficient manner. Traditionally, threadgroup barrier and threadgroup memory is required to perform data exchanges, but its a lot more heavier than finer granularity exchanges like quadgroup or simdgroup functions. When would you decide to use a quadgroup over a simdgroup function? When all you need is to exchange among 4 threads in a row. The finer granularity you use, the more efficient it is. If you are in compute, I would probably always use simdgroup, just to get the best bang for your buck. I believe quadgroups are interesting for fragment workloads; I'm not sure what the availability and details are by heart. (rasterization, helper threads etc) I'm interested in the accelerated ray tracing features in Metal 3, but the specific session that would contain more details is tomorrow. Are these changes specifically designed for realtime raytracing in a rasterized renderer, or are they general enough to be used on their own to ray trace entire scenes? I see that there are general purpose ray tracing features in older versions of Metal, I'm specifically curious how these intersect with the Metal 3 ray tracing features. Metal RT is intended for use in both pure ray traced renderers and hybrid renderers that ray trace from rasterized content. This year's features are applicable to both with a focus on improving performance with some quality of life and ease of use features too! We have a Q&A session on Friday if you\u2019d like to ask additional questions following the release of the video. Some of our samples, such as https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal|Accelerating Ray Tracing Using Metal and https://developer.apple.com/documentation/metal/metal_sample_code_library/control_the_ray_tracing_process_using_intersection_queries|Control the Ray Tracing Process Using Intersection Queries do pure ray tracing, and https://developer.apple.com/documentation/metal/metal_sample_code_library/rendering_reflections_in_real_time_using_ray_tracing|Rendering reflections in real time using ray tracing uses a more hybrid model. If anyone would like to have more of an in-depth/follow-up discussion, we still have a few Lab slots available! https://developer.apple.com/wwdc22/labs/ As far as I can tell iOS devices still run OpenGLES apps just fine. Is there a hard deadline when this will not be the case any more? We can't comment on future plans, but we recommend porting existing OpenGL-based applications to Metal to take advantage of the latest features available on our devices. Are there any recommended formats or workflows for dealing with 3D textures in Metal? Can you clarify more about \u201cworkflows\u201d? Are you trying to integrate with an existing asset pipeline? Is there a specific thing you are trying to do? I believe you can manually fill the texture one slice at a time if needed. If you are not interpolating along the 3rd axis, you might want to use a 2D Texture Array instead. Just be careful with blit API, Simple APIs don\u2019t work on 3D textures you need full swing blit copy with one full slice calculated not whole texture! I believe MTLTextureLoader supports KTX files, including 3D texture formats. Thanks for the pointers. I'm just starting to explore ideas. I had exported a PNG with vertical slices from a voxel editor. Thanks for the pointer on KTX! I'll look into that Importing one slice at a time works as well! If you are doing uncompressed data (voxel art etc), that would work. If you are going more \"natural\" 3D textures, beware that you probably want a compressed format as it makes a big difference in memory size and performance. Right. Are there any gotchas using sparse textures for 3D textures? I don't believe so - it should effectively work the same. Are all of the Metal 3 features available on all GPUs? Do Vega GPUs and UHD 630 get mesh shaders with Metal 3? \u2022 Metal 3 hardware support can be found in Discover Metal 3 https://developer.apple.com/videos/play/wwdc2022/10066/ \u2022 Specific features (Mesh Shaders, Metal FX) may have a different hardware support. For example, Mesh Shaders should be supported on MTLGPUFamilyApple7 and MTLGPUFamilyMac2. Please refer to their associated session for more information. ie, for Mesh Shaders : https://developer.apple.com/videos/play/wwdc2022/10162/ Quick question: What happened to the Metal Language Version build setting? Seemed it disappeared one year... Hi Steven, this looks a bug, could you please file a feedback request for us to track it? In the meanwhile you can work around this by passing -std=ios-metalX.X to your .metal file compilation flags Same for macOS? ~I think for macOS it is -std=osx-metalX.X ~ Looks like it may be back in Xcode 14, would it be fixed for 13.4? For macOS, it is -std=macos-metalX.Y until MSL 3.0 where we have unified language and it is -std=metal3.0 thank you Hey there! I have been working with a highly programmatic SceneKit nodes and geometries (I know, I know, old hat), and I have been suggested by some friendly helpful developers at the WWDC Apple Park event (Hello again, Rintaro!) that a pathway to help some of my performance problems would be to 'rewrite' some of my core components with ARKit primitives, or perhaps Metal. Unfortunately, I don't really know where I would begin that conversion, or even if doing the work would net the performance benefits I'm seeking. The context of this question is my AR/VR visualization project which uses 10's of thousands of small geometries and nodes (shared as much possible) to render individual text glyphs like a sheet of paper ( http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat ). A good, solid first step would be: What would be a suggested migration pattern from SCNNode/SCNPlane/UIImage to similar Metal/ARKit patterns? Are you using individual nodes for each letter/glyph? If so, I would recommend \"baking\" the glyphs into a vertex/index buffer at run-time and then inserting the entire mesh as a single node. So for instance, you would read in the text, then generate the mesh for a single page, and add that page to your scene as a single mesh (that you can then modify as if it is a static mesh). Alternatively, you can simply generate a large image with all the text on it using our Text APIs and simply apply it as a texture/Image. But the latter would take up more memory as you are effectively creating a full-sized texture each time. Jaap, thanks for your time! That\u2019s exactly the case - at the moment, I render each glyph as a node, and then flatten() them into a single geometry. I have a large texture method too, and it does work, but indeed has the memory issue you\u2019re mentioning. The flatten() has the advantage that is actually seems to create that mesh from the nodes, but I think it does it in a non-optimized-for-my-use-case-way. If I were to generate the mesh, is there something that I could use as a sample to convert a node\u2019s general presentation / backing geometry to a mesh? I apologize if that\u2019s a vague question. Essentially, I\u2019m looking to see if there\u2019s some intermediary step I can take that would allow me to migrate from the patterns I have now, even if the translation would be slow, just to build out some observations to make better hypotheses from. That would be something like, \u201ca function that takes a parent node and generates a mesh\u201d or something along that. Also, at the moment I \u2018hot swap\u2019 between the glyphs and the flattened nodes to get that boost in FPS. I need to keep the nodes around, though, because they\u2019re actionable individually - updating positions, backing contents, that kinda thing. It seems like I\u2019d be able to do the same thing with a mesh by dynamically updating the mesh itself for things like positioning, and then figuring out how to map texture coordinates to those dynamically. I was even looking at \u2018textureCoordinates\u2019 as a potential option, but that exists on geometry , and all of my nodes with the same glyph share a geometry, again to keep total counts down. Hi <@U03JRPTDF6U>, sorry for the late reply, as we closed down the Q&A, but if you want to discuss this more, please drop by the Q&A on Friday, or sign up for a Developer Lab, which is a 1:1 in-depth chat with one of our developers! Just to quickly reply to your question, I do agree that you probably want to do this manually to remove a lot of the overhead. I'm not sure how familiar you are with mesh representations, but you should probably be able to do this by effectively writing your own \"flatten\" function. Without knowing where most of the performance is lost, I would suggest reading the nodes data manually and then composing the vertices for the mesh yourself. This would require you to manually create vertex and index data for your mesh and then upload and bind these to your Mesh. I would suggest putting your question up in the Labs and indicate this is a SceneKit question - I think those folks will be better equipped to point out the best path :) With MetalFX are there any concerns or issues when dealing with transparency/volumetrics when doing temporal upscaling? Would it sometimes be better to use Spatial upscaling and MSAA in some cases with a lot of transparency? Temporal AA requires full motion and depth information to produce the best result. Transparent objects usually do not provide that, so TAA will have insufficient input data to process everything correctly. Similar logic applies to reflections. The choice will depend on your particular setup of the rendering pipeline. You could apply temporal upscaling to lower resolution color buffer with opaque objects only and then render transparent objects and volumetrics on top (in output resolution). That makes sense, thanks. Would the upscalers also work for targets other than the final tone mapped image, ie an AO pass? Yes, you can do that. Just allocate separate scaling effect instances for those. Can ScreenCaptureKit be used by two apps at once, each capturing all or parts of the screen? Hi Matt! Fantastic question! ScreenCaptureKit allows for any number of active streams from any number of clients capturing any combinations of filters! Great thanks. Is system-level permission required even if an app only wants to capture /itself/ (i.e. only its own window(s)), and not anything external? Hi Michael! Yes, system level permission (one time) is required for the initial time any applications wants to use ScreenCaptureKit. Even if the application is capturing itself, it'll need to get the SCShareableContent list in order to create a filter, which requires system level permission on the initial use. Gotcha, gracias! Our video conferencing app offers screen sharing functionality, but we want the screen share video to exclude certain windows (e.g., A/V controls and floating participant videos). This is easily solvable with content filtering in ScreenCaptureKit, but what is the recommended way to get this behavior with older versions of macOS that do not support ScreenCaptureKit? You can use the screen capture portion of ScreenCaptureKit as of 12.3, and send it to other users using earlier versions of macOS. In order to get the maximum screen capture performance with ScreenCaptureKit, you'll need 12.3 or later. To achieve similar behavior in previous versions of macOS, you can use the CGWindow API to capture video, but not audio. Specifically, CGWindowListCreate() and CGWindowListCreateImage(). They\u2019re much slower than the other APIs, but available back to earlier MacOS versions. If you want to display the interactions with menu at the top of macOS, does that constrain you to just capturing a display (vs. application or window) in order to show that? You can use init(display:including:exceptingWindows:) , but this will capture the surface the size of the desktop. The app will be positioned as it is in the desktop, and the menu bar will be at the top of the capture. Ah, brilliant! Thank you! I make media /playback/ systems, but haven't really yet played with /producing/ media\u2013 could you give me an idea of what the next steps would be (in terms of macOS frameworks/API usage), to go from ScreenCaptureKit's sample buffers, to, say, a 'playback-ready' H264/HEVC+AAC HLS output? Hi Michael! ScreenCaptureKit gives samples back as CMSampleBuffers. So if you wanted to make it playback ready, I would suggest you use AVAssetWriter to create a movie. This should allow you create a movie with the output you wish For HLS, you're going to use VideoToolBox to fragmented mp4 files that you can then upload to the server and update your HLS playlist And if you need a CBR-based stream, Ventura added hardware-encoding-based H.264 and HEVC CBR support, so you can offload the work from the CPU. <@U03H3GXFRSB> This is incredibly helpful! You, in two paragraphs, have just demystified something for me that has, honestly, just been a bit of an \u201cI can\u2019t do that\u2026\u201d topic so far :ok_hand::skin-tone-3: <@U03HF6TL5L5> Ah, as in; pre-Ventura H.264/HEVC hardware-encoding was VBR-only, or? Correct! I think it's technically ABR (instead of VBR), but I'm no encoding expert, so grain of salt. When capturing windows only, is the window's drop shadow captured, or only the actual frame of the windows? Using init(desktopIndependentWindow:) , you will get back an IOSurface that is the size of the window exactly. No drop shadow included. Oh well. Thanks This is definitely a lesser-used feature in the existing CGWindowList API, but it would be great to see it in ScreenCaptureKit, as well. Please submit a feedback request for this! Sounds like a good idea. Would be great to be able to make screen captures (either screenshots or screen recordings) in HDR! Thanks Chris! Please file a feedback in the feedback assistant for new features for ScreenCaptureKit. We're super excited about it, and we hope it can meet all of your needs. Will do; thanks! One thing that wasn't totally clear to me from the videos: Obviously, audio-capture is per-application, since audio is not associated with a window. If I exclude only some windows of an application from capture, does that mean I don't get any audio? If you\u2019re using display capture, and exclude any window of an app, then you will get system audio minus the whole app\u2019s audio. We also will cover this and other scenarios in our advanced talk \u201cTake ScreenCaptureKit to the Next Level\u201d :slightly_smiling_face: Thanks. I think this is fine for my typical use-case, but might be problematic for some others Another suggestion is to have a separate stream with the audio you want. If you have a particular use case or scenario you think we could address better, please file a feedback request! If I'm capturing all windows from a single application, will I also get XPC-hosted dialogs? I'm thinking of Open/Save panels, and the like, which were not easily captured with the CGWindowList API, since they are associated with a different PID than the application windows. It should capture things like the Open/Save dialog. If it doesn't, make sure to file a feedback request! I\u2019ll check, and let y\u2019all know. Just to reiterate my pledge - please please push mip bias as a part of the sampler for next metal releases. I think even GL had it :) Please submit your request via Feedback Assistant. Sure! Why use a constant jitter offset for a set of pixels instead of a random one that would probably make the result more smoother? Using a fixed set of low-discrepancy jitter offsets ensures that the space of potential jitters is covered relatively evenly for any given window in time. Thank you <@U03HJ4DLF7D> for your answer! Oh I see... You get a good representation of jitter space, even under this coupling between neighbouring offsets? Interesting... Wonderful presentation @Kevin! Thank you very much! The technology works beautifully with slow moving scenes. Could you comment on the impact of using temporal information on quick moving scenes or quick changing content? Should there be some adaptive setting with respect to the amount of previous pixels used based on the richness/rate of change of the scene's content? Thanks. If the scene is truly moving super quick, where there's very little temporal information that can be reused, the result will simply not be as high resolution and anti-aliased. We do expect this to work as well as the amount of information that's present and for things to be more refined as motion slows. Thank you <@U03HJ4HULBC> for your answer. Yes, I guess this is expected! Fast moving content is always a challenge with these technologies... Once again, thank you for a wonderful, easy to watch and beautifully prepared presentation. (Really nice and illustrative clips! :slightly_smiling_face: ) Machine learning has shown great results with respect to upscaling both in terms of quality as well as performance. Do you see implementing such a functionality in MetalFX over the near future? We cannot comment on particular implementation details of MetalFX framework. Thank you <@U03H3GQDUP9> for your answer. :slightly_smiling_face: I wouldn't say using ML for upscaling is an implementation detail, but rather a radically different approach to classical upscaling methods. Of course, it has its own implementation details, should one opts to actually build it (that naturally can't be shared). However, my question was about the approach itself and wether it is under consideration (and, of course, not any specifics on its implementation). Note that the answer is yeah, machine learning is used. you can see the models at /System/Library/Frameworks/MetalFX.framework/Versions/A/Resources/brnet_v31_quant . Hopefully that clarifies things a bit. Hi <@U03J07WJLRK>! Thank you very much for the reference. I will check it out! :slightly_smiling_face: I couldn't help noticing watching the No Man's Sky clip that there was quite a bit of smearing and things looked fairly low quality on the big camera move. Are these kind of camera moves a big concern with this technique. I understand TAA can have issues with smearing and ghosting. Depending on the network connection you have, the video quality may be worse and lower in quality. Can you point to us on which time-stamp in the video you saw the smearing effect? I think I had downloaded the presentation to my iPhone and watched. The timestamp was around 23:35, though looking at it more closely it could very well be getting destroyed by video compression. Are you maybe referring to 21:35 timestamp? The session video is 22:11 in length Yes, so sorry What lab is best for asking SceneKit questions this year? I think Games technologies Q&A would be the best fit. Please look in the schedule to see when is the next session. Thank you! What is the best way to render biplanar EDR cvpixelbuffer from avfoundation in Metal? Is better to convert pixelbuffer into mtltexture via compute, or sample from 2 mtltextures created from each plane? To render EDR CVPixelBuffer contents using Metal you\u2019d need following: \u2022 Setup CAMetalLayer to accept EDR values using CAMetalLayer.wantsExtendedDynamicRangeContent property set to YES \u2022 Create MTLTexture instances that shares corresponding IOSurface from CVPixelBuffer \u2022 Use them for rendering Thanks for the explanation about EDR:) And what about performance difference between using two planes in a shader directly, and converting it to rgb beforehand? I can\u2019t predict that to be honest :slightly_smiling_face: It might completely depend on the context and use case of your particular scenario. Ok, thanks!) I\u2019ve thought there was a well known best practice:) What's the best way to build a full screen game experience on the Mac? Should we still be using CGDisplayCapture? Or the AppKit full screen window support? Both seem to have pros and cons. For example: AppKit allows the menu bar to pop and for easy access. But CGDisplayCapture can use the area around the notch. Using the AppKit APIs will give your users the experience they expect, especially if they have customized Spaces. Use presentation options to control things like the menu bar behavior: https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions|https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions Thanks! I'm having a few issues with presentation options but that seems like a lab I should book for later. :smiley: Yes, please do! Regarding the display shape, look into safe area insets: https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets|https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets I just confirmed with the responsible engineering team that as long as nothing else is drawing atop your full screen NSWindow, you will be on the fastest-performance drawing path through the compositor. Thanks. Is there a way to display in the insets using NSWindow's toggleFullScreen? We don't need to draw in the insets, but wanted to try it as a possible full screen style. You can always draw within the full size of your window. The safe area insets are informative about places where your drawing might be clipped. Thanks. It's possible I've configured my layout to be constrained into the safe area. I'll check it out! Hello everybody! Is there any sample or instructions how to make offscreen render using Metal? For example: I have a scene and I want to save it as png file. How can I do it? Can I control resolution of saved file? To offscreen render a scene and save it as a PNG file, you may first render the scene into a MTLTexture with the desired resolution. The texture will need to be a <https://developer.apple.com/documentation/metal/mtltextureusage/1515701-rendertarget |render target>. Then there are a few different ways to get a CGImage from the MTLTexture: A quick way will be to <https://developer.apple.com/documentation/coreimage/ciimage/1437890-init |initialize a CIImage> with the MTLTexture and then <https://developer.apple.com/documentation/coreimage/cicontext/1437784-createcgimage |use a CIContext to create the CGImage>, which you can use to write to disk. Oh, thank you so much for the answer!!! Can you leverage the screen capture API to cast data that is being rendered off screen? eg two cameras in a scene, one camera view is your ipad screen, another camera view would be streamed to apple TV. each camera are viewing different location but in the same scene. Hi Simon, Can you elaborate a bit on what you mean by cameras and scenes? Do you mean a scene inside an app/game? Do you mean physical cameras capturing multiple feeds at once and aggregating them somewhere? Scene as in a level in a game or asset in a 3D engine (MTKView), Cameras would be digital viewports/view frustums Think Blender scene, with digital cameras SCK capture comes from the display pipeline, so content thats captured must have gone through that to be eligible. Does that make sense? a yeah it does, so that would not work then. Will have to read up some of the docs. Thanks for the info :slightly_smiling_face: Yeah it sounds like you were trying to \u2018save\u2019 doing some computations, but that won\u2019t work for this scenario :smiley: cut corners where you can :smile: it makes sense if you were trying to show say, \u2018behind me\u2019 or maybe a companion VR viewer for the non immersed person, etc, so we feel ya :smiley: Its an interesting use case, feel free to log a Feedback requesting an answer for the high level problem so we can take a look ! I'm sorry, I wasn't able to watch the video, so just wanted to confirm: with ScreenCaptureKit we're able to capture the mixed system sound, or filter in or out the applications that we want or don't want to include, right? For example capture sounds from my own application + Zoom call already mixed for us, basically replacing things like Blackhole + Multi-Output/Device etc? Yes, absolutely! In this case, you might want to use init(display:excludingApplications:exceptingWindows:) , where you can capture all audio, excluding the applications that you don't want in the audio. CARenderer was not previously available on iOS (only macOS). But Xcode 14 and docs on the web now show it as available since iOS 2.0! https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor|https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor Is this a mistake, or is it now available to older devices if built with the iOS 16 SDK? It\u2019s not a mistake, and the availability annotations are correct! CARenderer is available on all devices. You can back-deploy your code built with the iOS 16 SDK to devices running older OSes. That\u2019s great news! (But no wonder I was so confused yesterday..!) Thanks Does the Overview need a bit of editing then? > For real-time output you should use an instance of NSView to host the layer-tree. Good catch! Please file Feedback about the documentation :slightly_smiling_face: what is the fastest way to get something on screen with Metal? is there way to draw directly to screen instead of metal layer contents and then wait for frame composing? The fastest way to get your drawing on-screen is to create a full-screen CAMetalLayer. You can also optionally set displaySyncEnabled to false on your layer to render faster than the display\u2019s refresh rate at the cost of potential artifacts. Be sure to take advantage of a ProMotion display if the user has it, too! You can set a minimum and maximum CADisplayLink callback rate using a CAFrameRateRange, for instance. Might anyone be able to speak to whether it makes sense to combine indirect command buffers ( https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc|https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc ) with bindless rendering and dynamic data? Specifically, I am trying to avoid a lot of draw calls. With the SOC architecture, it seems passing data is not really expensive, so I figure per-frame I could just specify some parameters and generate draw calls on the GPU, as in the example. -broad question, I realize, but I'd appreciate some thoughts on how to reduce draw calls for rapidly-updated and/or deleted vertex data. I also checked mesh shaders, but I think those are overkill for my use case. Hi Karl! Metal 3 adds support for starting raytracing work from ICBs. ICBs give you more flexibility at rendering time, but this flexibility does come with a cost that will vary according to your specific use case. For example ICBs will give you the edge when you are CPU-bound. As with all things related to performance, we recommend you profile your game (or app) :slightly_smiling_face: Hi! Thanks for the reply, and I agree I should always profile. To clarify, I\u2019m not doing raytracing, so perhaps this is pre-optimization, but while WWDC is happening, I thought I\u2019d ask a follow-up question: Since you mention ICB-use cost, what would you say the common tradeoffs are with ICBs? Also, do you imagine them working well with bindless rendering? For example, let\u2019s many textures and my entire scene in a buffer within an argument buffer. Would it then make sense to use an ICB to issue draw calls on the scene stored that way? However, I suspect that ICBs and bindless are orthogonal. Thanks again for your time! When will the activity feed be available? The activity feed is available today in iOS/iPadOS/tvOS 16 beta and macOS Ventura beta. :video_game::star2: Hello! :wave: Playing around with the new ScreenCaptureKit I'm experiencing problems with TCC, revoking access to Screen Recording while the app's running (domain: com.apple.ScreenCaptureKit.SCStreamErrorDomain, code -3801 message: \"The user declined TCCs for application, window, display capture\"). Confusingly, when my app launched it did have the proper access. Anyone around knowing under which special circumstances the system may suddenly revoke access to Screen Recording for an already running app? There shouldn\u2019t be any reason why an app has their Screen Recording tcc revoked - besides the user denying the app permission. Could you please file a feedback request, with full steps to reproduce the issue? :slightly_smiling_face: That was my impression, too. Thanks for clarifying, <@U03HF6TK2J1>! Unfortunately, there doesn't appear to be a clear pattern to this. Usually, it just takes some time of the app sitting around to get the revocation suddenly, maybe an hour or two. Does it have anything to do with the process being a Service Management Login Item and LSUIElement=1 background process? It's hard for us to say for sure. But in lieu of a reproducible case, a sysdiagnose shortly after the time of the issue should help us figure out what might have caused it! I see, well then I'll file the bug report, thank you. Amazing new API by the way, I'm sure lots of developers are going to benefit greatly from it. :slightly_smiling_face: Glad you like the new framework! Hey all! Question is related to SceneKit + UIView / SwiftUI view layers. There has been some historical functionality allowing the layers of views not presented in a typical hierarchy to be presented as the material contents of geometries like SCNPlanes. One example is this repo here: https://github.com/sarangborude/SwiftUIARView|https://github.com/sarangborude/SwiftUIARView . Unfortunately, the technique here doesn't seem to work anymore a couple of years later. I'm wondering what might be possible to bring this back to usage, as I have a TON of use cases I'd love to explore with this! Thank you so much as always! Hi <@U03JRPTDF6U> I'm not seeing why SceneKit + SwiftUI shouldn't be working in this situation still. If you have a moment and can submit to https://feedbackassistant.apple.com and reference that GitHub project, it would be great for us to get this to the right folks inside Apple. If you do this soon and can post the FB number back here, that would help me make sure it routes. <@U03H3GPSE6B> Heya, and thanks for the good news, haha! I\u2019ll file it right now and add some more information about what I\u2019m trying to do. Also, this repo is a great resource as a sample, but I\u2019m curious if there might be some tighter documentation about how this is working? For example, there\u2019s some fun magic happening in the hosting of the controller and when it gets added, and I think at least part of my issue is I\u2019m not mimicking the pattern correctly. Also, here\u2019s that Feedback! FB10141722 thank you! Of course. And hey, in the mean time, might you know of any docs that might be related to this? I\u2019m thinking of giving this another go soon, hehe. Happy to sit back and relax though to hear from the Feedback side :smiley: For a relatively simple game (low poly graphics, simple textures, no AI), which framework would you suggest I use to develop in Swift? All logic would be custom, so no need for scripts library or similar features. SceneKit seems like an obvious choice as it has the functionality I need, but I'm not sure how actively it is still being maintained and supported? Hi <@U03HMCD9UQ7> If SceneKit is a good fit for your needs, it's a great choice! It is still being maintained and supported. If in the future you outgrow its functionality, you will have learned a lot of key parts of dealing with transforms, models, and geometry which you can use if you want to drop down to Metal and ModelIO, or move up to a higher-level framework or tool. On the topic of maintained and supported, should we assume SpriteKit is also still supported and maintained? Main reason I am asking is because I have submitted quite a few SpriteKit-related issues through Feedback Assistant, and I have not received any feedback at all on any of them. I dearly hope so as SpriteKit is core to my app. I add touches and logic and page layout on top but it's all SK, all the way down. Hoping for an answer to this, I will tag <@U03H3GPSE6B> . Nat, can you please check the previous 3 replies here, regarding SpriteKit? Thanks. https://github.com/AndyDentFree/SpriteKittenly is where I post my SpriteKit explorations of tech, following my philosophy of having lots of little test apps, as useful regression checks. Sorry for the slow reply - yes SpriteKit is still maintained and available! Thanks Nat. This is good news. I hope for maybe a little bit more maintaining in the future, maybe some of the reported bugs in the Feedback Assistant can be looked at. Thanks again! Hey <@U03JZ2H3NNR> do you also publish your bugs in somewhere open to the rest of us non-Apple employees to see? I ran into a doozy with SpriteKit that was (strictly speaking) my bug but was encouraged to lodge a Feedback when I discussed with an engineer during the Tech Talks last year, as that is such weird behaviour we should probably try to fix it anyway . (:blush: at realising it's been a few months & still haven't lodged that). You can read the entire saga https://medium.com/touchgram/oops-hitting-a-5yo-apple-bug-17d2703519f4 Given my massive dependency on SpriteKit I'd like to know about any and all lurking gremlins :pray: <@U03JELM0ZNV> Yes, I usually publish them on both Dev Forums and StackOverflow. You can see some of them here: https://developer.apple.com/forums/profile/calin I have more but to be honest I kinda gave up reporting them because, like I said, I am not getting any feedback, so it seems like a waste of my time, as it does take some time to file a properly documented bug report. Instead I am simply trying to work around them. I was thinking to maybe try on Twitter too. My twitter is @upurupu if you want to keep in contact. I'll definitely read your Medium post. For example, one bug I was hoping will get fixed (maybe it was, haven't tried it yet), is that isPaused , when passed to a SpriteView , doesn't seem to affect the state. See https://stackoverflow.com/questions/69610165/spriteview-doesnt-pause-scene-on-state-change/69610906#69610906 Great, will stay in touch. I am very active on Twitter as @andydentperth I have yet to get into SwiftUI as my app is iOS12+ so all UIKit but am just on the cusp. Thanks to feedback this WWDC will be trying to host SpriteKit for some screens. Just got a demo going with SwiftUI working inside an iMessage app extension at https://github.com/AndyDentFree/im-plausibilities/tree/master/imUrlDataAppSUI Is there a way to send a raw MTLTexture or a specific view to ScreenCaptureKit, or some lower-level API related to it? For example, let's say I have a real-time graphics application written using Metal, and I want to stream content containing only audience-relevant elements in my scene, rendered to one texture. I broadcast that texture. In another renderpass, I composite some diagnostic info only relevant to me locally that I don't want to share with others. How might this be achieved? It's a use case I keep running into (asymmetric views and perspectives.) Going even further, maybe I am developing a game and I want to render a completely different perspective in my scene from a different camera. I send that texture to the stream, but maybe I render from a different perspective locally. Hi Karl, SCK is dependent on the display pipeline so the content needs to be rendered before it can be captured. Ah, I see, so it sounds like I\u2019d need to roll my own solution for something along the lines of what I described. We discussed this a little bit yesterday too, in https://wwdc22.slack.com/archives/C03H77PER5G/p1654728707927069 You should log a https://developer.apple.com/bug-reporting/|feedback detailing your use case so we can consider it for future improvements! Will do! <@U03HWP445CH> Thanks for the link, I was to late yesterday to ask where I should submit the use case <@U03HWP445CH> <@U03HZ4EJJ05> I wrote this hopefully compelling argument: FB10143711 (ScreenCaptureKit Suggestion: Selective streaming of textures for Asymmetric Views) Thanks! We've got it Would ScreenCaptureKit be appropriate for a remote desktop/screen sharing application? Hi <@U03HZ4HF31T>, yes it would be appropriate! Did you have some specific scenarios you were concerned it might not accommodate? Just looking at something that would provide better performance than vnc (not difficult!) and add audio for remote working on a mac. Obviously keyboard/mouse input would need to be handled seperately. Screen sharing is not quite the same use case as bi directional content flow. Handling where the mouse is, capturing input and passing it back would require a not-insignificant amount of work To split your question, it would be appropriate for screen sharing but more challenging for VNC where you want to interface with the source from the destination Since audio was mentioned too, I'm assuming it's still a quite tough challenge to be able to capture the system's audio, right? May require writing an own low-level driver for it? No, you can get system or app audio! ScreenCaptureKit can include system audio as well I believe. Ahh beat me to it! wait what...? :flushed: I must've missed that part of the API You can include or exclude apps and capture all system audio. https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955572-excludescurrentprocessaudio The session also covers these use cases with specific examples https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955571-capturesaudio Given the amount of work required, I may provide feedback and hope/pray that the built in macos remote desktop will add audio! <@U03K0FHV4M8> Its much requested and long awaited, we are thrilled its surprising and delighting you :smiley: amazing! it doesn't allow capturing only the audio from one app though, does it? It does I believe. Yes, it allows you to specify just a single apps audio as well. <@U03HF6TL5L5> covers this use case specifically near the end of the session If you wanted to say, exclude a games audio and include your friends chat audio from another app, SCK would support that use case If you think about a capture session as being \"audio, video, or both\" you can setup a capture for a process and just drop the frames, making it an \"audio only\" capture. I'm blown away by this. We've all been waiting soooo long for this and now it's finally here. :pray::heart: If you don't set capturesAudio you have a video only capture and if you set it, you get both! really cool! thanks for all the explanations :stuck_out_tongue: The framework looks great. Congratulations on your hard work! And thanks for your help. Then its just how you want to combine \"including\" or \"excluding\" the calls like https://developer.apple.com/documentation/screencapturekit/sccontentfilter/3919807-init that lets you narrow in on what you want depending on your use case Are the docs going to be updated soon or why am I missing these audio APIs you're talking about there somehow? There is a sample project and videos that should help. When you go to https://developer.apple.com/documentation/screencapturekit and at the bottom search for \"Audio\", do you not see those results? The https://developer.apple.com/documentation/screencapturekit/capturing_screen_content_in_macos|sample app also includes Audio Well, I can see the functions about whether to capture audio, exclude the current process and the audio stream output type but not how I would exclude/include certain apps ah wait, those are not part of the ScreenCaptureKit API? Audio capture follows similar rules to video capture, however audio capture is at an application level. So your selection will have to be application-based. <@U03K0FHV4M8> You'd use the SCContentFilters for that init(display:excludingWindows:) or init(display:excludingApplications:exceptingWindows:) ahh, I think I got it now :grin::see_no_evil: The team notes \"Excluding a window will exclude all of that apps audio\" as a FYI that was the missing info, thanks! Yeah. I think Meng talks about this a bit in the \"Take ScreenCaptureKit to the next level\" WWDC talk, if you want to double check your understanding. Will definitely do I have a classic \"dumb question\" - is ScreenCaptureKit the macOS equivalent of ReplayKit? I want people to be able to create reaction videos of them playing touchgrams, which is running on SpriteKit. Hi Andrew! No question is \"dumb\"! So ReplayKit is also on macOS! but there are a lot of differences between ReplayKit and ScreenCaptureKit. ReplayKit has several features that aren't in ScreenCaptureKit, including broadcast extensions, Clips, and built in movie recording for in app use case Where ScreenCaptureKit differs is in its ability to capture far more content. ScreencaptureKit allows more than just in-application capture. It was designed to give you performant capture with custom filters for applications outside of your own So if you wanted someone to be able to capture video of their reactions (maybe using an iPhone clipped to the top of their laptop :wink: ) as well as playing a game experience, which would you use? ReplayKit has built in support for camera! so if you wanted to record just the singular application and their camera, you can do that with replaykit you could also do the same with screen capture kit as well! using AVCaptureDevice to get camera frames, you can get a custom filter of the content you want (more than 1 application) and then get the frames from the camera and using metal you can put them together and render that as well :smiley: Though you'd need something else for the camera capture part if you were using SCKit. Right, Johnny? correct <@U03HF6TL5L5>! you'd have to run an AVCaptureDevice (camera) to get frames from there All the user experience would be happening inside SpriteKit with audio playing via AVAudioPlayer (audio is in response to user actions, gestures, tapping parts of message, whatever has been specified by the original message author - it's user-generated content so utterly random) Is that likely to that break recording? not at all! ScreenCaptureKit and ReplayKit allows you to capture the applications audio, even if its played via AVAudioPlayer you'll get the application audio (I should just stop before the outstanding issues count on the tracker of every bright idea ever passes 1K) but is this robust with multiple players? Someone can have one or more background looping sounds start on entering a page and then have more short ones in reaction to touches - feels like I'm going to be pushing boundaries. I have a suspicion that making it easy for people to share videos of their play experience could be a huge accelerator, so this tech niggles at me :star-struck: is this local multiplayer? or the multiplayer instance played remotely? \"Multiple players\" is multiple sound players, sorry for the ambiguity. Not game players ahh got it, okay, and are these sound players coming from the same process? and by same process, I mean the same application process to which you are attempting to capture the video ? ReplayKit will only capture your applications audio, so if your application has several avplayers instances but its all in the process, ReplayKit will capture that audio for you Yes - Touchgram runs on top of SpriteKit with a document model where the message sender composes the experience, which can be utterly driven by what they want to do - think of it as a game authoring toolkit - meets Keynote - inside a message. The playback experience might be the person sending the message or, more likely, the receiver. Currently (deep breath), playback is only within iMessage as an app extension, which may complicate the ReplayKit scenario. However, it's relatively trivial to also have playback in the accompanying parent app and I'll be adding that soon. So inside one process is kinda true but as an app extension may have nuances ahhh okay, so if its coming from an app extension, then it will not get captured, as extensions are run as a separate process App extensions should have their own process, and that won\u2019t be captured it sounds like your application is built for iOS? It's an iMessage app extension but all the playback stuff plays on macOS as well. Although it's touch-driven, it's single-touches most of the time so is usable via mouse I'm planning to move to SwiftUI so started considering a Mac version :innocent: ahh okay! so ScreenCaptureKit is only for macOS it looks like the best fit for your application is in fact ReplayKit, you'll the audio from different processes / bundleID will not be captured Thanks for the clarification, and inspiration you are most welcome!! <@U03H3GXFRSB> Hey Johnny, it's Tobias from the one-on-one session today about AVAssetWriter producing corrupted files (the Teams related app). :wave: Thank you so much again for sharing your expertise with me and answering all of my questions. <@U03H3GXFRSB> I think I've figured out what the issue was. Turns out that I was attempting to append a sample buffer with the same CMTime twice due to time rounding reasons in the asset writer. Changing the timescale from 600 to something larger so far seems to have resolved the frame dropping entirely but as you've suggested I'll keep the check about whether the time of the last frame was before or equal to the current one in there just to be sure. In case you still see this, do my explanations make any sense to you? Either way, it was awesome talking to you. :nerd_face: <@U03K0FHV4M8> yay! Im sooo happy you were able to figure it out after our talk!! I want to record the whole screen but I keep getting the microphone active orange dot in the recording. How can I make a recording of a screen using ScreenCaptureKit without the orange dot? We are aware of feedback around the orange dot. ScreenCaptureKit adheres to the same system privacy and security requirements of other capture methods. OK thank you! Can ScreenCaptureKit capture an 'off-screen' window?\u2013 as in, if I wanted to capture some visuals that normally show in a window (1920x1080) in my app, but there's no need to show the user (because they just care about the /resulting/ captured output) & if so: does that seemingly /make sense/ as a use of SCK, or are there more 'direct' methods for 'app-internal' view capture you'd suggest instead? Hi! If you mean offscreen (but rendered), yes SCKit can capture it. If you mean something more like non rendered content like we discussed https://wwdc22.slack.com/archives/C03H77PER5G/p1654817389344279|here , then no If you drag a window partially offscreen, SCK can get the \"off screen\" content in its capture. Is that what you were after? If you want what the other thread discussed, be sure to log a feedback requesting it! Yep, it was! Thanks (I just remembered the one-time permission requirement, which would probably confuse the user for my use-case, as they wouldn\u2019t expect my app to be \u2018recording the screen\u2019 as such, so I\u2019ll do some Googling on existing \u2018app-owned\u2019 window/view capture options, if any) Your own app should have access to its own content without TCC permissions When I use SCContentFilter.init(desktopIndependentWindow:) in ScreenCaptureKit, the popup buttons, contextual menus, sheets that appear on the window are not captured. Is there any way to capture them in ScreenCaptureKit? Previously, I implemented using CGWindowListCopyWindowInfo([.optionIncludingWindow, .optionOnScreenAboveWindow], windowID) and CGImage.init?(windowListFromArrayScreenBounds:, windowArray:, imageOption:) API, but I would like to know how it is possible in ScreenCaptureKit. Additional pop-over items like these will not be shown with init(desktopIndependentWindow:) . If you would like to capture these, you should use an application-based capture. However, please submit a feedback request for this enhancement if you would like to see these inside of init(desktopIndependentWindow:) . How would you prefer the capture handle things like context menus that exceed the bounds of the window? I'd like to use Reality Composer (in the new Xcode) to generate simple USDZ shapes, then use the new Reality Converter to re-texture these shapes (because Reality Composer cannot add custom textures). In Reality Composer, you have to enable USDZ export in Preferences, but when I export a simple box to USDZ, Reality Converter cannot open it (Conversion failed: 1 error, which is unexpected/unknown). Is there a secret way to make this work? (Thanks for leaving questions open \u2014 I'm in Australia. I realise this not quite the right forum for this, but the right forum hasn't left questions open, and it's open around 2-3am my time.) Hi, I tried to reproduce the error as you described. This was the order of steps: \u2022 Create a new project in Reality Composer using the horizontal anchor. \u2022 Turn on \"Enable USDZ export\" in the settings. \u2022 Export the project to USDZ format \u2022 Open a Finder window and double-click the exported USDZ file. If this does not work, would you file a Feedback Assistant? In HLSL Texture2d and Texture2d_array are treated almost the same i.e. texture2d can be used instead of array one. However in metal the type is much more strict. Does it mean that the only way in Metal is to create respected texture view to convert types? The texture view looks to be performance hit :( Texture views allow you to use a single texture backing with more than one format (i.e. RGBA8 and RG16). When a texture is marked with the \u201cview\u201d option this can result in lossless compression being disabled. Can you explain in a little more detail what performance hit you are seeing? I actually don\u2019t need to change format let\u2019s say my shader always accepts only texture2d<float> then to push their texture2darray texture I will need to \u2018slice\u2019 it in slices even if I need always a zero slice it works btw though validator complains and by \u2018slice\u2019 I mean I need to make X slices of texture2d type and then pass that slice the perf hit is the necessity to slice the texture and disable of tex compression Ah, so you wish to use a single slice of a texture2d array \u2013 is it the requirement in MSL of putting Texture2D or Texture2DArray that you are bumping into? yes HLSL allows you to push texture2darray into texture2d in shader https://docs.microsoft.com/en-us/windows/win32/direct3d12/resource-binding-in-hlsl the problem is that engine we\u2019re porting uses mixed texture2d and texture2d_array types all around and in shader there is just texture2d and of course validator coughs may be there is some hidden trick to push tex2d_array slice as tex2d :wink: Hold tight so we can get a proper answer! (or ask more questions :slightly_smiling_face:) sure thing the alternative is to make an array of slices for each tex2d_array I\u2019d really want to avoid this but.. array of slices obtained by textureViewing.. Do you happen to have the print out of the validation layer for your use case so I can make sure I\u2019m looking at the same path? Basically it says wow wow wow you have MTTextureType2D_Array and in shader we have texture2d Sorry indeed I don\u2019t have exact line Ha no that was perfect. So I did confirm, because there is no reinterpretation of the pixels with taking a single slice of an array, MTLTextureUsagePixelFormatView should not be required! oh nice! hope validator won\u2019t bark so you think going \u2018array of slices\u2019 is the only way :wink: the thing is - without validator it works fine! i.e. shader texture2d and tex2d_array as input it shows first slice I was thinking about the idea of having the shader always take an array, too, and just passing in a single entry array, but I don\u2019t know how your shaders might get duplicated or used. and vice versa too! So you only need to use array entry 0 ever? sometimes nope by 90% of time yes Got it. To be clear, you still need to make a Texture View with slice N, but you don\u2019t have to set the MTLTextureUsagePixelFormatView bit, all righty!! hope views are cheap it\u2019s interesting whether I can do some tricks with new arg buffers to offset the GPU address of texture Should be good performance without any interpretation. And as implied above, if there is pixel interpretation then you lose things like lossless compression (which is what the PixelFormatView option is for). You can always find out which textures don't have the lossless compression in your GPU capture, by right clicking on the table header and choosing Texture->Lossless Compression when you're in the Memory view. Normally it shows Label, Insights, Type, Allocated Size, Storage Mode, Purgeable State, CPU Access, and Time Since Last Bound > to offset the GPU address of texture If I understand correctly the intent, offsetting the GPU address of a texture won\u2019t work in most cases and it\u2019s generally not recommended. Creating views is the way to go Yeah :slightly_smiling_face: I understand it\u2019s just fun to have GPU address finally > hope views are cheap Yes, creating a view is extremely cheap: there\u2019s no memory allocation / mapping, no trip to the kernel, etc. Like in GNM Having GPU pointer writing to arbitrary addresses, having Mac borked completely.. Fun! Still waiting btw for major tech breakthrough gents on your side 2022 is not a year for Metal to be able to kill mac in 1 second :wink: and right now I can do it easily Windows manages to survive In theory we have mechanisms to recover from a GPU crash pretty quickly. Also, we have mechanisms to handle some undefined behaviors more gently so we don\u2019t even need to crash at all I\u2019m specifically talking about Apple-Designed SoCs though yeah I understand still I code Mac games all days and I use eGPU btw just because it allows me to survive GPU external crashes but indeed hard crashes are so much better analysed on Apple Socs and yeah I think eGPU with Apple Soc is a good idea :wink: I've just moved from my old and hot Intel iMac, to an M1 Max MacBook Pro, which I'm loving! One snag - our games still run on OpenGL ES, but the iOS Simulator on Apple Silicon seems to crash when executing OpenGL ES commands. Is it Game Over? Do I finally need to move to Metal? Hi David! This is not expected. Please submit a report using the Feedback Assistant app and we\u2019ll take a look. Thank you! First, thanks for all your hard work and cool new APIs! I watched \"Meet distributed actors in Swift\", and I'm curious if there's currently a way to use distributed actors in a peer-to-peer GKMatch? Hi <@U03HMD2BP55>! I believe using distributed actors with GKMatch will have to wait until Swift actors support custom executors. But I encourage you to ask the friendly folks in the Swift lounge for a more authoritative answer! Hey there! Here's a basic one, with some context: I'm a mobile developer working for about a decade in the space, and I have very little experience with high-fidelity graphics and their libraries. I am highly interested in learning C++, especially to interact with the new metal-cpp tools. My question is, would the metal-cpp examples shared here in this lounge be a good starting point into learning enough C++ to be productive with Metal? Or, is there a better starting point that may be a more gradual or helpful introduction to the tools? I don't mind hitting the ground running so to speak, just curious to see what paths are most suggested for this! Thank you! Hi - let's chat a bit. I'm curious to know what languages you already have familiarity with - do you have experience with Objective-C++? Cheers Jared! I unfortunately don\u2019t have experience with Obj-C++, but I do with straight Obj-C from the pre-Swift years, heh. I also have a glancing familiarity with -C-like languages, but that\u2019s mostly syntactic pattern recognition over function (\u201coh, that thing is accessing a pointer, that\u2019s updating a field..\u201c) Ok, interesting. Well, as I'm sure you've seen there's a ton of resource for learning C++. If you're used to Obj-C, I think you'll understand the basic concepts pretty quickly. The reason I ask is because Metal is natively implemented as an Objective-C API. What's nice about Objective-C++ is that inter-op with C++ is super easy. Basically, you just rename your .m files to .mm files and you're using Objective-C++. You can now include C++ headers and start playing immediately. Now, if you really want to use Metal-cpp I don't want to discourage you at all. It's great to see developers interested. I think it's worth noting, though, that if you want to go the 'pure' C++ route, you'll likely need to end up wrapping some of our other platform libraries which are exposed as Obj-C or Swift. Ahhhh\u2026 I\u2019m seeing the boundary lines a little more clearly now. This makes a lot of sense. It seems like the learning path would be C++ -> [metal-cpp + interop wrapping], which isn\u2019t too bad at all. I have a really, really limited scope that I\u2019d like to try out first. Like, really limited: just rendering a plain ol\u2019 plane with a bunch of backed textures that act like a text font, and using the geometry bounds and coordinates to make the final mesh of an object animated. Backing up a bit, with more context, I currently have a working prototype in straight SceneKit land where my flow is essentially render a bunch of sibling nodes, flatten the hierarchy into a single mesh, and then keep the old nodes around when I want to do things like animate highlights and movement. My hope is that by picking up even some core metal-cpp, I\u2019ll be able to reach into the rendering fundamentals to get that basic use case down, with the bonus being more power under the hood for more complicated use cases down the road :wink: You don't necessarily need to adopt metal-cpp. You could continue interacting with Metal via its native Objective-C interface, but use C++ to integrate with other libraries from the same Objective-C++ source file. Here\u2019s a quick shot: the one of the left is in \u2018flatten\u2019 mode, with a single node that was flattened from a few hundred. The one on the right is in \u2018glyph\u2019 mode, where each node is rendered separately. <@U03J3UW6LSD> That\u2019s a really good point - I could use the Obj-C interface! Since that\u2019s the core, it almost seems like it makes sense to start with that first since the cpp lib is a tight wrapper around it. I think that would also make learning Metal a little easier. We do have some metal-cpp examples, but there are years of ObjC Metal resources out there. Maybe start with ObjC Metal, then move to Metal-cpp if you want that 'all one language' feeling. I think I\u2019ll do just that. I really appreciate the words of wisdom here - I was all ready to start down a quite intensive path, haha! If I may while I have an eye or two from ya, and just off the top of your head, no pressure: what sorts of primitives do you think I should be looking into to replicate this kind of behavior like the one above? In terms of triangles, meshes, texture coordinates, animating mesh coordinates, that kinda thing. I\u2019m biased because I\u2019ve spent years working with Apple\u2019s text rendering engine, but: I recommend using an actual text engine like Core Text or TextKit 2 to render to a texture. Lots of simplifying assumptions that hold true in English don't hold true across all languages. Another really interesting option is http://sluglibrary.com|Slug , which does correct Unicode rendering entirely on the GPU. But I don\u2019t know if there\u2019s a trial option. I would recommend you to take a look at the LearnMetalCPP samples( https://developer.apple.com/metal/LearnMetalCPP.zip ). It\u2019s written in metal-cpp but it provides a series of incremental graphics samples which can help you learn graphics and Metal from scratch, e.g draw a triangle, cube, texture etc. > Core Text or TextKit 2 to render to a texture <@U03J3UW6LSD> This 1_000x over! I would absolutely love to do something like this.. at the moment, I quite literally just create a CATextLayer with a single string character and render it out into a bitmap :sweat_smile: I\u2019ll start taking a look at those two kits to see what I can break, haha. I\u2019d need to render the text, and then understand the placement of those glyphs as they\u2019re rendered to make sure I can move them individually in space. <@U03HJ4DKQRY> You got it - this was the original reason I had the thought to try it, in that it seemed like it had that gradual build up of domain knowledge, through the lens of a different language. This might be a weekend project, haha. Question re Core Media IO Extension. How the host app can communicate with the extension? Can host/extension exchange IOSurface for example? What kind of information are you hoping to communicate? There might be a better lounge for this question. If you're trying to send video frames, the supported way to do that is CMIOExtensionStream . Just want to say HUUUGE thank you to all XCode GPU dev.team. You guys rock. I was testing first metal gpu capture and since that time it's miraculous improvements. Still ;) Pixel history - please make it happen And if this is possible can Apple team work at least a bit with RenderDoc author so we can replay DX11/12 traces on Mac? It's a very big task/very complex but it's super big pain to need a Windows with RenderDoc to check stuff. If only RenderDoc can run on Mac. Of course I'd be happy if it can replay Metal as well, but it's a different story! Hi - thanks for feedback. We'll definitely chat with the tools team about your ideas. I'm porting a game that happens to work when in a Windows virtual machine, so on my Mac Pro I keep Direct3D tooling/tracing in a VM running side by side with the Metal debugger. Really missing that workflow on Apple Silicon, but I also know there are no easy answers there. :slightly_frowning_face: Considering just Boot Camping my Mac Pro and keeping it as a remote Windows debug box when the time comes. Is there any documentation for the new Metal Pipeline Script JSON format? Mentioned in the \"Target and optimize GPU binaries with Metal 3\" session. Official documentation is coming soon! In the meantime here is a quick overview of creating a pre-compiled MTLBinaryArchive: First create a MTLBinaryArchive using the existing API. After that is done you can extract the Metal Pipeline JSON with: $ metal-source -flatbuffers=json theBinaryArchive.metallib -o /tmp/descriptors.mtlp-json Then you can use the mtlp-json file with metal to generate a MTLBinaryArchive offline that will work with all GPUs: $ metal shaders.metal -N descriptors.mtlp-json -o archive.metallib Here is a link to the talk for others who may be interested in more: https://developer.apple.com/videos/play/wwdc2022/10102/ I tried to extract the Metal Pipeline JSON from a harvested archive, but got metal-source: error: unsupported binary format I harvested the archive using (swift)... let lib = device.makeDefaultLibrary()! let desc = MTLRenderPipelineDescriptor() desc.vertexFunction = lib.makeFunction(name: \"vert_main\") desc.fragmentFunction = lib.makeFunction(name: \"frag_main\") desc.colorAttachments[0]?.pixelFormat = .bgra8Unorm let archdesc = MTLBinaryArchiveDescriptor() let archive = try device.makeBinaryArchive(descriptor: archdesc) try archive.addRenderPipelineFunctions(descriptor: desc) try archive.serialize(to: NSURL.fileURL(withPath: \"/Users/pwong/Downloads/x-game.metallib\")) Not sure if this is suppose to work yet on MacOS 13 beta/XCode 14 beta... also, I see that the workflow doesn't work at all without having a Metal pipeline JSON (applegpu-nt: note: [AGX] Plugin interface not implemented: AIRNTEmitExecutableImage error) Is there a way to opt out of QoS to avoid priority decay on the render thread if we are writing our application in Swift? The techniques we recommend in our tech talk on https://developer.apple.com/videos/play/tech-talks/110147/|tuning CPU job scheduling for Apple silicon games are also applicable to games written in Swift. The pthread APIs might not be the nicest to work with from Swift, but it should be possible to use them. Rendering the Mandelbrot set is an \"embarrassingly parallel\" :smile: task, so it seems perfect for the GPU. I wrote a Metal fragment shader to do this, but you can't zoom into the set too far because Metal only supports float , not double . I am a novice GPU programmer, so bear with me, but is there any way to do increased precision math on the GPU with Metal? I understand that GPUs are generally much slower with double-precision, if they even support it, but it seems like it would be useful for scientific computing at least. Thank you! Hi, thanks for your question. Since Metal doesn't support the double data type, you would have to use (or make) an arbitrary precision floating point library. Another option is to use 64-bit integers using fixed-point approaches. We would also encourage you to send a report with your use case using Feedback Assistant so the team can consider it as a future enhancement. Thanks for those suggestions, I will explore them! Also, I want to add to my answer that you could also experiment with the \"fast-math\" option that is normally turned on by default. If you disable it, you may get a little better precision in your results when calling sin, cos, sqrt, etc. It's not the same thing as a 64-bit floating point type of course, but handy if you're debugging a precision issue. Thanks <@U03J7T89SQG> , I didn't know about that option! Is there a way to reference templated compute functions from a compute pipeline descriptor such that the templated type inherits the bound texture's pixel format? Hi, this sounds like an interesting question. For example sake (using non-real code), I think you are trying to ask if you can do something like template &lt;typename T&gt; void computeShader(...) { ... } in your shader code and then reference this function using something like id &lt;MTLFunction&gt; function = [defaultLibrary newFunctionWithName:@\"computeShader&lt;MyType&gt;\"]; Is that what you are asking? You can use the host_name attribute and then define your template specializations. In the https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf|Metal Shading Language Specification , section 5.1.10 and 5.1.11 show how you can change the name that Metal will use to reference the function name. For example, you could try something like: template &lt;typename MyType&gt; kernel void computeShader(device MyType* output, constant MyType &amp;argument) { *output = argument; } template [[host_name(\"computeShader_int\")]] kernel void computeShader&lt;int&gt;(device int* output, constant int &amp;argument); template [[host_name(\"computeShader_float\")]] kernel void computeShader&lt;float&gt;(device float* output, constant float &amp;argument); And then reference the functions in your app with: id &lt;MTLFunction&gt; function1 = [defaultLibrary newFunctionWithName:@\"computeShader_int\"]; id &lt;MTLFunction&gt; function2 = [defaultLibrary newFunctionWithName:@\"computeShader_float\"]; <@U03J7T89SQG> Yes, your example illustrates what I was looking for. So sounds like I'd still need to create a compute pipeline state per pixel format? For context, I was hoping to build my compute pipeline state once up front and allow a consumer of an MPS-like framework provide textures with various pixel formats. Yes, you would need to create a pipeline state for each one. Something like: template &lt;typename T&gt; void computeShader(texture2d&lt;T&gt; myTexture [[texture(0)]]) { ... } where T is inferred from texture(0) And the reason for that is so the compiler can create an optimized version for each permutation you have. Yeah, makes sense. Would be cool if a compute pipeline state could be build with a set of template parameter permutations and then it selects the correct one based on bindings In fact, the term \"shader permutations\" refers to this specialization whether you're using templates, or function constants. If the pipeline is built too late though, it could trigger the compiler and that can result in performance problems. So, if you had a choice to compile your shaders before your app begins, versus on-demand, it's preferable to do it before. I see, so without template specialization in the shader source, the pipeline would have to compile versions for each of the permutations set on the pipeline state For the use case I outlined above, would you recommend lazily constructing the necessary pipeline state objects or building all possible versions once up front? If you know that you will need to use all possible ones, then you should do it once up front. Consider a real-time graphics application with many kinds of materials. If you were to compile the pipeline state objects when they were needed, then you may get stutters while you wait for them to compile. Great, thanks for your responses, very informative! Is generating the new JSON Pipelines Script suppose to work on MacOS 13 Beta and XCode 14 Beta? Mentioned in the \"Target and optimize GPU binaries with Metal 3\" session. Trying the sample terminal commands in the session, on a harvested metallib (super simply draw a point, with a vertex/fragment shader), errored out with metal-source: error: unsupported binary format . Thanks for the feedback! The team is looking into this issue. Is there a possibility to render 3D objects with SceneKit on the new Map? e.g. a Car driving down the road... Unfortunately this is not possible to do, but please file Feedback on this request. It seems like a great idea with lots of possibilities! The new object/mesh shader pipelines are pure genius. When will we get a chance to see an updated Metal Shading Language Spec so we can dig into the details? The spec was updated when we announced Metal 3! You can find it in the usual place https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf Yay!!! Do you have any interesting plans <@U03JK18HNR2>? We had some interesting discussions about terrain rendering earlier this week (you should be able to find it if you scroll all the way back up to the Wednesday session :stuck_out_tongue: ) I will definitely check that out! I don't have any specific ideas just yet. Still digesting everything, but I was properly blown away by the potential. This + MetalFX is an absolute game changer. In the new object and mesh shader stage, it looks like culling techniques can be refined greatly. But I didn\u2019t quite grasp the meshlet concept discussed on the presentation. Is there some external theory on this you could point us to for more on the concept? Or is this something that we will be able to do in the new pipeline stage? (Ie carve up a mesh into smaller chunks on the fly) Good question! The term refers to the general idea of splitting up meshes into smaller pieces in order to do efficient culling. This only really becomes a useful practice when culling is done on GPU (otherwise the CPU overhead would be prohibitive due to increased complexity). We don't supply an out-of-the-box solution for carving up mesh resources, and we leave that up to the developer. Personally I had pretty good (and quick) results using K-means clustering, but there are existing mesh tools out there that can help you prepare your meshlets. :) meshlet culling is a good search term to find out more from online resources :slightly_smiling_face: <@U03JK18HNR2> meshoptimizer has a set of functions to build meshlets: https://github.com/zeux/meshoptimizer#mesh-shading :raised_hands: One key observation is that meshlets have fixed upper bound on number of vertices and number of primitives, so they can be efficiently mapped to threadgroups of the same size(s); so chopping up the (potentially much larger) input meshes to fit into (multiple) meshlets has to be done up front (ideally at build-time of the assets) The library author also recorded a series of live streams on the topic, including GPU meshlet and view frustum culling: https://www.youtube.com/watch?v=KckRq7Rm3Mw Ha I was about to mention SG2015 :smile: Lots of interesting things, including using meshlets and GPU driven rendering for efficient shadows etc. One of my favorite talks! Working through implementations at the moment, which is why I'm very happy mesh shaders arrived! As for the second question, whether you can carve up meshes inside the pipeline, I don't see a reason why not. Mesh shader pipeline is a good fit for techniques that can be expressed as one or more mesh shader threadgroups (meaning, each such threadgroup maps 1:1 to a fixed size output mesh you declare in the shader), and you have the object shader stage to dynamically decide how many of those output meshes you will need. There is no particular input formats or intermediate data formats that the pipeline is tied to, so if your input is meshes in device memory, that could work. <@U03HJ3X8V43> <@U03JLQ9J0LB> <@U03HJ54DBT4> great resources! Thank you :) Why apple choose USDZ instead of GLTF? While USDz and GLTF are both excellent content delivery formats, USDz has the advantage in that it is a direct implementation of the USD format. A USDz file is a zip archive which contains a USD or USDc file along with all of its referenced resources such as textures, animations and shaders. This makes it very convenient for using with DCC packages that natively support USD, since there is no conversion or transcoding required. I think glTF is a great format, but it is not nearly as flexible as usd formats. USD is also widely supported on all sorts of Apple frameworks so you can save a lot of dev time using that as a spec for your file format. Model I/O, RealityKit etc. In my experience, the biggest pain point for USD lies in the tools the artists want to use. A lot of Blender enthusiasts for example are still lacking a fully featured USD export with skinned animation. The best you can do without a custom exporter is send the file to glTF and then use RealityConverter to get it to USD. But that isn't ideal for all use cases. Also another great thing about usd is the ability to convert to usda to get something human readable. That's saved me a lot of time pinpointing weird issues with geometry. Thanks guys! Is there sample code which uses a MTLSharedEvent ? I'm looking to test one of my compute pipelines which uses a shared event against a sample pipeline which uses the shared event correctly as I'm running into unexpected behavior We have sample code here: https://developer.apple.com/documentation/metal/memory_heaps/implementing_a_multistage_image_filter_using_heaps_and_events?language=objc We also mention shared event specifically here: https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_between_a_gpu_and_the_cpu?language=objc https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_across_multiple_devices_or_processes?language=objc If I render a scene to a MTLTexture with large resolution can I split scene to a small pieces and async render them or I need to render a whole scene al once? Very interesting question :slightly_smiling_face: There are a lot of interesting things you can do to do this somewhat efficiently The most basic way would just be to render the scene in multiple parts, each to a different render target (or viewport within a target). This would probably require you to first properly sort geometry to different draws One other way to do this, would be to use Vertex Amplification and Layered Rendering to emit vertices to multiple render targets at once (for all the vertices that are overlapping multiple viewports) Unless your render target is bigger than the allowed size though, you should just render everything at once where possible :slightly_smiling_face: The more you render at once, the more you gain efficiency. I'm curious, what is the reason you would need such a huge resolution? :slightly_smiling_face: (I believe our maximum texture size is 16x16k) The 16k is perfect to my case :grinning:. Last year I face the problem with render to a texture. I don\u2019t remember correct resolution, but if I set the resolution to 4k the test app is crashing. Other resolution (smaller or bigger) work perfect. Is there any way to get current max texture resolution directly in the app in runtime? <@U03HJ54DBT4> I work on sort of Standalone Render app and I need to figure out limits that I should set for export resolution, that the user cannot try to render image with bigger resolution. I don't believe so, but you can look at this table: https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf It appears that anything newer than Apple2 will support 16K (iPhone7+) <@U03HJ54DBT4> Thank you so much! How would you break down a scene into acceleration structures? I'm working on a small raytracing side project and I'm a bit stuck determining how a scene should be broken down. I'm using Model I/O to load some assets and then I'm planning to represent the scene with an instance acceleration structure, with each MDLMesh mapping to a primitive acceleration structure. Does this sound like an appropriate breakdown? Thanks for the question. Yes, this does sound like a proper mapping. You could fill out a MTLAccelerationStructureTriangleGeometryDescriptor for each submesh in an MDLMesh and then put those into geometryDescriptors in a MTLPrimitiveAccelerationStructureDescriptor Yeah that's pretty close to what I was planning actually then you would use the transformations for each mesh and put together a transformation buffer for MTLInstanceAccelerationStructureDescriptor Hello, are there any plans of supporting page faulting in Metal for the future? The purpose is to have an 1:1 CPU:GPU VA mapping, with even file mmap() being accessible from the GPU side, to be able to layer more high-level programming models on top. Hi Mohamed, we do not comment on the future of the API, but I would like to direct your attention to two features that may be sufficiently flexible for your use-case, Metal Fast Resource Loading to load data from files and Metal Sparse Textures to manage partial residency with page map and unmap support. We are also interested in learning more about your use-case, so please feel free to expand on it :slightly_smiling_face: Thanks The use case is porting HPC code from other platforms which rely on heterogenous memory management more extensively today, including the regular host heaps being accessible from the GPU. (eg. C++ standard parallelism on GPUs as deployed by some vendors) Wow that is indeed an interesting use-case, and to make it work with Metal 3 you would need some sort of shim to hide the memory management details. Please feel free to file a feedback request for us to track this. There's support for file mmap being made accessible to the GPU using https://developer.apple.com/documentation/metal/mtldevice/1433382-newbufferwithbytesnocopy?language=objc ; however it causes the VM region to be paged in entirely (which may not be what you want to achieve here, depending on the size of the file) > depending on the size of the file yeah pinned memory is useful - but is not what I'm after in this scenario :slightly_smiling_face: FB10133608 Will GPU Shader debugging for Mesh Shaders be ready for the Xcode 14 release? I tried debugging the sample code \"Adjusting the level of detail using Metal mesh shaders\" ( https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)|https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders) and 2 observations: 1. There didn't seem to be a way to select object or mesh shader to debug. 2. Attempting to debug the fragment shader, hung XCode. Many new and exciting features are always being planned to enable a better development experience for Metal. In the meantime if you encounter any issues we would highly encourage you to use the Feedback Assistant. Additionally, feel free to provide the case number here if you have filed feedback already. Thank you! Hey - Really love all the innovative Metal 3 features! I may be a little early asking, but I was curious about what\u2019s happening with MTLArgumentEncoders. Are they being deprecated across the board in Metal 3 such that we can take advantage of the streamlined API for most of our Metal 2 code? It also looks like some, but not all, of the methods are deprecated. For example, the versions that can set multiple buffers at once in a range. Is that correct? Hi Louis, thanks and glad you are enjoying Metal 3! MTLArgumentEncoders are indeed deprecated, however if you are targeting Tier 1 devices on Metal 2 you will still have to use them and disable the deprecation warnings, alternatively you could target only Tier 2 devices by performing a Metal 3 feature check. We'd also really appreciate it if you could file feedback for this use-case. Are there any cases we'd need to interact with MTLGPUHandle and MTLResourceID properties? I see them added in a bunch of places, but unsure where or by whom they'd be consumed. MTLGPUHandle is deprecated and we use MTLResourceID to access the underlying resource index of a resource. It's similar to the gpuAddress of a buffer. We use them in the bindless workflow for argument buffers. Instead of using the argument encoder's set[ResourceType] functionality, we can simply cast the contents of the argument buffer to the struct type, and map the resources directly. Please refer to https://developer.apple.com/wwdc22/10101 for more usage examples. Is there a way to have the equivalent of work_group_barrier (CLK_GLOBAL_MEM_FENCE, memory_scope_device) in Metal? (all threads on a device barriers, not threadgroup local, also a part of the Vulkan memory model) Hello, To barrier all threads within a threadgroup (equivalent to workgroup) properly ordering memory operations to device memory use threadgroup_barrier(mem_device) . If you want to barrier against all threads spawned, the proper way to achieve it in Metal is to split your operation into 2 sequentially dependent compute dispatches. Thanks. ok... that'll pose some intriguing work for me on the compiler side to make that work out :slightly_smiling_face: I'd use computeCommandEncoderWithDispatchType with the sequential dispatch type in that scenario right? are you talking about CLK_GLOBAL_MEM_FENCE or a scenario where one implements an actual across-grid synchronization primitive? there's indeed a cross-grid synchronisation primitive involved, and memory_scope_device is used to guarantee that My interpretation of the Vulkan spec for work_group_barrier is that the actual synchronization happens at workgroup boundary \u2014 i.e.: only between threads in the same workgroup (or threadgroup in Metal parlance). The memory fence behavior is kinda orthogonal and, in the case of CLK_GLOBAL_MEM_FENCE you are making sure that before any thread in a workgroup/threadgroup can proceed all in-flight device memory operations are visibile to the thread itself. That does not mean that a given thread will wait until all the other threads in the grid will reach the barrier to continue though See the SPV_KHR_vulkan_memory_model extension - more specifically the VulkanMemoryModelDeviceScopeKHR capability compute is my primary target instead of Vulkan though :slightly_smiling_face: Sorry, not really a Vulkan expert. In Metal unfortunately we do not currently support a shader-controller mechanism to barrier across all threads in a grid. In order to do that you would beed to split your operation into 2 sequential compute dispatches as mentioned by Kelvin. is it a known/already considered issue? do you mind if I file a radar about it? I\u2019m not aware of this particular feature request, but that does not mean it has not been discussed so far. If Vulkan offers you this functionality and you think it would be useful to have it in Metal as well, you can definitely file a radar for it, it will be screened by the GPU SW team. Thanks! FB10166471 Thanks! I've never used MTLRenderPipelineReflection before. What sort of use cases is it meant for? Could it be used to replace shared source/shader constants defining binding indices? MTLRenderPipelineReflection enables your application to reflect on shader bindings and allows you to perform data-driven tasks. For an example, you have a game engine and can pull out binding information to determine what resource needs bound where. This is super cool. I've historical just had a bunch of macros defining the binding indices. Will explore this approach. Thanks! It's also very handy if you have a lot of shaders you are constantly editing. By using reflection, your engine can intelligently bind the right data to the right slot. Less worry, less bugs! Sounds so nice! Heya! Quick shared post from the study-hall section, thank you much! I think I have a lot to learn with Metal now. I\u2019ll likely start with basic triangle patterns and moving those around with a compute kernel (which sounds so darn fun), and then start \u2018upgrading\u2019 to the mesh shader feature. Now that I\u2019ve said that, I wonder if it\u2019s a better idea to maybe start with mesh shaders? I.e., starting from the new-tool instead of building up to it? The context being I'm brand new to Metal, and am looking to do some fairly simple direct interactions with geometry positions, and eventually texture manipulations for things like highlighting areas. Both would work! While mesh shaders would be more efficient in most cases, if you are just starting out with compute I would suggest starting with a basic Compute->Vertex->Fragment pipeline, where your compute shader simply writes out the vertex buffer (and optionally index buffer). You can read back the written vertices and inspect the data. I feel that would be a great way to slowly build up to mesh shaders :slightly_smiling_face: If you have handle on it, you can further optimize your work with mesh shaders (this requires a bit more planning on the side of thread group sizes, mesh sizes etc.) This is excellent, thank you so much for the info. My original concern of going down an inconvenient path is now assuaged, haha. It seems like I\u2019ll get a lot of bang-for-my-buck by writing up my kernel, getting to know the API, and then \u201cdiscovering\u201d where the new tools fit in to how I\u2019ve tried to build things. At that point, even if I have built something that doesn\u2019t quite fit the pattern, I have that core understanding built up to diagnose and refactor as needed. It\u2019ll be quite interesting to start transitioning everything from basic SceneKit nodes and geometries to custom Metal ones\u2026 I feel like I may finally have the right path to get to the right tools for performance. If you happen to have any kinda favorite notes, pages, articles, or docs on some potential equivalences or built in support for SceneKit -> Metal or vice versa, I would really appreciate that as well! Apologies I somehow didn't see this pop up! I'm not super familiar with SceneKit, but we do have interop between them. I would definitely suggest just playing around with Metal to get the hang of it. We had a similar question on Wednesday, where someone wanted to optimize their SceneKit scene by manually constructing complex meshes from scratch instead of relying on huge node graphs. I'll try to find it for you Haha, no worries at all - quite literally thousands of fellow Apple-heads looking to chat, I\u2019m just happy to be a part! :smiley: That. Is. AWESOME! It sounds right on track with that I\u2019m talking about. I think this is going to be a weekend of Metal experimentation.. the more I ask about it from my current place, the more I\u2019m itching to see what I can do with it. Be sure to drop by the forums for help/questions :slightly_smiling_face: Totally. In the past, I\u2019ve only posted there when I\u2019m in \u201cdire straights\u201d so to speak, haha. No reason I can\u2019t ask simpler questions along the way too :smile: <@U03HJ54DBT4> Heya, one more question if you don\u2019t mind :sweat_smile: Sorry if this was asked 1_000_000 times already, but is there a significant difference in using Swift for interacting with Metal as opposed to Objective-C ? I see lots of write ups that use Swift to interact with it, including shader definitions. I would suggest using Swift if possible, since that interfaces much nices with stuff like SwiftUI and other technologies. :slightly_smiling_face: Wooooooooo! No square brackets for ME this weekend! :confetti_ball: :man_dancing: :dancer: :dancers: :confetti_ball: I'm trying to track down a rare Metal crash in our app. We're seeing this in two completely independent subsystems when deallocating Metal buffers, invariably vertex or index buffers. Hello David, could you please provide some further details about the crash? Is it a CPU crash pointing at the Metal library or is it a GPU crash? Thanks Posted in the study hall, but the common part of the stack trace is: Thread 0 Crashed: 0 libobjc.A.dylib 0x000000019c8b7470 objc_release + 16 1 IOGPU 0x00000001cf8059d8 -[IOGPUMetalResource dealloc] + 204 2 IOGPU 0x00000001cf80672c -[IOGPUMetalBuffer dealloc] + 288 3 AGXMetalA10 0x00000001df28ab28 0x1df26b000 + 129832 It's rare, so I don't think it's an over-release on our part. Thanks! Do you have the whole stack trace by chance? Also, did you try to turn on zombie object detection in Xcode or Instruments so far? These reports are coming from the production app and we haven't been able to reproduce it. Sanitizing stack traces, standby It might still be useful to follow the instructions in this page: https://developer.apple.com/documentation/xcode/investigating-crashes-for-zombie-objects , specifically the linked page https://help.apple.com/instruments/mac/current/#/dev612e6956 and I mean, on your development environment Right, like I said we can't repro it, but running with zombies enabled is a good idea. Based on the signature of your stack track this is quite likely a zombie object. A full stack trace would be useful to understand where in the driver we issue that release Here ya go, had to anonymize it: Thread 0 Crashed: 0 libobjc.A.dylib 0x000000019c8b7470 objc_release + 16 1 IOGPU 0x00000001cf8059d8 -[IOGPUMetalResource dealloc] + 204 2 IOGPU 0x00000001cf80672c -[IOGPUMetalBuffer dealloc] + 288 3 AGXMetalA10 0x00000001df28ab28 0x1df26b000 + 129832 4 MyApp 0x00000001059f2234 myapp::profile::MetalVertexBuffer::~MetalVertexBuffer() (<http://MetalRenderer.mm:1516|MetalRenderer.mm:1516>) 5 MyApp 0x00000001059f225c myapp::profile::MetalVertexBuffer::~MetalVertexBuffer() (<http://MetalRenderer.mm:1515|MetalRenderer.mm:1515>) 6 MyApp 0x00000001059e8afc myapp::profile::Shape::~Shape() (Shape.cpp:36) 7 MyApp 0x0000000105803444 std::__1::enable_if&lt;(__is_cpp17_forward_iterator&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*&gt;::value) &amp;&amp; (is_constructible&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;, std::__1::iterator_traits&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*&gt;::reference&gt;::value), void&gt;::type std::__1::vector&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt; &gt; &gt;::assign&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*&gt;(std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*, std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*) (memory:2506) 8 MyApp 0x00000001059d9f18 myapp::profile::TerrainLayer::updateNode(myapp::profile::SceneManager*) (TerrainLayer.cpp:46) 9 MyApp 0x00000001059dd494 myapp::profile::SceneNode::updateNode(myapp::profile::SceneManager*) (SceneNode.cpp:47) 10 MyApp 0x00000001059dc538 myapp::profile::ModelController::update(double) (ModelController.cpp:55) 11 MyApp 0x00000001059f6a38 -[TTMProfileModelController(CPP) update:] (<http://TTMProfileModelController.mm:51|TTMProfileModelController.mm:51>) 12 MyApp 0x0000000105a00648 -[TTMProfileView renderLoop] (<http://TTMProfileView.mm:335|TTMProfileView.mm:335>) 13 QuartzCore 0x0000000188fe8378 CA::Display::DisplayLink::dispatch_items(unsigned long long, unsigned long long, unsigned long long) + 756 14 QuartzCore 0x0000000188feeb44 display_timer_callback(__CFMachPort*, void*, long, void*) + 364 15 CoreFoundation 0x00000001854c869c __CFMachPortPerform + 168 16 CoreFoundation 0x00000001855084ec __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__ + 52 17 CoreFoundation 0x000000018550badc __CFRunLoopDoSource1 + 584 18 CoreFoundation 0x00000001854c90f0 __CFRunLoopRun + 2372 19 CoreFoundation 0x00000001854dbe1c CFRunLoopRunSpecific + 568 20 GraphicsServices 0x00000001a576d9a0 GSEventRunModal + 156 21 UIKitCore 0x0000000187d0fb90 -[UIApplication _run] + 1076 22 UIKitCore 0x0000000187aa516c UIApplicationMain + 328 23 MyApp 0x0000000106f36e1c main (main.m:127) 24 ??? 0x0000000109580250 0x0 + 0 And another one (different system altogether) Thread 0 Crashed: 0 libobjc.A.dylib 0x00000001dc079ef8 objc_msgSend + 56 1 IOGPU 0x00000002242b87a0 -[IOGPUMetalResource dealloc] + 208 2 IOGPU 0x00000002242b9558 -[IOGPUMetalBuffer dealloc] + 316 3 AGXMetalA14 0x0000000236149964 0x235f5c000 + 2021732 4 MyApp 0x00000001055b21ac std::__1::__any_imp::_SmallHandler&lt;myapp::render::Buffer::Metal&gt;::__handle(std::__1::__any_imp::_Action, std::__1::any const*, std::__1::any*, std::type_info const*, void const*) (any:0) 5 MyApp 0x000000010559f808 std::__1::any::reset() (any:321) 6 MyApp 0x000000010559f66c myapp::render::Buffer::~Buffer() (Buffer.cpp:14) 7 MyApp 0x00000001055a2768 std::__1::unique_ptr&lt;std::__1::__hash_node&lt;std::__1::__hash_value_type&lt;myapp::Name, myapp::render::VertexBuffer&gt;, void*&gt;, std::__1::__hash_node_destructor&lt;std::__1::allocator&lt;std::__1::__hash_node&lt;std::__1::__hash_value_type&lt;myapp::Name, myapp::render::VertexBuffer&gt;, void*&gt; &gt; &gt; &gt;::~unique_ptr() (Buffer.h:188) 8 MyApp 0x00000001055a1924 myapp::render::Cache&lt;myapp::render::VertexBuffer&gt;::RemoveBuffer(myapp::Name const&amp;) (__hash_table:2498) 9 MyApp 0x0000000105668870 TTMKTileMetadata::deleteGLBuffers() (TTMKTileMetadata.cpp:178) 10 MyApp 0x00000001056d8248 -[TTMKMetalView doInContext:] (<http://TTMKMetalView.mm:46|TTMKMetalView.mm:46>) 11 MyApp 0x000000010569c694 -[TTMKMapView doInBackgroundContextThread:doImmediately:] (<http://TTMKMapView.mm:2958|TTMKMapView.mm:2958>) 12 MyApp 0x000000010568ba8c -[TTMKMapContainerDelegate doInBackgroundContextThread:doImmediately:] (<http://TTMKMapContainerDelegate.mm:20|TTMKMapContainerDelegate.mm:20>) 13 MyApp 0x00000001056176c0 TTMKMapContainer::drainTileCache(bool) (TTMKMapContainer.cpp:3393) 14 MyApp 0x0000000105613ca4 TTMKMapContainer::update(double, double) (TTMKMapContainer.cpp:1442) 15 MyApp 0x000000010569b6a4 -[TTMKMapView glDrawOversample] (<http://TTMKMapView.mm:2696|TTMKMapView.mm:2696>) 16 MyApp 0x000000010569b964 -[TTMKMapView drawWithDevice:toDrawable:pass:] (<http://TTMKMapView.mm:2747|TTMKMapView.mm:2747>) 17 MyApp 0x00000001056d85a0 -[TTMKMetalView drawRect:] (<http://TTMKMetalView.mm:144|TTMKMetalView.mm:144>) 18 MetalKit 0x00000002009a36d4 -[MTKView draw] + 140 19 MetalKit 0x00000002009b1fb8 -[MTKViewDisplayLinkTarget draw] + 36 20 QuartzCore 0x00000001c70d2324 CA::Display::DisplayLink::dispatch_items(unsigned long long, unsigned long long, unsigned long long) + 744 21 QuartzCore 0x00000001c722d174 CA::Display::DisplayLink::dispatch_deferred_display_links() + 344 22 UIKitCore 0x00000001c665f254 __setupUpdateSequence_block_invoke + 212 23 UIKitCore 0x00000001c5fd9084 _UIUpdateSequenceRun + 80 24 UIKitCore 0x00000001c665ecb0 schedulerStepScheduledMainSection + 140 25 UIKitCore 0x00000001c665e478 runloopSourceCallback + 88 26 CoreFoundation 0x00000001c344bf04 __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 24 27 CoreFoundation 0x00000001c345cc90 __CFRunLoopDoSource0 + 204 28 CoreFoundation 0x00000001c3396184 __CFRunLoopDoSources0 + 264 29 CoreFoundation 0x00000001c339bb4c __CFRunLoopRun + 824 30 CoreFoundation 0x00000001c33af6b8 CFRunLoopRunSpecific + 596 31 GraphicsServices 0x00000001df449374 GSEventRunModal + 160 32 UIKitCore 0x00000001c5d14e88 -[UIApplication _run] + 1096 33 UIKitCore 0x00000001c5a965ec UIApplicationMain + 360 34 MyApp 0x0000000106d495fc main (main.m:127) 35 ??? 0x0000000109679ce4 0x0 + 0 Note: deleteGLBuffers() is part of the legacy code hierarchy. Rendering path is all Metal. Thanks, this definitely looks like a zombie object It seems like your app is holding a reference to a MTLBuffer object that has been already released Yeah, that's what I thought too, but the code paths here manage Metal buffers independently and it seems unlikely totally different implementations would make the same subtle mistake. Static analysis turned up nothing suspicious. Do you use command buffers with unretained references or command buffers with retained references? Let me look up the creation code Also, do you have a common pool of Metal resources that lives across sub-systems? Not vertex or index buffers, at least. Creation happens in Obj-C++ code and looks like: auto em = std::any_cast &lt;Environment::Metal&gt; ( &amp;_environment-&gt;_metal ); id &lt;MTLDevice&gt; device = em-&gt;_device; auto mtl = std::any_cast &lt;Metal&gt; ( &amp;_metal ); mtl-&gt;_buffer = [device newBufferWithLength: sizeInBytes options: MTLResourceCPUCacheModeDefaultCache]; That should be retained, if I'm not mistatken. Sorry, what I meant is: how do you create MTLCommandBuffer objects? Do you by chance use MTLCommandQueue:commandBufferWithUnretainedReferences or MTLCommandQueue:commandBufferWithDescriptor setting commandBufferWithUnretainedReferences=YES ? No, I don't think so. Device and queue are created here: _metal.emplace &lt;Metal&gt; (); auto mtl = std::any_cast &lt;Metal&gt; ( &amp;_metal ); bool first = ( mtl-&gt;_device == nullptr ); id &lt;MTLDevice&gt; mtlDevice = *std::any_cast &lt;id &lt;MTLDevice&gt;&gt; ( &amp;device ); mtl-&gt;_device = mtlDevice; mtl-&gt;_queue = [mtlDevice newCommandQueue]; if ( first ) { initialize(); initializeMetalRendering(); } Then the command buffer is created here: void Environment::start() { auto mtl = std::any_cast &lt;Metal&gt; ( &amp;_metal ); mtl-&gt;_buffer = [mtl-&gt;_queue commandBuffer]; OK, that\u2019s a command buffer with retained resources then Based on the info I have I still think your best bet is to use the zombie detector Ok, I'll give that a shot, but we haven't been able to repro it ourselves. If we referred to a dealloc'd object we should at least see a crash locally I would think. Do you possibly have MetalVertexBuffer objects that do not contain an actual MTLBuffer object? Maybe you\u2019re not overreleasing, maybe you just have some smashed pointer or uninitialized data structure? Not sure, all good things to check When we moved from GL to Metal, all the Metal code was put in a single file. I can send that if you'd like to look it over. Not sure we have means for you to safely upload your source code here let me ask You could use feedback assistant Not sure where that is https://developer.apple.com/bug-reporting/ Have there been any changes to resource limits with Metal 3? Specifically the maximum number of buffers inside an argument buffer. Thank you! There is no limit on the number of buffer references stored in an argument buffer. We also made it easier to populate an argument buffer on the CPU by using the buffers' gpuAddress property; so argument encoders are no longer needed. Please note though, that you still need to actually allocate an argument buffer large enough to hold all the references, and resource creation (of the argument buffer, or all the other buffers) can fail when you run out of memory; your total resident memory set is still limited, but the argument buffer contents are not. The same applies to texture references stored in an argument buffer (use the MTLResourceID property instead when populating the buffer on the CPU). Is there any list where I can find which devices support Metal 3? \u2022 Metal 3 hardware support can be found in Discover Metal 3 https://developer.apple.com/videos/play/wwdc2022/10066/ \u2022 Specific features (Mesh Shaders, Metal FX) may have a different hardware support. For example, Mesh Shaders should be supported on MTLGPUFamilyApple7 and MTLGPUFamilyMac2. Please refer to their associated session for more information. ie, for Mesh Shaders : https://developer.apple.com/videos/play/wwdc2022/10162/ <@U03HWP3Q65P> Thanks! No problem! Looking forward to seeing what y'all make with Metal 3 :raised_hands: I've noticed that in compute shader access to a 2d texture is faster, than access to a buffer. (Due to a lot more of a cache missed). How does caching strategy work for textures vs buffers in compute? Can we specify one (as i've pretty good idea what parts of the buffer will be accessed from which thread) Textures are optimized for localized sampling (so sampling things that are more \"together\" in 2D space). It definitely makes sense to store \"2D data\" in a texture. :slightly_smiling_face: Alternatively, you can manually encode 2D data in a \"1D buffer\" using a more optimized order of data using Morton Ordering (Z-ordering). This will make data that is close in 2D also end up close in \"1D\". You are totally free to sample textures in compute, and it will have the same performance characteristics as in a fragment function. https://en.wikipedia.org/wiki/Z-order_curve Does textures use the same hardware as buffers? If requesting from a buffer is different than requesting from a texture, there may be a point in \u201cparallelizing\u201d data access Since textures and buffers tend to reside in the same memory, that would just end up being a more divergent access pattern. :slightly_smiling_face: If you have 2D localized access pattern, I would suggest using textures, unless your data is not compatible with existing texture formats. We also discussed matching the layout of your threadgroups with your buffers in https://developer.apple.com/videos/play/wwdc2022/10159|Scale compute workloads across Apple GPUs I have a 1.5d data. I have really wide, but not very tall texture. ATM i\u2019m using 10-15k by 20 (just twenty, without thousand) And i\u2019m dispatching 10-15k by 1 threads. ATM i\u2019m limited by max texture width on iOS devices, and would like to explore ideas on how to efficiently scale my kernel) Ah I see! In that case, it might be interesting to use a buffer, but store your data interleaved (so in \"strips\" of 20). That would still give you pretty good localized access. And as always it is a good idea to take a critical look at the data size per entry. (use half if possible etc) that was my original implementation, and it was 10-20% slower than textures:( Data size is one possibility I haven\u2019t explored yet, as my algorithms performs poorly on halfs, but I haven\u2019t tried computing in f32, and storing in f16 Hmmm interesting! Are you sure your texture is the same effective data size? Are you using linear interpolation etc? If you are doing a \"linear\" access across your compute grid, I would expect a simple interleaved scheme to work pretty well Hmm, since it was a year ago, I may used not a linear pattern (as my data can be from 1 to 20 tall, I may have packed it tightly) I can check, and if there is major difference in performance, I can file a feedback Sounds good! Good luck :slightly_smiling_face: Just wanted to add: a colleague of mine reminded me that textures often use a separate cache, so you could definitely see some performance difference between the two. Hope it helps :) Does ScreenCaptureKit support pulling audio from different processes than the video? There are some use cases where audio comes from one process but the window and graphics come from another process, this is common in situations like CrossOver and Wine. This feature if not supported would be very useful. ScreenCaptureKit will get audio associated with application, so if the application\u2019s parent process spawns another process to render audio, SCK should also capture that audio. If this doesn\u2019t work in your case, please file a feedback request :slightly_smiling_face:. Thank you very much! No problem :slightly_smiling_face:","title":"graphics and games"},{"location":"wwdc22/graphics-and-games-lounge.html#graphics-and-games-lounge-qas","text":"","title":"graphics-and-games-lounge QAs"},{"location":"wwdc22/graphics-and-games-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/graphics-and-games-lounge.html#there-any-main-tradeoffs-between-tessellation-and-mesh-shading-for-vertex-creation","text":"Mesh Shading has a more flexible workflow and allows for dynamic scaling of your workload through the object shader stage. In terms of performance, Mesh Shading allows a more pipelined flow directly from the Mesh Shader stage to the rasterizer, so you have full mesh visibility during vertex creation. On the other hand - I would suggest using tessellation if you are doing actual tessellation of an existing geometry pipeline. For users who are doing procedural work that doesn't directly map to tessellation per-se, I would definitely suggest Mesh Shaders. The new https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders?language=objc|sample code we published shows how mesh shaders work and you can get an idea of the difference. The sample code generates Bezier patches and uses mesh shaders to control the level of detail. While the patches are generated on the CPU, you will see that it would not be a huge leap to do that in the object stage or mesh stage.","title":"There any main tradeoffs between tessellation and mesh shading for vertex creation?"},{"location":"wwdc22/graphics-and-games-lounge.html#not-much-of-a-question-because-im-just-starting-to-move-towards-rendering-but-id-like-to-say-thanks-to-everyone-involved-with-this-the-documentation-tooling-and-apis-seems-better-than-ever-and-the-metalfx-upscaling-seems-a-game-changer-i-can-see-some-foveated-rendering-hacks-being-used-with-this-api-and-using-different-lod-with-the-fast-resource-cheers-lads-awesome-work","text":"Thanks for the feedback. We are curious about your idea \"I can see some foveated rendering hacks being used with this API\". Can you elaborate more on it? Using the same concept that VR googles are starting now but on a flat screen. Render the main point of interest in 4k and moving away from it, create lower zones (almost like a gradient) where the borders can be rendered at like 1080p with high anti alias combining with lower LOD. Again, starting so not even sure if it\u2019s possible. For foveated rendering there\u2019s also a variable rasterization rate API in Metal to help with that https://developer.apple.com/documentation/metal/render_passes/rendering_at_different_rasterization_rates Thanks <@U03HHP77WUB>, there\u2019s not even the need for hacks then. Cheers. <@U03J76T9WG6> Just a quick note: MetalFX Upscaling is designed with fixed rasterization rate in mind. You can find more details in the MetalFX session tomorrow. Cheers. I\u2019ll keep an eye on it. Cheers <@U03HW8Y0RFB>.","title":"Not much of a question because I'm just starting to move towards rendering but I'd like to say thanks to everyone involved with this. The documentation, tooling and APIs seems better than ever and the MetalFX Upscaling seems a game changer. I can see some foveated rendering hacks being used with this API and using different LOD with the fast resource. Cheers lads, awesome work."},{"location":"wwdc22/graphics-and-games-lounge.html#what-are-the-package-size-considerations-when-shipping-with-offline-compiled-metal-shaders-do-these-need-to-be-updated-over-time-to-keep-up-with-any-driver-or-software-updates-for-a-particular-piece-of-hardware","text":"Hi <@U03J9S1L38W>, 1. Regarding package size considerations, optimizing for your particular deployment platform and particular set of supported devices is what make most sense. 2. Please refer to the video \u201cTarget and optimize GPU binaries with Metal 3\u201d available tomorrow for information on this.","title":"What are the package size considerations when shipping with offline compiled metal shaders?  Do these need to be updated over time to keep up with any driver or software updates for a particular piece of hardware?"},{"location":"wwdc22/graphics-and-games-lounge.html#how-do-i-start-creating-anything-in-metal-3-can-you-please-show-me-code-for-a-hello-world-type-metal-3-maybe-a-box-with-a-shaded-gradient-of-color-inside","text":"Yup! We have a list of examples with source code at https://developer.apple.com/metal/sample-code/ You can also create a new project in Xcode, select the Game template and pick Game Technology: Metal , which will create a new project that renders a cube in Metal Excellent! The sample code link will do well. Thanks And just created the Metal cube project in Xcode; this is a huge boost to getting me started using Metal 3! Awesome! :tada: If you have any questions throughout the week please let us know :smile: I also recommend checking out https://developer.apple.com/documentation/metal/debugging_tools to help you debug any issues you encounter","title":"How do I start creating anything in Metal 3? Can you please show me code for a \"Hello, World!\" type Metal 3; maybe a box with a shaded gradient of color inside."},{"location":"wwdc22/graphics-and-games-lounge.html#hi-are-metal-3-features-exclusive-to-apple-silicon-or-do-they-also-work-on-other-processors-eg-a-series-on-ios-or-the-gpus-in-intel-based-macs","text":"hello! thanks for the question, Alessandro That information is in the video at approx 12m 45s Thank you very much!","title":"Hi, are Metal 3 features exclusive to Apple Silicon or do they also work on other processors (e.g A-series on iOS, or the GPUs in Intel-based Macs)?"},{"location":"wwdc22/graphics-and-games-lounge.html#whats-the-best-way-to-get-started-with-metal-as-a-beginner-in-the-graphics-field","text":"Hello, we have a bunch of great samples for getting started with Metal going from the very basics all the way to more complicated rendering. https://developer.apple.com/metal/sample-code/ The samples under \"Metal Fundamentals\" are a great place to get started. Thank you very much! Will look into it.","title":"What's the best way to get started with Metal as a beginner in the graphics field?"},{"location":"wwdc22/graphics-and-games-lounge.html#im-learning-graphics-coming-from-swift-and-am-really-interested-in-ray-tracing-what-should-i-learn-first-to-learn-how-to-make-a-hybrid-renderer-in-metal","text":"Hi Ethan! In addition to this year's Raytracing content, we also had a session fully dedicated to the topic of Hybrid Rendering in last year's WWDC. Hopefully you find it a good introduction to the topic - https://developer.apple.com/videos/play/wwdc2021/10150/ . In particular, the session shows how to do your \"primaries\" using rasterization, and then how to use that as the basis for RT Shadows, Ambient Occlusion, and Mirror-like Reflections. This sample is also great for beginners https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal (it's not in Swift though :sweat_smile: sorry!) Thanks! Metal makes Ray tracing really exciting since you can now pair it with MetalFX. Going to be interesting!","title":"I\u2019m learning graphics (coming from Swift) and am really interested in Ray tracing. What should I learn first to learn how to make a hybrid renderer in Metal?"},{"location":"wwdc22/graphics-and-games-lounge.html#are-there-any-memory-and-texture-format-considerations-when-using-metalfx-im-guessing-the-historical-frames-need-to-be-stored-in-some-way-though-the-trade-off-would-be-smaller-intermediate-render-targets","text":"For more details on MetalFX Upscaling, please watch the session tomorrow. Thanks. Thanks! Will be sure to check it out. Also will be interesting if this could be used for some intermediate render targets like an AO pass. If you have any further questions, join us in the digital lounge for the session 1PM-2PM PST. We'd be happy to answer any further questions.","title":"Are there any memory and texture format considerations when using MetalFX?  I'm guessing the historical frames need to be stored in some way, though the trade off would be smaller intermediate render targets."},{"location":"wwdc22/graphics-and-games-lounge.html#since-the-shaders-are-compiled-at-project-build-time-and-compilation-speed-is-no-longer-as-critical-is-that-extra-headroom-used-to-allow-the-compiler-to-try-to-optimize-the-shaders-further-or-is-does-it-produce-identical-machine-code-as-when-building-it-at-runtime","text":"Independent from offline compilation, we did introduce a new optimization level -Os (size) as an option, but there is not additional optimization that occurs during runtime. The produced code will be identical. Understood, thanks! Also, be sure to check out \u201cTarget and optimize GPU binaries with Metal 3\u201d available tomorrow for more information!","title":"Since the shaders are compiled at project build time and compilation speed is no longer as critical, is that extra headroom used to allow the compiler to try to optimize the shaders further? Or is does it produce identical machine code as when building it at runtime?"},{"location":"wwdc22/graphics-and-games-lounge.html#hi-great-wwdc-session-question-do-you-have-a-hello-world-example-for-making-shaders-using-apple-technologies-thank-you","text":"Please see the article \" https://developer.apple.com/documentation/metal/developing_and_debugging_metal_shaders?language=objc|Developing and Debugging Metal Shaders \" that will introduce you to the Xcode's excellent shader debugger. Then you can try out the Game Template if you want to get a Metal app running. And you can check out our https://developer.apple.com/documentation/metal/metal_sample_code_library|sample code library that have many kinds of Metal samples (and shaders) you can learn from.","title":"Hi! Great wwdc session, question: do you have a hello world example for making shaders using Apple technologies? Thank you!"},{"location":"wwdc22/graphics-and-games-lounge.html#can-you-explain-how-to-use-triple-buffering-for-complicated-scene-with-2d-overlayui-and-3d-what-kind-of-data-i-should-store-in-buffers-in-vulkan-examples-they-use-framebuffer-but-metal-dont-have-any-similar-entity","text":"Could you give a bit more context? In general all interfacing with the compositing is done through the Drawable. (usually through a MTKView in an app). Are you referring to manually triple-buffering GPU resources for CPU synchronization? Yeah, indeed. I wanna fill some data on CPU side while frame rendering (inflight frames) and pass filled buffer to command queue. In apple examples I see how to use inflight frames to fill camera data to buffers, but camera data isn't complicated. Im newbie in Metal and trying to create my own render engine. One way to do this is to have a set of data for each frame in flight. You can then use a mutex (or other thread-safe structure) to keep track of what sets are in flight by: \u2022 marking them when you use them in a command bufferand \u2022 unmarking them in the completion handler of the command buffer. If you run out of free sets, you can simply wait on the mutex and halt rendering on CPU until GPU has caught up. Ok, I will try it! Thanks a lot. check synchronize https://developer.apple.com/documentation/metal/mtlblitcommandencoder/1400775-synchronize|link and the various ways of signalling and reading data between buffers on GPU and CPU https://developer.apple.com/documentation/metal/mtlbuffer|link Also, if you wanted a example of this pattern check out the game template in XCode by creating a new project in Xcode, selecting the Game template and picking Game Technology: Metal . In that template check how _inFlightSemaphore is used to multi buffer the uniforms buffer. From the Dynamic Terrain Sample code: // We allow up to three command buffers to be in flight on GPU before we wait static const NSUInteger kMaxBuffersInFlight = 3; _device = device; _commandQueue = [_device newCommandQueue]; _startTime = [NSDate date]; _inFlightSemaphore = dispatch_semaphore_create (kMaxBuffersInFlight); _frameAllocator = new AAPLAllocator (device, 1024 * 1024 * 16, kMaxBuffersInFlight); _uniforms_gpu = _frameAllocator-&gt;allocBuffer &lt;AAPLUniforms&gt; (1); You can see we explicitly limit our in-flight sets to 3 in this example. ( https://developer.apple.com/documentation/metal/buffers/rendering_terrain_dynamically_with_argument_buffers|link to sample code ) The main idea is that you shouldn\u2019t modify data with the CPU if the GPU might be reading it, so using that semaphore with a completion handler (per the examples above) allows you to know when at least one of your 3 copies of any data are able to be modified. If you are not using the CPU to modify textures/buffers on the fly then there isn\u2019t a need for this semaphore. -[CAMetalLayer nextDrawable] will automatically provide up to 3 drawables and will limit an app from getting too far ahead (if you render faster than the screen refresh). Thanks! I will watch linked examples asap.","title":"Can you explain how to use triple buffering for complicated scene with 2d (overlay/ui) and 3d? What kind of data I should store in buffers? In Vulkan examples they use framebuffer, but metal dont have any similar entity."},{"location":"wwdc22/graphics-and-games-lounge.html#didnt-see-it-earlier-new-to-slack-not-sure-if-this-is-the-right-place-to-ask-but-i-am-still-looking-through-the-plugins-but-would-it-be-able-to-use-bluetooth-or-related-tech-to-share-say-an-item-with-another-player-that-also-has-the-gameapp-like-if-there-is-a-unity-plugin-from-apple-that-does-this","text":"Unfortunately we don't currently offer a plug-in that exposes this functionality, but this is certainly an interesting idea. Can you tell me more about what you're trying to do. Are you looking for something that behaves like the share sheet? It's more of a thought experiment at this point, but I kind of want to create a fantasy/rpg game and want to incentivize working together in real life to defeat certain bosses or challenges. My goal is to not rely on having any servers if possible. I think having bluetooth access might be a way for people to create a party, like in a typical 4 player rpg game like final fantasy. For example, if you are a healer. You can only heal or buff other people most of the time. In this case to beat certain challenges you would need to meet up with other people to be able to progress.","title":"Didn't see it earlier, new to Slack:  \"Not sure if this is the right place to ask, but I am still looking through the plugins. But would it be able to use Bluetooth or related tech to share say an item with another player that also has the game/app. Like if there is a Unity plugin from apple that does this. \""},{"location":"wwdc22/graphics-and-games-lounge.html#whats-the-earliest-ios-you-can-use-this-with","text":"The plug-ins are compatible as far back as iOS 13 as long as the underlying framework is also supported. For example, the PHASE framework was new in iOS 15, so is not supported prior to that version. Thanks. (My current game supports iPad 2 still, iOS 9\u2026 But for new projects, that\u2019s probably fine.) Keep in mind the plug-ins are open-source, so feel free to make modifications if needed to help with your specific backwards compatibility goals.","title":"What's the earliest iOS you can use this with?"},{"location":"wwdc22/graphics-and-games-lounge.html#if-were-making-feature-requests-i-really-like-the-ability-to-email-bug-reports-from-my-game-with-mfmailcomposeviewcontroller","text":"Thanks for the feedback. We're definitely tracking feature requests, so this input is super valuable. Though that might be really tricky with Unity \u2014 I use it in UIKit-based games. Yeah, we'll have to investigate how we might surface the functionality. It's certainly interesting - but there may be a more Unity-esque approach to solving your issue as well. I believe Unity has some integrated bug report metrics that you can leverage as a Unity developer. We did it in Kobold2D (Cocos2D) years ago, too. Email is nice for indies because you don\u2019t need to worry about a back end.","title":"If we're making feature requests: I really like the ability to email bug reports from my game with MFMailComposeViewController!"},{"location":"wwdc22/graphics-and-games-lounge.html#a-question-on-the-accessability-plugin-for-unity-is-it-only-available-for-apple-devices-or-can-it-also-be-used-for-windows-xbox-etc-reason-for-asking-is-that-it-would-be-great-to-see-a-cross-platform-plugin-to-increase-the-reach-of-accessability-making-it-easier-to-prioritize-for-stakeholders","text":"Hi Denny, thanks for the perspective. Currently this plug-in is only available for Apple platforms as it ties into foundational Apple accessibility technologies. Ok! I did miss to say it in the question--it's really great to see the accessibility plugin :-)","title":"A question on the Accessability plugin for Unity. Is it only available for Apple devices or can it also be used for Windows, Xbox etc. Reason for asking is that it would be great to see a cross platform plugin to increase the reach of accessability (making it easier to prioritize for stakeholders)"},{"location":"wwdc22/graphics-and-games-lounge.html#what-is-the-recommended-framework-for-connecting-to-another-player-stably-for-a-multiplayer-match-not-the-old-turn-based-games-but-a-modern-approach-to-having-a-5-10-minute-connection-for-pvp-would-arkit-realitykit-be-better-today-than-multipeer-connectivity-which-can-be-flaky-with-frequent-disconnects","text":"Hi - GameKit currently provides real time multiplayer support and is currently used in a number of games. Coincidentally, we've just shared this functionality as part of the Unity plug-ins made available today. Besides that, GameKit provides a few way for our users to play with each other: \u2022 Auto matching. \u2022 Inviting Game Center friends via http://Messages.app|Messages.app or push notifications. \u2022 Or play with nearby friends. You will be able to see some games in Apple Arcade utilizing the multiplayer functionality provided by gamekit.","title":"What is the recommended framework for connecting to another player stably for a multiplayer match? Not the old turn-based games, but a modern approach to having a 5-10 minute connection for PvP. (Would ARKit, RealityKit be better today than multipeer connectivity, which can be flaky with frequent disconnects?)"},{"location":"wwdc22/graphics-and-games-lounge.html#hi-it-looks-like-the-documentation-for-the-unity-plugins-has-not-been-submitted-to-the-github-repo-all-the-documentation-links-are-broken","text":"Thanks for the report! We\u2019ll look into this. Hi, thanks for pointing out! Documentation links should now work again.","title":"Hi! It looks like the documentation for the unity plugins has not been submitted to the GitHub repo -- all the documentation links are broken."},{"location":"wwdc22/graphics-and-games-lounge.html#ive-had-a-quick-browse-of-the-accessibility-unity-plugin-and-noticed-that-its-only-supporting-ios-and-tvos-is-macos-planned-as-well-there-are-also-some-performance-concerns-with-the-code-that-id-love-to-see-resolved-will-github-prs-be-considered","text":"We don't currently support macOS, but you can always file a bug through feedback assistant for us to consider these requests. Hi Alex, this is very appreciated. Unfortunately we don't currently accept external PRs - but we do really take your feedback to heart, so keep the feedback coming and we'll do our best to address in the mean time. Great to hear. Just based on a cursory scan through the codebase, the most serious performance issues are allocating in the mono heap with calls like GetComponentsInChildren<Renderer>() (there are allocation-free versions of these available), and serializing rects and points to native code via strings. Fantastic. Thanks for already helping to make these plug-ins better. Would love to get some more detail on how the element ordering is working too. It looks like the plugin determines an element ordering based on screen-space positioning which seems quite complex; however my project already knows the logical layout and grouping of every UI element; is there a mechanism for using this actual logical ordering? That is a great feedback! Currently no, but we will add this to our feedback list to allow honoring element ordering provided by developers:thumbsup: I haven\u2019t looked at the Accessibility plugin yet but it\u2019s the one I\u2019m most excited about. My games are playable via VoiceOver, and I definitely override -accessibilityElements to do things like give a consistent navigation order when the visuals are mirrored.","title":"I've had a quick browse of the Accessibility Unity plugin and noticed that it's only supporting iOS and tvOS. Is macOS planned as well? There are also some performance concerns with the code that I'd love to see resolved, will GitHub PR's be considered?"},{"location":"wwdc22/graphics-and-games-lounge.html#i-look-into-unity-plugins-and-have-a-relative-question-why-you-using-swift-instead-of-objc-in-applecore-module-why-swift-with-all-unmanaged-things-better-than-objc-but-thanks-for-that-bridging-i-grab-information-how-to-use-_cdecl","text":"Thanks for asking Vladislav! We used Swift because it is the best language for Apple platforms. Glad you learned something new too! Yey! That is very great answer for me! Thanks Brett","title":"I look into unity plugins and have a relative question: Why you using swift instead of objc in AppleCore module? Why Swift with all Unmanaged things better, than ObjC?   But thanks for that bridging! I grab information how to use _cdecl :)"},{"location":"wwdc22/graphics-and-games-lounge.html#is-wheel-support-only-on-mac-the-api-seems-to-be-available-on-all-platforms-except-apple-watch-but-ive-only-seen-it-mentioned-with-regard-to-mac-am-i-misreading-something","text":"Yes, racing wheels are only supported on macOS at this time. If you\u2019re interested in supporting racing wheels in your game on other platforms, please file feedback to let us know!","title":"Is wheel support only on Mac? The API seems to be available on all platforms except Apple Watch, but I\u2019ve only seen it mentioned with regard to Mac. Am I misreading something?"},{"location":"wwdc22/graphics-and-games-lounge.html#could-mesh-shaders-be-used-for-procedural-terrain-generation-with-continuous-lod-currently-i-use-a-tessellation-compute-shader-which-also-does-some-basic-culling-followed-by-a-vertexfragment-shader","text":"You can definitely use Mesh Shaders to do this! - The great thing about Mesh Shaders is that you are not limited to the fixed tessellation layout; you can kick off an Object Shader to calculate what tiles to populate (and which ones to leave out for custom tiles), and then dynamically schedule Mesh Shader grids to create the individual tile geometry. :) This way, you could effectively generate and cull your entire terrain on the GPU Is there any specific issues you are worried/thinking about? Fantastic! Are there any more details/docs on mesh shaders? I\u2019m having trouble finding much about it. One of the problems I have at the moment is culling. I pass in some quads to the tessellator, then do frustum transforms to try to figure out whether the quad would be onscreen or not (using -1 for the tessellation factor if I want to cull it). Then my vertex shader displaces vertices with the procedural function and does the transforms again, and it all feels a bit clunky. I know there\u2019s a session tomorrow on mesh shaders, but I\u2019m having trouble understanding the basic behavior of them atm. There will be a full session on Mesh Shaders tomorrow! And you can always drop back into the next Q&A on friday if you have any follow-up. My suggestion for doing these kinds of things, is to assume each \"tile\" has a AABB of the min/max displacement within the segment (pre-calc if needed), then use the Object Shader (which is a full featured compute shader) to generate work items for tiles of various sizes etc. Then run a Mesh Shader Grid with a TG per tile to generate. You can simply cull your AABB in the OS and you should never have any overhead aside from the AABB check :slightly_smiling_face: You can effectively have the OS TG fill the payload buffer with an array of TileDesc structs (or something like that), then have each MS TG pick up a TileDesc to expand into a mesh and send it straight to the rasterizer Tomorrow's Mesh Shader session will be here: https://developer.apple.com/videos/play/wwdc2022/10162/ Documentation will be updated in the near future, and I believe a sample project will be available along with the session. OK, this sounds good! Is there any integration with the existing tessellation pipeline, or would I need to somehow implement something like it in the OS or MS? I believe you have to do a manual tessellation in your MS. If I can make a suggestion, a doubling straight forward quad tessellation scheme while you fade in height dat through manual mip levels might be a very good and quick way of getting a simple-but-smooth solution. The nice thing about OS/MS is that you can nicely debug both the tile scheduling and the mesh generation separately, as they are both basically compute kernels you can run individually. Hmm I see. It definitely sounds like something to try out. I love the built-in tessellator because it distinguishes between internal and edge tessellation which lets me avoid cracks in the terrain. I guess I would just need to do a bit more work in my MS to make that work. I wonder if the current fixed tessellator could be exposed as a function we could call from our MS\u2026 I think the main issue with the classic tessellation scheme is that it tends to generate noisy silhouettes when tessellation smoothly increases. (you get this wave-effect as the vertices move across the displacement samples). There should be a way to integrate an external \"classic tessellation function\" into the MS kernel to generate a list of 2D triples for a given tessellation. I wouldn't be surprised if there are examples of this online. Yes the swimming vertices are an issue. Although it\u2019s an issue in my case anyway as I\u2019m using a single square mesh to render an entire spherical terrain from ground to space by \u201cwrapping\u201d it over the sphere depending on distance and what is potentially visible. My solution is to buy better GPUs and increase the vertex count to minimise the sample aliasing.. :smile: Thanks for your help Jaap, looking forward to the session tomorrow and I\u2019ll probably be back for more Q&A following it! If you wanna have more of an in-depth discussion, we still have a few Lab openings to sign up! https://developer.apple.com/wwdc22/labs/ These are in-depth 1:1 discussions :)","title":"Could mesh shaders be used for procedural terrain generation with continuous LOD? Currently I use a tessellation compute shader (which also does some basic culling) followed by a vertex/fragment shader."},{"location":"wwdc22/graphics-and-games-lounge.html#how-should-we-approach-issues-like-shader-debugger-crashes-and-gpu-faults-we-dont-know-where-to-look-for-hints","text":"The best thing to do for debugger crashes is to create a feedback assistant with steps to reproduce. As for GPU faults, I would encourage you to try enabling shader validation for your application. We already have all validation turned on (no results), and we use FA all the time, but what do we do about these \"game over\" issues in the meantime? I would encourage you to try the new betas with your use case, we improved a lot of areas there If you are still having troubles, maybe you could create new FBA tickets, attach a gputrace and post the numbers here? One example of GPU Fault FB9968899 Is the GPU fault happening at application runtime or while you are working with the gputrace? The fault is a separate issue, but I mention both because there are no diagnostics we can understand But this is a common occurrence for us, something fails catastrophically with no hints and we don't know how to proceed Does this happen on the latest OSs? I haven't tried the fault on Ventura because it can lock things up if not quickly resolved, may result in a loginwindow watchdog timeout (force reboot) Are you using raytracing or ICBs? No raytracing, I assume ICB is indirect command buffer, also no Which GPU architecture is this Steven? Same app worked fine on Intel (AMD GPU) fails badly on M1 for no apparent reason But I'm really asking about methodology, because we want to keep making forward progress without DTS all the time The crash log on M1 appears to indicate GPU firmware detected lockup, meaning that from the software perspective the GPU is hung for some time hence it is rebooted. Do you by chance implement a concurrent producer/consumer pattern via atomics on M1? It uses atomics, but I wouldn't call it producer/consumer, it's just an atomic allocator to a buffer We found entirely by chance that increasing the number of allocators from \"1\" fixed it But have no idea why or how, etc Are you ever in a situation where you rely on forward progress guarantees to access the allocator? Not sure what that means from a GPU standpoint E.g.: would threads spin until memory is available or do you just assume there\u2019s always enough memory? My mention of \"forward progress\" above just means development progress, not getting stuck on crashes there is no spinning, just atomics though we check the allocated value hasn't gone off the end of the array, we discard in that case there is nothing \"consuming\" Oh coincidentally you also used the term \u201cforward progress\u201d, although I meant the property of a program to make progress no matter what There is no concurrent producer/consumer I see, then it\u2019s likely not the problem I thought, thanks for clarifying Have you tried running without shader validation enabled, just the API validation? Also, one of the ways GPU Faults happen when there\u2019s basically unpaged memory reads/writes, basically out of bounds reads/writes or access to a memory that\u2019s not resident - meaning the underlying memory of a resource that wasn\u2019t made resident using useResource or useHeap I don't think that explains how increasing the allocators fixed it 1 = crash, 4 = ok I realize there may be more parallelism on TBDR, but some kind of hint would help us out List of FB for reference FB10020742, FB10020198, FB10014465, FB9968922, FB9968899 Had a quick look at the shader code in the Metal library contained in the app you attached to FB9968899 and I noticed a function called transparency_populate that loops on a linked list. Does this function get called in the workload that hangs? That's in the second \"resolve\" pass, which is called but shouldn't be directly affected by the number of allocators or their values transparency_add , transparency_add_counter , transparency_write , etc are the ones affected If you remove entirely the second \u201cresolve\u201d pass, do you still see a GPU fault though? I could try that, but it seems unlikely if you're looking for sample code, check the attachment to FB9968922 The reason I\u2019m asking is that that transparency_populate relies on the data structure to be well formed to complete, otherwise it might spin forever That's not an issue we see It's an A-Buffer implementation, a 2D buffer of head pointers (inited to -1) then a linked list Yes, I can see the A-buffer structure for sure. At first glance the code looks sane to me, so I guess we need to repro locally to understand what\u2019s going on FYI, just triggered in Ventura, will be attaching the reports Looks like some diagnostics are missing from the messages? 2022-06-08 13:18:09.498390-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang) 2022-06-08 13:18:09.498450-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang) 2022-06-08 13:18:09.498713-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim) 2022-06-08 13:18:09.498734-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim) 2022-06-08 13:18:09.498867-0400 CADApp[8724:128585] GPU Soft Fault count: 1 2022-06-08 13:18:09.559102-0400 CADApp[8724:128580] [Metal Diagnostics] __message__: MTLCommandBuffer \"Main Loop\" execution failed: The commands associated with an encoder caused an error __:::__ __delegate_identifier__: GPUToolsDiagnostics 2022-06-08 13:18:09.559156-0400 CADApp[8724:128580] [Metal Diagnostics] __message__: MTLCommandBuffer \"Callout 3\" execution failed: The commands associated with the encoder were affected by an error, which may or may not have been caused by the commands themselves, and failed to execute in full __:::__ __delegate_identifier__: GPUToolsDiagnostics Thanks for the update. What message do you think is missing though? Seems like __:::__ __delegate_identifier__: is missing the identifier? I see what you mean, could you please add this message to the original FB? also, I managed to reproduce the lock up locally, so it\u2019s easy enough for somebody to bottom that out I updated FB9968899 at 1:22PM EDT GPU Soft Fault count: 1 kinda identifies that theres something either accessing out of bounds or at 0 <@U03HLFDFS20> I guess I would expect some kind of diagnostic for that rather than an unknown issue. Also wouldn't that affect AMD equally? There\u2019s a possibility that amd reads out of bounds, but doesn\u2019t page fault What I\u2019m seeing locally is definitely a hang, not an out-of-bounds though I guess the current answer is going to be that we would need to investigate these feedback assistant reports That's fine, though my original question was a little different And to answer your initial question, the fastest way to reach us is through developer forums ok I will start using that more Unfortunately there might not be a single solution to progress when you are faced with these issues Thanks for the feedback Definitely thanks for the feedback. Somebody will be investigating the GPU hang soon hopefully","title":"How should we approach issues like Shader Debugger crashes and GPU faults? We don't know where to look for hints"},{"location":"wwdc22/graphics-and-games-lounge.html#can-we-somehow-cancel-work-committed-to-gpu-for-example-we-commited-some-neural-net-processing-example-of-huge-work-and-we-received-memory-warning-from-the-system-can-we-somehow-stop-execution","text":"Hi Pavel, unfortunately no facility exists to cancel committed GPU work. You can use Indirect Command Buffers and NOP out the commands before execution though, could that be a work around? (Also we'd appreciate it if you can file a feedback request with this scenario for us to evaluate.)","title":"Can we somehow cancel work committed to GPU? For example we commit\u0435ed some neural net processing (example of huge work) and we received memory warning from the system, can we somehow stop execution?"},{"location":"wwdc22/graphics-and-games-lounge.html#what-is-the-best-way-to-denoise-the-ray-traced-frame-in-metal-is-mpssvgfdenoiser-recommended-for-this-purpose","text":"MPSSVGFDenoiser will work for denoising the full application frame. Typically you want a denoiser targeted at each ray traced effect, focusing on the visibility signal for shadows and ambient occlusion, and a separate denoiser for reflections aware of how to reproject reflected content. thanks for the response :thumbsup: Denoising is a rapidly moving field and there's several open source options that you could look at adding to your application! Those are new topics for me, so sorry if it is a silly question, but what is the latest best way to organize global illumination passes with metal ray tracing then? In my inexperienced understanding it was like: accumulate the light from ~3 bounces and apply denoising to the whole result Not a silly question at all! Accumulating light from multiple bounces will result in an approximation to global illumination and will likely be noisy depending on how you define the rays for each bounce. Typically for later bounces you could accept something more diffuse for your global illumination representation - there's more diffuse representations such as https://jcgt.org/published/0008/02/01/|Dynamic Diffuse Global Illumination","title":"What is the best way to denoise the Ray traced frame in metal? Is MPSSVGFDenoiser recommended for this purpose?"},{"location":"wwdc22/graphics-and-games-lounge.html#does-mfxspatialscalingeffect-support-pq-encoded-hdr-color-specifically-pixel-format-mtlpixelformatbgr10a2unorm-and-colorspace-kcgcolorspaceitur_2100_pq-if-so-which-colorprocessingmode-should-be-used","text":"HDR colorProcessingMode should be used in that case.","title":"Does MFXSpatialScalingEffect support PQ encoded HDR color? Specifically, pixel format MTLPixelFormatBGR10A2Unorm and colorspace kCGColorSpaceITUR_2100_PQ? If so, which colorProcessingMode should be used?"},{"location":"wwdc22/graphics-and-games-lounge.html#are-there-any-demos-using-the-mtlparallelrendercommandencoder-is-there-a-possibility-of-supporting-a-parallel-compute-encoder-in-the-future","text":"Hi, thanks for your question. Currently, we do not have a MTLParallelRenderCommandEncoder sample. When you said a parallel compute encoder, can you tell us more about your use case? We do support a compute encoder that executes dispatches in parallel on GPU but not encoding compute dispatches in parallel on CPU. I suppose I don't have a use case in mind :sweat_smile: but since Metal compute seems to becoming more popular (GPU-driven pipelines, ICBs, new machine learning stuff) I was just curious if a parallel encoder would be available. I suppose though it might not actually be necessary if compute encoding is not a bottleneck Typically what we've seen of GPU compute is that encoding is quite light weight while the GPU work is heavy. Encoding is just feeding pointers to GPU shader cores of where all the data lives and then GPU go on a massive parallel data processing/computing spree. :smile:","title":"Are there any demos using the MTLParallelRenderCommandEncoder? Is there a possibility of supporting a parallel compute encoder in the future?"},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-a-limit-to-the-scale-factor-outputwidth-inputwidth-that-mfxspatialscalingeffect-can-apply","text":"We recommend using a scale factor that's between > 1x && <= 2x. Just an FYI, we have a digital lounge session dedicated to MetalFX from 1PM-2PM PST today. Cheers. Alright, thanks. I'll try to check in later. :slightly_smiling_face:","title":"Is there a limit to the scale factor (outputWidth / inputWidth) that MFXSpatialScalingEffect can apply?"},{"location":"wwdc22/graphics-and-games-lounge.html#whats-the-most-performant-way-to-copy-a-calayer-into-a-mtltexture-on-ios","text":"Can you elaborate on your use case? What kind of layer are you trying to copy, and does your app provide the original content of this layer? We\u2019re trying to render Lottie animations into a Metal texture interactively in real-time. Our goal is for a user to be able to drag a Lottie animation around a canvas as it plays and have it composited with other Metal content. The Lottie library we\u2019re using renders to a CALayer and we need to copy it into a MTLTexture. Do you know what kind of layer they\u2019re rendering into? It matters significantly whether the library is also using a CAMetalLayer or if it is using the traditional CALayer drawing APIs. I think it\u2019s not a CAMetalLayer, which is the problem. I think it\u2019s using CAShapeLayer, CATextLayer, etc. (which are much simpler than implementing text rendering for instance). But it seems to be tricky to then get the result from the CALayer into Metal. On macOS CARenderer seems to permit this but not in iOS Most generally, the only way to capture the composited appearance of a layer is via CARenderer. You will have difficulty syncing the framerate of this renderer with the display. CARenderer is available on iOS, can you elaborate on your difficulty with it? Hmm.. I could have sworn that CARenderer::rendererWithMTLTexture was not available on iOS.. but the docs says it has been there since iOS 11. Well, I think that\u2019s the way to do it, right? One difference I can think of between macOS and iOS is that iOS makes much heavier use of implicit CATransactions. If you try to render the layer tree while the main queue\u2019s implicit transaction is still open, you might not capture the latest drawing. On iOS, to guarantee correct drawing, use UIGraphicsImageRenderer in conjunction with -[UIView drawViewHierarchyInRect:afterScreenUpdates:YES] . You can then use the resulting UIImage to populate an MTLTexture. This will not be as fast as using CARenderer directly but it is more likely to be resilient to the complexities of how iOS apps use Core Animation. But do try CARenderer first. :slightly_smiling_face: Thanks, I\u2019ll take another look! Good luck! If neither approach works for you, please file Feedback so the teams involved can understand more about your use case.","title":"What's the most performant way to copy a CALayer into a MTLTexture on iOS?"},{"location":"wwdc22/graphics-and-games-lounge.html#the-new-edr-support-in-metal-for-ios-looks-really-exciting-but-on-what-current-devices-can-we-test-this-on","text":"Devices with an XDR display currently support EDR as long as the device brightness isn't set too high (among other factors) which would clamp those EDR values. Please see https://developer.apple.com/documentation/metal/hdr_content/determining_support_for_edr_values?language=objc for more details. On macOS, you can read the maximumPotentialExtendedDynamicRangeColorComponentValue property on an NSScreen object for that display. And on iOS, you can use UIScreen.potentialEDRHeadroom . Is the iPhone 13 Pro included in this? I haven't tested it, but I think you should get a large EDR headroom on iPhone 13 pro. We also recommend to poll the value of UIScreen.currentEDRHeadroom (for iOS) or NSScreen.maximumExtendedDynamicRangeColorComponentValue (for macOS) to accommodate for possible display brightness changes that might affect available headroom. Those can be caused for ex. by the user interaction - changing the screen brightness or by the system if auto-brightness setting is enabled or as a result of thermal event when device has to throttle.","title":"The new EDR support in Metal for iOS looks really exciting, but on what current devices can we test this on?"},{"location":"wwdc22/graphics-and-games-lounge.html#more-out-of-curiosity-what-are-the-potential-use-cases-of-quadgroup-functions-ive-only-seen-one-video-mentioning-them-discovering-advances-in-metal-for-a15-bionic-where-theyre-used-to-reduce-the-number-of-texture-reads-per-thread-are-there-any-other-use-common-use-cases","text":"It can be used for any workload where you want to spread orthogonal work across a group; for instance, you can have each thread in a quad cull a part of a light list, then use quadgroup functions to peek at each others' lists and sum lights. The gist of quadgroup and simdgroup functions are to perform data exchange in a more efficient manner. Traditionally, threadgroup barrier and threadgroup memory is required to perform data exchanges, but its a lot more heavier than finer granularity exchanges like quadgroup or simdgroup functions. When would you decide to use a quadgroup over a simdgroup function? When all you need is to exchange among 4 threads in a row. The finer granularity you use, the more efficient it is. If you are in compute, I would probably always use simdgroup, just to get the best bang for your buck. I believe quadgroups are interesting for fragment workloads; I'm not sure what the availability and details are by heart. (rasterization, helper threads etc)","title":"More out of curiosity, what are the potential use cases of quadgroup functions? I've only seen one video mentioning them (\"Discovering advances in Metal for A15 Bionic\") where they're used to reduce the number of texture reads per thread. Are there any other use \"common\" use cases?"},{"location":"wwdc22/graphics-and-games-lounge.html#im-interested-in-the-accelerated-ray-tracing-features-in-metal-3-but-the-specific-session-that-would-contain-more-details-is-tomorrow-are-these-changes-specifically-designed-for-realtime-raytracing-in-a-rasterized-renderer-or-are-they-general-enough-to-be-used-on-their-own-to-ray-trace-entire-scenes-i-see-that-there-are-general-purpose-ray-tracing-features-in-older-versions-of-metal-im-specifically-curious-how-these-intersect-with-the-metal-3-ray-tracing-features","text":"Metal RT is intended for use in both pure ray traced renderers and hybrid renderers that ray trace from rasterized content. This year's features are applicable to both with a focus on improving performance with some quality of life and ease of use features too! We have a Q&A session on Friday if you\u2019d like to ask additional questions following the release of the video. Some of our samples, such as https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal|Accelerating Ray Tracing Using Metal and https://developer.apple.com/documentation/metal/metal_sample_code_library/control_the_ray_tracing_process_using_intersection_queries|Control the Ray Tracing Process Using Intersection Queries do pure ray tracing, and https://developer.apple.com/documentation/metal/metal_sample_code_library/rendering_reflections_in_real_time_using_ray_tracing|Rendering reflections in real time using ray tracing uses a more hybrid model. If anyone would like to have more of an in-depth/follow-up discussion, we still have a few Lab slots available! https://developer.apple.com/wwdc22/labs/","title":"I'm interested in the accelerated ray tracing features in Metal 3, but the specific session that would contain more details is tomorrow. Are these changes specifically designed for realtime raytracing in a rasterized renderer, or are they general enough to be used on their own to ray trace entire scenes? I see that there are general purpose ray tracing features in older versions of Metal, I'm specifically curious how these intersect with the Metal 3 ray tracing features."},{"location":"wwdc22/graphics-and-games-lounge.html#as-far-as-i-can-tell-ios-devices-still-run-opengles-apps-just-fine-is-there-a-hard-deadline-when-this-will-not-be-the-case-any-more","text":"We can't comment on future plans, but we recommend porting existing OpenGL-based applications to Metal to take advantage of the latest features available on our devices.","title":"As far as I can tell iOS devices still run OpenGLES apps just fine. Is there a hard deadline when this will not be the case any more?"},{"location":"wwdc22/graphics-and-games-lounge.html#are-there-any-recommended-formats-or-workflows-for-dealing-with-3d-textures-in-metal","text":"Can you clarify more about \u201cworkflows\u201d? Are you trying to integrate with an existing asset pipeline? Is there a specific thing you are trying to do? I believe you can manually fill the texture one slice at a time if needed. If you are not interpolating along the 3rd axis, you might want to use a 2D Texture Array instead. Just be careful with blit API, Simple APIs don\u2019t work on 3D textures you need full swing blit copy with one full slice calculated not whole texture! I believe MTLTextureLoader supports KTX files, including 3D texture formats. Thanks for the pointers. I'm just starting to explore ideas. I had exported a PNG with vertical slices from a voxel editor. Thanks for the pointer on KTX! I'll look into that Importing one slice at a time works as well! If you are doing uncompressed data (voxel art etc), that would work. If you are going more \"natural\" 3D textures, beware that you probably want a compressed format as it makes a big difference in memory size and performance. Right. Are there any gotchas using sparse textures for 3D textures? I don't believe so - it should effectively work the same.","title":"Are there any recommended formats or workflows for dealing with 3D textures in Metal?"},{"location":"wwdc22/graphics-and-games-lounge.html#are-all-of-the-metal-3-features-available-on-all-gpus-do-vega-gpus-and-uhd-630-get-mesh-shaders-with-metal-3","text":"\u2022 Metal 3 hardware support can be found in Discover Metal 3 https://developer.apple.com/videos/play/wwdc2022/10066/ \u2022 Specific features (Mesh Shaders, Metal FX) may have a different hardware support. For example, Mesh Shaders should be supported on MTLGPUFamilyApple7 and MTLGPUFamilyMac2. Please refer to their associated session for more information. ie, for Mesh Shaders : https://developer.apple.com/videos/play/wwdc2022/10162/","title":"Are all of the Metal 3 features available on all GPUs?  Do Vega GPUs and UHD 630 get mesh shaders with Metal 3?"},{"location":"wwdc22/graphics-and-games-lounge.html#quick-question-what-happened-to-the-metal-language-version-build-setting-seemed-it-disappeared-one-year","text":"Hi Steven, this looks a bug, could you please file a feedback request for us to track it? In the meanwhile you can work around this by passing -std=ios-metalX.X to your .metal file compilation flags Same for macOS? ~I think for macOS it is -std=osx-metalX.X ~ Looks like it may be back in Xcode 14, would it be fixed for 13.4? For macOS, it is -std=macos-metalX.Y until MSL 3.0 where we have unified language and it is -std=metal3.0 thank you","title":"Quick question: What happened to the Metal Language Version build setting? Seemed it disappeared one year..."},{"location":"wwdc22/graphics-and-games-lounge.html#hey-there-i-have-been-working-with-a-highly-programmatic-scenekit-nodes-and-geometries-i-know-i-know-old-hat-and-i-have-been-suggested-by-some-friendly-helpful-developers-at-the-wwdc-apple-park-event-hello-again-rintaro-that-a-pathway-to-help-some-of-my-performance-problems-would-be-to-rewrite-some-of-my-core-components-with-arkit-primitives-or-perhaps-metal-unfortunately-i-dont-really-know-where-i-would-begin-that-conversion-or-even-if-doing-the-work-would-net-the-performance-benefits-im-seeking-the-context-of-this-question-is-my-arvr-visualization-project-which-uses-10s-of-thousands-of-small-geometries-and-nodes-shared-as-much-possible-to-render-individual-text-glyphs-like-a-sheet-of-paper-httpgithubcomtikimcfeelookatthatgithubcomtikimcfeelookatthat-a-good-solid-first-step-would-be-what-would-be-a-suggested-migration-pattern-from-scnnodescnplaneuiimage-to-similar-metalarkit-patterns","text":"Are you using individual nodes for each letter/glyph? If so, I would recommend \"baking\" the glyphs into a vertex/index buffer at run-time and then inserting the entire mesh as a single node. So for instance, you would read in the text, then generate the mesh for a single page, and add that page to your scene as a single mesh (that you can then modify as if it is a static mesh). Alternatively, you can simply generate a large image with all the text on it using our Text APIs and simply apply it as a texture/Image. But the latter would take up more memory as you are effectively creating a full-sized texture each time. Jaap, thanks for your time! That\u2019s exactly the case - at the moment, I render each glyph as a node, and then flatten() them into a single geometry. I have a large texture method too, and it does work, but indeed has the memory issue you\u2019re mentioning. The flatten() has the advantage that is actually seems to create that mesh from the nodes, but I think it does it in a non-optimized-for-my-use-case-way. If I were to generate the mesh, is there something that I could use as a sample to convert a node\u2019s general presentation / backing geometry to a mesh? I apologize if that\u2019s a vague question. Essentially, I\u2019m looking to see if there\u2019s some intermediary step I can take that would allow me to migrate from the patterns I have now, even if the translation would be slow, just to build out some observations to make better hypotheses from. That would be something like, \u201ca function that takes a parent node and generates a mesh\u201d or something along that. Also, at the moment I \u2018hot swap\u2019 between the glyphs and the flattened nodes to get that boost in FPS. I need to keep the nodes around, though, because they\u2019re actionable individually - updating positions, backing contents, that kinda thing. It seems like I\u2019d be able to do the same thing with a mesh by dynamically updating the mesh itself for things like positioning, and then figuring out how to map texture coordinates to those dynamically. I was even looking at \u2018textureCoordinates\u2019 as a potential option, but that exists on geometry , and all of my nodes with the same glyph share a geometry, again to keep total counts down. Hi <@U03JRPTDF6U>, sorry for the late reply, as we closed down the Q&A, but if you want to discuss this more, please drop by the Q&A on Friday, or sign up for a Developer Lab, which is a 1:1 in-depth chat with one of our developers! Just to quickly reply to your question, I do agree that you probably want to do this manually to remove a lot of the overhead. I'm not sure how familiar you are with mesh representations, but you should probably be able to do this by effectively writing your own \"flatten\" function. Without knowing where most of the performance is lost, I would suggest reading the nodes data manually and then composing the vertices for the mesh yourself. This would require you to manually create vertex and index data for your mesh and then upload and bind these to your Mesh. I would suggest putting your question up in the Labs and indicate this is a SceneKit question - I think those folks will be better equipped to point out the best path :)","title":"Hey there! I have been working with a highly programmatic SceneKit nodes and geometries (I know, I know, old hat), and I have been suggested by some friendly helpful developers at the WWDC Apple Park event (Hello again, Rintaro!) that a pathway to help some of my performance problems would be to 'rewrite' some of my core components with ARKit primitives, or perhaps Metal. Unfortunately, I don't really know where I would begin that conversion, or even if doing the work would net the performance benefits I'm seeking. The context of this question is my AR/VR visualization project which uses 10's of thousands of small geometries and nodes (shared as much possible) to render individual text glyphs like a sheet of paper (http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat). A good, solid first step would be: What would be a suggested migration pattern from SCNNode/SCNPlane/UIImage to similar Metal/ARKit patterns?"},{"location":"wwdc22/graphics-and-games-lounge.html#with-metalfx-are-there-any-concerns-or-issues-when-dealing-with-transparencyvolumetrics-when-doing-temporal-upscaling-would-it-sometimes-be-better-to-use-spatial-upscaling-and-msaa-in-some-cases-with-a-lot-of-transparency","text":"Temporal AA requires full motion and depth information to produce the best result. Transparent objects usually do not provide that, so TAA will have insufficient input data to process everything correctly. Similar logic applies to reflections. The choice will depend on your particular setup of the rendering pipeline. You could apply temporal upscaling to lower resolution color buffer with opaque objects only and then render transparent objects and volumetrics on top (in output resolution). That makes sense, thanks. Would the upscalers also work for targets other than the final tone mapped image, ie an AO pass? Yes, you can do that. Just allocate separate scaling effect instances for those.","title":"With MetalFX are there any concerns or issues when dealing with transparency/volumetrics when doing temporal upscaling?  Would it sometimes be better to use Spatial upscaling and MSAA in some cases with a lot of transparency?"},{"location":"wwdc22/graphics-and-games-lounge.html#can-screencapturekit-be-used-by-two-apps-at-once-each-capturing-all-or-parts-of-the-screen","text":"Hi Matt! Fantastic question! ScreenCaptureKit allows for any number of active streams from any number of clients capturing any combinations of filters! Great thanks.","title":"Can ScreenCaptureKit be used by two apps at once, each capturing all or parts of the screen?"},{"location":"wwdc22/graphics-and-games-lounge.html#is-system-level-permission-required-even-if-an-app-only-wants-to-capture-itself-ie-only-its-own-windows-and-not-anything-external","text":"Hi Michael! Yes, system level permission (one time) is required for the initial time any applications wants to use ScreenCaptureKit. Even if the application is capturing itself, it'll need to get the SCShareableContent list in order to create a filter, which requires system level permission on the initial use. Gotcha, gracias!","title":"Is system-level permission required even if an app only wants to capture /itself/ (i.e. only its own window(s)), and not anything external?"},{"location":"wwdc22/graphics-and-games-lounge.html#our-video-conferencing-app-offers-screen-sharing-functionality-but-we-want-the-screen-share-video-to-exclude-certain-windows-eg-av-controls-and-floating-participant-videos-this-is-easily-solvable-with-content-filtering-in-screencapturekit-but-what-is-the-recommended-way-to-get-this-behavior-with-older-versions-of-macos-that-do-not-support-screencapturekit","text":"You can use the screen capture portion of ScreenCaptureKit as of 12.3, and send it to other users using earlier versions of macOS. In order to get the maximum screen capture performance with ScreenCaptureKit, you'll need 12.3 or later. To achieve similar behavior in previous versions of macOS, you can use the CGWindow API to capture video, but not audio. Specifically, CGWindowListCreate() and CGWindowListCreateImage(). They\u2019re much slower than the other APIs, but available back to earlier MacOS versions.","title":"Our video conferencing app offers screen sharing functionality, but we want the screen share video to exclude certain windows (e.g., A/V controls and floating participant videos). This is easily solvable with content filtering in ScreenCaptureKit, but what is the recommended way to get this behavior with older versions of macOS that do not support ScreenCaptureKit?"},{"location":"wwdc22/graphics-and-games-lounge.html#if-you-want-to-display-the-interactions-with-menu-at-the-top-of-macos-does-that-constrain-you-to-just-capturing-a-display-vs-application-or-window-in-order-to-show-that","text":"You can use init(display:including:exceptingWindows:) , but this will capture the surface the size of the desktop. The app will be positioned as it is in the desktop, and the menu bar will be at the top of the capture. Ah, brilliant! Thank you!","title":"If you want to display the interactions with menu at the top of macOS, does that constrain you to just capturing a display (vs. application or window) in order to show that?"},{"location":"wwdc22/graphics-and-games-lounge.html#i-make-media-playback-systems-but-havent-really-yet-played-with-producing-media-could-you-give-me-an-idea-of-what-the-next-steps-would-be-in-terms-of-macos-frameworksapi-usage-to-go-from-screencapturekits-sample-buffers-to-say-a-playback-ready-h264hevcaac-hls-output","text":"Hi Michael! ScreenCaptureKit gives samples back as CMSampleBuffers. So if you wanted to make it playback ready, I would suggest you use AVAssetWriter to create a movie. This should allow you create a movie with the output you wish For HLS, you're going to use VideoToolBox to fragmented mp4 files that you can then upload to the server and update your HLS playlist And if you need a CBR-based stream, Ventura added hardware-encoding-based H.264 and HEVC CBR support, so you can offload the work from the CPU. <@U03H3GXFRSB> This is incredibly helpful! You, in two paragraphs, have just demystified something for me that has, honestly, just been a bit of an \u201cI can\u2019t do that\u2026\u201d topic so far :ok_hand::skin-tone-3: <@U03HF6TL5L5> Ah, as in; pre-Ventura H.264/HEVC hardware-encoding was VBR-only, or? Correct! I think it's technically ABR (instead of VBR), but I'm no encoding expert, so grain of salt.","title":"I make media /playback/ systems, but haven't really yet played with /producing/ media\u2013 could you give me an idea of what the next steps would be (in terms of macOS frameworks/API usage), to go from ScreenCaptureKit's sample buffers, to, say, a 'playback-ready' H264/HEVC+AAC HLS output?"},{"location":"wwdc22/graphics-and-games-lounge.html#when-capturing-windows-only-is-the-windows-drop-shadow-captured-or-only-the-actual-frame-of-the-windows","text":"Using init(desktopIndependentWindow:) , you will get back an IOSurface that is the size of the window exactly. No drop shadow included. Oh well. Thanks This is definitely a lesser-used feature in the existing CGWindowList API, but it would be great to see it in ScreenCaptureKit, as well. Please submit a feedback request for this! Sounds like a good idea.","title":"When capturing windows only, is the window's drop shadow captured, or only the actual frame of the windows?"},{"location":"wwdc22/graphics-and-games-lounge.html#would-be-great-to-be-able-to-make-screen-captures-either-screenshots-or-screen-recordings-in-hdr","text":"Thanks Chris! Please file a feedback in the feedback assistant for new features for ScreenCaptureKit. We're super excited about it, and we hope it can meet all of your needs. Will do; thanks!","title":"Would be great to be able to make screen captures (either screenshots or screen recordings) in HDR!"},{"location":"wwdc22/graphics-and-games-lounge.html#one-thing-that-wasnt-totally-clear-to-me-from-the-videos-obviously-audio-capture-is-per-application-since-audio-is-not-associated-with-a-window-if-i-exclude-only-some-windows-of-an-application-from-capture-does-that-mean-i-dont-get-any-audio","text":"If you\u2019re using display capture, and exclude any window of an app, then you will get system audio minus the whole app\u2019s audio. We also will cover this and other scenarios in our advanced talk \u201cTake ScreenCaptureKit to the Next Level\u201d :slightly_smiling_face: Thanks. I think this is fine for my typical use-case, but might be problematic for some others Another suggestion is to have a separate stream with the audio you want. If you have a particular use case or scenario you think we could address better, please file a feedback request!","title":"One thing that wasn't totally clear to me from the videos: Obviously, audio-capture is per-application, since audio is not associated with a window. If I exclude only some windows of an application from capture, does that mean I don't get any audio?"},{"location":"wwdc22/graphics-and-games-lounge.html#if-im-capturing-all-windows-from-a-single-application-will-i-also-get-xpc-hosted-dialogs-im-thinking-of-opensave-panels-and-the-like-which-were-not-easily-captured-with-the-cgwindowlist-api-since-they-are-associated-with-a-different-pid-than-the-application-windows","text":"It should capture things like the Open/Save dialog. If it doesn't, make sure to file a feedback request! I\u2019ll check, and let y\u2019all know.","title":"If I'm capturing all windows from a single application, will I also get XPC-hosted dialogs? I'm thinking of Open/Save panels, and the like, which were not easily captured with the CGWindowList API, since they are associated with a different PID than the application windows."},{"location":"wwdc22/graphics-and-games-lounge.html#just-to-reiterate-my-pledge-please-please-push-mip-bias-as-a-part-of-the-sampler-for-next-metal-releases-i-think-even-gl-had-it","text":"Please submit your request via Feedback Assistant. Sure!","title":"Just to reiterate my pledge - please please push mip bias as a part of the sampler for next metal releases. I think even GL had it :)"},{"location":"wwdc22/graphics-and-games-lounge.html#why-use-a-constant-jitter-offset-for-a-set-of-pixels-instead-of-a-random-one-that-would-probably-make-the-result-more-smoother","text":"Using a fixed set of low-discrepancy jitter offsets ensures that the space of potential jitters is covered relatively evenly for any given window in time. Thank you <@U03HJ4DLF7D> for your answer! Oh I see... You get a good representation of jitter space, even under this coupling between neighbouring offsets? Interesting...","title":"Why use a constant jitter offset for a set of pixels instead of a random one that would probably make the result more smoother?"},{"location":"wwdc22/graphics-and-games-lounge.html#wonderful-presentation-kevin-thank-you-very-much-the-technology-works-beautifully-with-slow-moving-scenes-could-you-comment-on-the-impact-of-using-temporal-information-on-quick-moving-scenes-or-quick-changing-content-should-there-be-some-adaptive-setting-with-respect-to-the-amount-of-previous-pixels-used-based-on-the-richnessrate-of-change-of-the-scenes-content","text":"Thanks. If the scene is truly moving super quick, where there's very little temporal information that can be reused, the result will simply not be as high resolution and anti-aliased. We do expect this to work as well as the amount of information that's present and for things to be more refined as motion slows. Thank you <@U03HJ4HULBC> for your answer. Yes, I guess this is expected! Fast moving content is always a challenge with these technologies... Once again, thank you for a wonderful, easy to watch and beautifully prepared presentation. (Really nice and illustrative clips! :slightly_smiling_face: )","title":"Wonderful presentation @Kevin! Thank you very much! The technology works beautifully with slow moving scenes. Could you comment on the impact of using temporal information on quick moving scenes or quick changing content? Should there be some adaptive setting with respect to the amount of previous pixels used based on the richness/rate of change of the scene's content?"},{"location":"wwdc22/graphics-and-games-lounge.html#machine-learning-has-shown-great-results-with-respect-to-upscaling-both-in-terms-of-quality-as-well-as-performance-do-you-see-implementing-such-a-functionality-in-metalfx-over-the-near-future","text":"We cannot comment on particular implementation details of MetalFX framework. Thank you <@U03H3GQDUP9> for your answer. :slightly_smiling_face: I wouldn't say using ML for upscaling is an implementation detail, but rather a radically different approach to classical upscaling methods. Of course, it has its own implementation details, should one opts to actually build it (that naturally can't be shared). However, my question was about the approach itself and wether it is under consideration (and, of course, not any specifics on its implementation). Note that the answer is yeah, machine learning is used. you can see the models at /System/Library/Frameworks/MetalFX.framework/Versions/A/Resources/brnet_v31_quant . Hopefully that clarifies things a bit. Hi <@U03J07WJLRK>! Thank you very much for the reference. I will check it out! :slightly_smiling_face:","title":"Machine learning has shown great results with respect to upscaling both in terms of quality as well as performance. Do you see implementing such a functionality in MetalFX over the near future?"},{"location":"wwdc22/graphics-and-games-lounge.html#i-couldnt-help-noticing-watching-the-no-mans-sky-clip-that-there-was-quite-a-bit-of-smearing-and-things-looked-fairly-low-quality-on-the-big-camera-move-are-these-kind-of-camera-moves-a-big-concern-with-this-technique-i-understand-taa-can-have-issues-with-smearing-and-ghosting","text":"Depending on the network connection you have, the video quality may be worse and lower in quality. Can you point to us on which time-stamp in the video you saw the smearing effect? I think I had downloaded the presentation to my iPhone and watched. The timestamp was around 23:35, though looking at it more closely it could very well be getting destroyed by video compression. Are you maybe referring to 21:35 timestamp? The session video is 22:11 in length Yes, so sorry","title":"I couldn't help noticing watching the No Man's Sky clip that there was quite a bit of smearing and things looked fairly low quality on the big camera move.  Are these kind of camera moves a big concern with this technique.  I understand TAA can have issues with smearing and ghosting."},{"location":"wwdc22/graphics-and-games-lounge.html#what-lab-is-best-for-asking-scenekit-questions-this-year","text":"I think Games technologies Q&A would be the best fit. Please look in the schedule to see when is the next session. Thank you!","title":"What lab is best for asking SceneKit questions this year?"},{"location":"wwdc22/graphics-and-games-lounge.html#what-is-the-best-way-to-render-biplanar-edr-cvpixelbuffer-from-avfoundation-in-metal-is-better-to-convert-pixelbuffer-into-mtltexture-via-compute-or-sample-from-2-mtltextures-created-from-each-plane","text":"To render EDR CVPixelBuffer contents using Metal you\u2019d need following: \u2022 Setup CAMetalLayer to accept EDR values using CAMetalLayer.wantsExtendedDynamicRangeContent property set to YES \u2022 Create MTLTexture instances that shares corresponding IOSurface from CVPixelBuffer \u2022 Use them for rendering Thanks for the explanation about EDR:) And what about performance difference between using two planes in a shader directly, and converting it to rgb beforehand? I can\u2019t predict that to be honest :slightly_smiling_face: It might completely depend on the context and use case of your particular scenario. Ok, thanks!) I\u2019ve thought there was a well known best practice:)","title":"What is the best way to render biplanar EDR cvpixelbuffer from avfoundation in Metal? Is better to convert pixelbuffer into mtltexture via compute, or sample from 2 mtltextures created from each plane?"},{"location":"wwdc22/graphics-and-games-lounge.html#whats-the-best-way-to-build-a-full-screen-game-experience-on-the-mac-should-we-still-be-using-cgdisplaycapture-or-the-appkit-full-screen-window-support-both-seem-to-have-pros-and-cons-for-example-appkit-allows-the-menu-bar-to-pop-and-for-easy-access-but-cgdisplaycapture-can-use-the-area-around-the-notch","text":"Using the AppKit APIs will give your users the experience they expect, especially if they have customized Spaces. Use presentation options to control things like the menu bar behavior: https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions|https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions Thanks! I'm having a few issues with presentation options but that seems like a lab I should book for later. :smiley: Yes, please do! Regarding the display shape, look into safe area insets: https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets|https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets I just confirmed with the responsible engineering team that as long as nothing else is drawing atop your full screen NSWindow, you will be on the fastest-performance drawing path through the compositor. Thanks. Is there a way to display in the insets using NSWindow's toggleFullScreen? We don't need to draw in the insets, but wanted to try it as a possible full screen style. You can always draw within the full size of your window. The safe area insets are informative about places where your drawing might be clipped. Thanks. It's possible I've configured my layout to be constrained into the safe area. I'll check it out!","title":"What's the best way to build a full screen game experience on the Mac? Should we still be using CGDisplayCapture? Or the AppKit full screen window support? Both seem to have pros and cons. For example: AppKit allows the menu bar to pop and for easy access. But CGDisplayCapture can use the area around the notch."},{"location":"wwdc22/graphics-and-games-lounge.html#hello-everybody-is-there-any-sample-or-instructions-how-to-make-offscreen-render-using-metal-for-example-i-have-a-scene-and-i-want-to-save-it-as-png-file-how-can-i-do-it-can-i-control-resolution-of-saved-file","text":"To offscreen render a scene and save it as a PNG file, you may first render the scene into a MTLTexture with the desired resolution. The texture will need to be a <https://developer.apple.com/documentation/metal/mtltextureusage/1515701-rendertarget |render target>. Then there are a few different ways to get a CGImage from the MTLTexture: A quick way will be to <https://developer.apple.com/documentation/coreimage/ciimage/1437890-init |initialize a CIImage> with the MTLTexture and then <https://developer.apple.com/documentation/coreimage/cicontext/1437784-createcgimage |use a CIContext to create the CGImage>, which you can use to write to disk. Oh, thank you so much for the answer!!!","title":"Hello everybody! Is there any sample or instructions how to make offscreen render using Metal? For example: I have a scene and I want to save it as png file. How can I do it? Can I control resolution of saved file?"},{"location":"wwdc22/graphics-and-games-lounge.html#can-you-leverage-the-screen-capture-api-to-cast-data-that-is-being-rendered-off-screen-eg-two-cameras-in-a-scene-one-camera-view-is-your-ipad-screen-another-camera-view-would-be-streamed-to-apple-tv-each-camera-are-viewing-different-location-but-in-the-same-scene","text":"Hi Simon, Can you elaborate a bit on what you mean by cameras and scenes? Do you mean a scene inside an app/game? Do you mean physical cameras capturing multiple feeds at once and aggregating them somewhere? Scene as in a level in a game or asset in a 3D engine (MTKView), Cameras would be digital viewports/view frustums Think Blender scene, with digital cameras SCK capture comes from the display pipeline, so content thats captured must have gone through that to be eligible. Does that make sense? a yeah it does, so that would not work then. Will have to read up some of the docs. Thanks for the info :slightly_smiling_face: Yeah it sounds like you were trying to \u2018save\u2019 doing some computations, but that won\u2019t work for this scenario :smiley: cut corners where you can :smile: it makes sense if you were trying to show say, \u2018behind me\u2019 or maybe a companion VR viewer for the non immersed person, etc, so we feel ya :smiley: Its an interesting use case, feel free to log a Feedback requesting an answer for the high level problem so we can take a look !","title":"Can you leverage the screen capture API to cast data that is being rendered off screen? eg two cameras in a scene, one camera view is your ipad screen, another camera view would be streamed to apple TV. each camera are viewing different location but in the same scene."},{"location":"wwdc22/graphics-and-games-lounge.html#im-sorry-i-wasnt-able-to-watch-the-video-so-just-wanted-to-confirm-with-screencapturekit-were-able-to-capture-the-mixed-system-sound-or-filter-in-or-out-the-applications-that-we-want-or-dont-want-to-include-right-for-example-capture-sounds-from-my-own-application-zoom-call-already-mixed-for-us-basically-replacing-things-like-blackhole-multi-outputdevice-etc","text":"Yes, absolutely! In this case, you might want to use init(display:excludingApplications:exceptingWindows:) , where you can capture all audio, excluding the applications that you don't want in the audio.","title":"I'm sorry, I wasn't able to watch the video, so just wanted to confirm: with ScreenCaptureKit we're able to capture the mixed system sound, or filter in or out the applications that we want or don't want to include, right? For example capture sounds from my own application + Zoom call already mixed for us, basically replacing things like Blackhole + Multi-Output/Device etc?"},{"location":"wwdc22/graphics-and-games-lounge.html#carenderer-was-not-previously-available-on-ios-only-macos-but-xcode-14-and-docs-on-the-web-now-show-it-as-available-since-ios-20-httpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minorhttpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minor-is-this-a-mistake-or-is-it-now-available-to-older-devices-if-built-with-the-ios-16-sdk","text":"It\u2019s not a mistake, and the availability annotations are correct! CARenderer is available on all devices. You can back-deploy your code built with the iOS 16 SDK to devices running older OSes. That\u2019s great news! (But no wonder I was so confused yesterday..!) Thanks Does the Overview need a bit of editing then? > For real-time output you should use an instance of NSView to host the layer-tree. Good catch! Please file Feedback about the documentation :slightly_smiling_face:","title":"CARenderer was not previously available on iOS (only macOS). But Xcode 14 and docs on the web now show it as available since iOS 2.0!  https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor|https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor  Is this a mistake, or is it now available to older devices if built with the iOS 16 SDK?"},{"location":"wwdc22/graphics-and-games-lounge.html#what-is-the-fastest-way-to-get-something-on-screen-with-metal-is-there-way-to-draw-directly-to-screen-instead-of-metal-layer-contents-and-then-wait-for-frame-composing","text":"The fastest way to get your drawing on-screen is to create a full-screen CAMetalLayer. You can also optionally set displaySyncEnabled to false on your layer to render faster than the display\u2019s refresh rate at the cost of potential artifacts. Be sure to take advantage of a ProMotion display if the user has it, too! You can set a minimum and maximum CADisplayLink callback rate using a CAFrameRateRange, for instance.","title":"what is the fastest way to get something on screen with Metal? is there way to draw directly to screen instead of metal layer contents and then wait for frame composing?"},{"location":"wwdc22/graphics-and-games-lounge.html#might-anyone-be-able-to-speak-to-whether-it-makes-sense-to-combine-indirect-command-buffers-httpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjchttpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjc-with-bindless-rendering-and-dynamic-data-specifically-i-am-trying-to-avoid-a-lot-of-draw-calls-with-the-soc-architecture-it-seems-passing-data-is-not-really-expensive-so-i-figure-per-frame-i-could-just-specify-some-parameters-and-generate-draw-calls-on-the-gpu-as-in-the-example-broad-question-i-realize-but-id-appreciate-some-thoughts-on-how-to-reduce-draw-calls-for-rapidly-updated-andor-deleted-vertex-data-i-also-checked-mesh-shaders-but-i-think-those-are-overkill-for-my-use-case","text":"Hi Karl! Metal 3 adds support for starting raytracing work from ICBs. ICBs give you more flexibility at rendering time, but this flexibility does come with a cost that will vary according to your specific use case. For example ICBs will give you the edge when you are CPU-bound. As with all things related to performance, we recommend you profile your game (or app) :slightly_smiling_face: Hi! Thanks for the reply, and I agree I should always profile. To clarify, I\u2019m not doing raytracing, so perhaps this is pre-optimization, but while WWDC is happening, I thought I\u2019d ask a follow-up question: Since you mention ICB-use cost, what would you say the common tradeoffs are with ICBs? Also, do you imagine them working well with bindless rendering? For example, let\u2019s many textures and my entire scene in a buffer within an argument buffer. Would it then make sense to use an ICB to issue draw calls on the scene stored that way? However, I suspect that ICBs and bindless are orthogonal. Thanks again for your time!","title":"Might anyone be able to speak to whether it makes sense to combine indirect command buffers (https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc|https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc ) with bindless rendering and dynamic data? Specifically, I am trying to avoid a lot of draw calls. With the SOC architecture, it seems passing data is not really expensive, so I figure per-frame I could just specify some parameters and generate draw calls on the GPU, as in the example.  -broad question, I realize, but I'd appreciate some thoughts on how to reduce draw calls for rapidly-updated and/or deleted vertex data.  I also checked mesh shaders, but I think those are overkill for my use case."},{"location":"wwdc22/graphics-and-games-lounge.html#when-will-the-activity-feed-be-available","text":"The activity feed is available today in iOS/iPadOS/tvOS 16 beta and macOS Ventura beta. :video_game::star2:","title":"When will the activity feed be available?"},{"location":"wwdc22/graphics-and-games-lounge.html#hello-wave-playing-around-with-the-new-screencapturekit-im-experiencing-problems-with-tcc-revoking-access-to-screen-recording-while-the-apps-running-domain-comapplescreencapturekitscstreamerrordomain-code-3801-message-the-user-declined-tccs-for-application-window-display-capture-confusingly-when-my-app-launched-it-did-have-the-proper-access-anyone-around-knowing-under-which-special-circumstances-the-system-may-suddenly-revoke-access-to-screen-recording-for-an-already-running-app","text":"There shouldn\u2019t be any reason why an app has their Screen Recording tcc revoked - besides the user denying the app permission. Could you please file a feedback request, with full steps to reproduce the issue? :slightly_smiling_face: That was my impression, too. Thanks for clarifying, <@U03HF6TK2J1>! Unfortunately, there doesn't appear to be a clear pattern to this. Usually, it just takes some time of the app sitting around to get the revocation suddenly, maybe an hour or two. Does it have anything to do with the process being a Service Management Login Item and LSUIElement=1 background process? It's hard for us to say for sure. But in lieu of a reproducible case, a sysdiagnose shortly after the time of the issue should help us figure out what might have caused it! I see, well then I'll file the bug report, thank you. Amazing new API by the way, I'm sure lots of developers are going to benefit greatly from it. :slightly_smiling_face: Glad you like the new framework!","title":"Hello! :wave: Playing around with the new ScreenCaptureKit I'm experiencing problems with TCC, revoking access to Screen Recording while the app's running  (domain: com.apple.ScreenCaptureKit.SCStreamErrorDomain, code -3801 message: \"The user declined TCCs for application, window, display capture\"). Confusingly, when my app launched it did have the proper access.  Anyone around knowing under which special circumstances the system may suddenly revoke access to Screen Recording for an already running app?"},{"location":"wwdc22/graphics-and-games-lounge.html#hey-all-question-is-related-to-scenekit-uiview-swiftui-view-layers-there-has-been-some-historical-functionality-allowing-the-layers-of-views-not-presented-in-a-typical-hierarchy-to-be-presented-as-the-material-contents-of-geometries-like-scnplanes-one-example-is-this-repo-here-httpsgithubcomsarangborudeswiftuiarviewhttpsgithubcomsarangborudeswiftuiarview-unfortunately-the-technique-here-doesnt-seem-to-work-anymore-a-couple-of-years-later-im-wondering-what-might-be-possible-to-bring-this-back-to-usage-as-i-have-a-ton-of-use-cases-id-love-to-explore-with-this-thank-you-so-much-as-always","text":"Hi <@U03JRPTDF6U> I'm not seeing why SceneKit + SwiftUI shouldn't be working in this situation still. If you have a moment and can submit to https://feedbackassistant.apple.com and reference that GitHub project, it would be great for us to get this to the right folks inside Apple. If you do this soon and can post the FB number back here, that would help me make sure it routes. <@U03H3GPSE6B> Heya, and thanks for the good news, haha! I\u2019ll file it right now and add some more information about what I\u2019m trying to do. Also, this repo is a great resource as a sample, but I\u2019m curious if there might be some tighter documentation about how this is working? For example, there\u2019s some fun magic happening in the hosting of the controller and when it gets added, and I think at least part of my issue is I\u2019m not mimicking the pattern correctly. Also, here\u2019s that Feedback! FB10141722 thank you! Of course. And hey, in the mean time, might you know of any docs that might be related to this? I\u2019m thinking of giving this another go soon, hehe. Happy to sit back and relax though to hear from the Feedback side :smiley:","title":"Hey all! Question is related to SceneKit + UIView / SwiftUI view layers. There has been some historical functionality allowing the layers of views not presented in a typical hierarchy to be presented as the material contents of geometries like SCNPlanes. One example is this repo here: https://github.com/sarangborude/SwiftUIARView|https://github.com/sarangborude/SwiftUIARView . Unfortunately, the technique here doesn't seem to work anymore a couple of years later. I'm wondering what might be possible to bring this back to usage, as I have a TON of use cases I'd love to explore with this! Thank you so much as always!"},{"location":"wwdc22/graphics-and-games-lounge.html#for-a-relatively-simple-game-low-poly-graphics-simple-textures-no-ai-which-framework-would-you-suggest-i-use-to-develop-in-swift-all-logic-would-be-custom-so-no-need-for-scripts-library-or-similar-features-scenekit-seems-like-an-obvious-choice-as-it-has-the-functionality-i-need-but-im-not-sure-how-actively-it-is-still-being-maintained-and-supported","text":"Hi <@U03HMCD9UQ7> If SceneKit is a good fit for your needs, it's a great choice! It is still being maintained and supported. If in the future you outgrow its functionality, you will have learned a lot of key parts of dealing with transforms, models, and geometry which you can use if you want to drop down to Metal and ModelIO, or move up to a higher-level framework or tool. On the topic of maintained and supported, should we assume SpriteKit is also still supported and maintained? Main reason I am asking is because I have submitted quite a few SpriteKit-related issues through Feedback Assistant, and I have not received any feedback at all on any of them. I dearly hope so as SpriteKit is core to my app. I add touches and logic and page layout on top but it's all SK, all the way down. Hoping for an answer to this, I will tag <@U03H3GPSE6B> . Nat, can you please check the previous 3 replies here, regarding SpriteKit? Thanks. https://github.com/AndyDentFree/SpriteKittenly is where I post my SpriteKit explorations of tech, following my philosophy of having lots of little test apps, as useful regression checks. Sorry for the slow reply - yes SpriteKit is still maintained and available! Thanks Nat. This is good news. I hope for maybe a little bit more maintaining in the future, maybe some of the reported bugs in the Feedback Assistant can be looked at. Thanks again! Hey <@U03JZ2H3NNR> do you also publish your bugs in somewhere open to the rest of us non-Apple employees to see? I ran into a doozy with SpriteKit that was (strictly speaking) my bug but was encouraged to lodge a Feedback when I discussed with an engineer during the Tech Talks last year, as that is such weird behaviour we should probably try to fix it anyway . (:blush: at realising it's been a few months & still haven't lodged that). You can read the entire saga https://medium.com/touchgram/oops-hitting-a-5yo-apple-bug-17d2703519f4 Given my massive dependency on SpriteKit I'd like to know about any and all lurking gremlins :pray: <@U03JELM0ZNV> Yes, I usually publish them on both Dev Forums and StackOverflow. You can see some of them here: https://developer.apple.com/forums/profile/calin I have more but to be honest I kinda gave up reporting them because, like I said, I am not getting any feedback, so it seems like a waste of my time, as it does take some time to file a properly documented bug report. Instead I am simply trying to work around them. I was thinking to maybe try on Twitter too. My twitter is @upurupu if you want to keep in contact. I'll definitely read your Medium post. For example, one bug I was hoping will get fixed (maybe it was, haven't tried it yet), is that isPaused , when passed to a SpriteView , doesn't seem to affect the state. See https://stackoverflow.com/questions/69610165/spriteview-doesnt-pause-scene-on-state-change/69610906#69610906 Great, will stay in touch. I am very active on Twitter as @andydentperth I have yet to get into SwiftUI as my app is iOS12+ so all UIKit but am just on the cusp. Thanks to feedback this WWDC will be trying to host SpriteKit for some screens. Just got a demo going with SwiftUI working inside an iMessage app extension at https://github.com/AndyDentFree/im-plausibilities/tree/master/imUrlDataAppSUI","title":"For a relatively simple game (low poly graphics, simple textures, no AI), which framework would you suggest I use to develop in Swift? All logic would be custom, so no need for scripts library or similar features. SceneKit seems like an obvious choice as it has the functionality I need, but I'm not sure how actively it is still being maintained and supported?"},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-a-way-to-send-a-raw-mtltexture-or-a-specific-view-to-screencapturekit-or-some-lower-level-api-related-to-it-for-example-lets-say-i-have-a-real-time-graphics-application-written-using-metal-and-i-want-to-stream-content-containing-only-audience-relevant-elements-in-my-scene-rendered-to-one-texture-i-broadcast-that-texture-in-another-renderpass-i-composite-some-diagnostic-info-only-relevant-to-me-locally-that-i-dont-want-to-share-with-others-how-might-this-be-achieved-its-a-use-case-i-keep-running-into-asymmetric-views-and-perspectives-going-even-further-maybe-i-am-developing-a-game-and-i-want-to-render-a-completely-different-perspective-in-my-scene-from-a-different-camera-i-send-that-texture-to-the-stream-but-maybe-i-render-from-a-different-perspective-locally","text":"Hi Karl, SCK is dependent on the display pipeline so the content needs to be rendered before it can be captured. Ah, I see, so it sounds like I\u2019d need to roll my own solution for something along the lines of what I described. We discussed this a little bit yesterday too, in https://wwdc22.slack.com/archives/C03H77PER5G/p1654728707927069 You should log a https://developer.apple.com/bug-reporting/|feedback detailing your use case so we can consider it for future improvements! Will do! <@U03HWP445CH> Thanks for the link, I was to late yesterday to ask where I should submit the use case <@U03HWP445CH> <@U03HZ4EJJ05> I wrote this hopefully compelling argument: FB10143711 (ScreenCaptureKit Suggestion: Selective streaming of textures for Asymmetric Views) Thanks! We've got it","title":"Is there a way to send a raw MTLTexture or a specific view to ScreenCaptureKit, or some lower-level API related to it? For example, let's say I have a real-time graphics application written using Metal, and I want to stream content containing only audience-relevant elements in my scene, rendered to one texture.  I broadcast that texture. In another renderpass, I composite some diagnostic info only relevant to me locally that I don't want to share with others.  How might this be achieved? It's a use case I keep running into (asymmetric views and perspectives.)  Going even further, maybe I am developing a game and I want to render a completely different perspective in my scene from a different camera. I send that texture to the stream, but maybe I render from a different perspective locally."},{"location":"wwdc22/graphics-and-games-lounge.html#would-screencapturekit-be-appropriate-for-a-remote-desktopscreen-sharing-application","text":"Hi <@U03HZ4HF31T>, yes it would be appropriate! Did you have some specific scenarios you were concerned it might not accommodate? Just looking at something that would provide better performance than vnc (not difficult!) and add audio for remote working on a mac. Obviously keyboard/mouse input would need to be handled seperately. Screen sharing is not quite the same use case as bi directional content flow. Handling where the mouse is, capturing input and passing it back would require a not-insignificant amount of work To split your question, it would be appropriate for screen sharing but more challenging for VNC where you want to interface with the source from the destination Since audio was mentioned too, I'm assuming it's still a quite tough challenge to be able to capture the system's audio, right? May require writing an own low-level driver for it? No, you can get system or app audio! ScreenCaptureKit can include system audio as well I believe. Ahh beat me to it! wait what...? :flushed: I must've missed that part of the API You can include or exclude apps and capture all system audio. https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955572-excludescurrentprocessaudio The session also covers these use cases with specific examples https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955571-capturesaudio Given the amount of work required, I may provide feedback and hope/pray that the built in macos remote desktop will add audio! <@U03K0FHV4M8> Its much requested and long awaited, we are thrilled its surprising and delighting you :smiley: amazing! it doesn't allow capturing only the audio from one app though, does it? It does I believe. Yes, it allows you to specify just a single apps audio as well. <@U03HF6TL5L5> covers this use case specifically near the end of the session If you wanted to say, exclude a games audio and include your friends chat audio from another app, SCK would support that use case If you think about a capture session as being \"audio, video, or both\" you can setup a capture for a process and just drop the frames, making it an \"audio only\" capture. I'm blown away by this. We've all been waiting soooo long for this and now it's finally here. :pray::heart: If you don't set capturesAudio you have a video only capture and if you set it, you get both! really cool! thanks for all the explanations :stuck_out_tongue: The framework looks great. Congratulations on your hard work! And thanks for your help. Then its just how you want to combine \"including\" or \"excluding\" the calls like https://developer.apple.com/documentation/screencapturekit/sccontentfilter/3919807-init that lets you narrow in on what you want depending on your use case Are the docs going to be updated soon or why am I missing these audio APIs you're talking about there somehow? There is a sample project and videos that should help. When you go to https://developer.apple.com/documentation/screencapturekit and at the bottom search for \"Audio\", do you not see those results? The https://developer.apple.com/documentation/screencapturekit/capturing_screen_content_in_macos|sample app also includes Audio Well, I can see the functions about whether to capture audio, exclude the current process and the audio stream output type but not how I would exclude/include certain apps ah wait, those are not part of the ScreenCaptureKit API? Audio capture follows similar rules to video capture, however audio capture is at an application level. So your selection will have to be application-based. <@U03K0FHV4M8> You'd use the SCContentFilters for that init(display:excludingWindows:) or init(display:excludingApplications:exceptingWindows:) ahh, I think I got it now :grin::see_no_evil: The team notes \"Excluding a window will exclude all of that apps audio\" as a FYI that was the missing info, thanks! Yeah. I think Meng talks about this a bit in the \"Take ScreenCaptureKit to the next level\" WWDC talk, if you want to double check your understanding. Will definitely do","title":"Would ScreenCaptureKit be appropriate for a remote desktop/screen sharing application?"},{"location":"wwdc22/graphics-and-games-lounge.html#i-have-a-classic-dumb-question-is-screencapturekit-the-macos-equivalent-of-replaykit-i-want-people-to-be-able-to-create-reaction-videos-of-them-playing-touchgrams-which-is-running-on-spritekit","text":"Hi Andrew! No question is \"dumb\"! So ReplayKit is also on macOS! but there are a lot of differences between ReplayKit and ScreenCaptureKit. ReplayKit has several features that aren't in ScreenCaptureKit, including broadcast extensions, Clips, and built in movie recording for in app use case Where ScreenCaptureKit differs is in its ability to capture far more content. ScreencaptureKit allows more than just in-application capture. It was designed to give you performant capture with custom filters for applications outside of your own So if you wanted someone to be able to capture video of their reactions (maybe using an iPhone clipped to the top of their laptop :wink: ) as well as playing a game experience, which would you use? ReplayKit has built in support for camera! so if you wanted to record just the singular application and their camera, you can do that with replaykit you could also do the same with screen capture kit as well! using AVCaptureDevice to get camera frames, you can get a custom filter of the content you want (more than 1 application) and then get the frames from the camera and using metal you can put them together and render that as well :smiley: Though you'd need something else for the camera capture part if you were using SCKit. Right, Johnny? correct <@U03HF6TL5L5>! you'd have to run an AVCaptureDevice (camera) to get frames from there All the user experience would be happening inside SpriteKit with audio playing via AVAudioPlayer (audio is in response to user actions, gestures, tapping parts of message, whatever has been specified by the original message author - it's user-generated content so utterly random) Is that likely to that break recording? not at all! ScreenCaptureKit and ReplayKit allows you to capture the applications audio, even if its played via AVAudioPlayer you'll get the application audio (I should just stop before the outstanding issues count on the tracker of every bright idea ever passes 1K) but is this robust with multiple players? Someone can have one or more background looping sounds start on entering a page and then have more short ones in reaction to touches - feels like I'm going to be pushing boundaries. I have a suspicion that making it easy for people to share videos of their play experience could be a huge accelerator, so this tech niggles at me :star-struck: is this local multiplayer? or the multiplayer instance played remotely? \"Multiple players\" is multiple sound players, sorry for the ambiguity. Not game players ahh got it, okay, and are these sound players coming from the same process? and by same process, I mean the same application process to which you are attempting to capture the video ? ReplayKit will only capture your applications audio, so if your application has several avplayers instances but its all in the process, ReplayKit will capture that audio for you Yes - Touchgram runs on top of SpriteKit with a document model where the message sender composes the experience, which can be utterly driven by what they want to do - think of it as a game authoring toolkit - meets Keynote - inside a message. The playback experience might be the person sending the message or, more likely, the receiver. Currently (deep breath), playback is only within iMessage as an app extension, which may complicate the ReplayKit scenario. However, it's relatively trivial to also have playback in the accompanying parent app and I'll be adding that soon. So inside one process is kinda true but as an app extension may have nuances ahhh okay, so if its coming from an app extension, then it will not get captured, as extensions are run as a separate process App extensions should have their own process, and that won\u2019t be captured it sounds like your application is built for iOS? It's an iMessage app extension but all the playback stuff plays on macOS as well. Although it's touch-driven, it's single-touches most of the time so is usable via mouse I'm planning to move to SwiftUI so started considering a Mac version :innocent: ahh okay! so ScreenCaptureKit is only for macOS it looks like the best fit for your application is in fact ReplayKit, you'll the audio from different processes / bundleID will not be captured Thanks for the clarification, and inspiration you are most welcome!! <@U03H3GXFRSB> Hey Johnny, it's Tobias from the one-on-one session today about AVAssetWriter producing corrupted files (the Teams related app). :wave: Thank you so much again for sharing your expertise with me and answering all of my questions. <@U03H3GXFRSB> I think I've figured out what the issue was. Turns out that I was attempting to append a sample buffer with the same CMTime twice due to time rounding reasons in the asset writer. Changing the timescale from 600 to something larger so far seems to have resolved the frame dropping entirely but as you've suggested I'll keep the check about whether the time of the last frame was before or equal to the current one in there just to be sure. In case you still see this, do my explanations make any sense to you? Either way, it was awesome talking to you. :nerd_face: <@U03K0FHV4M8> yay! Im sooo happy you were able to figure it out after our talk!!","title":"I have a classic \"dumb question\" - is ScreenCaptureKit the macOS equivalent of ReplayKit? I want people to be able to create reaction videos of them playing touchgrams, which is running on SpriteKit."},{"location":"wwdc22/graphics-and-games-lounge.html#i-want-to-record-the-whole-screen-but-i-keep-getting-the-microphone-active-orange-dot-in-the-recording-how-can-i-make-a-recording-of-a-screen-using-screencapturekit-without-the-orange-dot","text":"We are aware of feedback around the orange dot. ScreenCaptureKit adheres to the same system privacy and security requirements of other capture methods. OK thank you!","title":"I want to record the whole screen but I keep getting the microphone active orange dot in the recording. How can I make a recording of a screen using ScreenCaptureKit without the orange dot?"},{"location":"wwdc22/graphics-and-games-lounge.html#can-screencapturekit-capture-an-off-screen-window-as-in-if-i-wanted-to-capture-some-visuals-that-normally-show-in-a-window-1920x1080-in-my-app-but-theres-no-need-to-show-the-user-because-they-just-care-about-the-resulting-captured-output-if-so-does-that-seemingly-make-sense-as-a-use-of-sck-or-are-there-more-direct-methods-for-app-internal-view-capture-youd-suggest-instead","text":"Hi! If you mean offscreen (but rendered), yes SCKit can capture it. If you mean something more like non rendered content like we discussed https://wwdc22.slack.com/archives/C03H77PER5G/p1654817389344279|here , then no If you drag a window partially offscreen, SCK can get the \"off screen\" content in its capture. Is that what you were after? If you want what the other thread discussed, be sure to log a feedback requesting it! Yep, it was! Thanks (I just remembered the one-time permission requirement, which would probably confuse the user for my use-case, as they wouldn\u2019t expect my app to be \u2018recording the screen\u2019 as such, so I\u2019ll do some Googling on existing \u2018app-owned\u2019 window/view capture options, if any) Your own app should have access to its own content without TCC permissions","title":"Can ScreenCaptureKit capture an 'off-screen' window?\u2013 as in, if I wanted to capture some visuals that normally show in a window (1920x1080) in my app, but there's no need to show the user (because they just care about the /resulting/ captured output)  &amp; if so: does that seemingly /make sense/ as a use of SCK, or are there more 'direct' methods for 'app-internal' view capture you'd suggest instead?"},{"location":"wwdc22/graphics-and-games-lounge.html#when-i-use-sccontentfilterinitdesktopindependentwindow-in-screencapturekit-the-popup-buttons-contextual-menus-sheets-that-appear-on-the-window-are-not-captured-is-there-any-way-to-capture-them-in-screencapturekit-previously-i-implemented-using-cgwindowlistcopywindowinfooptionincludingwindow-optiononscreenabovewindow-windowid-and-cgimageinitwindowlistfromarrayscreenbounds-windowarray-imageoption-api-but-i-would-like-to-know-how-it-is-possible-in-screencapturekit","text":"Additional pop-over items like these will not be shown with init(desktopIndependentWindow:) . If you would like to capture these, you should use an application-based capture. However, please submit a feedback request for this enhancement if you would like to see these inside of init(desktopIndependentWindow:) . How would you prefer the capture handle things like context menus that exceed the bounds of the window?","title":"When I use SCContentFilter.init(desktopIndependentWindow:) in ScreenCaptureKit, the popup buttons, contextual menus, sheets that appear on the window are not captured. Is there any way to capture them in ScreenCaptureKit?  Previously, I implemented using CGWindowListCopyWindowInfo([.optionIncludingWindow, .optionOnScreenAboveWindow], windowID) and CGImage.init?(windowListFromArrayScreenBounds:, windowArray:, imageOption:) API, but I would like to know how it is possible in ScreenCaptureKit."},{"location":"wwdc22/graphics-and-games-lounge.html#id-like-to-use-reality-composer-in-the-new-xcode-to-generate-simple-usdz-shapes-then-use-the-new-reality-converter-to-re-texture-these-shapes-because-reality-composer-cannot-add-custom-textures-in-reality-composer-you-have-to-enable-usdz-export-in-preferences-but-when-i-export-a-simple-box-to-usdz-reality-converter-cannot-open-it-conversion-failed-1-error-which-is-unexpectedunknown-is-there-a-secret-way-to-make-this-work-thanks-for-leaving-questions-open-im-in-australia-i-realise-this-not-quite-the-right-forum-for-this-but-the-right-forum-hasnt-left-questions-open-and-its-open-around-2-3am-my-time","text":"Hi, I tried to reproduce the error as you described. This was the order of steps: \u2022 Create a new project in Reality Composer using the horizontal anchor. \u2022 Turn on \"Enable USDZ export\" in the settings. \u2022 Export the project to USDZ format \u2022 Open a Finder window and double-click the exported USDZ file. If this does not work, would you file a Feedback Assistant?","title":"I'd like to use Reality Composer (in the new Xcode) to generate simple USDZ shapes, then use the new Reality Converter to re-texture these shapes (because Reality Composer cannot add custom textures). In Reality Composer, you have to enable USDZ export in Preferences, but when I export a simple box to USDZ, Reality Converter cannot open it (Conversion failed: 1 error, which is unexpected/unknown). Is there a secret way to make this work?  (Thanks for leaving questions open \u2014 I'm in Australia. I realise this not quite the right forum for this, but the right forum hasn't left questions open, and it's open around 2-3am my time.)"},{"location":"wwdc22/graphics-and-games-lounge.html#in-hlsl-texture2d-and-texture2d_array-are-treated-almost-the-same-ie-texture2d-can-be-used-instead-of-array-one-however-in-metal-the-type-is-much-more-strict-does-it-mean-that-the-only-way-in-metal-is-to-create-respected-texture-view-to-convert-types-the-texture-view-looks-to-be-performance-hit","text":"Texture views allow you to use a single texture backing with more than one format (i.e. RGBA8 and RG16). When a texture is marked with the \u201cview\u201d option this can result in lossless compression being disabled. Can you explain in a little more detail what performance hit you are seeing? I actually don\u2019t need to change format let\u2019s say my shader always accepts only texture2d<float> then to push their texture2darray texture I will need to \u2018slice\u2019 it in slices even if I need always a zero slice it works btw though validator complains and by \u2018slice\u2019 I mean I need to make X slices of texture2d type and then pass that slice the perf hit is the necessity to slice the texture and disable of tex compression Ah, so you wish to use a single slice of a texture2d array \u2013 is it the requirement in MSL of putting Texture2D or Texture2DArray that you are bumping into? yes HLSL allows you to push texture2darray into texture2d in shader https://docs.microsoft.com/en-us/windows/win32/direct3d12/resource-binding-in-hlsl the problem is that engine we\u2019re porting uses mixed texture2d and texture2d_array types all around and in shader there is just texture2d and of course validator coughs may be there is some hidden trick to push tex2d_array slice as tex2d :wink: Hold tight so we can get a proper answer! (or ask more questions :slightly_smiling_face:) sure thing the alternative is to make an array of slices for each tex2d_array I\u2019d really want to avoid this but.. array of slices obtained by textureViewing.. Do you happen to have the print out of the validation layer for your use case so I can make sure I\u2019m looking at the same path? Basically it says wow wow wow you have MTTextureType2D_Array and in shader we have texture2d Sorry indeed I don\u2019t have exact line Ha no that was perfect. So I did confirm, because there is no reinterpretation of the pixels with taking a single slice of an array, MTLTextureUsagePixelFormatView should not be required! oh nice! hope validator won\u2019t bark so you think going \u2018array of slices\u2019 is the only way :wink: the thing is - without validator it works fine! i.e. shader texture2d and tex2d_array as input it shows first slice I was thinking about the idea of having the shader always take an array, too, and just passing in a single entry array, but I don\u2019t know how your shaders might get duplicated or used. and vice versa too! So you only need to use array entry 0 ever? sometimes nope by 90% of time yes Got it. To be clear, you still need to make a Texture View with slice N, but you don\u2019t have to set the MTLTextureUsagePixelFormatView bit, all righty!! hope views are cheap it\u2019s interesting whether I can do some tricks with new arg buffers to offset the GPU address of texture Should be good performance without any interpretation. And as implied above, if there is pixel interpretation then you lose things like lossless compression (which is what the PixelFormatView option is for). You can always find out which textures don't have the lossless compression in your GPU capture, by right clicking on the table header and choosing Texture->Lossless Compression when you're in the Memory view. Normally it shows Label, Insights, Type, Allocated Size, Storage Mode, Purgeable State, CPU Access, and Time Since Last Bound > to offset the GPU address of texture If I understand correctly the intent, offsetting the GPU address of a texture won\u2019t work in most cases and it\u2019s generally not recommended. Creating views is the way to go Yeah :slightly_smiling_face: I understand it\u2019s just fun to have GPU address finally > hope views are cheap Yes, creating a view is extremely cheap: there\u2019s no memory allocation / mapping, no trip to the kernel, etc. Like in GNM Having GPU pointer writing to arbitrary addresses, having Mac borked completely.. Fun! Still waiting btw for major tech breakthrough gents on your side 2022 is not a year for Metal to be able to kill mac in 1 second :wink: and right now I can do it easily Windows manages to survive In theory we have mechanisms to recover from a GPU crash pretty quickly. Also, we have mechanisms to handle some undefined behaviors more gently so we don\u2019t even need to crash at all I\u2019m specifically talking about Apple-Designed SoCs though yeah I understand still I code Mac games all days and I use eGPU btw just because it allows me to survive GPU external crashes but indeed hard crashes are so much better analysed on Apple Socs and yeah I think eGPU with Apple Soc is a good idea :wink:","title":"In HLSL Texture2d and Texture2d_array are treated almost the same i.e. texture2d can be used instead of array one. However in metal the type is much more strict. Does it mean that the only way in Metal is to create respected texture view to convert types? The texture view looks to be performance hit :("},{"location":"wwdc22/graphics-and-games-lounge.html#ive-just-moved-from-my-old-and-hot-intel-imac-to-an-m1-max-macbook-pro-which-im-loving-one-snag-our-games-still-run-on-opengl-es-but-the-ios-simulator-on-apple-silicon-seems-to-crash-when-executing-opengl-es-commands-is-it-game-over-do-i-finally-need-to-move-to-metal","text":"Hi David! This is not expected. Please submit a report using the Feedback Assistant app and we\u2019ll take a look. Thank you!","title":"I've just moved from my old and hot Intel iMac, to an M1 Max MacBook Pro, which I'm loving! One snag - our games still run on OpenGL ES, but the iOS Simulator on Apple Silicon seems to crash when executing OpenGL ES commands. Is it Game Over? Do I finally need to move to Metal?"},{"location":"wwdc22/graphics-and-games-lounge.html#first-thanks-for-all-your-hard-work-and-cool-new-apis-i-watched-meet-distributed-actors-in-swift-and-im-curious-if-theres-currently-a-way-to-use-distributed-actors-in-a-peer-to-peer-gkmatch","text":"Hi <@U03HMD2BP55>! I believe using distributed actors with GKMatch will have to wait until Swift actors support custom executors. But I encourage you to ask the friendly folks in the Swift lounge for a more authoritative answer!","title":"First, thanks for all your hard work and cool new APIs!  I watched \"Meet distributed actors in Swift\", and I'm curious if there's currently a way to use distributed actors in a peer-to-peer GKMatch?"},{"location":"wwdc22/graphics-and-games-lounge.html#hey-there-heres-a-basic-one-with-some-context-im-a-mobile-developer-working-for-about-a-decade-in-the-space-and-i-have-very-little-experience-with-high-fidelity-graphics-and-their-libraries-i-am-highly-interested-in-learning-c-especially-to-interact-with-the-new-metal-cpp-tools-my-question-is-would-the-metal-cpp-examples-shared-here-in-this-lounge-be-a-good-starting-point-into-learning-enough-c-to-be-productive-with-metal-or-is-there-a-better-starting-point-that-may-be-a-more-gradual-or-helpful-introduction-to-the-tools-i-dont-mind-hitting-the-ground-running-so-to-speak-just-curious-to-see-what-paths-are-most-suggested-for-this-thank-you","text":"Hi - let's chat a bit. I'm curious to know what languages you already have familiarity with - do you have experience with Objective-C++? Cheers Jared! I unfortunately don\u2019t have experience with Obj-C++, but I do with straight Obj-C from the pre-Swift years, heh. I also have a glancing familiarity with -C-like languages, but that\u2019s mostly syntactic pattern recognition over function (\u201coh, that thing is accessing a pointer, that\u2019s updating a field..\u201c) Ok, interesting. Well, as I'm sure you've seen there's a ton of resource for learning C++. If you're used to Obj-C, I think you'll understand the basic concepts pretty quickly. The reason I ask is because Metal is natively implemented as an Objective-C API. What's nice about Objective-C++ is that inter-op with C++ is super easy. Basically, you just rename your .m files to .mm files and you're using Objective-C++. You can now include C++ headers and start playing immediately. Now, if you really want to use Metal-cpp I don't want to discourage you at all. It's great to see developers interested. I think it's worth noting, though, that if you want to go the 'pure' C++ route, you'll likely need to end up wrapping some of our other platform libraries which are exposed as Obj-C or Swift. Ahhhh\u2026 I\u2019m seeing the boundary lines a little more clearly now. This makes a lot of sense. It seems like the learning path would be C++ -> [metal-cpp + interop wrapping], which isn\u2019t too bad at all. I have a really, really limited scope that I\u2019d like to try out first. Like, really limited: just rendering a plain ol\u2019 plane with a bunch of backed textures that act like a text font, and using the geometry bounds and coordinates to make the final mesh of an object animated. Backing up a bit, with more context, I currently have a working prototype in straight SceneKit land where my flow is essentially render a bunch of sibling nodes, flatten the hierarchy into a single mesh, and then keep the old nodes around when I want to do things like animate highlights and movement. My hope is that by picking up even some core metal-cpp, I\u2019ll be able to reach into the rendering fundamentals to get that basic use case down, with the bonus being more power under the hood for more complicated use cases down the road :wink: You don't necessarily need to adopt metal-cpp. You could continue interacting with Metal via its native Objective-C interface, but use C++ to integrate with other libraries from the same Objective-C++ source file. Here\u2019s a quick shot: the one of the left is in \u2018flatten\u2019 mode, with a single node that was flattened from a few hundred. The one on the right is in \u2018glyph\u2019 mode, where each node is rendered separately. <@U03J3UW6LSD> That\u2019s a really good point - I could use the Obj-C interface! Since that\u2019s the core, it almost seems like it makes sense to start with that first since the cpp lib is a tight wrapper around it. I think that would also make learning Metal a little easier. We do have some metal-cpp examples, but there are years of ObjC Metal resources out there. Maybe start with ObjC Metal, then move to Metal-cpp if you want that 'all one language' feeling. I think I\u2019ll do just that. I really appreciate the words of wisdom here - I was all ready to start down a quite intensive path, haha! If I may while I have an eye or two from ya, and just off the top of your head, no pressure: what sorts of primitives do you think I should be looking into to replicate this kind of behavior like the one above? In terms of triangles, meshes, texture coordinates, animating mesh coordinates, that kinda thing. I\u2019m biased because I\u2019ve spent years working with Apple\u2019s text rendering engine, but: I recommend using an actual text engine like Core Text or TextKit 2 to render to a texture. Lots of simplifying assumptions that hold true in English don't hold true across all languages. Another really interesting option is http://sluglibrary.com|Slug , which does correct Unicode rendering entirely on the GPU. But I don\u2019t know if there\u2019s a trial option. I would recommend you to take a look at the LearnMetalCPP samples( https://developer.apple.com/metal/LearnMetalCPP.zip ). It\u2019s written in metal-cpp but it provides a series of incremental graphics samples which can help you learn graphics and Metal from scratch, e.g draw a triangle, cube, texture etc. > Core Text or TextKit 2 to render to a texture <@U03J3UW6LSD> This 1_000x over! I would absolutely love to do something like this.. at the moment, I quite literally just create a CATextLayer with a single string character and render it out into a bitmap :sweat_smile: I\u2019ll start taking a look at those two kits to see what I can break, haha. I\u2019d need to render the text, and then understand the placement of those glyphs as they\u2019re rendered to make sure I can move them individually in space. <@U03HJ4DKQRY> You got it - this was the original reason I had the thought to try it, in that it seemed like it had that gradual build up of domain knowledge, through the lens of a different language. This might be a weekend project, haha.","title":"Hey there! Here's a basic one, with some context: I'm a mobile developer working for about a decade in the space, and I have very little experience with high-fidelity graphics and their libraries. I am highly interested in learning C++, especially to interact with the new metal-cpp tools. My question is, would the metal-cpp examples shared here in this lounge be a good starting point into learning enough C++ to be productive with Metal? Or, is there a better starting point that may be a more gradual or helpful introduction to the tools? I don't mind hitting the ground running so to speak, just curious to see what paths are most suggested for this! Thank you!"},{"location":"wwdc22/graphics-and-games-lounge.html#question-re-core-media-io-extension-how-the-host-app-can-communicate-with-the-extension-can-hostextension-exchange-iosurface-for-example","text":"What kind of information are you hoping to communicate? There might be a better lounge for this question. If you're trying to send video frames, the supported way to do that is CMIOExtensionStream .","title":"Question re Core Media IO Extension. How the host app can communicate with the extension? Can host/extension exchange IOSurface for example?"},{"location":"wwdc22/graphics-and-games-lounge.html#just-want-to-say-huuuge-thank-you-to-all-xcode-gpu-devteam-you-guys-rock-i-was-testing-first-metal-gpu-capture-and-since-that-time-its-miraculous-improvements-still-pixel-history-please-make-it-happen-and-if-this-is-possible-can-apple-team-work-at-least-a-bit-with-renderdoc-author-so-we-can-replay-dx1112-traces-on-mac-its-a-very-big-taskvery-complex-but-its-super-big-pain-to-need-a-windows-with-renderdoc-to-check-stuff-if-only-renderdoc-can-run-on-mac-of-course-id-be-happy-if-it-can-replay-metal-as-well-but-its-a-different-story","text":"Hi - thanks for feedback. We'll definitely chat with the tools team about your ideas. I'm porting a game that happens to work when in a Windows virtual machine, so on my Mac Pro I keep Direct3D tooling/tracing in a VM running side by side with the Metal debugger. Really missing that workflow on Apple Silicon, but I also know there are no easy answers there. :slightly_frowning_face: Considering just Boot Camping my Mac Pro and keeping it as a remote Windows debug box when the time comes.","title":"Just want to say HUUUGE thank you to all XCode GPU dev.team. You guys rock. I was testing first metal gpu capture and since that time it's miraculous improvements.  Still ;)  Pixel history - please make it happen And if this is possible can Apple team work at least a bit with RenderDoc author so we can replay DX11/12 traces on Mac? It's a very big task/very complex but it's super big pain to need a Windows with RenderDoc to check stuff. If only RenderDoc can run on Mac. Of course I'd be happy if it can replay Metal as well, but it's a different story!"},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-any-documentation-for-the-new-metal-pipeline-script-json-format-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session","text":"Official documentation is coming soon! In the meantime here is a quick overview of creating a pre-compiled MTLBinaryArchive: First create a MTLBinaryArchive using the existing API. After that is done you can extract the Metal Pipeline JSON with: $ metal-source -flatbuffers=json theBinaryArchive.metallib -o /tmp/descriptors.mtlp-json Then you can use the mtlp-json file with metal to generate a MTLBinaryArchive offline that will work with all GPUs: $ metal shaders.metal -N descriptors.mtlp-json -o archive.metallib Here is a link to the talk for others who may be interested in more: https://developer.apple.com/videos/play/wwdc2022/10102/ I tried to extract the Metal Pipeline JSON from a harvested archive, but got metal-source: error: unsupported binary format I harvested the archive using (swift)... let lib = device.makeDefaultLibrary()! let desc = MTLRenderPipelineDescriptor() desc.vertexFunction = lib.makeFunction(name: \"vert_main\") desc.fragmentFunction = lib.makeFunction(name: \"frag_main\") desc.colorAttachments[0]?.pixelFormat = .bgra8Unorm let archdesc = MTLBinaryArchiveDescriptor() let archive = try device.makeBinaryArchive(descriptor: archdesc) try archive.addRenderPipelineFunctions(descriptor: desc) try archive.serialize(to: NSURL.fileURL(withPath: \"/Users/pwong/Downloads/x-game.metallib\")) Not sure if this is suppose to work yet on MacOS 13 beta/XCode 14 beta... also, I see that the workflow doesn't work at all without having a Metal pipeline JSON (applegpu-nt: note: [AGX] Plugin interface not implemented: AIRNTEmitExecutableImage error)","title":"Is there any documentation for the new Metal Pipeline Script JSON format?   Mentioned in the \"Target and optimize GPU binaries with Metal 3\" session."},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-a-way-to-opt-out-of-qos-to-avoid-priority-decay-on-the-render-thread-if-we-are-writing-our-application-in-swift","text":"The techniques we recommend in our tech talk on https://developer.apple.com/videos/play/tech-talks/110147/|tuning CPU job scheduling for Apple silicon games are also applicable to games written in Swift. The pthread APIs might not be the nicest to work with from Swift, but it should be possible to use them.","title":"Is there a way to opt out of QoS to avoid priority decay on the render thread if we are writing our application in Swift?"},{"location":"wwdc22/graphics-and-games-lounge.html#rendering-the-mandelbrot-set-is-an-embarrassingly-parallel-smile-task-so-it-seems-perfect-for-the-gpu-i-wrote-a-metal-fragment-shader-to-do-this-but-you-cant-zoom-into-the-set-too-far-because-metal-only-supports-float-not-double-i-am-a-novice-gpu-programmer-so-bear-with-me-but-is-there-any-way-to-do-increased-precision-math-on-the-gpu-with-metal-i-understand-that-gpus-are-generally-much-slower-with-double-precision-if-they-even-support-it-but-it-seems-like-it-would-be-useful-for-scientific-computing-at-least-thank-you","text":"Hi, thanks for your question. Since Metal doesn't support the double data type, you would have to use (or make) an arbitrary precision floating point library. Another option is to use 64-bit integers using fixed-point approaches. We would also encourage you to send a report with your use case using Feedback Assistant so the team can consider it as a future enhancement. Thanks for those suggestions, I will explore them! Also, I want to add to my answer that you could also experiment with the \"fast-math\" option that is normally turned on by default. If you disable it, you may get a little better precision in your results when calling sin, cos, sqrt, etc. It's not the same thing as a 64-bit floating point type of course, but handy if you're debugging a precision issue. Thanks <@U03J7T89SQG> , I didn't know about that option!","title":"Rendering the Mandelbrot set is an \"embarrassingly parallel\" :smile: task, so it seems perfect for the GPU. I wrote a Metal fragment shader to do this, but you can't zoom into the set too far because Metal only supports float, not double. I am a novice GPU programmer, so bear with me, but is there any way to do increased precision math on the GPU with Metal? I understand that GPUs are generally much slower with double-precision, if they even support it, but it seems like it would be useful for scientific computing at least. Thank you!"},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-a-way-to-reference-templated-compute-functions-from-a-compute-pipeline-descriptor-such-that-the-templated-type-inherits-the-bound-textures-pixel-format","text":"Hi, this sounds like an interesting question. For example sake (using non-real code), I think you are trying to ask if you can do something like template &lt;typename T&gt; void computeShader(...) { ... } in your shader code and then reference this function using something like id &lt;MTLFunction&gt; function = [defaultLibrary newFunctionWithName:@\"computeShader&lt;MyType&gt;\"]; Is that what you are asking? You can use the host_name attribute and then define your template specializations. In the https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf|Metal Shading Language Specification , section 5.1.10 and 5.1.11 show how you can change the name that Metal will use to reference the function name. For example, you could try something like: template &lt;typename MyType&gt; kernel void computeShader(device MyType* output, constant MyType &amp;argument) { *output = argument; } template [[host_name(\"computeShader_int\")]] kernel void computeShader&lt;int&gt;(device int* output, constant int &amp;argument); template [[host_name(\"computeShader_float\")]] kernel void computeShader&lt;float&gt;(device float* output, constant float &amp;argument); And then reference the functions in your app with: id &lt;MTLFunction&gt; function1 = [defaultLibrary newFunctionWithName:@\"computeShader_int\"]; id &lt;MTLFunction&gt; function2 = [defaultLibrary newFunctionWithName:@\"computeShader_float\"]; <@U03J7T89SQG> Yes, your example illustrates what I was looking for. So sounds like I'd still need to create a compute pipeline state per pixel format? For context, I was hoping to build my compute pipeline state once up front and allow a consumer of an MPS-like framework provide textures with various pixel formats. Yes, you would need to create a pipeline state for each one. Something like: template &lt;typename T&gt; void computeShader(texture2d&lt;T&gt; myTexture [[texture(0)]]) { ... } where T is inferred from texture(0) And the reason for that is so the compiler can create an optimized version for each permutation you have. Yeah, makes sense. Would be cool if a compute pipeline state could be build with a set of template parameter permutations and then it selects the correct one based on bindings In fact, the term \"shader permutations\" refers to this specialization whether you're using templates, or function constants. If the pipeline is built too late though, it could trigger the compiler and that can result in performance problems. So, if you had a choice to compile your shaders before your app begins, versus on-demand, it's preferable to do it before. I see, so without template specialization in the shader source, the pipeline would have to compile versions for each of the permutations set on the pipeline state For the use case I outlined above, would you recommend lazily constructing the necessary pipeline state objects or building all possible versions once up front? If you know that you will need to use all possible ones, then you should do it once up front. Consider a real-time graphics application with many kinds of materials. If you were to compile the pipeline state objects when they were needed, then you may get stutters while you wait for them to compile. Great, thanks for your responses, very informative!","title":"Is there a way to reference templated compute functions from a compute pipeline descriptor such that the templated type inherits the bound texture's pixel format?"},{"location":"wwdc22/graphics-and-games-lounge.html#is-generating-the-new-json-pipelines-script-suppose-to-work-on-macos-13-beta-and-xcode-14-beta-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session-trying-the-sample-terminal-commands-in-the-session-on-a-harvested-metallib-super-simply-draw-a-point-with-a-vertexfragment-shader-errored-out-with-metal-source-error-unsupported-binary-format","text":"Thanks for the feedback! The team is looking into this issue.","title":"Is generating the new JSON Pipelines Script suppose to work on MacOS 13 Beta and XCode 14 Beta? Mentioned in the \"Target and optimize GPU binaries with Metal 3\" session.  Trying the sample terminal commands in the session, on a harvested metallib (super simply draw a point, with a vertex/fragment shader), errored out with metal-source: error: unsupported binary format."},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-a-possibility-to-render-3d-objects-with-scenekit-on-the-new-map-eg-a-car-driving-down-the-road","text":"Unfortunately this is not possible to do, but please file Feedback on this request. It seems like a great idea with lots of possibilities!","title":"Is there a possibility to render 3D objects with SceneKit on the new Map? e.g. a Car driving down the road..."},{"location":"wwdc22/graphics-and-games-lounge.html#the-new-objectmesh-shader-pipelines-are-pure-genius-when-will-we-get-a-chance-to-see-an-updated-metal-shading-language-spec-so-we-can-dig-into-the-details","text":"The spec was updated when we announced Metal 3! You can find it in the usual place https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf Yay!!! Do you have any interesting plans <@U03JK18HNR2>? We had some interesting discussions about terrain rendering earlier this week (you should be able to find it if you scroll all the way back up to the Wednesday session :stuck_out_tongue: ) I will definitely check that out! I don't have any specific ideas just yet. Still digesting everything, but I was properly blown away by the potential. This + MetalFX is an absolute game changer.","title":"The new object/mesh shader pipelines are pure genius. When will we get a chance to see an updated Metal Shading Language Spec so we can dig into the details?"},{"location":"wwdc22/graphics-and-games-lounge.html#in-the-new-object-and-mesh-shader-stage-it-looks-like-culling-techniques-can-be-refined-greatly-but-i-didnt-quite-grasp-the-meshlet-concept-discussed-on-the-presentation-is-there-some-external-theory-on-this-you-could-point-us-to-for-more-on-the-concept-or-is-this-something-that-we-will-be-able-to-do-in-the-new-pipeline-stage-ie-carve-up-a-mesh-into-smaller-chunks-on-the-fly","text":"Good question! The term refers to the general idea of splitting up meshes into smaller pieces in order to do efficient culling. This only really becomes a useful practice when culling is done on GPU (otherwise the CPU overhead would be prohibitive due to increased complexity). We don't supply an out-of-the-box solution for carving up mesh resources, and we leave that up to the developer. Personally I had pretty good (and quick) results using K-means clustering, but there are existing mesh tools out there that can help you prepare your meshlets. :) meshlet culling is a good search term to find out more from online resources :slightly_smiling_face: <@U03JK18HNR2> meshoptimizer has a set of functions to build meshlets: https://github.com/zeux/meshoptimizer#mesh-shading :raised_hands: One key observation is that meshlets have fixed upper bound on number of vertices and number of primitives, so they can be efficiently mapped to threadgroups of the same size(s); so chopping up the (potentially much larger) input meshes to fit into (multiple) meshlets has to be done up front (ideally at build-time of the assets) The library author also recorded a series of live streams on the topic, including GPU meshlet and view frustum culling: https://www.youtube.com/watch?v=KckRq7Rm3Mw Ha I was about to mention SG2015 :smile: Lots of interesting things, including using meshlets and GPU driven rendering for efficient shadows etc. One of my favorite talks! Working through implementations at the moment, which is why I'm very happy mesh shaders arrived! As for the second question, whether you can carve up meshes inside the pipeline, I don't see a reason why not. Mesh shader pipeline is a good fit for techniques that can be expressed as one or more mesh shader threadgroups (meaning, each such threadgroup maps 1:1 to a fixed size output mesh you declare in the shader), and you have the object shader stage to dynamically decide how many of those output meshes you will need. There is no particular input formats or intermediate data formats that the pipeline is tied to, so if your input is meshes in device memory, that could work. <@U03HJ3X8V43> <@U03JLQ9J0LB> <@U03HJ54DBT4> great resources! Thank you :)","title":"In the new object and mesh shader stage, it looks like culling techniques can be refined greatly. But I didn\u2019t quite grasp the meshlet concept discussed on the presentation. Is there some external theory on this you could point us to for more on the concept? Or is this something that we will be able to do in the new pipeline stage? (Ie carve up a mesh into smaller chunks on the fly)"},{"location":"wwdc22/graphics-and-games-lounge.html#why-apple-choose-usdz-instead-of-gltf","text":"While USDz and GLTF are both excellent content delivery formats, USDz has the advantage in that it is a direct implementation of the USD format. A USDz file is a zip archive which contains a USD or USDc file along with all of its referenced resources such as textures, animations and shaders. This makes it very convenient for using with DCC packages that natively support USD, since there is no conversion or transcoding required. I think glTF is a great format, but it is not nearly as flexible as usd formats. USD is also widely supported on all sorts of Apple frameworks so you can save a lot of dev time using that as a spec for your file format. Model I/O, RealityKit etc. In my experience, the biggest pain point for USD lies in the tools the artists want to use. A lot of Blender enthusiasts for example are still lacking a fully featured USD export with skinned animation. The best you can do without a custom exporter is send the file to glTF and then use RealityConverter to get it to USD. But that isn't ideal for all use cases. Also another great thing about usd is the ability to convert to usda to get something human readable. That's saved me a lot of time pinpointing weird issues with geometry. Thanks guys!","title":"Why apple choose USDZ instead of GLTF?"},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-sample-code-which-uses-a-mtlsharedevent-im-looking-to-test-one-of-my-compute-pipelines-which-uses-a-shared-event-against-a-sample-pipeline-which-uses-the-shared-event-correctly-as-im-running-into-unexpected-behavior","text":"We have sample code here: https://developer.apple.com/documentation/metal/memory_heaps/implementing_a_multistage_image_filter_using_heaps_and_events?language=objc We also mention shared event specifically here: https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_between_a_gpu_and_the_cpu?language=objc https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_across_multiple_devices_or_processes?language=objc","title":"Is there sample code which uses a MTLSharedEvent? I'm looking to test one of my compute pipelines which uses a shared event against a sample pipeline which uses the shared event correctly as I'm running into unexpected behavior"},{"location":"wwdc22/graphics-and-games-lounge.html#if-i-render-a-scene-to-a-mtltexture-with-large-resolution-can-i-split-scene-to-a-small-pieces-and-async-render-them-or-i-need-to-render-a-whole-scene-al-once","text":"Very interesting question :slightly_smiling_face: There are a lot of interesting things you can do to do this somewhat efficiently The most basic way would just be to render the scene in multiple parts, each to a different render target (or viewport within a target). This would probably require you to first properly sort geometry to different draws One other way to do this, would be to use Vertex Amplification and Layered Rendering to emit vertices to multiple render targets at once (for all the vertices that are overlapping multiple viewports) Unless your render target is bigger than the allowed size though, you should just render everything at once where possible :slightly_smiling_face: The more you render at once, the more you gain efficiency. I'm curious, what is the reason you would need such a huge resolution? :slightly_smiling_face: (I believe our maximum texture size is 16x16k) The 16k is perfect to my case :grinning:. Last year I face the problem with render to a texture. I don\u2019t remember correct resolution, but if I set the resolution to 4k the test app is crashing. Other resolution (smaller or bigger) work perfect. Is there any way to get current max texture resolution directly in the app in runtime? <@U03HJ54DBT4> I work on sort of Standalone Render app and I need to figure out limits that I should set for export resolution, that the user cannot try to render image with bigger resolution. I don't believe so, but you can look at this table: https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf It appears that anything newer than Apple2 will support 16K (iPhone7+) <@U03HJ54DBT4> Thank you so much!","title":"If I render a scene to a MTLTexture with large resolution can I split scene to a small pieces and async render them or I need to render a whole scene al once?"},{"location":"wwdc22/graphics-and-games-lounge.html#how-would-you-break-down-a-scene-into-acceleration-structures-im-working-on-a-small-raytracing-side-project-and-im-a-bit-stuck-determining-how-a-scene-should-be-broken-down-im-using-model-io-to-load-some-assets-and-then-im-planning-to-represent-the-scene-with-an-instance-acceleration-structure-with-each-mdlmesh-mapping-to-a-primitive-acceleration-structure-does-this-sound-like-an-appropriate-breakdown","text":"Thanks for the question. Yes, this does sound like a proper mapping. You could fill out a MTLAccelerationStructureTriangleGeometryDescriptor for each submesh in an MDLMesh and then put those into geometryDescriptors in a MTLPrimitiveAccelerationStructureDescriptor Yeah that's pretty close to what I was planning actually then you would use the transformations for each mesh and put together a transformation buffer for MTLInstanceAccelerationStructureDescriptor","title":"How would you break down a scene into acceleration structures? I'm working on a small raytracing side project and I'm a bit stuck determining how a scene should be broken down. I'm using Model I/O to load some assets and then I'm planning to represent the scene with an instance acceleration structure, with each MDLMesh mapping to a primitive acceleration structure. Does this sound like an appropriate breakdown?"},{"location":"wwdc22/graphics-and-games-lounge.html#hello-are-there-any-plans-of-supporting-page-faulting-in-metal-for-the-future-the-purpose-is-to-have-an-11-cpugpu-va-mapping-with-even-file-mmap-being-accessible-from-the-gpu-side-to-be-able-to-layer-more-high-level-programming-models-on-top","text":"Hi Mohamed, we do not comment on the future of the API, but I would like to direct your attention to two features that may be sufficiently flexible for your use-case, Metal Fast Resource Loading to load data from files and Metal Sparse Textures to manage partial residency with page map and unmap support. We are also interested in learning more about your use-case, so please feel free to expand on it :slightly_smiling_face: Thanks The use case is porting HPC code from other platforms which rely on heterogenous memory management more extensively today, including the regular host heaps being accessible from the GPU. (eg. C++ standard parallelism on GPUs as deployed by some vendors) Wow that is indeed an interesting use-case, and to make it work with Metal 3 you would need some sort of shim to hide the memory management details. Please feel free to file a feedback request for us to track this. There's support for file mmap being made accessible to the GPU using https://developer.apple.com/documentation/metal/mtldevice/1433382-newbufferwithbytesnocopy?language=objc ; however it causes the VM region to be paged in entirely (which may not be what you want to achieve here, depending on the size of the file) > depending on the size of the file yeah pinned memory is useful - but is not what I'm after in this scenario :slightly_smiling_face: FB10133608","title":"Hello, are there any plans of supporting page faulting in Metal for the future?   The purpose is to have an 1:1 CPU:GPU VA mapping, with even file mmap() being accessible from the GPU side, to be able to layer more high-level programming models on top."},{"location":"wwdc22/graphics-and-games-lounge.html#will-gpu-shader-debugging-for-mesh-shaders-be-ready-for-the-xcode-14-release-i-tried-debugging-the-sample-code-adjusting-the-level-of-detail-using-metal-mesh-shaders-httpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shadershttpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shaders-and-2-observations-1-there-didnt-seem-to-be-a-way-to-select-object-or-mesh-shader-to-debug-2-attempting-to-debug-the-fragment-shader-hung-xcode","text":"Many new and exciting features are always being planned to enable a better development experience for Metal. In the meantime if you encounter any issues we would highly encourage you to use the Feedback Assistant. Additionally, feel free to provide the case number here if you have filed feedback already. Thank you!","title":"Will GPU Shader debugging for Mesh Shaders be ready for the Xcode 14 release?  I tried debugging the sample code \"Adjusting the level of detail using Metal mesh shaders\" (https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)|https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders) and 2 observations: 1. There didn't seem to be a way to select object or mesh shader to debug. 2. Attempting to debug the fragment shader, hung XCode."},{"location":"wwdc22/graphics-and-games-lounge.html#hey-really-love-all-the-innovative-metal-3-features-i-may-be-a-little-early-asking-but-i-was-curious-about-whats-happening-with-mtlargumentencoders-are-they-being-deprecated-across-the-board-in-metal-3-such-that-we-can-take-advantage-of-the-streamlined-api-for-most-of-our-metal-2-code-it-also-looks-like-some-but-not-all-of-the-methods-are-deprecated-for-example-the-versions-that-can-set-multiple-buffers-at-once-in-a-range-is-that-correct","text":"Hi Louis, thanks and glad you are enjoying Metal 3! MTLArgumentEncoders are indeed deprecated, however if you are targeting Tier 1 devices on Metal 2 you will still have to use them and disable the deprecation warnings, alternatively you could target only Tier 2 devices by performing a Metal 3 feature check. We'd also really appreciate it if you could file feedback for this use-case.","title":"Hey - Really love all the innovative Metal 3 features! I may be a little early asking, but I was curious about what\u2019s happening with MTLArgumentEncoders. Are they being deprecated across the board in Metal 3 such that we can take advantage of the streamlined API for most of our Metal 2 code? It also looks like some, but not all, of the methods are deprecated. For example, the versions that can set multiple buffers at once in a range. Is that correct?"},{"location":"wwdc22/graphics-and-games-lounge.html#are-there-any-cases-wed-need-to-interact-with-mtlgpuhandle-and-mtlresourceid-properties-i-see-them-added-in-a-bunch-of-places-but-unsure-where-or-by-whom-theyd-be-consumed","text":"MTLGPUHandle is deprecated and we use MTLResourceID to access the underlying resource index of a resource. It's similar to the gpuAddress of a buffer. We use them in the bindless workflow for argument buffers. Instead of using the argument encoder's set[ResourceType] functionality, we can simply cast the contents of the argument buffer to the struct type, and map the resources directly. Please refer to https://developer.apple.com/wwdc22/10101 for more usage examples.","title":"Are there any cases we'd need to interact with MTLGPUHandle and MTLResourceID properties? I see them added in a bunch of places, but unsure where or by whom they'd be consumed."},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-a-way-to-have-the-equivalent-of-work_group_barrier-clk_global_mem_fence-memory_scope_device-in-metal-all-threads-on-a-device-barriers-not-threadgroup-local-also-a-part-of-the-vulkan-memory-model","text":"Hello, To barrier all threads within a threadgroup (equivalent to workgroup) properly ordering memory operations to device memory use threadgroup_barrier(mem_device) . If you want to barrier against all threads spawned, the proper way to achieve it in Metal is to split your operation into 2 sequentially dependent compute dispatches. Thanks. ok... that'll pose some intriguing work for me on the compiler side to make that work out :slightly_smiling_face: I'd use computeCommandEncoderWithDispatchType with the sequential dispatch type in that scenario right? are you talking about CLK_GLOBAL_MEM_FENCE or a scenario where one implements an actual across-grid synchronization primitive? there's indeed a cross-grid synchronisation primitive involved, and memory_scope_device is used to guarantee that My interpretation of the Vulkan spec for work_group_barrier is that the actual synchronization happens at workgroup boundary \u2014 i.e.: only between threads in the same workgroup (or threadgroup in Metal parlance). The memory fence behavior is kinda orthogonal and, in the case of CLK_GLOBAL_MEM_FENCE you are making sure that before any thread in a workgroup/threadgroup can proceed all in-flight device memory operations are visibile to the thread itself. That does not mean that a given thread will wait until all the other threads in the grid will reach the barrier to continue though See the SPV_KHR_vulkan_memory_model extension - more specifically the VulkanMemoryModelDeviceScopeKHR capability compute is my primary target instead of Vulkan though :slightly_smiling_face: Sorry, not really a Vulkan expert. In Metal unfortunately we do not currently support a shader-controller mechanism to barrier across all threads in a grid. In order to do that you would beed to split your operation into 2 sequential compute dispatches as mentioned by Kelvin. is it a known/already considered issue? do you mind if I file a radar about it? I\u2019m not aware of this particular feature request, but that does not mean it has not been discussed so far. If Vulkan offers you this functionality and you think it would be useful to have it in Metal as well, you can definitely file a radar for it, it will be screened by the GPU SW team. Thanks! FB10166471 Thanks!","title":"Is there a way to have the equivalent of work_group_barrier (CLK_GLOBAL_MEM_FENCE, memory_scope_device) in Metal?   (all threads on a device barriers, not threadgroup local, also a part of the Vulkan memory model)"},{"location":"wwdc22/graphics-and-games-lounge.html#ive-never-used-mtlrenderpipelinereflection-before-what-sort-of-use-cases-is-it-meant-for-could-it-be-used-to-replace-shared-sourceshader-constants-defining-binding-indices","text":"MTLRenderPipelineReflection enables your application to reflect on shader bindings and allows you to perform data-driven tasks. For an example, you have a game engine and can pull out binding information to determine what resource needs bound where. This is super cool. I've historical just had a bunch of macros defining the binding indices. Will explore this approach. Thanks! It's also very handy if you have a lot of shaders you are constantly editing. By using reflection, your engine can intelligently bind the right data to the right slot. Less worry, less bugs! Sounds so nice!","title":"I've never used MTLRenderPipelineReflection before. What sort of use cases is it meant for? Could it be used to replace shared source/shader constants defining binding indices?"},{"location":"wwdc22/graphics-and-games-lounge.html#heya-quick-shared-post-from-the-study-hall-section-thank-you-much-i-think-i-have-a-lot-to-learn-with-metal-now-ill-likely-start-with-basic-triangle-patterns-and-moving-those-around-with-a-compute-kernel-which-sounds-so-darn-fun-and-then-start-upgrading-to-the-mesh-shader-feature-now-that-ive-said-that-i-wonder-if-its-a-better-idea-to-maybe-start-with-mesh-shaders-ie-starting-from-the-new-tool-instead-of-building-up-to-it-the-context-being-im-brand-new-to-metal-and-am-looking-to-do-some-fairly-simple-direct-interactions-with-geometry-positions-and-eventually-texture-manipulations-for-things-like-highlighting-areas","text":"Both would work! While mesh shaders would be more efficient in most cases, if you are just starting out with compute I would suggest starting with a basic Compute->Vertex->Fragment pipeline, where your compute shader simply writes out the vertex buffer (and optionally index buffer). You can read back the written vertices and inspect the data. I feel that would be a great way to slowly build up to mesh shaders :slightly_smiling_face: If you have handle on it, you can further optimize your work with mesh shaders (this requires a bit more planning on the side of thread group sizes, mesh sizes etc.) This is excellent, thank you so much for the info. My original concern of going down an inconvenient path is now assuaged, haha. It seems like I\u2019ll get a lot of bang-for-my-buck by writing up my kernel, getting to know the API, and then \u201cdiscovering\u201d where the new tools fit in to how I\u2019ve tried to build things. At that point, even if I have built something that doesn\u2019t quite fit the pattern, I have that core understanding built up to diagnose and refactor as needed. It\u2019ll be quite interesting to start transitioning everything from basic SceneKit nodes and geometries to custom Metal ones\u2026 I feel like I may finally have the right path to get to the right tools for performance. If you happen to have any kinda favorite notes, pages, articles, or docs on some potential equivalences or built in support for SceneKit -> Metal or vice versa, I would really appreciate that as well! Apologies I somehow didn't see this pop up! I'm not super familiar with SceneKit, but we do have interop between them. I would definitely suggest just playing around with Metal to get the hang of it. We had a similar question on Wednesday, where someone wanted to optimize their SceneKit scene by manually constructing complex meshes from scratch instead of relying on huge node graphs. I'll try to find it for you Haha, no worries at all - quite literally thousands of fellow Apple-heads looking to chat, I\u2019m just happy to be a part! :smiley: That. Is. AWESOME! It sounds right on track with that I\u2019m talking about. I think this is going to be a weekend of Metal experimentation.. the more I ask about it from my current place, the more I\u2019m itching to see what I can do with it. Be sure to drop by the forums for help/questions :slightly_smiling_face: Totally. In the past, I\u2019ve only posted there when I\u2019m in \u201cdire straights\u201d so to speak, haha. No reason I can\u2019t ask simpler questions along the way too :smile: <@U03HJ54DBT4> Heya, one more question if you don\u2019t mind :sweat_smile: Sorry if this was asked 1_000_000 times already, but is there a significant difference in using Swift for interacting with Metal as opposed to Objective-C ? I see lots of write ups that use Swift to interact with it, including shader definitions. I would suggest using Swift if possible, since that interfaces much nices with stuff like SwiftUI and other technologies. :slightly_smiling_face: Wooooooooo! No square brackets for ME this weekend! :confetti_ball: :man_dancing: :dancer: :dancers: :confetti_ball:","title":"Heya! Quick shared post from the study-hall section, thank you much!  I think I have a lot to learn with Metal now. I\u2019ll likely start with basic triangle patterns and moving those around with a compute kernel (which sounds so darn fun), and then start \u2018upgrading\u2019 to the mesh shader feature.   Now that I\u2019ve said that, I wonder if it\u2019s a better idea to maybe start with mesh shaders? I.e., starting from the new-tool instead of building up to it? The context being I'm brand new to Metal, and am looking to do some fairly simple direct interactions with geometry positions, and eventually texture manipulations for things like highlighting areas."},{"location":"wwdc22/graphics-and-games-lounge.html#im-trying-to-track-down-a-rare-metal-crash-in-our-app-were-seeing-this-in-two-completely-independent-subsystems-when-deallocating-metal-buffers-invariably-vertex-or-index-buffers","text":"Hello David, could you please provide some further details about the crash? Is it a CPU crash pointing at the Metal library or is it a GPU crash? Thanks Posted in the study hall, but the common part of the stack trace is: Thread 0 Crashed: 0 libobjc.A.dylib 0x000000019c8b7470 objc_release + 16 1 IOGPU 0x00000001cf8059d8 -[IOGPUMetalResource dealloc] + 204 2 IOGPU 0x00000001cf80672c -[IOGPUMetalBuffer dealloc] + 288 3 AGXMetalA10 0x00000001df28ab28 0x1df26b000 + 129832 It's rare, so I don't think it's an over-release on our part. Thanks! Do you have the whole stack trace by chance? Also, did you try to turn on zombie object detection in Xcode or Instruments so far? These reports are coming from the production app and we haven't been able to reproduce it. Sanitizing stack traces, standby It might still be useful to follow the instructions in this page: https://developer.apple.com/documentation/xcode/investigating-crashes-for-zombie-objects , specifically the linked page https://help.apple.com/instruments/mac/current/#/dev612e6956 and I mean, on your development environment Right, like I said we can't repro it, but running with zombies enabled is a good idea. Based on the signature of your stack track this is quite likely a zombie object. A full stack trace would be useful to understand where in the driver we issue that release Here ya go, had to anonymize it: Thread 0 Crashed: 0 libobjc.A.dylib 0x000000019c8b7470 objc_release + 16 1 IOGPU 0x00000001cf8059d8 -[IOGPUMetalResource dealloc] + 204 2 IOGPU 0x00000001cf80672c -[IOGPUMetalBuffer dealloc] + 288 3 AGXMetalA10 0x00000001df28ab28 0x1df26b000 + 129832 4 MyApp 0x00000001059f2234 myapp::profile::MetalVertexBuffer::~MetalVertexBuffer() (<http://MetalRenderer.mm:1516|MetalRenderer.mm:1516>) 5 MyApp 0x00000001059f225c myapp::profile::MetalVertexBuffer::~MetalVertexBuffer() (<http://MetalRenderer.mm:1515|MetalRenderer.mm:1515>) 6 MyApp 0x00000001059e8afc myapp::profile::Shape::~Shape() (Shape.cpp:36) 7 MyApp 0x0000000105803444 std::__1::enable_if&lt;(__is_cpp17_forward_iterator&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*&gt;::value) &amp;&amp; (is_constructible&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;, std::__1::iterator_traits&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*&gt;::reference&gt;::value), void&gt;::type std::__1::vector&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt; &gt; &gt;::assign&lt;std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*&gt;(std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*, std::__1::shared_ptr&lt;myapp::mdk::RenderableTile&gt;*) (memory:2506) 8 MyApp 0x00000001059d9f18 myapp::profile::TerrainLayer::updateNode(myapp::profile::SceneManager*) (TerrainLayer.cpp:46) 9 MyApp 0x00000001059dd494 myapp::profile::SceneNode::updateNode(myapp::profile::SceneManager*) (SceneNode.cpp:47) 10 MyApp 0x00000001059dc538 myapp::profile::ModelController::update(double) (ModelController.cpp:55) 11 MyApp 0x00000001059f6a38 -[TTMProfileModelController(CPP) update:] (<http://TTMProfileModelController.mm:51|TTMProfileModelController.mm:51>) 12 MyApp 0x0000000105a00648 -[TTMProfileView renderLoop] (<http://TTMProfileView.mm:335|TTMProfileView.mm:335>) 13 QuartzCore 0x0000000188fe8378 CA::Display::DisplayLink::dispatch_items(unsigned long long, unsigned long long, unsigned long long) + 756 14 QuartzCore 0x0000000188feeb44 display_timer_callback(__CFMachPort*, void*, long, void*) + 364 15 CoreFoundation 0x00000001854c869c __CFMachPortPerform + 168 16 CoreFoundation 0x00000001855084ec __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__ + 52 17 CoreFoundation 0x000000018550badc __CFRunLoopDoSource1 + 584 18 CoreFoundation 0x00000001854c90f0 __CFRunLoopRun + 2372 19 CoreFoundation 0x00000001854dbe1c CFRunLoopRunSpecific + 568 20 GraphicsServices 0x00000001a576d9a0 GSEventRunModal + 156 21 UIKitCore 0x0000000187d0fb90 -[UIApplication _run] + 1076 22 UIKitCore 0x0000000187aa516c UIApplicationMain + 328 23 MyApp 0x0000000106f36e1c main (main.m:127) 24 ??? 0x0000000109580250 0x0 + 0 And another one (different system altogether) Thread 0 Crashed: 0 libobjc.A.dylib 0x00000001dc079ef8 objc_msgSend + 56 1 IOGPU 0x00000002242b87a0 -[IOGPUMetalResource dealloc] + 208 2 IOGPU 0x00000002242b9558 -[IOGPUMetalBuffer dealloc] + 316 3 AGXMetalA14 0x0000000236149964 0x235f5c000 + 2021732 4 MyApp 0x00000001055b21ac std::__1::__any_imp::_SmallHandler&lt;myapp::render::Buffer::Metal&gt;::__handle(std::__1::__any_imp::_Action, std::__1::any const*, std::__1::any*, std::type_info const*, void const*) (any:0) 5 MyApp 0x000000010559f808 std::__1::any::reset() (any:321) 6 MyApp 0x000000010559f66c myapp::render::Buffer::~Buffer() (Buffer.cpp:14) 7 MyApp 0x00000001055a2768 std::__1::unique_ptr&lt;std::__1::__hash_node&lt;std::__1::__hash_value_type&lt;myapp::Name, myapp::render::VertexBuffer&gt;, void*&gt;, std::__1::__hash_node_destructor&lt;std::__1::allocator&lt;std::__1::__hash_node&lt;std::__1::__hash_value_type&lt;myapp::Name, myapp::render::VertexBuffer&gt;, void*&gt; &gt; &gt; &gt;::~unique_ptr() (Buffer.h:188) 8 MyApp 0x00000001055a1924 myapp::render::Cache&lt;myapp::render::VertexBuffer&gt;::RemoveBuffer(myapp::Name const&amp;) (__hash_table:2498) 9 MyApp 0x0000000105668870 TTMKTileMetadata::deleteGLBuffers() (TTMKTileMetadata.cpp:178) 10 MyApp 0x00000001056d8248 -[TTMKMetalView doInContext:] (<http://TTMKMetalView.mm:46|TTMKMetalView.mm:46>) 11 MyApp 0x000000010569c694 -[TTMKMapView doInBackgroundContextThread:doImmediately:] (<http://TTMKMapView.mm:2958|TTMKMapView.mm:2958>) 12 MyApp 0x000000010568ba8c -[TTMKMapContainerDelegate doInBackgroundContextThread:doImmediately:] (<http://TTMKMapContainerDelegate.mm:20|TTMKMapContainerDelegate.mm:20>) 13 MyApp 0x00000001056176c0 TTMKMapContainer::drainTileCache(bool) (TTMKMapContainer.cpp:3393) 14 MyApp 0x0000000105613ca4 TTMKMapContainer::update(double, double) (TTMKMapContainer.cpp:1442) 15 MyApp 0x000000010569b6a4 -[TTMKMapView glDrawOversample] (<http://TTMKMapView.mm:2696|TTMKMapView.mm:2696>) 16 MyApp 0x000000010569b964 -[TTMKMapView drawWithDevice:toDrawable:pass:] (<http://TTMKMapView.mm:2747|TTMKMapView.mm:2747>) 17 MyApp 0x00000001056d85a0 -[TTMKMetalView drawRect:] (<http://TTMKMetalView.mm:144|TTMKMetalView.mm:144>) 18 MetalKit 0x00000002009a36d4 -[MTKView draw] + 140 19 MetalKit 0x00000002009b1fb8 -[MTKViewDisplayLinkTarget draw] + 36 20 QuartzCore 0x00000001c70d2324 CA::Display::DisplayLink::dispatch_items(unsigned long long, unsigned long long, unsigned long long) + 744 21 QuartzCore 0x00000001c722d174 CA::Display::DisplayLink::dispatch_deferred_display_links() + 344 22 UIKitCore 0x00000001c665f254 __setupUpdateSequence_block_invoke + 212 23 UIKitCore 0x00000001c5fd9084 _UIUpdateSequenceRun + 80 24 UIKitCore 0x00000001c665ecb0 schedulerStepScheduledMainSection + 140 25 UIKitCore 0x00000001c665e478 runloopSourceCallback + 88 26 CoreFoundation 0x00000001c344bf04 __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 24 27 CoreFoundation 0x00000001c345cc90 __CFRunLoopDoSource0 + 204 28 CoreFoundation 0x00000001c3396184 __CFRunLoopDoSources0 + 264 29 CoreFoundation 0x00000001c339bb4c __CFRunLoopRun + 824 30 CoreFoundation 0x00000001c33af6b8 CFRunLoopRunSpecific + 596 31 GraphicsServices 0x00000001df449374 GSEventRunModal + 160 32 UIKitCore 0x00000001c5d14e88 -[UIApplication _run] + 1096 33 UIKitCore 0x00000001c5a965ec UIApplicationMain + 360 34 MyApp 0x0000000106d495fc main (main.m:127) 35 ??? 0x0000000109679ce4 0x0 + 0 Note: deleteGLBuffers() is part of the legacy code hierarchy. Rendering path is all Metal. Thanks, this definitely looks like a zombie object It seems like your app is holding a reference to a MTLBuffer object that has been already released Yeah, that's what I thought too, but the code paths here manage Metal buffers independently and it seems unlikely totally different implementations would make the same subtle mistake. Static analysis turned up nothing suspicious. Do you use command buffers with unretained references or command buffers with retained references? Let me look up the creation code Also, do you have a common pool of Metal resources that lives across sub-systems? Not vertex or index buffers, at least. Creation happens in Obj-C++ code and looks like: auto em = std::any_cast &lt;Environment::Metal&gt; ( &amp;_environment-&gt;_metal ); id &lt;MTLDevice&gt; device = em-&gt;_device; auto mtl = std::any_cast &lt;Metal&gt; ( &amp;_metal ); mtl-&gt;_buffer = [device newBufferWithLength: sizeInBytes options: MTLResourceCPUCacheModeDefaultCache]; That should be retained, if I'm not mistatken. Sorry, what I meant is: how do you create MTLCommandBuffer objects? Do you by chance use MTLCommandQueue:commandBufferWithUnretainedReferences or MTLCommandQueue:commandBufferWithDescriptor setting commandBufferWithUnretainedReferences=YES ? No, I don't think so. Device and queue are created here: _metal.emplace &lt;Metal&gt; (); auto mtl = std::any_cast &lt;Metal&gt; ( &amp;_metal ); bool first = ( mtl-&gt;_device == nullptr ); id &lt;MTLDevice&gt; mtlDevice = *std::any_cast &lt;id &lt;MTLDevice&gt;&gt; ( &amp;device ); mtl-&gt;_device = mtlDevice; mtl-&gt;_queue = [mtlDevice newCommandQueue]; if ( first ) { initialize(); initializeMetalRendering(); } Then the command buffer is created here: void Environment::start() { auto mtl = std::any_cast &lt;Metal&gt; ( &amp;_metal ); mtl-&gt;_buffer = [mtl-&gt;_queue commandBuffer]; OK, that\u2019s a command buffer with retained resources then Based on the info I have I still think your best bet is to use the zombie detector Ok, I'll give that a shot, but we haven't been able to repro it ourselves. If we referred to a dealloc'd object we should at least see a crash locally I would think. Do you possibly have MetalVertexBuffer objects that do not contain an actual MTLBuffer object? Maybe you\u2019re not overreleasing, maybe you just have some smashed pointer or uninitialized data structure? Not sure, all good things to check When we moved from GL to Metal, all the Metal code was put in a single file. I can send that if you'd like to look it over. Not sure we have means for you to safely upload your source code here let me ask You could use feedback assistant Not sure where that is https://developer.apple.com/bug-reporting/","title":"I'm trying to track down a rare Metal crash in our app. We're seeing this in two completely independent subsystems when deallocating Metal buffers, invariably vertex or index buffers."},{"location":"wwdc22/graphics-and-games-lounge.html#have-there-been-any-changes-to-resource-limits-with-metal-3-specifically-the-maximum-number-of-buffers-inside-an-argument-buffer-thank-you","text":"There is no limit on the number of buffer references stored in an argument buffer. We also made it easier to populate an argument buffer on the CPU by using the buffers' gpuAddress property; so argument encoders are no longer needed. Please note though, that you still need to actually allocate an argument buffer large enough to hold all the references, and resource creation (of the argument buffer, or all the other buffers) can fail when you run out of memory; your total resident memory set is still limited, but the argument buffer contents are not. The same applies to texture references stored in an argument buffer (use the MTLResourceID property instead when populating the buffer on the CPU).","title":"Have there been any changes to resource limits with Metal 3? Specifically the maximum number of buffers inside an argument buffer. Thank you!"},{"location":"wwdc22/graphics-and-games-lounge.html#is-there-any-list-where-i-can-find-which-devices-support-metal-3","text":"\u2022 Metal 3 hardware support can be found in Discover Metal 3 https://developer.apple.com/videos/play/wwdc2022/10066/ \u2022 Specific features (Mesh Shaders, Metal FX) may have a different hardware support. For example, Mesh Shaders should be supported on MTLGPUFamilyApple7 and MTLGPUFamilyMac2. Please refer to their associated session for more information. ie, for Mesh Shaders : https://developer.apple.com/videos/play/wwdc2022/10162/ <@U03HWP3Q65P> Thanks! No problem! Looking forward to seeing what y'all make with Metal 3 :raised_hands:","title":"Is there any list where I can find which devices support Metal 3?"},{"location":"wwdc22/graphics-and-games-lounge.html#ive-noticed-that-in-compute-shader-access-to-a-2d-texture-is-faster-than-access-to-a-buffer-due-to-a-lot-more-of-a-cache-missed-how-does-caching-strategy-work-for-textures-vs-buffers-in-compute-can-we-specify-one-as-ive-pretty-good-idea-what-parts-of-the-buffer-will-be-accessed-from-which-thread","text":"Textures are optimized for localized sampling (so sampling things that are more \"together\" in 2D space). It definitely makes sense to store \"2D data\" in a texture. :slightly_smiling_face: Alternatively, you can manually encode 2D data in a \"1D buffer\" using a more optimized order of data using Morton Ordering (Z-ordering). This will make data that is close in 2D also end up close in \"1D\". You are totally free to sample textures in compute, and it will have the same performance characteristics as in a fragment function. https://en.wikipedia.org/wiki/Z-order_curve Does textures use the same hardware as buffers? If requesting from a buffer is different than requesting from a texture, there may be a point in \u201cparallelizing\u201d data access Since textures and buffers tend to reside in the same memory, that would just end up being a more divergent access pattern. :slightly_smiling_face: If you have 2D localized access pattern, I would suggest using textures, unless your data is not compatible with existing texture formats. We also discussed matching the layout of your threadgroups with your buffers in https://developer.apple.com/videos/play/wwdc2022/10159|Scale compute workloads across Apple GPUs I have a 1.5d data. I have really wide, but not very tall texture. ATM i\u2019m using 10-15k by 20 (just twenty, without thousand) And i\u2019m dispatching 10-15k by 1 threads. ATM i\u2019m limited by max texture width on iOS devices, and would like to explore ideas on how to efficiently scale my kernel) Ah I see! In that case, it might be interesting to use a buffer, but store your data interleaved (so in \"strips\" of 20). That would still give you pretty good localized access. And as always it is a good idea to take a critical look at the data size per entry. (use half if possible etc) that was my original implementation, and it was 10-20% slower than textures:( Data size is one possibility I haven\u2019t explored yet, as my algorithms performs poorly on halfs, but I haven\u2019t tried computing in f32, and storing in f16 Hmmm interesting! Are you sure your texture is the same effective data size? Are you using linear interpolation etc? If you are doing a \"linear\" access across your compute grid, I would expect a simple interleaved scheme to work pretty well Hmm, since it was a year ago, I may used not a linear pattern (as my data can be from 1 to 20 tall, I may have packed it tightly) I can check, and if there is major difference in performance, I can file a feedback Sounds good! Good luck :slightly_smiling_face: Just wanted to add: a colleague of mine reminded me that textures often use a separate cache, so you could definitely see some performance difference between the two. Hope it helps :)","title":"I've noticed that in compute shader access to a 2d texture is faster, than access to a buffer. (Due to a lot more of a cache missed). How does caching strategy work for textures vs buffers in compute? Can we specify one (as i've pretty good idea what parts of the buffer will be accessed from which thread)"},{"location":"wwdc22/graphics-and-games-lounge.html#does-screencapturekit-support-pulling-audio-from-different-processes-than-the-video-there-are-some-use-cases-where-audio-comes-from-one-process-but-the-window-and-graphics-come-from-another-process-this-is-common-in-situations-like-crossover-and-wine-this-feature-if-not-supported-would-be-very-useful","text":"ScreenCaptureKit will get audio associated with application, so if the application\u2019s parent process spawns another process to render audio, SCK should also capture that audio. If this doesn\u2019t work in your case, please file a feedback request :slightly_smiling_face:. Thank you very much! No problem :slightly_smiling_face:","title":"Does ScreenCaptureKit support pulling audio from different processes than the video? There are some use cases where audio comes from one process but the window and graphics come from another process, this is common in situations like CrossOver and Wine. This feature if not supported would be very useful."},{"location":"wwdc22/localization-and-internationalization-lounge.html","text":"localization-and-internationalization-lounge QAs by FeeTiki Sometimes there is a difference in font size(width/height) between LTR and RTL , how can we make this better? There are a couple of things you could be seeing and I\u2019d like to understand which it might be. The System Font for some languages like Arabic (which is RTL) uses a taller line height than other languages, which you will notice if you\u2019re using Text Styles and have multiline labels or even in the default cell heights in Table Views. In fact, we recommend using Text Styles to ensure that you do get this language-specific tailoring. Different System Fonts also may have different relative font weights and sizing and it\u2019s possible that what you\u2019re seeing is as a result of that. Do either of these sound like what you may be observing? yep, the first one. In that case, this is correct and expected behaviour, because different languages require different line heights to ensure that text doesn\u2019t overlap across lines and has room to breathe. Are you encountering any problems with this? Sometimes when let's say my use case when u have a cell with both English and Hebrew together, it looks abit weird, thought mby equaling their actual height of one of them is the best practice but i'm not sure. a 10.0 font size in some Hebrew font can be smaller than a 10,0 font size of SF English f.e That\u2019s true. For example, if we compare \u201cWelcome\u201d and \u201c\u05d1\u05e8\u05d5\u05db\u05d9\u05dd\u201c, Hebrew will not be as tall as the capital letters in English and the Hebrew letters are slightly taller than the small letters in English. This is something that varies by script and we cannot expect that the heights will be equal. Generally speaking, the way the System Fonts on iOS, macOS, etc. are designed, if you are using a Text Style, you should get a reasonable size for the text across various scripts. Thank you! To be clear, Hebrew does not use a different line height compared to English. It does, however, use a different UI font, which is why you're seeing this difference, as it is difficult to perfectly match up metrics between two independent fonts. Our app includes multiple tutorials. Each tutorial is in its own directory with tutorial data, images and Localizable.strings files for localized text for just this tutorial. This makes it easy for us to add new localized tutorials without having to modify the app's main Localizable.strings file. Is there a way to include these Localizable.strings files for the many tutorials in the output when we use Export for Localization in Xcode? Or are we going about this wrong? Hi! Can you elaborate a little on how you are setting up these files in your projects? For example, are they part of a target, or do you save them as a pre-created bundle? They are currently in reference folders in Xcode. In the main project and target. Our recommendation would be to convert these to bona fide bundle targets in your project. That would allow them to be properly extracted automatically. As long as the localizable assets are part of a \u201cCopy Resources\u201d phase in your target, they should be extracted when doing Product > Export for Localization. As <@U03HW7PE3SM> mentioned, you can have a \u201cBundle\u201d target for each of your tutorials and then include the localizable resources in each bundle. Then you can include these bundles in your main application target. I'm not familiar with the process of creating bundle targets. But we have over 1000 tutorials and that may be a lot of overhead. The root Tutorials reference folder is included in the Copy Bundle Resources build phase. But I don't see the Localized.strings files, which are in sub folders, in the results from Export for Localization. Yeah, unfortunately Xcode does not look through folder references for strings files \u2014 they are meant to add a folder to your product as-is, without any postprocessing. As <@U03HWDD6RED> mentioned, Xcode does not export the \u201cblue\u201d folder references. One thing you might be able to try is moving all of your tutorials into a Swift package. Xcode 14 adds support for exporting Swift packages for localization and Swift packages are represented by a folder on-disk. Any other suggestions for organizing tutorial or lesson files to be localized? We would like to use Xcode's great localization tools as much as possible. <@U03HJA9JVGA>\u2019s suggestion will let you do just that, for when you can adopt Xcode 14, and is probably your best bet going forward. You can include the \u201cTutorials\u201d Swift package inside of your app\u2019s Xcode project and then when you export for localization\u2014it should include all of the localizable assets and strings in the .xcloc Ok. Thanks! Give it a try and if you run into issues, please feel free to file a report with Feedback Assistant! Hi all! When editing an .xloc file in XCode, I'm missing one key feature: filtering or sorting the list of keys by whether or not they were translated into the second language. I seem to only be able to sort alphabetically. Am I missing something? Hi Jan! As you pointed out, Xcloc Editor supports sorting alphabetically and filtering by search terms, but does not support filtering by \u201ctranslated\u201d or \u201cuntranslated\u201d strings. This is great feedback though! If you would benefit from this feature, please file a feedback report and paste the ID in this thread. Thank you <@U03H3NEH4CX>, submitted at FB10114561 Perfect, thank you! Hi! Is there a built-in workflow in Xcode to localize settings bundles? In a way that Xcode would export all the relevant strings to .string files and the Export Localizations menu command would include those automatically into the exported .xliff files? Hi Daniel! Xcode supports localization export for Settings bundles that are named \u201cSettings.bundle\u201d and contain .strings files. If this isn\u2019t working for you, can you explain the setup you\u2019re using and the behavior you\u2019re experiencing? Hi Matt, ok, I understand what my problem was. As you mentioned, the strings from the Settings.bundle plist are actually exported correctly. I'm in addition using the third party https://github.com/futuretap/InAppSettingsKit|InAppSettings framework , which builds on the Settings.bundle format but supports additional key/values and can be embedded into the app. It was its InAppSettings.bundle where the strings weren't automatically exported from the plist. Thanks for the answer! Ahh, glad you were able to find the problem! Yes, thanks again! How would you handle the case where you want to provide variations in a language? For example I want to provide localization for the word colour. UK people would see colour, but US people would see color. Is there a way so all other text is shared but I could localize that one word? Hi! Unfortunately, once we find a strings table for a specific lproj, we use the contents of that table as they are without doing any more fallbacks. For example, you can localize a table for en_GB, but it has to contain all strings, even those that are the same as other English localizations. Seems like their should be a hierarchy in the string handling where you can have an overlay (e.g for GB) that only needs to contain the differences to another version (e.g US) Ah ok, that's too bad. Are there any plans to be able to do so? Is it worth it to file feedback about it? On the plus side, you can in fact localize for English variants, like English (UK), English (Ireland), and so on. Yeah, it would be wonderful to have a feedback! Thanks! Created FB10115590 :blush: How do we format and sort time values in a localized way? for example, if i'm showing a list of hours of operation for the week, the time strings will be different (24 hr vs 12 hr), and the order of the week days will be different (starting on monday vs sunday). Hi! For sorting, it would be best to sort based on the original binary data, then use .formatted() to produce a localized time and/or date. could you clarify what \u201coriginal binary data\u201d means? How are you storing the days/hours prior to localization? For example, is it a Date , or are you receiving the data in some other form? it would be a JSON from an API response in english, { day: 0, start: \"0700\", end: \"2300\" } and 0 means sunday as specified in the API Got it. You should be able to sort directly on those strings. For the first day of the week issue, you can check Calendar.firstWeekday . (on the current user default calendar) To produce formatted times, you'll need to convert the strings to a Date object, then use .formatted() with the appropriate options to produce the string you want. thank you!! You should be able to put a weekday/hour/minute in DateComponents and then use Calendar to make a Date from that. You can then make a DateInterval from two Dates for the range (e.g. \"9 AM - 5 PM\") and then use .formatted() on that. One more note: when using Calendar to convert DateComponents to Date , set the time zone to GMT to avoid Daylight Time issues. Same with .formatted() We have an app that has English only features. Let's imagine that's as an advanced assistant for editing English texts and our assistant doesn't support other languages. If we localize our app, would it improve user experience for people who use English as a second language, or would it only confuse them on why their main language is not supported, since we only expect English texts for now? Hi Roman, I think it\u2019ll depend on exactly what your app does. For example, hypothetically, if you have an app that points out grammar notes about English, it might still be useful for people to see that information in a language they find easier to understand than English. So, if you can share details, I might be able to help give you a more specific, helpful answer :slightly_smiling_face: Basically, it is an editor app, which has users' documents. When user creates the doc and fills it with a text, he receives grammar suggestions, as well as clarity, fluency, readability and other suggestions. Each suggestion is a card that has: \u2022 previous word/sentence and the resulting sentence \u2022 detailed explanation of why the user should apply this suggestion in that particular context - like because it sounds unconfident, or because it has a grammatical error, and so on Right\u2026 I think some users\u2014especially those who have less familiarity with English to begin with\u2014could certainly benefit from localization. However, you may want to be cautious about (a) which languages you choose, and (b) how you separate out the \u201ccontent language\u201d (English) from the \u201cUI language\u201d so that there\u2019s no confusion or aesthetically displeasing mixture of languages. I've been thinking about some extra flow to suggest users who are not from EFL countries to have some pieces of the content being translated to their native language, just like that hint on why they should apply that alert. However, I dunno if users would be ok with English interface since they already write some English texts and the interface is primitive enough to be understandable. But at the same time having some interface parts in the native language, like we as an app just forgot to translate everything :sweat_smile::sweat_smile: You\u2019re thinking about all the right things! And it\u2019s true that with English being the lingua franca in today\u2019s world, many people might be familiar enough with the English user interface such that they won\u2019t need localization. However, it\u2019s always handy to have choices, and your users can also go into Settings to switch your app to English if that\u2019s what they prefer. In terms of making sure that the localization doesn\u2019t look partial or incomplete, I would definitely suggest making sure that any \u201cchrome\u201d (i.e. UI elements) is translated consistently in each of the languages you target. For example, if the tables are turned and your app is actually for helping people with French, as a French learner, I would appreciate it if I could use it in English and see suggestions like this: > The verb bouffer could be too formal. Consider using d\u00e9jeuner or manger . I\u2019m limited by what I can do in Slack here stylistically, but you can see that I have marked the text in the content language in a different style than the UI language, which gives me clarity as the user as to what\u2019s going on and where the language boundaries are. That's exactly what I've been thinking about. Last one - if you will have those suggestions in English, having French texts, would you as a new user prefer having whole app interface in English? Wouldn't you start typing in English instead of French, since the app is fully localized?)) Yes, I suppose I might! It\u2019s hard to answer that in the abstract though without knowing exactly what the app looks and feels like. It all depends on which direction your app\u2019s design steers the user in. Yeah, well... I'm talking about Grammarly app for iOS. Would be great to learn your thoughts on that. Oh, interesting! I was guessing that this sounded like Grammarly! I would suggest looking at your app\u2019s usage and tackling one market as an initial foray, e.g. China or Japan, and see what feedback you get from your customers :slightly_smiling_face: Personally, I don't know that the app being localized in a given language would necessarily imply that I can input text in that language (though obviously it's ideal!). Taking for example our own products, we localize all features of the OS into all languages completely, including things like Apple Pay Later that might only be available in some regions. There's probably no right way to do this, but when working with localized strings in code, do you recommend using the new string initializer (which looks great btw!) directly throughout the app, or creating, for example, a file with enums that return those localized strings? We strongly recommend that you use String(localized:\u2026) , AttributedString(localized:\u2026) or in Objective-C NSLocalizedString and NSAttributedLocalizedString ! If you do that, Xcode can extract strings for you to provide to your localizers \u2014 it knows how to recognize these macros and initializers and how to produce a xcloc bundle that contains them all, and then reintegrate it into your project. String constants used with other mechanism will not be recognized by this mechanism, and you will have to manually handle extraction into .strings files and any changes going forward. Note that SwiftUI also uses the LocalizedStringKey type, and Xcode also knows about it. So, when you write Text(\"Hello, world!\") , since Text(\u2026) takes a LocalizedStringKey , Xcode will also know to extract it. In Swift, you can even use String interpolation to include variables directly in the initializer String(localized: \"Welcome to \\(newCity)\", comment:\"Large headline label welcoming the user to their new location, the variable refers to the city name.\") Please make sure to explain what the variable refers to, so that translators have that knowledge, too :slightly_smiling_face: Yup! This works in String(localized:\u2026) , AttributedString(localized:\u2026) and in SwiftUI. As long as you\u2019re using this setup, in general, you\u2019re golden \u2014 if you\u2019re using enums to represent your strings, just make sure that they\u2019re calling into these functions. In general, using enums may complicate your patches a little and interact weirdly with SwiftUI if you try to pass String s to SwiftUI initializers that expect localized keys. Hello. Can the Localizable.strings updates itself? The first time export and import is okay. However, when the project contains a Localizable.strings file then the next export will not contain new strings added. Hi ChakMing! The expected flow is as follows: 1. Have some strings in your project (either .strings files or strings in code) 2. Export Localizations, which includes all strings in an xcloc 3. Translate the xcloc 4. Import Localizations with the translated xcloc (This step updates/adds .strings files in/to the project.) 5. Exporting again should include all strings in the project (including those previously imported) in the xcloc If this is not working as expected, please file a feedback so that we can take a look. <@U03H3NEH4CX> Hello again. Here\u2019s what I found. At initial, the export will include all strings. If I imported the xcloc, it will generate Localizable Files. Then I add more words on SwiftUI, and export. The xcloc is still using the old localizable file. I thought it had been happening for a long time\u2026 Do you think it is a bug, or is this correct? So my expectation here is that the second export would include both the strings in your SwiftUI code and the strings in the .strings files that the import created. Xcode won\u2019t remove stale translations automatically, because those translated strings can be a valuable resource if you ever need to translate them again. However, please feel free to file a feedback request if you think we should change the flow here! I just re-export and seems it works this time! Thanks, <@U03H3NEH4CX> <@U03HBMCRX0E> Hello. How can we make sure the system and VoiceOver identify a certain text\u2019s language correctly. Is there a tag we can set somewhere? I\u2019m asking for both web and apps. It happens from time to time where the system falsely identifies Urdu for Arabic, or recently on iOS 16, Persian for Arabic. How can we improve this? If you know the language of your text, you can annotate it with that language. For web content, you can use the https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/lang|lang attribute, which can be applied to any HTML element, including &lt;p&gt; , &lt;span&gt; , and &lt;body&gt; . For app content, you can create an attributed string and set the NSLanguageIdentifierAttributeName to the language code of the corresponding text. If you have reproducible strings that are incorrectly identified, we would love to have feedback with those examples as well! Here\u2019s the feedback ID: FB10152753 It fails on everything. Here are some example texts: \u0645\u0646 \u0633\u0639\u06cc\u062f \u0647\u0633\u062a\u0645 \u0627\u0645\u0631\u0648\u0632 \u0628\u0631\u06cc\u0645 \u0631\u0633\u062a\u0648\u0631\u0627\u0646\u061f \u0686\u0647 \u062e\u0628\u0631\u061f <@U03HW7PE3SM> Thanks! We\u2019ll be sure to pass this along. What\u2019s the best way to use two custom fonts (non SF Pro for marketing reasons) for Arabic and Latin scripts? The most challenging part is that the text could be bilingual and it may come from the server. For instance, \u201c\u0628\u0647 WWDC \u062e\u0648\u0634 \u0622\u0645\u062f\u06cc\u062f\u201d. I want two fonts to be used for this snippet. It looks you forgot to answer this. You can do it by setting https://developer.apple.com/documentation/coretext/kctfontcascadelistattribute|kCTFontCascadeListAttribute , either have the primary font be the Latin font, then have the cascade list contain your Arabic font (the way we do it for our system), or the other way around. Another option is to use (NS)AttributedString and set the fonts on the individual segments of the string. The thing is I mostly don\u2019t have access to the text beforehand. Let\u2019s say it\u2019s coming from a server. Then the font fallback list (as above) is best. Note that in addition to the CoreText version above, there's also https://developer.apple.com/documentation/uikit/uifontdescriptorcascadelistattribute/|UIFontDescriptorCascadeListAttribute and https://developer.apple.com/documentation/appkit/nsfontcascadelistattribute/|NSFontCascadeListAttribute , if you're using higher-level text APIs. Is there a non-hacky way to force a single language for an app? Let\u2019s say the app only supports \u201car-SA\u201d and the user\u2019s device language is set to \u201cen-US\u201d. I used to set a value for a certain in key in UserDefaults at launch to make it work. You should not need to set any user defaults. Simply make sure that the developmentRegion in your project.pbxproj is set to the correct language and that all your localizable files are in the correct lproj. If your app does not have an lproj folder, you will need to be on iOS 16, macOS 13, tvOS 16, and watchOS 9 to get correct behavior. Is there a way to force the numeral system for an app? For instance, the user may have chosen \u2018\u06f1\u06f2\u06f3\u2019 as their numbering system of choice where it only make sense in our app to show numbers as \u2018123\u2019. One thing that\u2019s still missing and I\u2019d already filed a report as I remember, is choosing the numeral system when changing the language for a specific app. Now you can only change the numeral system when you change the device language as a whole. Simply don\u2019t localize the numbers (e.g. don\u2019t use a formatter) and the numbers should be preserved as the literals you are using in your code. Do you have a code sample to share to elaborate more on the issues you are seeing? I think I phrased my question wrong. For instance, install Whatsapp on a device whose locale is set to English - United States. Then, go to Whatsapp settings page inside system Settings and change its language to Persian. Now Open Whatsapp. You\u2019ll see the system keyboard has number row in Latin. Whereas in iMessage on the same device it\u2019s in \u06f1\u06f2\u06f3\u200d form. <@U03JPJ277SQ> IMHO, this may be a bug. Do you mind reporting this bug and sharing the FB # here? <@U03J83G0WQG> Here it is: FB9647599. I believe we already discussed this on Twitter as well. When I release my apps I can usually find some people to help me translate the initial release. But over time it makes it difficult for me to create updates which require new translation strings. Any best practices you would have there for indie developers that don't have a lot of resources to pay 20+ translators for each release? It is understood that adding languages to your application can become quite the commitment \u2014 not just a commitment in the moment, but also going forward. It does pay off to make a plan in which you can sit down and think whether you can do so sustainably for every single feature you do, including adding languages to your application. Sometimes, it is a valid choice to know that the time may just not be there every release, and to choose perhaps to support fewer languages. At other times, community-supported projects may be able to elicit contributions from the community that can help where a single developer may not, depending on what your relationship is with that community. But it is very much a commitment \u2014 often an economic one \u2014 you may want to think about as an ongoing task that requires care and maintenance. Just to clarify \u2014 you are not having issues with filtering out new strings from old ones, correct? in other words, using the Xcode import/export workflow, you should be able to only have to deal with new/changed strings on every release, rather than re-translating the entire app. There are third-party services that offer (human) translation as a service for your localizable strings. But you'll have to document your strings well, or they may get the context wrong and mistranslate. We saw that this year you're adding punctuation to Hebrew (Thank you for that! \u05ea\u05d5\u05d3\u05d4 \u05e8\u05d1\u05d4) Besides the obvious addition to the Hebrew Keyboard is there anything else we should know about that? Hi <@U03J1TN6WBD>, we have indeed added niqqud support to the iPhone & iPad keyboard layouts this year. There\u2019s nothing else new for Hebrew this year per se, but it\u2019s worth mentioning that Yiddish is also now supported as a separate keyboard language. Thank you! a dank Maybe a slightly tangential question, but are there learnings or strategies used during RTL localization that could be beneficial for adapting a UI with significant lateral elements for left handed use? Does \u201chandedness\u201d come into play when considering the design of RTL UIs? We treat Handedness and Layout Directionality as distinct elements and handle each of those in different ways. For example, for the One-Handed Keyboard, we offer both Left-Handed and Right-Handed modes. UI elements or gestures that indicate a progression that go left-to-right in an LTR language do mirror to go right-to-left in an RTL language. So, handedness does not come into play here. This is all fairly abstract, of course. If you have a concrete example that you want to ask about, do feel free to share it :) I guess, as a lefty, I never really understood how an adaptive layout could really change how someone uses a device until I started looking at how RTL languages are handled in the HIG. The chevrons in lists are meant to point towards the thumb you use to tap them :exploding_head:. I would imagine that the inverse could be true in RTL situations\u2026 has that changed your design strategy? For example, an iPad in portrait orientation puts the capture button on the right hand side. This seems like a place where handedness would take priority. But there are other situations where the inverse would be true? I didn't see specifics about gestures in the HIG article, so I was wondering if I just missed something or if there was some type of helpful underlying principle to keep in mind. Those are thought-provoking questions, <@U03JCRFMPHV>, and I likely won\u2019t have any satisfying answers to your larger question in this forum. It does seem like Handedness and how it applies to User Interface Design is a topic that I\u2019m not the best person to answer, though I\u2019ll see if there are any other colleagues of mine who may have an answer for you. The overall rule is that the design layout in RTL matches the flow that the user would expect elements to flow in the UI - including the text strings, animation, navigation (E.g. If the heat map of people that uses an LTR layout - e.g. website - is on the top left corner, it would be top right corner in RTL). The only thing that we should take care of is: Reachability: if the design intention is to have an element closer to the right hand - then it should not be changing because of the RTL layout (E.g. the keyboard layout does not change when you switch LTR or RTL / emojis entry point would be at the same exact place). Other elements that could impact our decisions while changing to RTL layout: 1. Memorization: If the UI component would be very connected to how user would remember things, it should not change (E.g. the order of the lock screen numbers remain the same in both RTL & LTR) 2. Images, videos, backgrounds should not also be impacted (E.g. the weather App background). <@U03JCRFMPHV> Let me know what do you think. This is very interesting, thank you for sharing some of the methodology in your design thinking. I was surprised to see elements like system level scroll bars flip to the left side. It's never been under my thumb before. Seems disadvantageous for RTL righties? <@U03J83F2TDE> That\u2019s a really great question <@U03JCRFMPHV> Two angles for this: (which are up for discussion too) :slightly_smiling_face: \u2022 In the layout, having scroll bars usually is on the other side where your attention should be. \u2022 And for reachability, I usually think about it device by device. On macOS - which I think is where scroll bars are used the most - there is no reachability issue as it is not touch based. Touch based devices are less likely to rely on it and it is used as an indicator more than an interactive piece of the UI as we use swipe instead. And even on iPad for example if we decide to use it, we mostly hold it with two hands and the left hand solves it too. Definitely. I can honestly say that having the scroll bar under my thumb for the first time\u2026 and to not have my scrolling thumb blocking the icons in lists for example, was a brand new experience. Especially on the larger phone, it makes reaching the compose button much easier :joy:. Interesting to see places where steps were taken to counteract reachability issues. Like floating the iPad portrait camera capture button to the right. Thank you for your thoughtful response! <@U03J83F2TDE> You\u2019re always welcome <@U03JCRFMPHV> Nice questions :D Hi there! Didn't have time to check out the new resources yet (but will do in the future for sure!). Could you tell me what are the main key points that we should take into account when designing apps targeting an international audience? Hi <@U03JRP87THN>, there\u2019s no succinct answer for this; so, I would highly recommend checking out https://developer.apple.com/videos/play/wwdc2018/201/?time=1665|Creating Apps for a Global Audience , https://developer.apple.com/videos/play/wwdc2022/10110/|Localization by example , and https://developer.apple.com/videos/play/wwdc2022/10107/|Get it Right (to Left) to build up an understanding of what you should take into account when designing apps for an international audience. <@U03JRP87THN> I would say, generally speaking look out for two things: 1. Language: design layouts may be impacted by RTL languages Orientation or the less dense text strings in CJK languages. Another example is that certain scripts require different bounding boxes or spacing whether less or more than latin (E.g. Thai needs more vertical spacing in the UI to avoid clipping). 2. Culture: What are the cultural behaviors and nuances that can impact the features you support, the hierarchy or the elements in the UI, or simply the visual language (E.g. colours that could be perceived differently in different cultures). Thank you! Will look into the resources. Should RTL (specifically Arabic) text work out of the box using SwiftUI on watchOS 8? Are there any configurations or SwiftUI properties that might break it? I have a Watch app and RTL seem to work fine (I can\u2019t read them but they look okay) but I\u2019ve gotten feedback that they fail for some users \u2014 Arabic text is \u201cbackwards\u201d and the characters aren\u2019t joined like they should be. I\u2019m just using Text so I can\u2019t see why this would happen. I can provide screenshots in the thread if possible. Hi Christopher! I think I may have an idea of what issue you\u2019re encountering, but to be sure, a screenshot would definitely help. Please feel free to share. Sure, here's the same text on watchOS and iOS. I've been unable to reproduce the this with my own strings but I don't have a copy of this exact text. I see; there are two different issues going on here. The Watch screenshot demonstrates broken and completely illegible Arabic text, and it\u2019s also not using the System Font. It\u2019s difficult to be sure about what\u2019s going on here without a sample app to reproduce this issue in. I am setting the Text\u2019s font using UIFont and a FontDescriptor so I'd guess that's the culprit One thing I will note is that in the Watch screenshot, the title \u0627\u0644\u062a\u062d\u0631\u064a\u0645 is rendering correctly, but the body text below that is what\u2019s completely broken. Not sure why it fails for some RTL strings and not others It\u2019s possible that the specific font you\u2019re using supports Arabic glyphs, but not the appropriate shaping. I should note that the alignment in both the iOS and watchOS screenshot is unnatural for Arabic, i.e. the text is left-aligned instead of right-aligned. Does this app localize into Arabic, i.e. have ar.lproj or is it localized into another language and this content is being loaded at runtime? The app is localized in Russian, which is what this user is set to. And then he's entered Arabic text. I'm using the system font but applying a design: Here's another example where (I think?) all the text is working fine: indeed that does look correct I see. Nothing jumps out to me immediately as being wrong here. I would really appreciate it if you could file a Feedback Report with a sample app which demonstrates this issue. I would love to look into this problem further. Yes, the second screenshot here looks correct; I also see that it\u2019s using the System Font. have you ever been able to reproduce this locally, or are your only reports of this from user(s)? I have not been able to reproduce (second screenshot is mine). I've asked the user to send me the text itself so I can try it. Unfortunately I haven't been able to copy that Arabic text out of the iPhone screenshot. To start, I think it would be good to file a feedback with as much of your watch app as you can reasonably include (with build instructions if needed beyond build and run) and these screenshots. Out of curiosity, was the iPhone screenshot above also from the same user? I happened to notice that one of the UI elements was rendering text using the Urdu font, and it\u2019s possible that that is the clue to when this issue reproduces, i.e. when Urdu is added to the preferred languages list (higher than Arabic, if Arabic is also present). Yes, that iPhone screenshot was from the same user. Okay, I'll put together a Feedback with whatever I can get. Thanks! Good spot! I was also wondering why the Arabic text was shaped like this on the navigation bar, but it\u2019s actually Urdu! Please do post the feedback number once you have that filed back in this thread. Thanks again! How can I right align Arabic text in push notifications efficiently when only the app language is Arabic not the entire device. Thanks for raising this use case. Can you please describe your use case in a bit more detail in a Feedback report, sharing the number here? We\u2019ll follow up there. I am working on an application which has two languages English and Arabic, now users can change the app language whenever they want. So ideally if the app language is English the text displayed in push notifications is left aligned as it should be, but when the app language is Arabic the text should be right aligned but it's not. Even when the app is in Arabic the text in push notifications is left aligned. While the rest of the app is completely right aligned. Thanks <@U03J6B7G2QN>, that is a great description. If you don\u2019t mind reporting that via the Feedback tool, it makes it easier for us to track a solution internally, and follow up with you beyond the Digital Lounge. Here FB10161536 Hi. Is the generated xcloc file will fetch all strings update in every export? Yes, running Export Localizations will re-process your source code to extract any newly added, removed, or changed strings and include them in the exported localization catalog. It will also include the existing translations when exporting for languages other than English. That\u2019s Weird. Becoz I found it only works at the 1st time. Then all xcloc will follow the version of the 1st import\u2026 https://wwdc22.slack.com/archives/C03H786M2V8/p1654879512571169?thread_ts=1654804988.358809&cid=C03H786M2V8 I can follow up in your other thread ok thx So if I have a SwiftUI or UIKit notes app that\u2019s not localized in any RTL languages but a user enters RTL text, am I correct that the expected behavior is that the text itself will appear RTL but will be left-aligned? Is there an easy way to get this to appear right-aligned, or would I have to analyze the language myself? Or is this undesirable behavior in an otherwise left-aligned app? On iOS, when a user enters text into a text field or text view, the alignment follows the default alignment of the keyboard, and you should not need to do anything extra on top of this. If that text is then displayed in a UILabel, for example, the alignment would be based on the UI language, rather than the content of the label itself. If you have a use case for something more automatic, please do file a feedback describing it in more detail. Okay thanks Hello Devs, We have localization Strings being delivered by Middle ware and get updated and stored in Application Directory on each app launch. Where we used to load them using \u2018Localizable. nocache\u2019 as mentioned on Apple Docs. However this slow down the Ul performance way too much. As it seems it would load whole strings map for each call to NSLocalizationString. Can you suggest a work around? Thanks Fahied Hi Muhammad! .nocache should only need to be used if: \u2022 the strings table has already been loaded during your app processes lifetime, \u2022 the table is known to have been updated based on some other signal Use of .nocache is expected to have some performance overhead so some techniques to reduce are : \u2022 Use it as sparingly as necessary (ie is it really user critical to get non-cached strings) \u2022 Consider re-organizing the strings tables into smaller tables that map to user activities so that unnecessary strings aren\u2019t being loaded into memory Hi Muhammad, can you also file a feedback report about your specific use case so we can look into enhancement in this area too? Sounds great, I ll do that. The problem is really If I load Strings without .nocache, It will not load the updated strings even though the same source files .strings is overriden or even you force load main bundle again. That's right. The strings files are cached by bundle when bundle is created. Since there's no way to really \"unload\" a bundle, the cached resources remained cached Depending on the scope of your project, you can consider having a separate XPC service to load the strings, and kill and relaunch the service when you get a notification when the strings are updated, if such feedback is available via your middle ware Would there be any possibility for iOS in term of XPC service? Ah.. sorry I didn't ask about your environment first. Unfortunately I don't think so I thought may be you guys holding on to something :smile: Not sure it's a bug bug if my development lanugage is english but if I don't add English in my Localization it going to display other language. The work around I did now is add empty english localization file for each file? Can you clarify your project setup? In general, your development localization should be one of your app localizations. Prior to iOS 16/macOS Ventura, it was required that your app bundle also contain a corresponding lproj folder for the development language, but you should not need any empty files.","title":"localization-and-internationalization-lounge QAs"},{"location":"wwdc22/localization-and-internationalization-lounge.html#localization-and-internationalization-lounge-qas","text":"","title":"localization-and-internationalization-lounge QAs"},{"location":"wwdc22/localization-and-internationalization-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/localization-and-internationalization-lounge.html#sometimes-there-is-a-difference-in-font-sizewidthheight-between-ltr-and-rtl-how-can-we-make-this-better","text":"There are a couple of things you could be seeing and I\u2019d like to understand which it might be. The System Font for some languages like Arabic (which is RTL) uses a taller line height than other languages, which you will notice if you\u2019re using Text Styles and have multiline labels or even in the default cell heights in Table Views. In fact, we recommend using Text Styles to ensure that you do get this language-specific tailoring. Different System Fonts also may have different relative font weights and sizing and it\u2019s possible that what you\u2019re seeing is as a result of that. Do either of these sound like what you may be observing? yep, the first one. In that case, this is correct and expected behaviour, because different languages require different line heights to ensure that text doesn\u2019t overlap across lines and has room to breathe. Are you encountering any problems with this? Sometimes when let's say my use case when u have a cell with both English and Hebrew together, it looks abit weird, thought mby equaling their actual height of one of them is the best practice but i'm not sure. a 10.0 font size in some Hebrew font can be smaller than a 10,0 font size of SF English f.e That\u2019s true. For example, if we compare \u201cWelcome\u201d and \u201c\u05d1\u05e8\u05d5\u05db\u05d9\u05dd\u201c, Hebrew will not be as tall as the capital letters in English and the Hebrew letters are slightly taller than the small letters in English. This is something that varies by script and we cannot expect that the heights will be equal. Generally speaking, the way the System Fonts on iOS, macOS, etc. are designed, if you are using a Text Style, you should get a reasonable size for the text across various scripts. Thank you! To be clear, Hebrew does not use a different line height compared to English. It does, however, use a different UI font, which is why you're seeing this difference, as it is difficult to perfectly match up metrics between two independent fonts.","title":"Sometimes there is a difference in font size(width/height) between LTR and RTL , how can we make this better?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#our-app-includes-multiple-tutorials-each-tutorial-is-in-its-own-directory-with-tutorial-data-images-and-localizablestrings-files-for-localized-text-for-just-this-tutorial-this-makes-it-easy-for-us-to-add-new-localized-tutorials-without-having-to-modify-the-apps-main-localizablestrings-file-is-there-a-way-to-include-these-localizablestrings-files-for-the-many-tutorials-in-the-output-when-we-use-export-for-localization-in-xcode-or-are-we-going-about-this-wrong","text":"Hi! Can you elaborate a little on how you are setting up these files in your projects? For example, are they part of a target, or do you save them as a pre-created bundle? They are currently in reference folders in Xcode. In the main project and target. Our recommendation would be to convert these to bona fide bundle targets in your project. That would allow them to be properly extracted automatically. As long as the localizable assets are part of a \u201cCopy Resources\u201d phase in your target, they should be extracted when doing Product > Export for Localization. As <@U03HW7PE3SM> mentioned, you can have a \u201cBundle\u201d target for each of your tutorials and then include the localizable resources in each bundle. Then you can include these bundles in your main application target. I'm not familiar with the process of creating bundle targets. But we have over 1000 tutorials and that may be a lot of overhead. The root Tutorials reference folder is included in the Copy Bundle Resources build phase. But I don't see the Localized.strings files, which are in sub folders, in the results from Export for Localization. Yeah, unfortunately Xcode does not look through folder references for strings files \u2014 they are meant to add a folder to your product as-is, without any postprocessing. As <@U03HWDD6RED> mentioned, Xcode does not export the \u201cblue\u201d folder references. One thing you might be able to try is moving all of your tutorials into a Swift package. Xcode 14 adds support for exporting Swift packages for localization and Swift packages are represented by a folder on-disk. Any other suggestions for organizing tutorial or lesson files to be localized? We would like to use Xcode's great localization tools as much as possible. <@U03HJA9JVGA>\u2019s suggestion will let you do just that, for when you can adopt Xcode 14, and is probably your best bet going forward. You can include the \u201cTutorials\u201d Swift package inside of your app\u2019s Xcode project and then when you export for localization\u2014it should include all of the localizable assets and strings in the .xcloc Ok. Thanks! Give it a try and if you run into issues, please feel free to file a report with Feedback Assistant!","title":"Our app includes multiple tutorials. Each tutorial is in its own directory with tutorial data, images and Localizable.strings files for localized text for just this tutorial. This makes it easy for us to add new localized tutorials without having to modify the app's main Localizable.strings file.  Is there a way to include these Localizable.strings files for the many tutorials in the output when we use Export for Localization in Xcode?  Or are we going about this wrong?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#hi-all-when-editing-an-xloc-file-in-xcode-im-missing-one-key-feature-filtering-or-sorting-the-list-of-keys-by-whether-or-not-they-were-translated-into-the-second-language-i-seem-to-only-be-able-to-sort-alphabetically-am-i-missing-something","text":"Hi Jan! As you pointed out, Xcloc Editor supports sorting alphabetically and filtering by search terms, but does not support filtering by \u201ctranslated\u201d or \u201cuntranslated\u201d strings. This is great feedback though! If you would benefit from this feature, please file a feedback report and paste the ID in this thread. Thank you <@U03H3NEH4CX>, submitted at FB10114561 Perfect, thank you!","title":"Hi all! When editing an .xloc file in XCode, I'm missing one key feature: filtering or sorting the list of keys by whether or not they were translated into the second language. I seem to only be able to sort alphabetically. Am I missing something?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#hi-is-there-a-built-in-workflow-in-xcode-to-localize-settings-bundles-in-a-way-that-xcode-would-export-all-the-relevant-strings-to-string-files-and-the-export-localizations-menu-command-would-include-those-automatically-into-the-exported-xliff-files","text":"Hi Daniel! Xcode supports localization export for Settings bundles that are named \u201cSettings.bundle\u201d and contain .strings files. If this isn\u2019t working for you, can you explain the setup you\u2019re using and the behavior you\u2019re experiencing? Hi Matt, ok, I understand what my problem was. As you mentioned, the strings from the Settings.bundle plist are actually exported correctly. I'm in addition using the third party https://github.com/futuretap/InAppSettingsKit|InAppSettings framework , which builds on the Settings.bundle format but supports additional key/values and can be embedded into the app. It was its InAppSettings.bundle where the strings weren't automatically exported from the plist. Thanks for the answer! Ahh, glad you were able to find the problem! Yes, thanks again!","title":"Hi! Is there a built-in workflow in Xcode to localize settings bundles? In a way that Xcode would export all the relevant strings to .string files and the Export Localizations menu command would include those automatically into the exported .xliff files?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#how-would-you-handle-the-case-where-you-want-to-provide-variations-in-a-language-for-example-i-want-to-provide-localization-for-the-word-colour-uk-people-would-see-colour-but-us-people-would-see-color-is-there-a-way-so-all-other-text-is-shared-but-i-could-localize-that-one-word","text":"Hi! Unfortunately, once we find a strings table for a specific lproj, we use the contents of that table as they are without doing any more fallbacks. For example, you can localize a table for en_GB, but it has to contain all strings, even those that are the same as other English localizations. Seems like their should be a hierarchy in the string handling where you can have an overlay (e.g for GB) that only needs to contain the differences to another version (e.g US) Ah ok, that's too bad. Are there any plans to be able to do so? Is it worth it to file feedback about it? On the plus side, you can in fact localize for English variants, like English (UK), English (Ireland), and so on. Yeah, it would be wonderful to have a feedback! Thanks! Created FB10115590 :blush:","title":"How would you handle the case where you want to provide variations in a language? For example I want to provide localization for the word colour. UK people would see colour, but US people would see color. Is there a way so all other text is shared but I could localize that one word?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#how-do-we-format-and-sort-time-values-in-a-localized-way-for-example-if-im-showing-a-list-of-hours-of-operation-for-the-week-the-time-strings-will-be-different-24-hr-vs-12-hr-and-the-order-of-the-week-days-will-be-different-starting-on-monday-vs-sunday","text":"Hi! For sorting, it would be best to sort based on the original binary data, then use .formatted() to produce a localized time and/or date. could you clarify what \u201coriginal binary data\u201d means? How are you storing the days/hours prior to localization? For example, is it a Date , or are you receiving the data in some other form? it would be a JSON from an API response in english, { day: 0, start: \"0700\", end: \"2300\" } and 0 means sunday as specified in the API Got it. You should be able to sort directly on those strings. For the first day of the week issue, you can check Calendar.firstWeekday . (on the current user default calendar) To produce formatted times, you'll need to convert the strings to a Date object, then use .formatted() with the appropriate options to produce the string you want. thank you!! You should be able to put a weekday/hour/minute in DateComponents and then use Calendar to make a Date from that. You can then make a DateInterval from two Dates for the range (e.g. \"9 AM - 5 PM\") and then use .formatted() on that. One more note: when using Calendar to convert DateComponents to Date , set the time zone to GMT to avoid Daylight Time issues. Same with .formatted()","title":"How do we format and sort time values in a localized way? for example, if i'm showing a list of hours of operation for the week, the time strings will be different (24 hr vs 12 hr), and the order of the week days will be different (starting on monday vs sunday)."},{"location":"wwdc22/localization-and-internationalization-lounge.html#we-have-an-app-that-has-english-only-features-lets-imagine-thats-as-an-advanced-assistant-for-editing-english-texts-and-our-assistant-doesnt-support-other-languages-if-we-localize-our-app-would-it-improve-user-experience-for-people-who-use-english-as-a-second-language-or-would-it-only-confuse-them-on-why-their-main-language-is-not-supported-since-we-only-expect-english-texts-for-now","text":"Hi Roman, I think it\u2019ll depend on exactly what your app does. For example, hypothetically, if you have an app that points out grammar notes about English, it might still be useful for people to see that information in a language they find easier to understand than English. So, if you can share details, I might be able to help give you a more specific, helpful answer :slightly_smiling_face: Basically, it is an editor app, which has users' documents. When user creates the doc and fills it with a text, he receives grammar suggestions, as well as clarity, fluency, readability and other suggestions. Each suggestion is a card that has: \u2022 previous word/sentence and the resulting sentence \u2022 detailed explanation of why the user should apply this suggestion in that particular context - like because it sounds unconfident, or because it has a grammatical error, and so on Right\u2026 I think some users\u2014especially those who have less familiarity with English to begin with\u2014could certainly benefit from localization. However, you may want to be cautious about (a) which languages you choose, and (b) how you separate out the \u201ccontent language\u201d (English) from the \u201cUI language\u201d so that there\u2019s no confusion or aesthetically displeasing mixture of languages. I've been thinking about some extra flow to suggest users who are not from EFL countries to have some pieces of the content being translated to their native language, just like that hint on why they should apply that alert. However, I dunno if users would be ok with English interface since they already write some English texts and the interface is primitive enough to be understandable. But at the same time having some interface parts in the native language, like we as an app just forgot to translate everything :sweat_smile::sweat_smile: You\u2019re thinking about all the right things! And it\u2019s true that with English being the lingua franca in today\u2019s world, many people might be familiar enough with the English user interface such that they won\u2019t need localization. However, it\u2019s always handy to have choices, and your users can also go into Settings to switch your app to English if that\u2019s what they prefer. In terms of making sure that the localization doesn\u2019t look partial or incomplete, I would definitely suggest making sure that any \u201cchrome\u201d (i.e. UI elements) is translated consistently in each of the languages you target. For example, if the tables are turned and your app is actually for helping people with French, as a French learner, I would appreciate it if I could use it in English and see suggestions like this: > The verb bouffer could be too formal. Consider using d\u00e9jeuner or manger . I\u2019m limited by what I can do in Slack here stylistically, but you can see that I have marked the text in the content language in a different style than the UI language, which gives me clarity as the user as to what\u2019s going on and where the language boundaries are. That's exactly what I've been thinking about. Last one - if you will have those suggestions in English, having French texts, would you as a new user prefer having whole app interface in English? Wouldn't you start typing in English instead of French, since the app is fully localized?)) Yes, I suppose I might! It\u2019s hard to answer that in the abstract though without knowing exactly what the app looks and feels like. It all depends on which direction your app\u2019s design steers the user in. Yeah, well... I'm talking about Grammarly app for iOS. Would be great to learn your thoughts on that. Oh, interesting! I was guessing that this sounded like Grammarly! I would suggest looking at your app\u2019s usage and tackling one market as an initial foray, e.g. China or Japan, and see what feedback you get from your customers :slightly_smiling_face: Personally, I don't know that the app being localized in a given language would necessarily imply that I can input text in that language (though obviously it's ideal!). Taking for example our own products, we localize all features of the OS into all languages completely, including things like Apple Pay Later that might only be available in some regions.","title":"We have an app that has English only features. Let's imagine that's as an advanced assistant for editing English texts and our assistant doesn't support other languages.  If we localize our app, would it improve user experience for people who use English as a second language, or would it only confuse them on why their main language is not supported, since we only expect English texts for now?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#theres-probably-no-right-way-to-do-this-but-when-working-with-localized-strings-in-code-do-you-recommend-using-the-new-string-initializer-which-looks-great-btw-directly-throughout-the-app-or-creating-for-example-a-file-with-enums-that-return-those-localized-strings","text":"We strongly recommend that you use String(localized:\u2026) , AttributedString(localized:\u2026) or in Objective-C NSLocalizedString and NSAttributedLocalizedString ! If you do that, Xcode can extract strings for you to provide to your localizers \u2014 it knows how to recognize these macros and initializers and how to produce a xcloc bundle that contains them all, and then reintegrate it into your project. String constants used with other mechanism will not be recognized by this mechanism, and you will have to manually handle extraction into .strings files and any changes going forward. Note that SwiftUI also uses the LocalizedStringKey type, and Xcode also knows about it. So, when you write Text(\"Hello, world!\") , since Text(\u2026) takes a LocalizedStringKey , Xcode will also know to extract it. In Swift, you can even use String interpolation to include variables directly in the initializer String(localized: \"Welcome to \\(newCity)\", comment:\"Large headline label welcoming the user to their new location, the variable refers to the city name.\") Please make sure to explain what the variable refers to, so that translators have that knowledge, too :slightly_smiling_face: Yup! This works in String(localized:\u2026) , AttributedString(localized:\u2026) and in SwiftUI. As long as you\u2019re using this setup, in general, you\u2019re golden \u2014 if you\u2019re using enums to represent your strings, just make sure that they\u2019re calling into these functions. In general, using enums may complicate your patches a little and interact weirdly with SwiftUI if you try to pass String s to SwiftUI initializers that expect localized keys.","title":"There's probably no right way to do this, but when working with localized strings in code, do you recommend using the new string initializer (which looks great btw!) directly throughout the app, or creating, for example, a file with enums that return those localized strings?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#hello-can-the-localizablestrings-updates-itself-the-first-time-export-and-import-is-okay-however-when-the-project-contains-a-localizablestrings-file-then-the-next-export-will-not-contain-new-strings-added","text":"Hi ChakMing! The expected flow is as follows: 1. Have some strings in your project (either .strings files or strings in code) 2. Export Localizations, which includes all strings in an xcloc 3. Translate the xcloc 4. Import Localizations with the translated xcloc (This step updates/adds .strings files in/to the project.) 5. Exporting again should include all strings in the project (including those previously imported) in the xcloc If this is not working as expected, please file a feedback so that we can take a look. <@U03H3NEH4CX> Hello again. Here\u2019s what I found. At initial, the export will include all strings. If I imported the xcloc, it will generate Localizable Files. Then I add more words on SwiftUI, and export. The xcloc is still using the old localizable file. I thought it had been happening for a long time\u2026 Do you think it is a bug, or is this correct? So my expectation here is that the second export would include both the strings in your SwiftUI code and the strings in the .strings files that the import created. Xcode won\u2019t remove stale translations automatically, because those translated strings can be a valuable resource if you ever need to translate them again. However, please feel free to file a feedback request if you think we should change the flow here! I just re-export and seems it works this time! Thanks, <@U03H3NEH4CX> <@U03HBMCRX0E>","title":"Hello. Can the Localizable.strings updates itself? The first time export and import is okay. However, when the project contains a  Localizable.strings file then the next export will not contain new strings added."},{"location":"wwdc22/localization-and-internationalization-lounge.html#hello-how-can-we-make-sure-the-system-and-voiceover-identify-a-certain-texts-language-correctly-is-there-a-tag-we-can-set-somewhere-im-asking-for-both-web-and-apps-it-happens-from-time-to-time-where-the-system-falsely-identifies-urdu-for-arabic-or-recently-on-ios-16-persian-for-arabic-how-can-we-improve-this","text":"If you know the language of your text, you can annotate it with that language. For web content, you can use the https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/lang|lang attribute, which can be applied to any HTML element, including &lt;p&gt; , &lt;span&gt; , and &lt;body&gt; . For app content, you can create an attributed string and set the NSLanguageIdentifierAttributeName to the language code of the corresponding text. If you have reproducible strings that are incorrectly identified, we would love to have feedback with those examples as well! Here\u2019s the feedback ID: FB10152753 It fails on everything. Here are some example texts: \u0645\u0646 \u0633\u0639\u06cc\u062f \u0647\u0633\u062a\u0645 \u0627\u0645\u0631\u0648\u0632 \u0628\u0631\u06cc\u0645 \u0631\u0633\u062a\u0648\u0631\u0627\u0646\u061f \u0686\u0647 \u062e\u0628\u0631\u061f <@U03HW7PE3SM> Thanks! We\u2019ll be sure to pass this along.","title":"Hello. How can we make sure the system and VoiceOver identify a certain text\u2019s language correctly. Is there a tag we can set somewhere? I\u2019m asking for both web and apps. It happens from time to time where the system falsely identifies Urdu for Arabic, or recently on iOS 16, Persian for Arabic. How can we improve this?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#whats-the-best-way-to-use-two-custom-fonts-non-sf-pro-for-marketing-reasons-for-arabic-and-latin-scripts-the-most-challenging-part-is-that-the-text-could-be-bilingual-and-it-may-come-from-the-server-for-instance-wwdc-i-want-two-fonts-to-be-used-for-this-snippet","text":"It looks you forgot to answer this. You can do it by setting https://developer.apple.com/documentation/coretext/kctfontcascadelistattribute|kCTFontCascadeListAttribute , either have the primary font be the Latin font, then have the cascade list contain your Arabic font (the way we do it for our system), or the other way around. Another option is to use (NS)AttributedString and set the fonts on the individual segments of the string. The thing is I mostly don\u2019t have access to the text beforehand. Let\u2019s say it\u2019s coming from a server. Then the font fallback list (as above) is best. Note that in addition to the CoreText version above, there's also https://developer.apple.com/documentation/uikit/uifontdescriptorcascadelistattribute/|UIFontDescriptorCascadeListAttribute and https://developer.apple.com/documentation/appkit/nsfontcascadelistattribute/|NSFontCascadeListAttribute , if you're using higher-level text APIs.","title":"What\u2019s the best way to use two custom fonts (non SF Pro for marketing reasons) for Arabic and Latin scripts? The most challenging part is that the text could be bilingual and it may come from the server. For instance, \u201c\u0628\u0647 WWDC \u062e\u0648\u0634 \u0622\u0645\u062f\u06cc\u062f\u201d. I want two fonts to be used for this snippet."},{"location":"wwdc22/localization-and-internationalization-lounge.html#is-there-a-non-hacky-way-to-force-a-single-language-for-an-app-lets-say-the-app-only-supports-ar-sa-and-the-users-device-language-is-set-to-en-us-i-used-to-set-a-value-for-a-certain-in-key-in-userdefaults-at-launch-to-make-it-work","text":"You should not need to set any user defaults. Simply make sure that the developmentRegion in your project.pbxproj is set to the correct language and that all your localizable files are in the correct lproj. If your app does not have an lproj folder, you will need to be on iOS 16, macOS 13, tvOS 16, and watchOS 9 to get correct behavior.","title":"Is there a non-hacky way to force a single language for an app? Let\u2019s say the app only supports \u201car-SA\u201d and the user\u2019s device language is set to \u201cen-US\u201d. I used to set a value for a certain in key in UserDefaults at launch to make it work."},{"location":"wwdc22/localization-and-internationalization-lounge.html#is-there-a-way-to-force-the-numeral-system-for-an-app-for-instance-the-user-may-have-chosen-as-their-numbering-system-of-choice-where-it-only-make-sense-in-our-app-to-show-numbers-as-123-one-thing-thats-still-missing-and-id-already-filed-a-report-as-i-remember-is-choosing-the-numeral-system-when-changing-the-language-for-a-specific-app-now-you-can-only-change-the-numeral-system-when-you-change-the-device-language-as-a-whole","text":"Simply don\u2019t localize the numbers (e.g. don\u2019t use a formatter) and the numbers should be preserved as the literals you are using in your code. Do you have a code sample to share to elaborate more on the issues you are seeing? I think I phrased my question wrong. For instance, install Whatsapp on a device whose locale is set to English - United States. Then, go to Whatsapp settings page inside system Settings and change its language to Persian. Now Open Whatsapp. You\u2019ll see the system keyboard has number row in Latin. Whereas in iMessage on the same device it\u2019s in \u06f1\u06f2\u06f3\u200d form. <@U03JPJ277SQ> IMHO, this may be a bug. Do you mind reporting this bug and sharing the FB # here? <@U03J83G0WQG> Here it is: FB9647599. I believe we already discussed this on Twitter as well.","title":"Is there a way to force the numeral system for an app? For instance, the user may have chosen \u2018\u06f1\u06f2\u06f3\u2019 as their numbering system of choice where it only make sense in our app to show numbers as \u2018123\u2019. One thing that\u2019s still missing and I\u2019d already filed a report as I remember, is choosing the numeral system when changing the language for a specific app. Now you can only change the numeral system when you change the device language as a whole."},{"location":"wwdc22/localization-and-internationalization-lounge.html#when-i-release-my-apps-i-can-usually-find-some-people-to-help-me-translate-the-initial-release-but-over-time-it-makes-it-difficult-for-me-to-create-updates-which-require-new-translation-strings-any-best-practices-you-would-have-there-for-indie-developers-that-dont-have-a-lot-of-resources-to-pay-20-translators-for-each-release","text":"It is understood that adding languages to your application can become quite the commitment \u2014 not just a commitment in the moment, but also going forward. It does pay off to make a plan in which you can sit down and think whether you can do so sustainably for every single feature you do, including adding languages to your application. Sometimes, it is a valid choice to know that the time may just not be there every release, and to choose perhaps to support fewer languages. At other times, community-supported projects may be able to elicit contributions from the community that can help where a single developer may not, depending on what your relationship is with that community. But it is very much a commitment \u2014 often an economic one \u2014 you may want to think about as an ongoing task that requires care and maintenance. Just to clarify \u2014 you are not having issues with filtering out new strings from old ones, correct? in other words, using the Xcode import/export workflow, you should be able to only have to deal with new/changed strings on every release, rather than re-translating the entire app. There are third-party services that offer (human) translation as a service for your localizable strings. But you'll have to document your strings well, or they may get the context wrong and mistranslate.","title":"When I release my apps I can usually find some people to help me translate the initial release. But over time it makes it difficult for me to create updates which require new translation strings. Any best practices you would have there for indie developers that don't have a lot of resources to pay 20+ translators for each release?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#we-saw-that-this-year-youre-adding-punctuation-to-hebrew-thank-you-for-that-besides-the-obvious-addition-to-the-hebrew-keyboard-is-there-anything-else-we-should-know-about-that","text":"Hi <@U03J1TN6WBD>, we have indeed added niqqud support to the iPhone & iPad keyboard layouts this year. There\u2019s nothing else new for Hebrew this year per se, but it\u2019s worth mentioning that Yiddish is also now supported as a separate keyboard language. Thank you! a dank","title":"We saw that this year you're adding punctuation to Hebrew (Thank you for that! \u05ea\u05d5\u05d3\u05d4 \u05e8\u05d1\u05d4) Besides the obvious addition to the Hebrew Keyboard is there anything else we should know about that?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#maybe-a-slightly-tangential-question-but-are-there-learnings-or-strategies-used-during-rtl-localization-that-could-be-beneficial-for-adapting-a-ui-with-significant-lateral-elements-for-left-handed-use-does-handedness-come-into-play-when-considering-the-design-of-rtl-uis","text":"We treat Handedness and Layout Directionality as distinct elements and handle each of those in different ways. For example, for the One-Handed Keyboard, we offer both Left-Handed and Right-Handed modes. UI elements or gestures that indicate a progression that go left-to-right in an LTR language do mirror to go right-to-left in an RTL language. So, handedness does not come into play here. This is all fairly abstract, of course. If you have a concrete example that you want to ask about, do feel free to share it :) I guess, as a lefty, I never really understood how an adaptive layout could really change how someone uses a device until I started looking at how RTL languages are handled in the HIG. The chevrons in lists are meant to point towards the thumb you use to tap them :exploding_head:. I would imagine that the inverse could be true in RTL situations\u2026 has that changed your design strategy? For example, an iPad in portrait orientation puts the capture button on the right hand side. This seems like a place where handedness would take priority. But there are other situations where the inverse would be true? I didn't see specifics about gestures in the HIG article, so I was wondering if I just missed something or if there was some type of helpful underlying principle to keep in mind. Those are thought-provoking questions, <@U03JCRFMPHV>, and I likely won\u2019t have any satisfying answers to your larger question in this forum. It does seem like Handedness and how it applies to User Interface Design is a topic that I\u2019m not the best person to answer, though I\u2019ll see if there are any other colleagues of mine who may have an answer for you. The overall rule is that the design layout in RTL matches the flow that the user would expect elements to flow in the UI - including the text strings, animation, navigation (E.g. If the heat map of people that uses an LTR layout - e.g. website - is on the top left corner, it would be top right corner in RTL). The only thing that we should take care of is: Reachability: if the design intention is to have an element closer to the right hand - then it should not be changing because of the RTL layout (E.g. the keyboard layout does not change when you switch LTR or RTL / emojis entry point would be at the same exact place). Other elements that could impact our decisions while changing to RTL layout: 1. Memorization: If the UI component would be very connected to how user would remember things, it should not change (E.g. the order of the lock screen numbers remain the same in both RTL & LTR) 2. Images, videos, backgrounds should not also be impacted (E.g. the weather App background). <@U03JCRFMPHV> Let me know what do you think. This is very interesting, thank you for sharing some of the methodology in your design thinking. I was surprised to see elements like system level scroll bars flip to the left side. It's never been under my thumb before. Seems disadvantageous for RTL righties? <@U03J83F2TDE> That\u2019s a really great question <@U03JCRFMPHV> Two angles for this: (which are up for discussion too) :slightly_smiling_face: \u2022 In the layout, having scroll bars usually is on the other side where your attention should be. \u2022 And for reachability, I usually think about it device by device. On macOS - which I think is where scroll bars are used the most - there is no reachability issue as it is not touch based. Touch based devices are less likely to rely on it and it is used as an indicator more than an interactive piece of the UI as we use swipe instead. And even on iPad for example if we decide to use it, we mostly hold it with two hands and the left hand solves it too. Definitely. I can honestly say that having the scroll bar under my thumb for the first time\u2026 and to not have my scrolling thumb blocking the icons in lists for example, was a brand new experience. Especially on the larger phone, it makes reaching the compose button much easier :joy:. Interesting to see places where steps were taken to counteract reachability issues. Like floating the iPad portrait camera capture button to the right. Thank you for your thoughtful response! <@U03J83F2TDE> You\u2019re always welcome <@U03JCRFMPHV> Nice questions :D","title":"Maybe a slightly tangential question, but are there learnings or strategies used during RTL localization that could be beneficial for adapting a UI with significant lateral elements for left handed use? Does \u201chandedness\u201d come into play when considering the design of RTL UIs?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#hi-there-didnt-have-time-to-check-out-the-new-resources-yet-but-will-do-in-the-future-for-sure-could-you-tell-me-what-are-the-main-key-points-that-we-should-take-into-account-when-designing-apps-targeting-an-international-audience","text":"Hi <@U03JRP87THN>, there\u2019s no succinct answer for this; so, I would highly recommend checking out https://developer.apple.com/videos/play/wwdc2018/201/?time=1665|Creating Apps for a Global Audience , https://developer.apple.com/videos/play/wwdc2022/10110/|Localization by example , and https://developer.apple.com/videos/play/wwdc2022/10107/|Get it Right (to Left) to build up an understanding of what you should take into account when designing apps for an international audience. <@U03JRP87THN> I would say, generally speaking look out for two things: 1. Language: design layouts may be impacted by RTL languages Orientation or the less dense text strings in CJK languages. Another example is that certain scripts require different bounding boxes or spacing whether less or more than latin (E.g. Thai needs more vertical spacing in the UI to avoid clipping). 2. Culture: What are the cultural behaviors and nuances that can impact the features you support, the hierarchy or the elements in the UI, or simply the visual language (E.g. colours that could be perceived differently in different cultures). Thank you! Will look into the resources.","title":"Hi there! Didn't have time to check out the new resources yet (but will do in the future for sure!). Could you tell me what are the main key points that we should take into account when designing apps targeting an international audience?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#should-rtl-specifically-arabic-text-work-out-of-the-box-using-swiftui-on-watchos-8-are-there-any-configurations-or-swiftui-properties-that-might-break-it-i-have-a-watch-app-and-rtl-seem-to-work-fine-i-cant-read-them-but-they-look-okay-but-ive-gotten-feedback-that-they-fail-for-some-users-arabic-text-is-backwards-and-the-characters-arent-joined-like-they-should-be-im-just-using-text-so-i-cant-see-why-this-would-happen-i-can-provide-screenshots-in-the-thread-if-possible","text":"Hi Christopher! I think I may have an idea of what issue you\u2019re encountering, but to be sure, a screenshot would definitely help. Please feel free to share. Sure, here's the same text on watchOS and iOS. I've been unable to reproduce the this with my own strings but I don't have a copy of this exact text. I see; there are two different issues going on here. The Watch screenshot demonstrates broken and completely illegible Arabic text, and it\u2019s also not using the System Font. It\u2019s difficult to be sure about what\u2019s going on here without a sample app to reproduce this issue in. I am setting the Text\u2019s font using UIFont and a FontDescriptor so I'd guess that's the culprit One thing I will note is that in the Watch screenshot, the title \u0627\u0644\u062a\u062d\u0631\u064a\u0645 is rendering correctly, but the body text below that is what\u2019s completely broken. Not sure why it fails for some RTL strings and not others It\u2019s possible that the specific font you\u2019re using supports Arabic glyphs, but not the appropriate shaping. I should note that the alignment in both the iOS and watchOS screenshot is unnatural for Arabic, i.e. the text is left-aligned instead of right-aligned. Does this app localize into Arabic, i.e. have ar.lproj or is it localized into another language and this content is being loaded at runtime? The app is localized in Russian, which is what this user is set to. And then he's entered Arabic text. I'm using the system font but applying a design: Here's another example where (I think?) all the text is working fine: indeed that does look correct I see. Nothing jumps out to me immediately as being wrong here. I would really appreciate it if you could file a Feedback Report with a sample app which demonstrates this issue. I would love to look into this problem further. Yes, the second screenshot here looks correct; I also see that it\u2019s using the System Font. have you ever been able to reproduce this locally, or are your only reports of this from user(s)? I have not been able to reproduce (second screenshot is mine). I've asked the user to send me the text itself so I can try it. Unfortunately I haven't been able to copy that Arabic text out of the iPhone screenshot. To start, I think it would be good to file a feedback with as much of your watch app as you can reasonably include (with build instructions if needed beyond build and run) and these screenshots. Out of curiosity, was the iPhone screenshot above also from the same user? I happened to notice that one of the UI elements was rendering text using the Urdu font, and it\u2019s possible that that is the clue to when this issue reproduces, i.e. when Urdu is added to the preferred languages list (higher than Arabic, if Arabic is also present). Yes, that iPhone screenshot was from the same user. Okay, I'll put together a Feedback with whatever I can get. Thanks! Good spot! I was also wondering why the Arabic text was shaped like this on the navigation bar, but it\u2019s actually Urdu! Please do post the feedback number once you have that filed back in this thread. Thanks again!","title":"Should RTL (specifically Arabic) text work out of the box using SwiftUI on watchOS 8? Are there any configurations or SwiftUI properties that might break it? I have a Watch app and RTL seem to work fine (I can\u2019t read them but they look okay) but I\u2019ve gotten feedback that they fail for some users \u2014 Arabic text is \u201cbackwards\u201d and the characters aren\u2019t joined like they should be. I\u2019m just using Text so I can\u2019t see why this would happen. I can provide screenshots in the thread if possible."},{"location":"wwdc22/localization-and-internationalization-lounge.html#how-can-i-right-align-arabic-text-in-push-notifications-efficiently-when-only-the-app-language-is-arabic-not-the-entire-device","text":"Thanks for raising this use case. Can you please describe your use case in a bit more detail in a Feedback report, sharing the number here? We\u2019ll follow up there. I am working on an application which has two languages English and Arabic, now users can change the app language whenever they want. So ideally if the app language is English the text displayed in push notifications is left aligned as it should be, but when the app language is Arabic the text should be right aligned but it's not. Even when the app is in Arabic the text in push notifications is left aligned. While the rest of the app is completely right aligned. Thanks <@U03J6B7G2QN>, that is a great description. If you don\u2019t mind reporting that via the Feedback tool, it makes it easier for us to track a solution internally, and follow up with you beyond the Digital Lounge. Here FB10161536","title":"How can I right align Arabic text in push notifications efficiently when only the app language is Arabic not the entire device."},{"location":"wwdc22/localization-and-internationalization-lounge.html#hi-is-the-generated-xcloc-file-will-fetch-all-strings-update-in-every-export","text":"Yes, running Export Localizations will re-process your source code to extract any newly added, removed, or changed strings and include them in the exported localization catalog. It will also include the existing translations when exporting for languages other than English. That\u2019s Weird. Becoz I found it only works at the 1st time. Then all xcloc will follow the version of the 1st import\u2026 https://wwdc22.slack.com/archives/C03H786M2V8/p1654879512571169?thread_ts=1654804988.358809&cid=C03H786M2V8 I can follow up in your other thread ok thx","title":"Hi. Is the generated xcloc file will fetch all strings update in every export?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#so-if-i-have-a-swiftui-or-uikit-notes-app-thats-not-localized-in-any-rtl-languages-but-a-user-enters-rtl-text-am-i-correct-that-the-expected-behavior-is-that-the-text-itself-will-appear-rtl-but-will-be-left-aligned-is-there-an-easy-way-to-get-this-to-appear-right-aligned-or-would-i-have-to-analyze-the-language-myself-or-is-this-undesirable-behavior-in-an-otherwise-left-aligned-app","text":"On iOS, when a user enters text into a text field or text view, the alignment follows the default alignment of the keyboard, and you should not need to do anything extra on top of this. If that text is then displayed in a UILabel, for example, the alignment would be based on the UI language, rather than the content of the label itself. If you have a use case for something more automatic, please do file a feedback describing it in more detail. Okay thanks","title":"So if I have a SwiftUI or UIKit notes app that\u2019s not localized in any RTL languages but a user enters RTL text, am I correct that the expected behavior is that the text itself will appear RTL but will be left-aligned? Is there an easy way to get this to appear right-aligned, or would I have to analyze the language myself? Or is this undesirable behavior in an otherwise left-aligned app?"},{"location":"wwdc22/localization-and-internationalization-lounge.html#hello-devs-we-have-localization-strings-being-delivered-by-middle-ware-and-get-updated-and-stored-in-application-directory-on-each-app-launch-where-we-used-to-load-them-using-localizable-nocache-as-mentioned-on-apple-docs-however-this-slow-down-the-ul-performance-way-too-much-as-it-seems-it-would-load-whole-strings-map-for-each-call-to-nslocalizationstring-can-you-suggest-a-work-around-thanks-fahied","text":"Hi Muhammad! .nocache should only need to be used if: \u2022 the strings table has already been loaded during your app processes lifetime, \u2022 the table is known to have been updated based on some other signal Use of .nocache is expected to have some performance overhead so some techniques to reduce are : \u2022 Use it as sparingly as necessary (ie is it really user critical to get non-cached strings) \u2022 Consider re-organizing the strings tables into smaller tables that map to user activities so that unnecessary strings aren\u2019t being loaded into memory Hi Muhammad, can you also file a feedback report about your specific use case so we can look into enhancement in this area too? Sounds great, I ll do that. The problem is really If I load Strings without .nocache, It will not load the updated strings even though the same source files .strings is overriden or even you force load main bundle again. That's right. The strings files are cached by bundle when bundle is created. Since there's no way to really \"unload\" a bundle, the cached resources remained cached Depending on the scope of your project, you can consider having a separate XPC service to load the strings, and kill and relaunch the service when you get a notification when the strings are updated, if such feedback is available via your middle ware Would there be any possibility for iOS in term of XPC service? Ah.. sorry I didn't ask about your environment first. Unfortunately I don't think so I thought may be you guys holding on to something :smile:","title":"Hello Devs,  We have localization Strings being delivered by Middle ware and get updated and stored in Application Directory on each app launch. Where we used to load them using \u2018Localizable. nocache\u2019 as mentioned on Apple Docs. However this slow down the Ul performance way too much. As it seems it would load whole strings map for each call to NSLocalizationString. Can you suggest a work around?  Thanks Fahied"},{"location":"wwdc22/localization-and-internationalization-lounge.html#not-sure-its-a-bug-bug-if-my-development-lanugage-is-english-but-if-i-dont-add-english-in-my-localization-it-going-to-display-other-language-the-work-around-i-did-now-is-add-empty-english-localization-file-for-each-file","text":"Can you clarify your project setup? In general, your development localization should be one of your app localizations. Prior to iOS 16/macOS Ventura, it was required that your app bundle also contain a corresponding lproj folder for the development language, but you should not need any empty files.","title":"Not sure it's a bug bug if my development lanugage is english but if I don't add English in my Localization it going to display other language. The work around I did now is add empty english localization file for each file?"},{"location":"wwdc22/localization-and-internationalization.html","text":"Localization and internationalization Lounge QAs by emin Sometimes there is a difference in font size(width/height) between LTR and RTL , how can we make this better? There are a couple of things you could be seeing and I\u2019d like to understand which it might be. The System Font for some languages like Arabic (which is RTL) uses a taller line height than other languages, which you will notice if you\u2019re using Text Styles and have multiline labels or even in the default cell heights in Table Views. In fact, we recommend using Text Styles to ensure that you do get this language-specific tailoring. Different System Fonts also may have different relative font weights and sizing and it\u2019s possible that what you\u2019re seeing is as a result of that. Do either of these sound like what you may be observing? yep, the first one. In that case, this is correct and expected behaviour, because different languages require different line heights to ensure that text doesn\u2019t overlap across lines and has room to breathe. Are you encountering any problems with this? Sometimes when let's say my use case when u have a cell with both English and Hebrew together, it looks abit weird, thought mby equaling their actual height of one of them is the best practice but i'm not sure. a 10.0 font size in some Hebrew font can be smaller than a 10,0 font size of SF English f.e That\u2019s true. For example, if we compare \u201cWelcome\u201d and \u201c\u05d1\u05e8\u05d5\u05db\u05d9\u05dd\u201c, Hebrew will not be as tall as the capital letters in English and the Hebrew letters are slightly taller than the small letters in English. This is something that varies by script and we cannot expect that the heights will be equal. Generally speaking, the way the System Fonts on iOS, macOS, etc. are designed, if you are using a Text Style, you should get a reasonable size for the text across various scripts. Thank you! To be clear, Hebrew does not use a different line height compared to English. It does, however, use a different UI font, which is why you're seeing this difference, as it is difficult to perfectly match up metrics between two independent fonts. Our app includes multiple tutorials. Each tutorial is in its own directory with tutorial data, images and Localizable.strings files for localized text for just this tutorial. This makes it easy for us to add new localized tutorials without having to modify the app's main Localizable.strings file. Is there a way to include these Localizable.strings files for the many tutorials in the output when we use Export for Localization in Xcode? Or are we going about this wrong? Hi! Can you elaborate a little on how you are setting up these files in your projects? For example, are they part of a target, or do you save them as a pre-created bundle? They are currently in reference folders in Xcode. In the main project and target. Our recommendation would be to convert these to bona fide bundle targets in your project. That would allow them to be properly extracted automatically. As long as the localizable assets are part of a \u201cCopy Resources\u201d phase in your target, they should be extracted when doing Product > Export for Localization. As <@U03HW7PE3SM> mentioned, you can have a \u201cBundle\u201d target for each of your tutorials and then include the localizable resources in each bundle. Then you can include these bundles in your main application target. I'm not familiar with the process of creating bundle targets. But we have over 1000 tutorials and that may be a lot of overhead. The root Tutorials reference folder is included in the Copy Bundle Resources build phase. But I don't see the Localized.strings files, which are in sub folders, in the results from Export for Localization. Yeah, unfortunately Xcode does not look through folder references for strings files \u2014 they are meant to add a folder to your product as-is, without any postprocessing. As <@U03HWDD6RED> mentioned, Xcode does not export the \u201cblue\u201d folder references. One thing you might be able to try is moving all of your tutorials into a Swift package. Xcode 14 adds support for exporting Swift packages for localization and Swift packages are represented by a folder on-disk. Any other suggestions for organizing tutorial or lesson files to be localized? We would like to use Xcode's great localization tools as much as possible. <@U03HJA9JVGA>\u2019s suggestion will let you do just that, for when you can adopt Xcode 14, and is probably your best bet going forward. You can include the \u201cTutorials\u201d Swift package inside of your app\u2019s Xcode project and then when you export for localization\u2014it should include all of the localizable assets and strings in the .xcloc Ok. Thanks! Give it a try and if you run into issues, please feel free to file a report with Feedback Assistant! Hi all! When editing an .xloc file in XCode, I'm missing one key feature: filtering or sorting the list of keys by whether or not they were translated into the second language. I seem to only be able to sort alphabetically. Am I missing something? Hi Jan! As you pointed out, Xcloc Editor supports sorting alphabetically and filtering by search terms, but does not support filtering by \u201ctranslated\u201d or \u201cuntranslated\u201d strings. This is great feedback though! If you would benefit from this feature, please file a feedback report and paste the ID in this thread. Thank you <@U03H3NEH4CX>, submitted at FB10114561 Perfect, thank you! Hi! Is there a built-in workflow in Xcode to localize settings bundles? In a way that Xcode would export all the relevant strings to .string files and the Export Localizations menu command would include those automatically into the exported .xliff files? Hi Daniel! Xcode supports localization export for Settings bundles that are named \u201cSettings.bundle\u201d and contain .strings files. If this isn\u2019t working for you, can you explain the setup you\u2019re using and the behavior you\u2019re experiencing? Hi Matt, ok, I understand what my problem was. As you mentioned, the strings from the Settings.bundle plist are actually exported correctly. I'm in addition using the third party https://github.com/futuretap/InAppSettingsKit|InAppSettings framework , which builds on the Settings.bundle format but supports additional key/values and can be embedded into the app. It was its InAppSettings.bundle where the strings weren't automatically exported from the plist. Thanks for the answer! Ahh, glad you were able to find the problem! Yes, thanks again! How would you handle the case where you want to provide variations in a language? For example I want to provide localization for the word colour. UK people would see colour, but US people would see color. Is there a way so all other text is shared but I could localize that one word? Hi! Unfortunately, once we find a strings table for a specific lproj, we use the contents of that table as they are without doing any more fallbacks. For example, you can localize a table for en_GB, but it has to contain all strings, even those that are the same as other English localizations. Seems like their should be a hierarchy in the string handling where you can have an overlay (e.g for GB) that only needs to contain the differences to another version (e.g US) Ah ok, that's too bad. Are there any plans to be able to do so? Is it worth it to file feedback about it? On the plus side, you can in fact localize for English variants, like English (UK), English (Ireland), and so on. Yeah, it would be wonderful to have a feedback! Thanks! Created FB10115590 :blush: How do we format and sort time values in a localized way? for example, if i'm showing a list of hours of operation for the week, the time strings will be different (24 hr vs 12 hr), and the order of the week days will be different (starting on monday vs sunday). Hi! For sorting, it would be best to sort based on the original binary data, then use .formatted() to produce a localized time and/or date. could you clarify what \u201coriginal binary data\u201d means? How are you storing the days/hours prior to localization? For example, is it a Date , or are you receiving the data in some other form? it would be a JSON from an API response in english, { day: 0, start: \"0700\", end: \"2300\" } and 0 means sunday as specified in the API Got it. You should be able to sort directly on those strings. For the first day of the week issue, you can check Calendar.firstWeekday . (on the current user default calendar) To produce formatted times, you'll need to convert the strings to a Date object, then use .formatted() with the appropriate options to produce the string you want. thank you!! You should be able to put a weekday/hour/minute in DateComponents and then use Calendar to make a Date from that. You can then make a DateInterval from two Dates for the range (e.g. \"9 AM - 5 PM\") and then use .formatted() on that. One more note: when using Calendar to convert DateComponents to Date , set the time zone to GMT to avoid Daylight Time issues. Same with .formatted() We have an app that has English only features. Let's imagine that's as an advanced assistant for editing English texts and our assistant doesn't support other languages. If we localize our app, would it improve user experience for people who use English as a second language, or would it only confuse them on why their main language is not supported, since we only expect English texts for now? Hi Roman, I think it\u2019ll depend on exactly what your app does. For example, hypothetically, if you have an app that points out grammar notes about English, it might still be useful for people to see that information in a language they find easier to understand than English. So, if you can share details, I might be able to help give you a more specific, helpful answer :slightly_smiling_face: Basically, it is an editor app, which has users' documents. When user creates the doc and fills it with a text, he receives grammar suggestions, as well as clarity, fluency, readability and other suggestions. Each suggestion is a card that has: \u2022 previous word/sentence and the resulting sentence \u2022 detailed explanation of why the user should apply this suggestion in that particular context - like because it sounds unconfident, or because it has a grammatical error, and so on Right\u2026 I think some users\u2014especially those who have less familiarity with English to begin with\u2014could certainly benefit from localization. However, you may want to be cautious about (a) which languages you choose, and (b) how you separate out the \u201ccontent language\u201d (English) from the \u201cUI language\u201d so that there\u2019s no confusion or aesthetically displeasing mixture of languages. I've been thinking about some extra flow to suggest users who are not from EFL countries to have some pieces of the content being translated to their native language, just like that hint on why they should apply that alert. However, I dunno if users would be ok with English interface since they already write some English texts and the interface is primitive enough to be understandable. But at the same time having some interface parts in the native language, like we as an app just forgot to translate everything :sweat_smile::sweat_smile: You\u2019re thinking about all the right things! And it\u2019s true that with English being the lingua franca in today\u2019s world, many people might be familiar enough with the English user interface such that they won\u2019t need localization. However, it\u2019s always handy to have choices, and your users can also go into Settings to switch your app to English if that\u2019s what they prefer. In terms of making sure that the localization doesn\u2019t look partial or incomplete, I would definitely suggest making sure that any \u201cchrome\u201d (i.e. UI elements) is translated consistently in each of the languages you target. For example, if the tables are turned and your app is actually for helping people with French, as a French learner, I would appreciate it if I could use it in English and see suggestions like this: > The verb bouffer could be too formal. Consider using d\u00e9jeuner or manger . I\u2019m limited by what I can do in Slack here stylistically, but you can see that I have marked the text in the content language in a different style than the UI language, which gives me clarity as the user as to what\u2019s going on and where the language boundaries are. That's exactly what I've been thinking about. Last one - if you will have those suggestions in English, having French texts, would you as a new user prefer having whole app interface in English? Wouldn't you start typing in English instead of French, since the app is fully localized?)) Yes, I suppose I might! It\u2019s hard to answer that in the abstract though without knowing exactly what the app looks and feels like. It all depends on which direction your app\u2019s design steers the user in. Yeah, well... I'm talking about Grammarly app for iOS. Would be great to learn your thoughts on that. Oh, interesting! I was guessing that this sounded like Grammarly! I would suggest looking at your app\u2019s usage and tackling one market as an initial foray, e.g. China or Japan, and see what feedback you get from your customers :slightly_smiling_face: Personally, I don't know that the app being localized in a given language would necessarily imply that I can input text in that language (though obviously it's ideal!). Taking for example our own products, we localize all features of the OS into all languages completely, including things like Apple Pay Later that might only be available in some regions. There's probably no right way to do this, but when working with localized strings in code, do you recommend using the new string initializer (which looks great btw!) directly throughout the app, or creating, for example, a file with enums that return those localized strings? We strongly recommend that you use String(localized:\u2026) , AttributedString(localized:\u2026) or in Objective-C NSLocalizedString and NSAttributedLocalizedString ! If you do that, Xcode can extract strings for you to provide to your localizers \u2014 it knows how to recognize these macros and initializers and how to produce a xcloc bundle that contains them all, and then reintegrate it into your project. String constants used with other mechanism will not be recognized by this mechanism, and you will have to manually handle extraction into .strings files and any changes going forward. Note that SwiftUI also uses the LocalizedStringKey type, and Xcode also knows about it. So, when you write Text(\"Hello, world!\") , since Text(\u2026) takes a LocalizedStringKey , Xcode will also know to extract it. In Swift, you can even use String interpolation to include variables directly in the initializer String(localized: \"Welcome to \\(newCity)\", comment:\"Large headline label welcoming the user to their new location, the variable refers to the city name.\") Please make sure to explain what the variable refers to, so that translators have that knowledge, too :slightly_smiling_face: Yup! This works in String(localized:\u2026) , AttributedString(localized:\u2026) and in SwiftUI. As long as you\u2019re using this setup, in general, you\u2019re golden \u2014 if you\u2019re using enums to represent your strings, just make sure that they\u2019re calling into these functions. In general, using enums may complicate your patches a little and interact weirdly with SwiftUI if you try to pass String s to SwiftUI initializers that expect localized keys. Hello. Can the Localizable.strings updates itself? The first time export and import is okay. However, when the project contains a Localizable.strings file then the next export will not contain new strings added. Hi ChakMing! The expected flow is as follows: 1. Have some strings in your project (either .strings files or strings in code) 2. Export Localizations, which includes all strings in an xcloc 3. Translate the xcloc 4. Import Localizations with the translated xcloc (This step updates/adds .strings files in/to the project.) 5. Exporting again should include all strings in the project (including those previously imported) in the xcloc If this is not working as expected, please file a feedback so that we can take a look. <@U03H3NEH4CX> Hello again. Here\u2019s what I found. At initial, the export will include all strings. If I imported the xcloc, it will generate Localizable Files. Then I add more words on SwiftUI, and export. The xcloc is still using the old localizable file. I thought it had been happening for a long time\u2026 Do you think it is a bug, or is this correct? So my expectation here is that the second export would include both the strings in your SwiftUI code and the strings in the .strings files that the import created. Xcode won\u2019t remove stale translations automatically, because those translated strings can be a valuable resource if you ever need to translate them again. However, please feel free to file a feedback request if you think we should change the flow here! I just re-export and seems it works this time! Thanks, <@U03H3NEH4CX> <@U03HBMCRX0E> Hello. How can we make sure the system and VoiceOver identify a certain text\u2019s language correctly. Is there a tag we can set somewhere? I\u2019m asking for both web and apps. It happens from time to time where the system falsely identifies Urdu for Arabic, or recently on iOS 16, Persian for Arabic. How can we improve this? If you know the language of your text, you can annotate it with that language. For web content, you can use the https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/lang|lang attribute, which can be applied to any HTML element, including &lt;p&gt; , &lt;span&gt; , and &lt;body&gt; . For app content, you can create an attributed string and set the NSLanguageIdentifierAttributeName to the language code of the corresponding text. If you have reproducible strings that are incorrectly identified, we would love to have feedback with those examples as well! Here\u2019s the feedback ID: FB10152753 It fails on everything. Here are some example texts: \u0645\u0646 \u0633\u0639\u06cc\u062f \u0647\u0633\u062a\u0645 \u0627\u0645\u0631\u0648\u0632 \u0628\u0631\u06cc\u0645 \u0631\u0633\u062a\u0648\u0631\u0627\u0646\u061f \u0686\u0647 \u062e\u0628\u0631\u061f <@U03HW7PE3SM> Thanks! We\u2019ll be sure to pass this along. What\u2019s the best way to use two custom fonts (non SF Pro for marketing reasons) for Arabic and Latin scripts? The most challenging part is that the text could be bilingual and it may come from the server. For instance, \u201c\u0628\u0647 WWDC \u062e\u0648\u0634 \u0622\u0645\u062f\u06cc\u062f\u201d. I want two fonts to be used for this snippet. It looks you forgot to answer this. You can do it by setting https://developer.apple.com/documentation/coretext/kctfontcascadelistattribute|kCTFontCascadeListAttribute , either have the primary font be the Latin font, then have the cascade list contain your Arabic font (the way we do it for our system), or the other way around. Another option is to use (NS)AttributedString and set the fonts on the individual segments of the string. The thing is I mostly don\u2019t have access to the text beforehand. Let\u2019s say it\u2019s coming from a server. Then the font fallback list (as above) is best. Note that in addition to the CoreText version above, there's also https://developer.apple.com/documentation/uikit/uifontdescriptorcascadelistattribute/|UIFontDescriptorCascadeListAttribute and https://developer.apple.com/documentation/appkit/nsfontcascadelistattribute/|NSFontCascadeListAttribute , if you're using higher-level text APIs. Is there a non-hacky way to force a single language for an app? Let\u2019s say the app only supports \u201car-SA\u201d and the user\u2019s device language is set to \u201cen-US\u201d. I used to set a value for a certain in key in UserDefaults at launch to make it work. You should not need to set any user defaults. Simply make sure that the developmentRegion in your project.pbxproj is set to the correct language and that all your localizable files are in the correct lproj. If your app does not have an lproj folder, you will need to be on iOS 16, macOS 13, tvOS 16, and watchOS 9 to get correct behavior. Is there a way to force the numeral system for an app? For instance, the user may have chosen \u2018\u06f1\u06f2\u06f3\u2019 as their numbering system of choice where it only make sense in our app to show numbers as \u2018123\u2019. One thing that\u2019s still missing and I\u2019d already filed a report as I remember, is choosing the numeral system when changing the language for a specific app. Now you can only change the numeral system when you change the device language as a whole. Simply don\u2019t localize the numbers (e.g. don\u2019t use a formatter) and the numbers should be preserved as the literals you are using in your code. Do you have a code sample to share to elaborate more on the issues you are seeing? I think I phrased my question wrong. For instance, install Whatsapp on a device whose locale is set to English - United States. Then, go to Whatsapp settings page inside system Settings and change its language to Persian. Now Open Whatsapp. You\u2019ll see the system keyboard has number row in Latin. Whereas in iMessage on the same device it\u2019s in \u06f1\u06f2\u06f3\u200d form. <@U03JPJ277SQ> IMHO, this may be a bug. Do you mind reporting this bug and sharing the FB # here? <@U03J83G0WQG> Here it is: FB9647599. I believe we already discussed this on Twitter as well. When I release my apps I can usually find some people to help me translate the initial release. But over time it makes it difficult for me to create updates which require new translation strings. Any best practices you would have there for indie developers that don't have a lot of resources to pay 20+ translators for each release? It is understood that adding languages to your application can become quite the commitment \u2014 not just a commitment in the moment, but also going forward. It does pay off to make a plan in which you can sit down and think whether you can do so sustainably for every single feature you do, including adding languages to your application. Sometimes, it is a valid choice to know that the time may just not be there every release, and to choose perhaps to support fewer languages. At other times, community-supported projects may be able to elicit contributions from the community that can help where a single developer may not, depending on what your relationship is with that community. But it is very much a commitment \u2014 often an economic one \u2014 you may want to think about as an ongoing task that requires care and maintenance. Just to clarify \u2014 you are not having issues with filtering out new strings from old ones, correct? in other words, using the Xcode import/export workflow, you should be able to only have to deal with new/changed strings on every release, rather than re-translating the entire app. There are third-party services that offer (human) translation as a service for your localizable strings. But you'll have to document your strings well, or they may get the context wrong and mistranslate. We saw that this year you're adding punctuation to Hebrew (Thank you for that! \u05ea\u05d5\u05d3\u05d4 \u05e8\u05d1\u05d4) Besides the obvious addition to the Hebrew Keyboard is there anything else we should know about that? Hi <@U03J1TN6WBD>, we have indeed added niqqud support to the iPhone & iPad keyboard layouts this year. There\u2019s nothing else new for Hebrew this year per se, but it\u2019s worth mentioning that Yiddish is also now supported as a separate keyboard language. Thank you! a dank Maybe a slightly tangential question, but are there learnings or strategies used during RTL localization that could be beneficial for adapting a UI with significant lateral elements for left handed use? Does \u201chandedness\u201d come into play when considering the design of RTL UIs? We treat Handedness and Layout Directionality as distinct elements and handle each of those in different ways. For example, for the One-Handed Keyboard, we offer both Left-Handed and Right-Handed modes. UI elements or gestures that indicate a progression that go left-to-right in an LTR language do mirror to go right-to-left in an RTL language. So, handedness does not come into play here. This is all fairly abstract, of course. If you have a concrete example that you want to ask about, do feel free to share it :) I guess, as a lefty, I never really understood how an adaptive layout could really change how someone uses a device until I started looking at how RTL languages are handled in the HIG. The chevrons in lists are meant to point towards the thumb you use to tap them :exploding_head:. I would imagine that the inverse could be true in RTL situations\u2026 has that changed your design strategy? For example, an iPad in portrait orientation puts the capture button on the right hand side. This seems like a place where handedness would take priority. But there are other situations where the inverse would be true? I didn't see specifics about gestures in the HIG article, so I was wondering if I just missed something or if there was some type of helpful underlying principle to keep in mind. Those are thought-provoking questions, <@U03JCRFMPHV>, and I likely won\u2019t have any satisfying answers to your larger question in this forum. It does seem like Handedness and how it applies to User Interface Design is a topic that I\u2019m not the best person to answer, though I\u2019ll see if there are any other colleagues of mine who may have an answer for you. The overall rule is that the design layout in RTL matches the flow that the user would expect elements to flow in the UI - including the text strings, animation, navigation (E.g. If the heat map of people that uses an LTR layout - e.g. website - is on the top left corner, it would be top right corner in RTL). The only thing that we should take care of is: Reachability: if the design intention is to have an element closer to the right hand - then it should not be changing because of the RTL layout (E.g. the keyboard layout does not change when you switch LTR or RTL / emojis entry point would be at the same exact place). Other elements that could impact our decisions while changing to RTL layout: 1. Memorization: If the UI component would be very connected to how user would remember things, it should not change (E.g. the order of the lock screen numbers remain the same in both RTL & LTR) 2. Images, videos, backgrounds should not also be impacted (E.g. the weather App background). <@U03JCRFMPHV> Let me know what do you think. This is very interesting, thank you for sharing some of the methodology in your design thinking. I was surprised to see elements like system level scroll bars flip to the left side. It's never been under my thumb before. Seems disadvantageous for RTL righties? <@U03J83F2TDE> That\u2019s a really great question <@U03JCRFMPHV> Two angles for this: (which are up for discussion too) :slightly_smiling_face: \u2022 In the layout, having scroll bars usually is on the other side where your attention should be. \u2022 And for reachability, I usually think about it device by device. On macOS - which I think is where scroll bars are used the most - there is no reachability issue as it is not touch based. Touch based devices are less likely to rely on it and it is used as an indicator more than an interactive piece of the UI as we use swipe instead. And even on iPad for example if we decide to use it, we mostly hold it with two hands and the left hand solves it too. Definitely. I can honestly say that having the scroll bar under my thumb for the first time\u2026 and to not have my scrolling thumb blocking the icons in lists for example, was a brand new experience. Especially on the larger phone, it makes reaching the compose button much easier :joy:. Interesting to see places where steps were taken to counteract reachability issues. Like floating the iPad portrait camera capture button to the right. Thank you for your thoughtful response! <@U03J83F2TDE> You\u2019re always welcome <@U03JCRFMPHV> Nice questions :D Hi there! Didn't have time to check out the new resources yet (but will do in the future for sure!). Could you tell me what are the main key points that we should take into account when designing apps targeting an international audience? Hi <@U03JRP87THN>, there\u2019s no succinct answer for this; so, I would highly recommend checking out https://developer.apple.com/videos/play/wwdc2018/201/?time=1665|Creating Apps for a Global Audience , https://developer.apple.com/videos/play/wwdc2022/10110/|Localization by example , and https://developer.apple.com/videos/play/wwdc2022/10107/|Get it Right (to Left) to build up an understanding of what you should take into account when designing apps for an international audience. <@U03JRP87THN> I would say, generally speaking look out for two things: 1. Language: design layouts may be impacted by RTL languages Orientation or the less dense text strings in CJK languages. Another example is that certain scripts require different bounding boxes or spacing whether less or more than latin (E.g. Thai needs more vertical spacing in the UI to avoid clipping). 2. Culture: What are the cultural behaviors and nuances that can impact the features you support, the hierarchy or the elements in the UI, or simply the visual language (E.g. colours that could be perceived differently in different cultures). Thank you! Will look into the resources. Should RTL (specifically Arabic) text work out of the box using SwiftUI on watchOS 8? Are there any configurations or SwiftUI properties that might break it? I have a Watch app and RTL seem to work fine (I can\u2019t read them but they look okay) but I\u2019ve gotten feedback that they fail for some users \u2014 Arabic text is \u201cbackwards\u201d and the characters aren\u2019t joined like they should be. I\u2019m just using Text so I can\u2019t see why this would happen. I can provide screenshots in the thread if possible. Hi Christopher! I think I may have an idea of what issue you\u2019re encountering, but to be sure, a screenshot would definitely help. Please feel free to share. Sure, here's the same text on watchOS and iOS. I've been unable to reproduce the this with my own strings but I don't have a copy of this exact text. I see; there are two different issues going on here. The Watch screenshot demonstrates broken and completely illegible Arabic text, and it\u2019s also not using the System Font. It\u2019s difficult to be sure about what\u2019s going on here without a sample app to reproduce this issue in. I am setting the Text\u2019s font using UIFont and a FontDescriptor so I'd guess that's the culprit One thing I will note is that in the Watch screenshot, the title \u0627\u0644\u062a\u062d\u0631\u064a\u0645 is rendering correctly, but the body text below that is what\u2019s completely broken. Not sure why it fails for some RTL strings and not others It\u2019s possible that the specific font you\u2019re using supports Arabic glyphs, but not the appropriate shaping. I should note that the alignment in both the iOS and watchOS screenshot is unnatural for Arabic, i.e. the text is left-aligned instead of right-aligned. Does this app localize into Arabic, i.e. have ar.lproj or is it localized into another language and this content is being loaded at runtime? The app is localized in Russian, which is what this user is set to. And then he's entered Arabic text. I'm using the system font but applying a design: Here's another example where (I think?) all the text is working fine: indeed that does look correct I see. Nothing jumps out to me immediately as being wrong here. I would really appreciate it if you could file a Feedback Report with a sample app which demonstrates this issue. I would love to look into this problem further. Yes, the second screenshot here looks correct; I also see that it\u2019s using the System Font. have you ever been able to reproduce this locally, or are your only reports of this from user(s)? I have not been able to reproduce (second screenshot is mine). I've asked the user to send me the text itself so I can try it. Unfortunately I haven't been able to copy that Arabic text out of the iPhone screenshot. To start, I think it would be good to file a feedback with as much of your watch app as you can reasonably include (with build instructions if needed beyond build and run) and these screenshots. Out of curiosity, was the iPhone screenshot above also from the same user? I happened to notice that one of the UI elements was rendering text using the Urdu font, and it\u2019s possible that that is the clue to when this issue reproduces, i.e. when Urdu is added to the preferred languages list (higher than Arabic, if Arabic is also present). Yes, that iPhone screenshot was from the same user. Okay, I'll put together a Feedback with whatever I can get. Thanks! Good spot! I was also wondering why the Arabic text was shaped like this on the navigation bar, but it\u2019s actually Urdu! Please do post the feedback number once you have that filed back in this thread. Thanks again! How can I right align Arabic text in push notifications efficiently when only the app language is Arabic not the entire device. Thanks for raising this use case. Can you please describe your use case in a bit more detail in a Feedback report, sharing the number here? We\u2019ll follow up there. I am working on an application which has two languages English and Arabic, now users can change the app language whenever they want. So ideally if the app language is English the text displayed in push notifications is left aligned as it should be, but when the app language is Arabic the text should be right aligned but it's not. Even when the app is in Arabic the text in push notifications is left aligned. While the rest of the app is completely right aligned. Thanks <@U03J6B7G2QN>, that is a great description. If you don\u2019t mind reporting that via the Feedback tool, it makes it easier for us to track a solution internally, and follow up with you beyond the Digital Lounge. Here FB10161536 Hi. Is the generated xcloc file will fetch all strings update in every export? Yes, running Export Localizations will re-process your source code to extract any newly added, removed, or changed strings and include them in the exported localization catalog. It will also include the existing translations when exporting for languages other than English. That\u2019s Weird. Becoz I found it only works at the 1st time. Then all xcloc will follow the version of the 1st import\u2026 https://wwdc22.slack.com/archives/C03H786M2V8/p1654879512571169?thread_ts=1654804988.358809&cid=C03H786M2V8 I can follow up in your other thread ok thx So if I have a SwiftUI or UIKit notes app that\u2019s not localized in any RTL languages but a user enters RTL text, am I correct that the expected behavior is that the text itself will appear RTL but will be left-aligned? Is there an easy way to get this to appear right-aligned, or would I have to analyze the language myself? Or is this undesirable behavior in an otherwise left-aligned app? On iOS, when a user enters text into a text field or text view, the alignment follows the default alignment of the keyboard, and you should not need to do anything extra on top of this. If that text is then displayed in a UILabel, for example, the alignment would be based on the UI language, rather than the content of the label itself. If you have a use case for something more automatic, please do file a feedback describing it in more detail. Okay thanks Hello Devs, We have localization Strings being delivered by Middle ware and get updated and stored in Application Directory on each app launch. Where we used to load them using \u2018Localizable. nocache\u2019 as mentioned on Apple Docs. However this slow down the Ul performance way too much. As it seems it would load whole strings map for each call to NSLocalizationString. Can you suggest a work around? Thanks Fahied Hi Muhammad! .nocache should only need to be used if: \u2022 the strings table has already been loaded during your app processes lifetime, \u2022 the table is known to have been updated based on some other signal Use of .nocache is expected to have some performance overhead so some techniques to reduce are : \u2022 Use it as sparingly as necessary (ie is it really user critical to get non-cached strings) \u2022 Consider re-organizing the strings tables into smaller tables that map to user activities so that unnecessary strings aren\u2019t being loaded into memory Hi Muhammad, can you also file a feedback report about your specific use case so we can look into enhancement in this area too? Sounds great, I ll do that. The problem is really If I load Strings without .nocache, It will not load the updated strings even though the same source files .strings is overriden or even you force load main bundle again. That's right. The strings files are cached by bundle when bundle is created. Since there's no way to really \"unload\" a bundle, the cached resources remained cached Depending on the scope of your project, you can consider having a separate XPC service to load the strings, and kill and relaunch the service when you get a notification when the strings are updated, if such feedback is available via your middle ware Would there be any possibility for iOS in term of XPC service? Ah.. sorry I didn't ask about your environment first. Unfortunately I don't think so I thought may be you guys holding on to something :smile: Not sure it's a bug bug if my development lanugage is english but if I don't add English in my Localization it going to display other language. The work around I did now is add empty english localization file for each file? Can you clarify your project setup? In general, your development localization should be one of your app localizations. Prior to iOS 16/macOS Ventura, it was required that your app bundle also contain a corresponding lproj folder for the development language, but you should not need any empty files.","title":"L10n & i18n"},{"location":"wwdc22/localization-and-internationalization.html#localization-and-internationalization-lounge-qas","text":"","title":"Localization and internationalization Lounge QAs"},{"location":"wwdc22/localization-and-internationalization.html#by-emin","text":"","title":"by emin"},{"location":"wwdc22/localization-and-internationalization.html#sometimes-there-is-a-difference-in-font-sizewidthheight-between-ltr-and-rtl-how-can-we-make-this-better","text":"There are a couple of things you could be seeing and I\u2019d like to understand which it might be. The System Font for some languages like Arabic (which is RTL) uses a taller line height than other languages, which you will notice if you\u2019re using Text Styles and have multiline labels or even in the default cell heights in Table Views. In fact, we recommend using Text Styles to ensure that you do get this language-specific tailoring. Different System Fonts also may have different relative font weights and sizing and it\u2019s possible that what you\u2019re seeing is as a result of that. Do either of these sound like what you may be observing? yep, the first one. In that case, this is correct and expected behaviour, because different languages require different line heights to ensure that text doesn\u2019t overlap across lines and has room to breathe. Are you encountering any problems with this? Sometimes when let's say my use case when u have a cell with both English and Hebrew together, it looks abit weird, thought mby equaling their actual height of one of them is the best practice but i'm not sure. a 10.0 font size in some Hebrew font can be smaller than a 10,0 font size of SF English f.e That\u2019s true. For example, if we compare \u201cWelcome\u201d and \u201c\u05d1\u05e8\u05d5\u05db\u05d9\u05dd\u201c, Hebrew will not be as tall as the capital letters in English and the Hebrew letters are slightly taller than the small letters in English. This is something that varies by script and we cannot expect that the heights will be equal. Generally speaking, the way the System Fonts on iOS, macOS, etc. are designed, if you are using a Text Style, you should get a reasonable size for the text across various scripts. Thank you! To be clear, Hebrew does not use a different line height compared to English. It does, however, use a different UI font, which is why you're seeing this difference, as it is difficult to perfectly match up metrics between two independent fonts.","title":"Sometimes there is a difference in font size(width/height) between LTR and RTL , how can we make this better?"},{"location":"wwdc22/localization-and-internationalization.html#our-app-includes-multiple-tutorials-each-tutorial-is-in-its-own-directory-with-tutorial-data-images-and-localizablestrings-files-for-localized-text-for-just-this-tutorial-this-makes-it-easy-for-us-to-add-new-localized-tutorials-without-having-to-modify-the-apps-main-localizablestrings-file-is-there-a-way-to-include-these-localizablestrings-files-for-the-many-tutorials-in-the-output-when-we-use-export-for-localization-in-xcode-or-are-we-going-about-this-wrong","text":"Hi! Can you elaborate a little on how you are setting up these files in your projects? For example, are they part of a target, or do you save them as a pre-created bundle? They are currently in reference folders in Xcode. In the main project and target. Our recommendation would be to convert these to bona fide bundle targets in your project. That would allow them to be properly extracted automatically. As long as the localizable assets are part of a \u201cCopy Resources\u201d phase in your target, they should be extracted when doing Product > Export for Localization. As <@U03HW7PE3SM> mentioned, you can have a \u201cBundle\u201d target for each of your tutorials and then include the localizable resources in each bundle. Then you can include these bundles in your main application target. I'm not familiar with the process of creating bundle targets. But we have over 1000 tutorials and that may be a lot of overhead. The root Tutorials reference folder is included in the Copy Bundle Resources build phase. But I don't see the Localized.strings files, which are in sub folders, in the results from Export for Localization. Yeah, unfortunately Xcode does not look through folder references for strings files \u2014 they are meant to add a folder to your product as-is, without any postprocessing. As <@U03HWDD6RED> mentioned, Xcode does not export the \u201cblue\u201d folder references. One thing you might be able to try is moving all of your tutorials into a Swift package. Xcode 14 adds support for exporting Swift packages for localization and Swift packages are represented by a folder on-disk. Any other suggestions for organizing tutorial or lesson files to be localized? We would like to use Xcode's great localization tools as much as possible. <@U03HJA9JVGA>\u2019s suggestion will let you do just that, for when you can adopt Xcode 14, and is probably your best bet going forward. You can include the \u201cTutorials\u201d Swift package inside of your app\u2019s Xcode project and then when you export for localization\u2014it should include all of the localizable assets and strings in the .xcloc Ok. Thanks! Give it a try and if you run into issues, please feel free to file a report with Feedback Assistant!","title":"Our app includes multiple tutorials. Each tutorial is in its own directory with tutorial data, images and Localizable.strings files for localized text for just this tutorial. This makes it easy for us to add new localized tutorials without having to modify the app's main Localizable.strings file.  Is there a way to include these Localizable.strings files for the many tutorials in the output when we use Export for Localization in Xcode?  Or are we going about this wrong?"},{"location":"wwdc22/localization-and-internationalization.html#hi-all-when-editing-an-xloc-file-in-xcode-im-missing-one-key-feature-filtering-or-sorting-the-list-of-keys-by-whether-or-not-they-were-translated-into-the-second-language-i-seem-to-only-be-able-to-sort-alphabetically-am-i-missing-something","text":"Hi Jan! As you pointed out, Xcloc Editor supports sorting alphabetically and filtering by search terms, but does not support filtering by \u201ctranslated\u201d or \u201cuntranslated\u201d strings. This is great feedback though! If you would benefit from this feature, please file a feedback report and paste the ID in this thread. Thank you <@U03H3NEH4CX>, submitted at FB10114561 Perfect, thank you!","title":"Hi all! When editing an .xloc file in XCode, I'm missing one key feature: filtering or sorting the list of keys by whether or not they were translated into the second language. I seem to only be able to sort alphabetically. Am I missing something?"},{"location":"wwdc22/localization-and-internationalization.html#hi-is-there-a-built-in-workflow-in-xcode-to-localize-settings-bundles-in-a-way-that-xcode-would-export-all-the-relevant-strings-to-string-files-and-the-export-localizations-menu-command-would-include-those-automatically-into-the-exported-xliff-files","text":"Hi Daniel! Xcode supports localization export for Settings bundles that are named \u201cSettings.bundle\u201d and contain .strings files. If this isn\u2019t working for you, can you explain the setup you\u2019re using and the behavior you\u2019re experiencing? Hi Matt, ok, I understand what my problem was. As you mentioned, the strings from the Settings.bundle plist are actually exported correctly. I'm in addition using the third party https://github.com/futuretap/InAppSettingsKit|InAppSettings framework , which builds on the Settings.bundle format but supports additional key/values and can be embedded into the app. It was its InAppSettings.bundle where the strings weren't automatically exported from the plist. Thanks for the answer! Ahh, glad you were able to find the problem! Yes, thanks again!","title":"Hi! Is there a built-in workflow in Xcode to localize settings bundles? In a way that Xcode would export all the relevant strings to .string files and the Export Localizations menu command would include those automatically into the exported .xliff files?"},{"location":"wwdc22/localization-and-internationalization.html#how-would-you-handle-the-case-where-you-want-to-provide-variations-in-a-language-for-example-i-want-to-provide-localization-for-the-word-colour-uk-people-would-see-colour-but-us-people-would-see-color-is-there-a-way-so-all-other-text-is-shared-but-i-could-localize-that-one-word","text":"Hi! Unfortunately, once we find a strings table for a specific lproj, we use the contents of that table as they are without doing any more fallbacks. For example, you can localize a table for en_GB, but it has to contain all strings, even those that are the same as other English localizations. Seems like their should be a hierarchy in the string handling where you can have an overlay (e.g for GB) that only needs to contain the differences to another version (e.g US) Ah ok, that's too bad. Are there any plans to be able to do so? Is it worth it to file feedback about it? On the plus side, you can in fact localize for English variants, like English (UK), English (Ireland), and so on. Yeah, it would be wonderful to have a feedback! Thanks! Created FB10115590 :blush:","title":"How would you handle the case where you want to provide variations in a language? For example I want to provide localization for the word colour. UK people would see colour, but US people would see color. Is there a way so all other text is shared but I could localize that one word?"},{"location":"wwdc22/localization-and-internationalization.html#how-do-we-format-and-sort-time-values-in-a-localized-way-for-example-if-im-showing-a-list-of-hours-of-operation-for-the-week-the-time-strings-will-be-different-24-hr-vs-12-hr-and-the-order-of-the-week-days-will-be-different-starting-on-monday-vs-sunday","text":"Hi! For sorting, it would be best to sort based on the original binary data, then use .formatted() to produce a localized time and/or date. could you clarify what \u201coriginal binary data\u201d means? How are you storing the days/hours prior to localization? For example, is it a Date , or are you receiving the data in some other form? it would be a JSON from an API response in english, { day: 0, start: \"0700\", end: \"2300\" } and 0 means sunday as specified in the API Got it. You should be able to sort directly on those strings. For the first day of the week issue, you can check Calendar.firstWeekday . (on the current user default calendar) To produce formatted times, you'll need to convert the strings to a Date object, then use .formatted() with the appropriate options to produce the string you want. thank you!! You should be able to put a weekday/hour/minute in DateComponents and then use Calendar to make a Date from that. You can then make a DateInterval from two Dates for the range (e.g. \"9 AM - 5 PM\") and then use .formatted() on that. One more note: when using Calendar to convert DateComponents to Date , set the time zone to GMT to avoid Daylight Time issues. Same with .formatted()","title":"How do we format and sort time values in a localized way? for example, if i'm showing a list of hours of operation for the week, the time strings will be different (24 hr vs 12 hr), and the order of the week days will be different (starting on monday vs sunday)."},{"location":"wwdc22/localization-and-internationalization.html#we-have-an-app-that-has-english-only-features-lets-imagine-thats-as-an-advanced-assistant-for-editing-english-texts-and-our-assistant-doesnt-support-other-languages-if-we-localize-our-app-would-it-improve-user-experience-for-people-who-use-english-as-a-second-language-or-would-it-only-confuse-them-on-why-their-main-language-is-not-supported-since-we-only-expect-english-texts-for-now","text":"Hi Roman, I think it\u2019ll depend on exactly what your app does. For example, hypothetically, if you have an app that points out grammar notes about English, it might still be useful for people to see that information in a language they find easier to understand than English. So, if you can share details, I might be able to help give you a more specific, helpful answer :slightly_smiling_face: Basically, it is an editor app, which has users' documents. When user creates the doc and fills it with a text, he receives grammar suggestions, as well as clarity, fluency, readability and other suggestions. Each suggestion is a card that has: \u2022 previous word/sentence and the resulting sentence \u2022 detailed explanation of why the user should apply this suggestion in that particular context - like because it sounds unconfident, or because it has a grammatical error, and so on Right\u2026 I think some users\u2014especially those who have less familiarity with English to begin with\u2014could certainly benefit from localization. However, you may want to be cautious about (a) which languages you choose, and (b) how you separate out the \u201ccontent language\u201d (English) from the \u201cUI language\u201d so that there\u2019s no confusion or aesthetically displeasing mixture of languages. I've been thinking about some extra flow to suggest users who are not from EFL countries to have some pieces of the content being translated to their native language, just like that hint on why they should apply that alert. However, I dunno if users would be ok with English interface since they already write some English texts and the interface is primitive enough to be understandable. But at the same time having some interface parts in the native language, like we as an app just forgot to translate everything :sweat_smile::sweat_smile: You\u2019re thinking about all the right things! And it\u2019s true that with English being the lingua franca in today\u2019s world, many people might be familiar enough with the English user interface such that they won\u2019t need localization. However, it\u2019s always handy to have choices, and your users can also go into Settings to switch your app to English if that\u2019s what they prefer. In terms of making sure that the localization doesn\u2019t look partial or incomplete, I would definitely suggest making sure that any \u201cchrome\u201d (i.e. UI elements) is translated consistently in each of the languages you target. For example, if the tables are turned and your app is actually for helping people with French, as a French learner, I would appreciate it if I could use it in English and see suggestions like this: > The verb bouffer could be too formal. Consider using d\u00e9jeuner or manger . I\u2019m limited by what I can do in Slack here stylistically, but you can see that I have marked the text in the content language in a different style than the UI language, which gives me clarity as the user as to what\u2019s going on and where the language boundaries are. That's exactly what I've been thinking about. Last one - if you will have those suggestions in English, having French texts, would you as a new user prefer having whole app interface in English? Wouldn't you start typing in English instead of French, since the app is fully localized?)) Yes, I suppose I might! It\u2019s hard to answer that in the abstract though without knowing exactly what the app looks and feels like. It all depends on which direction your app\u2019s design steers the user in. Yeah, well... I'm talking about Grammarly app for iOS. Would be great to learn your thoughts on that. Oh, interesting! I was guessing that this sounded like Grammarly! I would suggest looking at your app\u2019s usage and tackling one market as an initial foray, e.g. China or Japan, and see what feedback you get from your customers :slightly_smiling_face: Personally, I don't know that the app being localized in a given language would necessarily imply that I can input text in that language (though obviously it's ideal!). Taking for example our own products, we localize all features of the OS into all languages completely, including things like Apple Pay Later that might only be available in some regions.","title":"We have an app that has English only features. Let's imagine that's as an advanced assistant for editing English texts and our assistant doesn't support other languages.  If we localize our app, would it improve user experience for people who use English as a second language, or would it only confuse them on why their main language is not supported, since we only expect English texts for now?"},{"location":"wwdc22/localization-and-internationalization.html#theres-probably-no-right-way-to-do-this-but-when-working-with-localized-strings-in-code-do-you-recommend-using-the-new-string-initializer-which-looks-great-btw-directly-throughout-the-app-or-creating-for-example-a-file-with-enums-that-return-those-localized-strings","text":"We strongly recommend that you use String(localized:\u2026) , AttributedString(localized:\u2026) or in Objective-C NSLocalizedString and NSAttributedLocalizedString ! If you do that, Xcode can extract strings for you to provide to your localizers \u2014 it knows how to recognize these macros and initializers and how to produce a xcloc bundle that contains them all, and then reintegrate it into your project. String constants used with other mechanism will not be recognized by this mechanism, and you will have to manually handle extraction into .strings files and any changes going forward. Note that SwiftUI also uses the LocalizedStringKey type, and Xcode also knows about it. So, when you write Text(\"Hello, world!\") , since Text(\u2026) takes a LocalizedStringKey , Xcode will also know to extract it. In Swift, you can even use String interpolation to include variables directly in the initializer String(localized: \"Welcome to \\(newCity)\", comment:\"Large headline label welcoming the user to their new location, the variable refers to the city name.\") Please make sure to explain what the variable refers to, so that translators have that knowledge, too :slightly_smiling_face: Yup! This works in String(localized:\u2026) , AttributedString(localized:\u2026) and in SwiftUI. As long as you\u2019re using this setup, in general, you\u2019re golden \u2014 if you\u2019re using enums to represent your strings, just make sure that they\u2019re calling into these functions. In general, using enums may complicate your patches a little and interact weirdly with SwiftUI if you try to pass String s to SwiftUI initializers that expect localized keys.","title":"There's probably no right way to do this, but when working with localized strings in code, do you recommend using the new string initializer (which looks great btw!) directly throughout the app, or creating, for example, a file with enums that return those localized strings?"},{"location":"wwdc22/localization-and-internationalization.html#hello-can-the-localizablestrings-updates-itself-the-first-time-export-and-import-is-okay-however-when-the-project-contains-a-localizablestrings-file-then-the-next-export-will-not-contain-new-strings-added","text":"Hi ChakMing! The expected flow is as follows: 1. Have some strings in your project (either .strings files or strings in code) 2. Export Localizations, which includes all strings in an xcloc 3. Translate the xcloc 4. Import Localizations with the translated xcloc (This step updates/adds .strings files in/to the project.) 5. Exporting again should include all strings in the project (including those previously imported) in the xcloc If this is not working as expected, please file a feedback so that we can take a look. <@U03H3NEH4CX> Hello again. Here\u2019s what I found. At initial, the export will include all strings. If I imported the xcloc, it will generate Localizable Files. Then I add more words on SwiftUI, and export. The xcloc is still using the old localizable file. I thought it had been happening for a long time\u2026 Do you think it is a bug, or is this correct? So my expectation here is that the second export would include both the strings in your SwiftUI code and the strings in the .strings files that the import created. Xcode won\u2019t remove stale translations automatically, because those translated strings can be a valuable resource if you ever need to translate them again. However, please feel free to file a feedback request if you think we should change the flow here! I just re-export and seems it works this time! Thanks, <@U03H3NEH4CX> <@U03HBMCRX0E>","title":"Hello. Can the Localizable.strings updates itself? The first time export and import is okay. However, when the project contains a  Localizable.strings file then the next export will not contain new strings added."},{"location":"wwdc22/localization-and-internationalization.html#hello-how-can-we-make-sure-the-system-and-voiceover-identify-a-certain-texts-language-correctly-is-there-a-tag-we-can-set-somewhere-im-asking-for-both-web-and-apps-it-happens-from-time-to-time-where-the-system-falsely-identifies-urdu-for-arabic-or-recently-on-ios-16-persian-for-arabic-how-can-we-improve-this","text":"If you know the language of your text, you can annotate it with that language. For web content, you can use the https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/lang|lang attribute, which can be applied to any HTML element, including &lt;p&gt; , &lt;span&gt; , and &lt;body&gt; . For app content, you can create an attributed string and set the NSLanguageIdentifierAttributeName to the language code of the corresponding text. If you have reproducible strings that are incorrectly identified, we would love to have feedback with those examples as well! Here\u2019s the feedback ID: FB10152753 It fails on everything. Here are some example texts: \u0645\u0646 \u0633\u0639\u06cc\u062f \u0647\u0633\u062a\u0645 \u0627\u0645\u0631\u0648\u0632 \u0628\u0631\u06cc\u0645 \u0631\u0633\u062a\u0648\u0631\u0627\u0646\u061f \u0686\u0647 \u062e\u0628\u0631\u061f <@U03HW7PE3SM> Thanks! We\u2019ll be sure to pass this along.","title":"Hello. How can we make sure the system and VoiceOver identify a certain text\u2019s language correctly. Is there a tag we can set somewhere? I\u2019m asking for both web and apps. It happens from time to time where the system falsely identifies Urdu for Arabic, or recently on iOS 16, Persian for Arabic. How can we improve this?"},{"location":"wwdc22/localization-and-internationalization.html#whats-the-best-way-to-use-two-custom-fonts-non-sf-pro-for-marketing-reasons-for-arabic-and-latin-scripts-the-most-challenging-part-is-that-the-text-could-be-bilingual-and-it-may-come-from-the-server-for-instance-wwdc-i-want-two-fonts-to-be-used-for-this-snippet","text":"It looks you forgot to answer this. You can do it by setting https://developer.apple.com/documentation/coretext/kctfontcascadelistattribute|kCTFontCascadeListAttribute , either have the primary font be the Latin font, then have the cascade list contain your Arabic font (the way we do it for our system), or the other way around. Another option is to use (NS)AttributedString and set the fonts on the individual segments of the string. The thing is I mostly don\u2019t have access to the text beforehand. Let\u2019s say it\u2019s coming from a server. Then the font fallback list (as above) is best. Note that in addition to the CoreText version above, there's also https://developer.apple.com/documentation/uikit/uifontdescriptorcascadelistattribute/|UIFontDescriptorCascadeListAttribute and https://developer.apple.com/documentation/appkit/nsfontcascadelistattribute/|NSFontCascadeListAttribute , if you're using higher-level text APIs.","title":"What\u2019s the best way to use two custom fonts (non SF Pro for marketing reasons) for Arabic and Latin scripts? The most challenging part is that the text could be bilingual and it may come from the server. For instance, \u201c\u0628\u0647 WWDC \u062e\u0648\u0634 \u0622\u0645\u062f\u06cc\u062f\u201d. I want two fonts to be used for this snippet."},{"location":"wwdc22/localization-and-internationalization.html#is-there-a-non-hacky-way-to-force-a-single-language-for-an-app-lets-say-the-app-only-supports-ar-sa-and-the-users-device-language-is-set-to-en-us-i-used-to-set-a-value-for-a-certain-in-key-in-userdefaults-at-launch-to-make-it-work","text":"You should not need to set any user defaults. Simply make sure that the developmentRegion in your project.pbxproj is set to the correct language and that all your localizable files are in the correct lproj. If your app does not have an lproj folder, you will need to be on iOS 16, macOS 13, tvOS 16, and watchOS 9 to get correct behavior.","title":"Is there a non-hacky way to force a single language for an app? Let\u2019s say the app only supports \u201car-SA\u201d and the user\u2019s device language is set to \u201cen-US\u201d. I used to set a value for a certain in key in UserDefaults at launch to make it work."},{"location":"wwdc22/localization-and-internationalization.html#is-there-a-way-to-force-the-numeral-system-for-an-app-for-instance-the-user-may-have-chosen-as-their-numbering-system-of-choice-where-it-only-make-sense-in-our-app-to-show-numbers-as-123-one-thing-thats-still-missing-and-id-already-filed-a-report-as-i-remember-is-choosing-the-numeral-system-when-changing-the-language-for-a-specific-app-now-you-can-only-change-the-numeral-system-when-you-change-the-device-language-as-a-whole","text":"Simply don\u2019t localize the numbers (e.g. don\u2019t use a formatter) and the numbers should be preserved as the literals you are using in your code. Do you have a code sample to share to elaborate more on the issues you are seeing? I think I phrased my question wrong. For instance, install Whatsapp on a device whose locale is set to English - United States. Then, go to Whatsapp settings page inside system Settings and change its language to Persian. Now Open Whatsapp. You\u2019ll see the system keyboard has number row in Latin. Whereas in iMessage on the same device it\u2019s in \u06f1\u06f2\u06f3\u200d form. <@U03JPJ277SQ> IMHO, this may be a bug. Do you mind reporting this bug and sharing the FB # here? <@U03J83G0WQG> Here it is: FB9647599. I believe we already discussed this on Twitter as well.","title":"Is there a way to force the numeral system for an app? For instance, the user may have chosen \u2018\u06f1\u06f2\u06f3\u2019 as their numbering system of choice where it only make sense in our app to show numbers as \u2018123\u2019. One thing that\u2019s still missing and I\u2019d already filed a report as I remember, is choosing the numeral system when changing the language for a specific app. Now you can only change the numeral system when you change the device language as a whole."},{"location":"wwdc22/localization-and-internationalization.html#when-i-release-my-apps-i-can-usually-find-some-people-to-help-me-translate-the-initial-release-but-over-time-it-makes-it-difficult-for-me-to-create-updates-which-require-new-translation-strings-any-best-practices-you-would-have-there-for-indie-developers-that-dont-have-a-lot-of-resources-to-pay-20-translators-for-each-release","text":"It is understood that adding languages to your application can become quite the commitment \u2014 not just a commitment in the moment, but also going forward. It does pay off to make a plan in which you can sit down and think whether you can do so sustainably for every single feature you do, including adding languages to your application. Sometimes, it is a valid choice to know that the time may just not be there every release, and to choose perhaps to support fewer languages. At other times, community-supported projects may be able to elicit contributions from the community that can help where a single developer may not, depending on what your relationship is with that community. But it is very much a commitment \u2014 often an economic one \u2014 you may want to think about as an ongoing task that requires care and maintenance. Just to clarify \u2014 you are not having issues with filtering out new strings from old ones, correct? in other words, using the Xcode import/export workflow, you should be able to only have to deal with new/changed strings on every release, rather than re-translating the entire app. There are third-party services that offer (human) translation as a service for your localizable strings. But you'll have to document your strings well, or they may get the context wrong and mistranslate.","title":"When I release my apps I can usually find some people to help me translate the initial release. But over time it makes it difficult for me to create updates which require new translation strings. Any best practices you would have there for indie developers that don't have a lot of resources to pay 20+ translators for each release?"},{"location":"wwdc22/localization-and-internationalization.html#we-saw-that-this-year-youre-adding-punctuation-to-hebrew-thank-you-for-that-besides-the-obvious-addition-to-the-hebrew-keyboard-is-there-anything-else-we-should-know-about-that","text":"Hi <@U03J1TN6WBD>, we have indeed added niqqud support to the iPhone & iPad keyboard layouts this year. There\u2019s nothing else new for Hebrew this year per se, but it\u2019s worth mentioning that Yiddish is also now supported as a separate keyboard language. Thank you! a dank","title":"We saw that this year you're adding punctuation to Hebrew (Thank you for that! \u05ea\u05d5\u05d3\u05d4 \u05e8\u05d1\u05d4) Besides the obvious addition to the Hebrew Keyboard is there anything else we should know about that?"},{"location":"wwdc22/localization-and-internationalization.html#maybe-a-slightly-tangential-question-but-are-there-learnings-or-strategies-used-during-rtl-localization-that-could-be-beneficial-for-adapting-a-ui-with-significant-lateral-elements-for-left-handed-use-does-handedness-come-into-play-when-considering-the-design-of-rtl-uis","text":"We treat Handedness and Layout Directionality as distinct elements and handle each of those in different ways. For example, for the One-Handed Keyboard, we offer both Left-Handed and Right-Handed modes. UI elements or gestures that indicate a progression that go left-to-right in an LTR language do mirror to go right-to-left in an RTL language. So, handedness does not come into play here. This is all fairly abstract, of course. If you have a concrete example that you want to ask about, do feel free to share it :) I guess, as a lefty, I never really understood how an adaptive layout could really change how someone uses a device until I started looking at how RTL languages are handled in the HIG. The chevrons in lists are meant to point towards the thumb you use to tap them :exploding_head:. I would imagine that the inverse could be true in RTL situations\u2026 has that changed your design strategy? For example, an iPad in portrait orientation puts the capture button on the right hand side. This seems like a place where handedness would take priority. But there are other situations where the inverse would be true? I didn't see specifics about gestures in the HIG article, so I was wondering if I just missed something or if there was some type of helpful underlying principle to keep in mind. Those are thought-provoking questions, <@U03JCRFMPHV>, and I likely won\u2019t have any satisfying answers to your larger question in this forum. It does seem like Handedness and how it applies to User Interface Design is a topic that I\u2019m not the best person to answer, though I\u2019ll see if there are any other colleagues of mine who may have an answer for you. The overall rule is that the design layout in RTL matches the flow that the user would expect elements to flow in the UI - including the text strings, animation, navigation (E.g. If the heat map of people that uses an LTR layout - e.g. website - is on the top left corner, it would be top right corner in RTL). The only thing that we should take care of is: Reachability: if the design intention is to have an element closer to the right hand - then it should not be changing because of the RTL layout (E.g. the keyboard layout does not change when you switch LTR or RTL / emojis entry point would be at the same exact place). Other elements that could impact our decisions while changing to RTL layout: 1. Memorization: If the UI component would be very connected to how user would remember things, it should not change (E.g. the order of the lock screen numbers remain the same in both RTL & LTR) 2. Images, videos, backgrounds should not also be impacted (E.g. the weather App background). <@U03JCRFMPHV> Let me know what do you think. This is very interesting, thank you for sharing some of the methodology in your design thinking. I was surprised to see elements like system level scroll bars flip to the left side. It's never been under my thumb before. Seems disadvantageous for RTL righties? <@U03J83F2TDE> That\u2019s a really great question <@U03JCRFMPHV> Two angles for this: (which are up for discussion too) :slightly_smiling_face: \u2022 In the layout, having scroll bars usually is on the other side where your attention should be. \u2022 And for reachability, I usually think about it device by device. On macOS - which I think is where scroll bars are used the most - there is no reachability issue as it is not touch based. Touch based devices are less likely to rely on it and it is used as an indicator more than an interactive piece of the UI as we use swipe instead. And even on iPad for example if we decide to use it, we mostly hold it with two hands and the left hand solves it too. Definitely. I can honestly say that having the scroll bar under my thumb for the first time\u2026 and to not have my scrolling thumb blocking the icons in lists for example, was a brand new experience. Especially on the larger phone, it makes reaching the compose button much easier :joy:. Interesting to see places where steps were taken to counteract reachability issues. Like floating the iPad portrait camera capture button to the right. Thank you for your thoughtful response! <@U03J83F2TDE> You\u2019re always welcome <@U03JCRFMPHV> Nice questions :D","title":"Maybe a slightly tangential question, but are there learnings or strategies used during RTL localization that could be beneficial for adapting a UI with significant lateral elements for left handed use? Does \u201chandedness\u201d come into play when considering the design of RTL UIs?"},{"location":"wwdc22/localization-and-internationalization.html#hi-there-didnt-have-time-to-check-out-the-new-resources-yet-but-will-do-in-the-future-for-sure-could-you-tell-me-what-are-the-main-key-points-that-we-should-take-into-account-when-designing-apps-targeting-an-international-audience","text":"Hi <@U03JRP87THN>, there\u2019s no succinct answer for this; so, I would highly recommend checking out https://developer.apple.com/videos/play/wwdc2018/201/?time=1665|Creating Apps for a Global Audience , https://developer.apple.com/videos/play/wwdc2022/10110/|Localization by example , and https://developer.apple.com/videos/play/wwdc2022/10107/|Get it Right (to Left) to build up an understanding of what you should take into account when designing apps for an international audience. <@U03JRP87THN> I would say, generally speaking look out for two things: 1. Language: design layouts may be impacted by RTL languages Orientation or the less dense text strings in CJK languages. Another example is that certain scripts require different bounding boxes or spacing whether less or more than latin (E.g. Thai needs more vertical spacing in the UI to avoid clipping). 2. Culture: What are the cultural behaviors and nuances that can impact the features you support, the hierarchy or the elements in the UI, or simply the visual language (E.g. colours that could be perceived differently in different cultures). Thank you! Will look into the resources.","title":"Hi there! Didn't have time to check out the new resources yet (but will do in the future for sure!). Could you tell me what are the main key points that we should take into account when designing apps targeting an international audience?"},{"location":"wwdc22/localization-and-internationalization.html#should-rtl-specifically-arabic-text-work-out-of-the-box-using-swiftui-on-watchos-8-are-there-any-configurations-or-swiftui-properties-that-might-break-it-i-have-a-watch-app-and-rtl-seem-to-work-fine-i-cant-read-them-but-they-look-okay-but-ive-gotten-feedback-that-they-fail-for-some-users-arabic-text-is-backwards-and-the-characters-arent-joined-like-they-should-be-im-just-using-text-so-i-cant-see-why-this-would-happen-i-can-provide-screenshots-in-the-thread-if-possible","text":"Hi Christopher! I think I may have an idea of what issue you\u2019re encountering, but to be sure, a screenshot would definitely help. Please feel free to share. Sure, here's the same text on watchOS and iOS. I've been unable to reproduce the this with my own strings but I don't have a copy of this exact text. I see; there are two different issues going on here. The Watch screenshot demonstrates broken and completely illegible Arabic text, and it\u2019s also not using the System Font. It\u2019s difficult to be sure about what\u2019s going on here without a sample app to reproduce this issue in. I am setting the Text\u2019s font using UIFont and a FontDescriptor so I'd guess that's the culprit One thing I will note is that in the Watch screenshot, the title \u0627\u0644\u062a\u062d\u0631\u064a\u0645 is rendering correctly, but the body text below that is what\u2019s completely broken. Not sure why it fails for some RTL strings and not others It\u2019s possible that the specific font you\u2019re using supports Arabic glyphs, but not the appropriate shaping. I should note that the alignment in both the iOS and watchOS screenshot is unnatural for Arabic, i.e. the text is left-aligned instead of right-aligned. Does this app localize into Arabic, i.e. have ar.lproj or is it localized into another language and this content is being loaded at runtime? The app is localized in Russian, which is what this user is set to. And then he's entered Arabic text. I'm using the system font but applying a design: Here's another example where (I think?) all the text is working fine: indeed that does look correct I see. Nothing jumps out to me immediately as being wrong here. I would really appreciate it if you could file a Feedback Report with a sample app which demonstrates this issue. I would love to look into this problem further. Yes, the second screenshot here looks correct; I also see that it\u2019s using the System Font. have you ever been able to reproduce this locally, or are your only reports of this from user(s)? I have not been able to reproduce (second screenshot is mine). I've asked the user to send me the text itself so I can try it. Unfortunately I haven't been able to copy that Arabic text out of the iPhone screenshot. To start, I think it would be good to file a feedback with as much of your watch app as you can reasonably include (with build instructions if needed beyond build and run) and these screenshots. Out of curiosity, was the iPhone screenshot above also from the same user? I happened to notice that one of the UI elements was rendering text using the Urdu font, and it\u2019s possible that that is the clue to when this issue reproduces, i.e. when Urdu is added to the preferred languages list (higher than Arabic, if Arabic is also present). Yes, that iPhone screenshot was from the same user. Okay, I'll put together a Feedback with whatever I can get. Thanks! Good spot! I was also wondering why the Arabic text was shaped like this on the navigation bar, but it\u2019s actually Urdu! Please do post the feedback number once you have that filed back in this thread. Thanks again!","title":"Should RTL (specifically Arabic) text work out of the box using SwiftUI on watchOS 8? Are there any configurations or SwiftUI properties that might break it? I have a Watch app and RTL seem to work fine (I can\u2019t read them but they look okay) but I\u2019ve gotten feedback that they fail for some users \u2014 Arabic text is \u201cbackwards\u201d and the characters aren\u2019t joined like they should be. I\u2019m just using Text so I can\u2019t see why this would happen. I can provide screenshots in the thread if possible."},{"location":"wwdc22/localization-and-internationalization.html#how-can-i-right-align-arabic-text-in-push-notifications-efficiently-when-only-the-app-language-is-arabic-not-the-entire-device","text":"Thanks for raising this use case. Can you please describe your use case in a bit more detail in a Feedback report, sharing the number here? We\u2019ll follow up there. I am working on an application which has two languages English and Arabic, now users can change the app language whenever they want. So ideally if the app language is English the text displayed in push notifications is left aligned as it should be, but when the app language is Arabic the text should be right aligned but it's not. Even when the app is in Arabic the text in push notifications is left aligned. While the rest of the app is completely right aligned. Thanks <@U03J6B7G2QN>, that is a great description. If you don\u2019t mind reporting that via the Feedback tool, it makes it easier for us to track a solution internally, and follow up with you beyond the Digital Lounge. Here FB10161536","title":"How can I right align Arabic text in push notifications efficiently when only the app language is Arabic not the entire device."},{"location":"wwdc22/localization-and-internationalization.html#hi-is-the-generated-xcloc-file-will-fetch-all-strings-update-in-every-export","text":"Yes, running Export Localizations will re-process your source code to extract any newly added, removed, or changed strings and include them in the exported localization catalog. It will also include the existing translations when exporting for languages other than English. That\u2019s Weird. Becoz I found it only works at the 1st time. Then all xcloc will follow the version of the 1st import\u2026 https://wwdc22.slack.com/archives/C03H786M2V8/p1654879512571169?thread_ts=1654804988.358809&cid=C03H786M2V8 I can follow up in your other thread ok thx","title":"Hi. Is the generated xcloc file will fetch all strings update in every export?"},{"location":"wwdc22/localization-and-internationalization.html#so-if-i-have-a-swiftui-or-uikit-notes-app-thats-not-localized-in-any-rtl-languages-but-a-user-enters-rtl-text-am-i-correct-that-the-expected-behavior-is-that-the-text-itself-will-appear-rtl-but-will-be-left-aligned-is-there-an-easy-way-to-get-this-to-appear-right-aligned-or-would-i-have-to-analyze-the-language-myself-or-is-this-undesirable-behavior-in-an-otherwise-left-aligned-app","text":"On iOS, when a user enters text into a text field or text view, the alignment follows the default alignment of the keyboard, and you should not need to do anything extra on top of this. If that text is then displayed in a UILabel, for example, the alignment would be based on the UI language, rather than the content of the label itself. If you have a use case for something more automatic, please do file a feedback describing it in more detail. Okay thanks","title":"So if I have a SwiftUI or UIKit notes app that\u2019s not localized in any RTL languages but a user enters RTL text, am I correct that the expected behavior is that the text itself will appear RTL but will be left-aligned? Is there an easy way to get this to appear right-aligned, or would I have to analyze the language myself? Or is this undesirable behavior in an otherwise left-aligned app?"},{"location":"wwdc22/localization-and-internationalization.html#hello-devs-we-have-localization-strings-being-delivered-by-middle-ware-and-get-updated-and-stored-in-application-directory-on-each-app-launch-where-we-used-to-load-them-using-localizable-nocache-as-mentioned-on-apple-docs-however-this-slow-down-the-ul-performance-way-too-much-as-it-seems-it-would-load-whole-strings-map-for-each-call-to-nslocalizationstring-can-you-suggest-a-work-around-thanks-fahied","text":"Hi Muhammad! .nocache should only need to be used if: \u2022 the strings table has already been loaded during your app processes lifetime, \u2022 the table is known to have been updated based on some other signal Use of .nocache is expected to have some performance overhead so some techniques to reduce are : \u2022 Use it as sparingly as necessary (ie is it really user critical to get non-cached strings) \u2022 Consider re-organizing the strings tables into smaller tables that map to user activities so that unnecessary strings aren\u2019t being loaded into memory Hi Muhammad, can you also file a feedback report about your specific use case so we can look into enhancement in this area too? Sounds great, I ll do that. The problem is really If I load Strings without .nocache, It will not load the updated strings even though the same source files .strings is overriden or even you force load main bundle again. That's right. The strings files are cached by bundle when bundle is created. Since there's no way to really \"unload\" a bundle, the cached resources remained cached Depending on the scope of your project, you can consider having a separate XPC service to load the strings, and kill and relaunch the service when you get a notification when the strings are updated, if such feedback is available via your middle ware Would there be any possibility for iOS in term of XPC service? Ah.. sorry I didn't ask about your environment first. Unfortunately I don't think so I thought may be you guys holding on to something :smile:","title":"Hello Devs,  We have localization Strings being delivered by Middle ware and get updated and stored in Application Directory on each app launch. Where we used to load them using \u2018Localizable. nocache\u2019 as mentioned on Apple Docs. However this slow down the Ul performance way too much. As it seems it would load whole strings map for each call to NSLocalizationString. Can you suggest a work around?  Thanks Fahied"},{"location":"wwdc22/localization-and-internationalization.html#not-sure-its-a-bug-bug-if-my-development-lanugage-is-english-but-if-i-dont-add-english-in-my-localization-it-going-to-display-other-language-the-work-around-i-did-now-is-add-empty-english-localization-file-for-each-file","text":"Can you clarify your project setup? In general, your development localization should be one of your app localizations. Prior to iOS 16/macOS Ventura, it was required that your app bundle also contain a corresponding lproj folder for the development language, but you should not need any empty files.","title":"Not sure it's a bug bug if my development lanugage is english but if I don't add English in my Localization it going to display other language. The work around I did now is add empty english localization file for each file?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html","text":"object-capture-and-room-plan-lounge QAs by FeeTiki Hi! I'm just wondering if you have suggested guidance for capturing objects that are on a rotating platform, to minimize post-production work. Is it advised to try to frame the platform out of the image as best as possible, to post-process each image and crop out the platform, wrap the platform in a similar color to the background, etc.? We suggest to use a uniform color platform which is different than the object and keep the object inside the frame, but as large as possible. Use object masking to make PhotogrammetrySession remove the background https://developer.apple.com/documentation/realitykit/photogrammetrysession/configuration-swift.struct/isobjectmaskingenabled For difficult objects you can also provide manual masks. Please see https://developer.apple.com/documentation/realitykit/photogrammetrysample/objectmask Thank you, <@U03J7GGBZDW>! Since I have the camera on a tripod and object(s) on a platform, doing some degree of post-processing or masking might be a viable solution when passing the photos into the Object Capture API. <@U03J20E7UBV> To get the best outcome from post-processing, please keep the exif in the images and do not crop or resize the images. can I get point cloud during updating progress of request or can only get point cloud data after request complete ? There is a separate request for point cloud and model. You can get the point cloud output before the model is done. How can I crop the model after it is made? We provide a Geometry field on the model reconstruction Request that will allow you to specify the cropping region during the model creation step. We show this flow in last year's WWDC session ( https://developer.apple.com/videos/play/wwdc2021/10076/ ). To recap, a GUI app can be made with the API that will let you request a preview model, arrange the reconstruction volume, and then get the cropped output. We recommend cropping during reconstruction since this allows our algorithms to focus the texture and mesh resolution on areas that are being reconstructed. Also note that this year we added the ability to specify an OrientedBoundedBox in the request to allow a more general cropping volume. can I choose how many or range or point clouds to generate? The number of points or the range of the points in a point cloud request is automatically determined by the algorithms based on the input images. Is it possible to add custom textures to the mesh map? For example, a furniture piece I scanned is of material Oak color, can I change it to a glossy silver color? You can edit the texture map generated by the PhotogrammetrySession and assign it to the obj model by Reality Converter or USDZ Tools https://developer.apple.com/augmented-reality/tools/ . If you need more information on usdz, please join \u201cThe Q&A: USD\u201d from 5-7pm today. Can you clarify the underlying differences for a RoomCaptureView and a RoomCaptureSession ? RoomCaptureView seems like it is a UIView subclass that handles all of the underlying logic for scanning and creation of the 3D model for output, whereas RoomCaptureSession appears to be a custom implementation that allows us to customize the process. What would be the use case for a custom RoomCaptureSession ? RoomCaptureView is a plug and play view that you can use in your app RoomCaptureSession let\u2019s you subscribe to updates using a delegate and gives you finer grain control over the surfaces and objects that RoomPlan detects. RoomCapture session can be used for filtering out surfaces or objects and rendering them as per your liking. I'm just now trying to get up to speed. Is it possible to use a room plan as an anchor for future AR sessions? A particular use case would be tying an AR experience to the architecture and physical properties of a room. It is an interesting idea but this question may be better suited for tomorrow RoomPlan Q&A. Thank you! Is there a point of creating new models using this year's APIs (highResolution?) with last year's photos? should it get me a better model? We added algorithmic improvements to this year's Object Capture API, so you may see improvements to your model if you re-run this year's API on images you already captured last year. Perusing the room capture demo, would we have access to the view just like an ARView, so we have finer control over objects and experiences in it? Looking at the documentation doesn\u2019t yield anything useful This is not supported using a RoomCaptureView . But you can have fine control using the data only API and creating a custom ARView (or SCNView or MTKView ) How can I convert the .usdz file generated by object capture to a format that I can edit with blender, such as .fbx or .usd? The Object Capture API can directly provide OBJ and USD (ascii) outputs if you provide a folder URL instead of a USDZ filename when requesting the modelFile . The maps will also be placed into this folder along with the OBJ and USD. Is ARKit 6's new session.captureHighResolutionFrame intended for use with an Object Capture experience? It's not clear if session.captureHighResolutionFrame captures just the high resolution camera feed, or the camera feed with overlaid AR content. The captureHighResolutionFrame can be used by developers creating their own Object Capture app using ARKit to get higher quality images to be used for Object Capture. It captures the high-res camera feed, returning an ARFrame with a high-res image along with other information such as the camera tracking results. Using this API you just get the camera feed though and not the overlaid content in AR Thanks, <@U03J7GBRQ0G>! Does the output file format (I'm not sure if it's a HEIC or PNG, etc.) also include depth/gravity data baked in? You simply get an ARFrame and can access the CVPixelBuffer through that to eventually save it to the format of your choice https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframe Got it! Thanks so much for the helpful information! Does the output USDZ file from a RoomPlan session maintain some sort of a model hierarchy where the object types (and not surface types) could be modified/removed at a later time, or is the USDZ flattened? These are great questions for the Q&A session for RoomPlan tomorrow. There are two choices for you: You can either tune in at 9-11am or 4-6pm Pacific Time. Alternatively, you can also request a 1:1 lab appointment on the developer app for this Thursday. Thank you! Will focus all future questions on Object Capture! If someone were interested in filtering out [.objects] from a captured RoomPlan, would it be advised to do this in the func captureSession(_ session: RoomCaptureSession, didEndWith data: CapturedRoomData, error: Error?) method, just prior to exporting to a USDZ model, or at some other time during the capture process? Another excellent question. It would be great to have you join us at the RoomPlan Q&A session tomorrow where we can cover this and lots of other questions on this topic. It all depends if you\u2019re using RoomCaptureView or not: if you are, there\u2019s no way of filtering out objects from live scan rendering, so you\u2019ll have to do it prior to exporting to a USDZ. If you have your own custom view (using Data Only API), We\u2019d recommend doing it directly in the live view (so that end users understand that objects won\u2019t be part of the final USDZ) That makes sense, thank you very much! :slightly_smiling_face: As someone who is new to both those tools, but is working in Swift/Metal realm, Is the API of both of them accessible to have the functionality added to other applications? Object Capture and Room Plan are both public Swift API\u2019s intended to be used by developers to make their own applications. We have sample code to https://developer.apple.com/documentation/realitykit/taking_pictures_for_3d_object_capture|take pictures for 3D object capture and https://developer.apple.com/documentation/realitykit/creating_a_photogrammetry_command-line_app|creating a photogrammetry command-line app on Mac from last year. This year we add another sample code for https://developer.apple.com/documentation/realitykit/using_object_capture_assets_in_realitykit|using object capture assets in RealityKit . Thanks you Are there any USD prim naming guarantees for the output room? For example, could the naming schema suddenly change in iOS 16.2? There\u2019s no guarantee that the naming schema will remain the same over time, but we do our best to maintain consistency and compatibility over time. Are there any hints about the kinds of result degredation we can expect if we use it outside of residential settings? e.g. if we can live without \"objects\", or high confidence levels... Current technology is designed to support interior of residential homes. Does RoomBuilder.ConfigurationOptions.beautifyObjects affect captured room surfaces, or just objects? Just objects Is there a recommended workflow for performing photogrammetry to capture textures alongside roomplan? Great question! We don\u2019t have a recommended workflow for photogrammetry with RoomPlan. It is a great idea for developers to explore! Should we be able to provide our own ARSession to the RoomCaptureSession? I attempted to set the arSession property with a custom configuration running, but room reconstruction never begins. I noticed without the custom ARSession, the RoomCaptureSession creates a RoomPlan.ARRoomCaptureConfiguration, with custom scene understanding techniques (keyframe with color). If this is a bug, I'd be happy to file feedback. @Jack: ARSession property in RoomCaptureSession is intended to give you a handle to give developers the ability to customize the rendering detected CapturedRoom in the camera preview. It's not meant to be overwritten. Should we create a RoomBuilder per CapturedRoomData, or are their advantages to reusing the same room builder instance? This is an option that can be left up to the developer to implement for what makes sense for the design of their app. There aren\u2019t RoomBuilder specific advantages or disadvantages to reusing the same instance. Can sceneDepth depth maps be captured at the same time as as a RoomPlan scan? We enable sceneDepth in underlying arsession in roomCaptureSession. Therefore yes, you could also access it via roomCaptureSession.arSession Thank you <@U03HWCNKLRF>! Is there a way to get the generated usdz model in actual world size? Currently the exported model comes out to be very very small in real world scale This is listed as a known issue in Beta 1 I noticed that capture only identifies a single floor with the configuration in the code sample. Would it be possible to combine multiple captures to capture multiple floors in one model with the same origin reference? Great question! The current technology is designed to support single room and single floor scan. Combining different scans would be a great direction for developer to explore. Thank you <@U03J9L3CG2X>! For the PhotogrammetrySession API, is there any way to geo-reference the result? (so that we can place model on a map accurately based on photos capture location) This could be a good question for the Object Capture Digital Lounge on Friday 10am-12pm. Can this be used for scanning multiple rooms in a single scan? We are currently only supporting a single room scan Is it possible to real time texture the RoomPlan objects before or after export? Currently we do not provide texture of 3D objects. Not sure if this was asked, but is it possible to add textures to the resulting CapturedRoom (Maybe by using the data API) to create a room that looks closer to the real thing? The CapturedRoom struct itself does not allow adding textures but your custom visualizer can parse the structure to get dimension and locations and add textures. Is there any recommendations on manipulating RoomCaptureData for custom display? Like rendering as 2D plans, or even using it as an overlay on the real world after the fact? (Imagining replacing the chair objects with photorealistic chairs, or fantastical objects for games). Both CapturedRoom and CapturedRoomData are read-only. CapturedRoomData is intended for generating the final room model with RoomBuilder. During capture, you can parse the CapturedRoom structures and add your custom visualization (eg. photorealist chairs, fantastical objects as you mentioned) on the real world. I'm just now trying to get up to speed. Is it possible to use a room plan as an anchor for future AR sessions? A particular use case would be tying an AR experience to the architecture and physical properties of a room. RoomPlan is not an ARAnchor in current design. Thanks for suggestion. We will take into consideration. Any other suggestions, by chance? Could this be reasonably solved with an archived world map, distributed to client devices, either in the app binary or over the network? Hi all, super stoked to watch the RoomPlan session yesterday. I am quite new to ARKit and the USD filetype so forgive me if this question is out of scope for this session. What would be the best way to create a 2D floor-plan (image) from the outputted USD file on-device? We've got ways we could do this on our servers, but the ability to process this on device would be amazing. USD file contains 3D dimension and transformation information. Therefore, you could do projection to 2D in any arbitrary angle. For 2D floorplan, simply projection from y coordinate should work. Alex, are the dimensions available directly from the USD file? Is there sample code for this? yes, dimensions are inside usdz file. You could import the exported usdz. I think you could find more info in documentation. Would it be possible to choose which components to display from RoomCaptureView and adjust location and sizing on screen? Like for example only show the mini-map render and adjust it's location and size? The Model (or mini-map as described) is not currently adjustable in the RoomCaptureView , but can be disabled optionally by setting roomCaptureView.isModelEnabled = false and a custom model could be implemented if desired using a custom visualizer. This also seems like a good opportunity to use the data-only version of the API and implement a completely custom visualizer for the application Thank you <@U03HHNW0MRR>! Is this API improving based on usage (ML?) No Are there any recommendations on converting the resulting usdz file to something like an obj file to be used in Blender? The current structure of a usd file and folders of usda files don't seem to import nicely The current API does not support OBJ, developers are encouraged to explore using existing tools for conversion. Are there corrections being done to fix close to 90 angles to 90, and parallel, perpendicular geometries by the API? When user finish scanning and generating final result, the post processing would try to generate output as close to reality as possible. Are we able to capture ARMeshAnchors while running a RoomPlan capture session at the same time? No, you cannot capture ARMeshAnchors while running RoomCaptureSession. Thank you <@U03HAQGCXC7>! You're welcome <@U03K6CZC8LQ>. It would be a cool feature to have the capture be right to the right anchor Definitely would like something slightly more general, still with rooms and objects, but more \"industrial\" applicability. Even if the confidence is low and it's more surfaces than objects with categories it would be very useful to us. Thanks Thanks for the question, can you clarify more what is the \"industrial\" application? Do you mean support commercial store scanning? Not just residential rooms with sinks, fireplaces, etc but more like a factory floor or other industrial setup. These may be small rooms with windows and doors, but the objects inside aren't home furnishings. We will have to see how well RoomPlan v1 works when we test it Got it, thanks Steven for the clarification! Is there any documentation on what the output USDZ structure is like? I brought my Room Plan USDZ output into RealityKit and looked at the children, and it\u2019s very well organized. Just wondering if there\u2019s any documentation to know how we might remove certain subcomponents (like walls, windows, etc). Thank you for trying out USDZ file so quickly! Currently there is no documentation for USDZ structure. Thanks for the feedback and we will take it into consideration Thank you, <@U03HWCN4S0Z>! I\u2019m missing ceilings and floors in the export and the metersPerUnit header detail in all files <@U03J7B27ADD>: We don't support ceilings and floors. Thanks for noticing the missing metersPerUnit header in usd files. We've logged it in our database. Thank you so much for taking the time to answer our questions!! This is incredibly helpful We\u2019re more than happy to see developers, such as yourself, are interested in RoomPlan, and would do our best possible to support your adoption of it in your project. How are dimensions mapped in the in the dimensions simd_float3 for CapturedRoom.Object and CapturedRoom.Surface? Do these values represent height, width, and depth? If so in which order? <@U03K6CZC8LQ>: The dimension value order is width, height, and length. How can we get a scanned object tag/label (chair/fireplace...) <@U03JTPRQ66Q>: Scanned object's label is a enum typed property named category in CapturedRoom.Object Is there any guidance on how \"floors\" are calculated/rendered in a Room Plan capture? Does the .floor type exist, or in the rendered CapturedRoom , is the output adding a general plane as the floor? We currently don\u2019t support floor type in the framework. Developers are encouraged to explore custom visualizations! Thank you, <@U03HWCN4S0Z>! Thanks for the question Can custom object classifications be implemented for RoomPlan other than the default object recognition? What level of ML integration can it support? the current classification model in the pipeline is not a module. It cannot be easily replaced by an external model. is the accuracy of the RoomPlan the same level of accuracy as the LiDAR scan? aside from the known issue with the scale being 100 times off, how much difference is it from real scale when scaled back appropriately when it's exported from USDZ to FBX or OBJ. We are constantly working on improving the accuracy of the RoomPlan results. RoomPlan is intended for simplified parametric representation of the room. It does not have the same level of accuracy as LiDAR scans. We do have a lot to improve to support applications that have high accuracy requirement. Why isn't the object capture API available on iOS? Please ask during the Object Capture Digital Lounge on Friday June 10th from 10am-12pm PST, or ask in one of the Object Capture Labs. This Digital Lounge is currently for RoomPlan. Aside from the recommendation of a 30\u2019 x 30\u2019 room maximum, is there an imposed maximum area size limit with how RoomPlan can capture new objects and surfaces? No, there is nothing that will stop you from scanning a large area While you will be able to scan larger areas, there is still impact that will happen with drift and device heat for larger areas makes sense, thank you <@U03HHNW0MRR>! Can we store/export the original mesh while using the RoomPlan API alongside the RoomPlan CapturedRoom? Also, can RoomPlan be run concurrently while also capturing a mesh with Lidar? <@U03J98VF2K1>: RoomPlan doesn't export mesh data. You'll have to run reconstruction mesh while RoomCaptureSession isn't active. Would we still be able to extract the depth map, color image, color camera position, lidar camera position? <@U03J98VF2K1>: Yes, you can still assign yourself as a ARSessionDelegate and get those ARKit's information. Awesome thank you :slightly_smiling_face: I\u2019m currently able to read the color camera position, is there lidar camera position information as well? <@U03HAQGCXC7> Additionally, which camera position is being returned when doing a Room Scan with lidar? <@U03J98VF2K1>: The camera position is from the wide angled lens. Is that the only camera position we can receive? No lidar camera position information is being exposed? <@U03J98VF2K1>: Yes, to the best of my knowledge, you only get the position from the camera that you are seeing on the screen. Since this is intrinsically ARKit information, you could confirm it in ARKit channel. Ah thanks, I\u2019ll give that a try tomorrow :slightly_smiling_face: Is it possible to get the underlying mesh information during or after a RoomPlan scan? It\u2019s not possible to get underlying mesh information while RoomCaptureSession is active. Thank you :slightly_smiling_face: I played around with the configurations during a RoomPlan, namely RoomBuilder. What does .beautifyObjects actually do? I exported two captured rooms with the option on and off. The results looked the same. .beautifyObjects on vs off is to align chairs along with table. Thanks! yes... if you try a scene with chairs around a table (like a dining set), you can see the difference.. the beautified chairs is shorter than table, and evenly distributed and align along table. Ah that\u2019s why I didn\u2019t see any difference, I only had one chair and it was pushed into my desk Is it possible to leverage the object detection data separately from the room measurements (walls, doors, windows)? We have an app that uses ArKit + LiDAR to capture precise measurements, but would like to augment that with RoomPlan. <@U03K881J7DX>: Object detection results are presented in the array property objects in CapturedRoom struct. You can just fish them out separately. I\u2019ve see that y\u2019all recommend a custom visualizer. Would it be possible to add AR objects at the same time the user is taking measurements? Say, real-time substituting chairs, beds, walls, etc? Yeah, using a custom visualizer it would totally be possible to substitute objects. A great opportunity for developers to explore! I probably wouldn't be able to use RealityKit to place those objects with my custom visualizer correct? Because it would be busy with the RoomPlan? Correct, the custom visualizer would replace RoomCaptureView and utilize the data only API to perform the visualizations. Can an app access the ARSession.currentFrame property on the RoomCaptureSession.arSession (for instance to obtain the RGB frame, depth, or camera pose) as usual? If so, is there any way to obtain the latest frame when available other than just... polling at 60Hz? Since the RoomCaptureSession owns the ARSessionDelegate, which is usually how the latest frame would be obtained. <@U03HVDET8S2>: Yes, you can still get all of ARKit's information through ARSessionDelegate. RoomCaptureSession doesn't prevent client from becoming the delegate. Oh, nice! I just assumed it did -- thanks! Do you plan on supporting additional ceiling types beyond flat? Sloped, peaked, tray/coffered? Good question! We only support flat ceiling. It would be a great direction for developers to explore. +1 to this. My ceiling is sloped about 1\" over 10' to allow draining in a common direction. I\u2019m sure it would confuse any attempt to clean up angles to 90\u00b0, etc. Thanks for the feedback. We will take it into consideration Can you please let us know if this RoomPlan is built on top of ARKit planes mesh? Room Plan is built upon various ARKit technologies Can I assume that it using ARKit planes information? No, sorry we can not comment on implementation details and they are also subject to change What is the smallest wall surface length that is supported? We're had difficulty capturing walls that less than 1 foot in length (not height). We don't set an explicit limit on the length of the smallest walls. But thanks for the feedback. We will take this into consideration. Are the RoomPlan models as accurate as the ARMeshAnchors or is accuracy being sacrificed for clean geometry? We haven\u2019t compared the accuracy between RoomPlan models with ARMeshAnchors. A RoomPlan model is intended to be a simplified parametric representation of the room. We are trying to make the final model to be as close to the real world as possible. But there are some abstractions or simplifications such as representing wall/window/door/opening as planes without thickness. Thank you <@U03HWCN4S0Z>! Is there any documentation for extracting wall dimensions from the USDZ file from within swift on iOS? I can import the USDZ into Xcode and easily see any manipulate the data, but I\u2019m not seeing how to access that programmatically. Using RealityKit you can load the USDZ as an Entity and then perform evaluations on it. Please note that there is a known issue with Beta 1 on the scale of dimensions in a USDZ file. https://developer.apple.com/documentation/realitykit/loading-entities-from-a-file Additionally, you are able to obtain the dimension values directly from the CapturedRoom surfaces and objects without needing to export and load the USDZ Thank you, are there any particular resources within RealityKit that I should start looking into? Yeah that link above provides documentation on how to load a USDZ file as an Entity, and then you can walk through the child Entities to find your desired objects and surfaces. Here is the documentation on an Entity https://developer.apple.com/documentation/realitykit/entity Perfect, thank you! As might be a common use case, do you have any guidance for how one might implement a \"measurement\" tool to measure from one point to another (I presume using a raycast to determine those points, but not sure where I'd go from there to measure in meters in feet the distance between two points)? Good question! Looks like you already have some ideas. Feel free to explore the possibilities. Can the ARSession provided by RoomCaptureSession be supplied to the new NearbyInteraction NISession.setARSession? This would be useful to, say, localize a UWB beacon within a room being scanned. <@U03HVDET8S2>: You can certainly supply the ARSession provided by RoomCaptureSession to other object. The intention is for users to get existing ARKit supplied information out of it. But if the ARSession is used to run ARConfiguration, RoomCaptureSession will stop automatically. That makes sense; just wanted to clarify. Thanks again <@U03HAQGCXC7> Would it be possible to apply textures to the CapturedRoom? Great question! That will be a good use case for data-only API with custom visualization. I have turned off different things in the configurations, like instructions. Is that what data-only API means or is that something else? Oh, sorry for the confusion. The data-only API is mainly referring to RoomCaptureSession and RoomCaptureSessionDelegate protocol, with which you can get CapturedRoom during the scan and apply textures yourself. For final model, you will also get CapturedRoom from RoomBuilder.capturedRoom(from:) to apply textures. Ah that makes sense. thanks :slightly_smiling_face: So when applying the textures, is that the same as grabbing an ARFrame and aligning to some timestamp to apply? Is there a 3D processing library coming to iOS any time soon (make meshes watertight). Is that anything you guys have on your radar? Thanks, we don\u2019t discuss future releases of Apple products. But we\u2019d love your feedback and suggestion. Please file your feedback here to get it into our system. https://developer.apple.com/bug-reporting/ I am aware. Will file this to put it on your teams radar Thank you! On Full Reconstruction Displacement maps always turn out Red. They are about 100MB large but contain no useable data. I checked all the different Color channels In the full detail level we provide the following texture maps: diffuse, normal, AO, roughness, displacement . Displacement is useful for some use cases. In case you do not need some of the texture maps you could try to output uncompressed USDA and individual texture maps in a folder (by providing a folder URL). That makes it easier to choose a subset of the texture maps. Thanks <@U03J7GGBZDW>. We would actually like to use displacement for rendering with NIRA but the displacement map we get from Full Quality doesn't have any useable information. The displacement map is a single channel output. We can look at your data if you share it via feature request. Yeah I thought it is in the red channel but checked all channels and couldn't find any output. We filed a bug for it FB10124152 Can we use a Bounding Box on the PointCloud to pass to PhotogrammetrySession before starting mesh reconstruction to cut down on time for reconstruction? The .bounds Request should be used to get the initial reconstruction bounding box. This can be used in an GUI iterative workflow as shown in the WWDC 21 Object Capture video to then send the desired reconstruction volume box for the reconstruction to use by adding it into the Geometry property of the Request . Note that this year we added OrientedBoundingBox to allow more general reconstruction boxes (not axis-aligned). We recommend to use the .preview Detail along with .bounds for this, but it should also be possible to use the point cloud as well. The key is that the volume specified in Geometry is relative to the initial coordinate system defined by the .bounds Request . Intermediate results in the session are cached, so subsequent requests should be faster than the initial ones as well. For the PhotogrammetrySession API, is there a way to Geo-reference the results? So that we can put generated 3d models on a map, especially with drone captured imagery. We do not provide automatic geo-referencing functionality in the API. If this is a feature you would like to see, please submit to us a request in the Feedback Assistant. Alright! Will submit a feedback! You could just get the location using existing APIs though, right? https://developer.apple.com/documentation/corelocation Well you could get the locations of the images yes, but not the generated model right? I was thinking of capturing the location of the device as it captures the room. Obviously, GPS inside doesn't always work well... Sorry, I was focused on RoomPlan. You're looking at ObjectCapture :) Yeah, the last time I played with PhotogrammetrySession, I used a bunch of drone imagery of a building, and the results were great but sadly no option to geo reference it. My team used to do a lot with drone capture - using drone images to create accurate 3D roof measurements. I'd be happy to see if anyone has ideas that could help short of re-creating what we did. I don't think they want us sharing contact info on here, but you can look me up on LinkedIn - Jeff Lewis w/Verisk Drone imagery is a great use case for photogrammetry.:+1::skin-tone-2: <@U03K881J7DX> I work in a local startup that works on photogrammetry hence my interest in this subject. Don\u2019t use LinkedIn much but I think I found your profile I have used a drone for photogrammetry that includes GPS and altitude in it's metadata. The resulting models are always reconstructed high in the Y axis as if it is using the altitude data. Does the API use altitude? This also seems to suggest that the APA can use GPS data to aid in reconstruction, but not return it. https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata Btw <@U03JSLCQU04> you might want to use the workflow feature for your question as the Apple Engineers may not be monitoring this thread Done, thank you <@U03J21CNQ1G> Hello all! I'm checking the PhotogrammetrySession.process(request:) new doc and it seems that now we can use it on iOS devices ( https://developer.apple.com/documentation/realitykit/photogrammetrysession/process(requests:)?changes=latest_major).|https://developer.apple.com/documentation/realitykit/photogrammetrysession/process(requests:)?changes=latest_major). Will I be able to use Object Capture on an iPad? Is there some limitations like needed M1? Thanks!!! This is an error in the documentation that will be addressed. Object Capture is only available on macOS. I'd love to create both images for Object Capture and an ARReferenceObject so that I can later show virtual content registered on both the real object and the USDZ from Object Capture. After all, both give me access to a raw point cloud. While there doesn't seem to be an official API for such a use-case I was wondering if you have anything you can point me to do register these point clouds. Are there any assumptions I can make, e.g. on the origin of the created USDZ model? It is an interesting idea. The two point clouds are different data structures and come from different algorithms. We do not provide currently algorithms for point cloud alignment but there are a several open source packages for this. We are happy about feedback, please submit also a feature request for it. Thanks! Does anyone knows particular algorithms or search queries that could point me in the right direction? FB10160265 On Full Reconstruction and large image data sets (300 images+) we experienced timeouts and failed jobs and when a job finished it sometimes produced no Albedo Texture - is it possible the gpu runs out of memory and cannot produce the high resolution texture (8K) It is difficult to diagnose and depends also on the other workload which you have on the GPU. Please provide feedback with your data. That will allow us to look into this. FB10124598 I've noticed when I set my phone down on the table with the camera facing down, didEndWith gets called automatically. Is there a way to prevent this from happening? Since RoomCaptureSession relies on ARSession , this cannot be prevented since if tracking is failing, didEnd will automatically be called. However, you can take action based on the error provided in the didEnd delegate. There is an option to add EXIF metadata to samples in object capture (like focal length, GPS position, etc), but there is no example or documentation on how this data should be formatted, and no feedback from the API that this data is successfully being used. Is it possible to provide an example of this? ImageIO is a great framework to read and write image metadata, including EXIF metadata. Please see https://developer.apple.com/documentation/imageio Thank you for the advice! I may need to clarify better. If I am creating a photogrammetry sample manually with CVPixelBuffers, there is an option to add metadata as a [String : any] dictionary. Should the metadata be a float, int, string? https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata That is good feedback for our documentation, thanks I am using the photogrammetry cli application called \"Hello Photogrammetry\" that you have in your documentation. It all starts well by producing messages like the following to indicate progress \"2022-06-09 22:30:24.675274-0500 HelloPhotogrammetry[42340:2622322] [HelloPhotogrammetry] Progress(request = modelFile(url: file:///Users/Merficius/Downloads/Test/Food/food-high-heic-meat.usdz, detail: RealityFoundation.PhotogrammetrySession.Request.Detail.full, geometry: nil) = 0.013281\" at a good constant rate, but then it stops the progress and starts spamming this kind of message \"2022-06-09 22:43:41.325189-0500 HelloPhotogrammetry[42340:2663624] VPA info: plugin is INTEL, AVD_id = 1080020, AVD_api.Create:0x24c2bdf2c\", after a while it resumes to the first kind of message and then goes back to the second message, then to the first message, etc. Is this normal behavior? Thanks for the feedback. Please provide data (images, logs, machine info etc.) to developer forum https://developer.apple.com/bug-reporting/ or Feedback Assistant https://feedbackassistant.apple.com for us to look into. Is there a way to feed a folder of images and masks to the Photogrammetry API, or will I need to instantiate each image and mask individually? I know I can provide a folder, I just wasn\u2019t sure if there is a way for the API to know which images are masks. The PhotogrammetrySession when provided with a folder will assume all image files in the folder are RGB images of the object to be reconstructed, so you cannot mix them all in one folder if you use the folder input init. There is currently not a way to specify masks using the folder input to a session. To provide masks, you will need to use the custom input PhotogrammetrySession that takes a lazy sequence of PhotogrammetrySample objects that you create yourself in your app. You will have to load the images yourself in this case and they can be stored whereever you wish, in the same folder, in a database, etc. Got it, thank you, <@U03HL4SJY5S>! (Reposted/clarified from threads) This seems to suggest that GPS metadata can be added to a sample to aid in reconstruction. https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata|https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata Does it help with the scale and altitude of a reconstruction? The dictionary key should be a string, but what data type should the value be added in? Float, double, int? Yes providing GPS information is useful and can help with the reconstruction, especially for drone use cases. We use the same formatting as ImageIO. Is there a way to force the API to use a specific GPU? The API does not support to specify GPUs. If you have this feature request in your use case, please consider filing a request in Feedback Assistant https://feedbackassistant.apple.com or developer forum https://developer.apple.com/bug-reporting/ If \"object masking\" is enabled when processing a Photogrammetry model, but there are no object masks provided, does the algorithm create its own mask? yes if this value is enabled, but the samples don\u2019t contain object masks, Object Capture attempts to automatically create a mask algorithmically. If it\u2019s unable to create a mask, Object Capture reverts to reconstructing the object using the entire image. More information can be found in the documentation here: https://developer.apple.com/documentation/realitykit/photogrammetrysession/configuration-swift.struct/isobjectmaskingenabled Got it, thank you <@U03J7GGBZDW> <@U03HL4SJY5S>! Our App captures rooms. We use SceneKit to draw lines representing Wall edges. We want to stay with our wall capture method. But auto capturing everything else is enticing. I\u2019ve plugged in a RoomCaptureSession into the app. I want to use point data to draw lines with SCNNodes. But when the callbacks get called(e.g. didUpdate), there does not seem to be any useful point data for a a door. Even with a high confidence level, in the didUpdate method when I look inside the CapturedRoom, I\u2019m seeing 0 values for CompletedEdges, PolygonCorners and PolygonEdges. Is there a particular callBack method I should be using? It looks like didUpdate and didChange are the main methods that get called. But I\u2019m not seeing any valuable point data in them. You can use transform and dimensions paremeters to draw lines. The 4 corners can be inferred from those 2 parameters: the first column of the transform is the \"right\" vector, and second is the \"up\" vector. The fourth column is the position of the wall/door/opening/window/object Combining those unit vectors with the dimensions vector will give you the corners Is there still a 1000 image hard limit in photogrammetry sessions? Previously the API would stop processing images after 1000 regardless of their resolution, or how much memory was in the system. I have tests both 2 megapixel up to 12 megapixel datasets, on Intel, M1, and M1 Max chips. Thanks for the feedback. What is the desired upper limit for your use case? It would be great if you can provide more info about your use case if the current set-up is limiting, at Feedback Assistant https://feedbackassistant.apple.com or developer forum https://developer.apple.com/bug-reporting/ .","title":"object capture and room plan"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#object-capture-and-room-plan-lounge-qas","text":"","title":"object-capture-and-room-plan-lounge QAs"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#hi-im-just-wondering-if-you-have-suggested-guidance-for-capturing-objects-that-are-on-a-rotating-platform-to-minimize-post-production-work-is-it-advised-to-try-to-frame-the-platform-out-of-the-image-as-best-as-possible-to-post-process-each-image-and-crop-out-the-platform-wrap-the-platform-in-a-similar-color-to-the-background-etc","text":"We suggest to use a uniform color platform which is different than the object and keep the object inside the frame, but as large as possible. Use object masking to make PhotogrammetrySession remove the background https://developer.apple.com/documentation/realitykit/photogrammetrysession/configuration-swift.struct/isobjectmaskingenabled For difficult objects you can also provide manual masks. Please see https://developer.apple.com/documentation/realitykit/photogrammetrysample/objectmask Thank you, <@U03J7GGBZDW>! Since I have the camera on a tripod and object(s) on a platform, doing some degree of post-processing or masking might be a viable solution when passing the photos into the Object Capture API. <@U03J20E7UBV> To get the best outcome from post-processing, please keep the exif in the images and do not crop or resize the images.","title":"Hi!  I'm just wondering if you have suggested guidance for capturing objects that are on a rotating platform, to minimize post-production work.  Is it advised to try to frame the platform out of the image as best as possible, to post-process each image and crop out the platform, wrap the platform in a similar color to the background, etc.?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-i-get-point-cloud-during-updating-progress-of-request-or-can-only-get-point-cloud-data-after-request-complete","text":"There is a separate request for point cloud and model. You can get the point cloud output before the model is done.","title":"can I get point cloud during updating progress of request or can only get point cloud data after request complete ?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#how-can-i-crop-the-model-after-it-is-made","text":"We provide a Geometry field on the model reconstruction Request that will allow you to specify the cropping region during the model creation step. We show this flow in last year's WWDC session ( https://developer.apple.com/videos/play/wwdc2021/10076/ ). To recap, a GUI app can be made with the API that will let you request a preview model, arrange the reconstruction volume, and then get the cropped output. We recommend cropping during reconstruction since this allows our algorithms to focus the texture and mesh resolution on areas that are being reconstructed. Also note that this year we added the ability to specify an OrientedBoundedBox in the request to allow a more general cropping volume.","title":"How can I crop the model after it is made?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-i-choose-how-many-or-range-or-point-clouds-to-generate","text":"The number of points or the range of the points in a point cloud request is automatically determined by the algorithms based on the input images.","title":"can I choose how many or range or point clouds to generate?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-it-possible-to-add-custom-textures-to-the-mesh-map-for-example-a-furniture-piece-i-scanned-is-of-material-oak-color-can-i-change-it-to-a-glossy-silver-color","text":"You can edit the texture map generated by the PhotogrammetrySession and assign it to the obj model by Reality Converter or USDZ Tools https://developer.apple.com/augmented-reality/tools/ . If you need more information on usdz, please join \u201cThe Q&A: USD\u201d from 5-7pm today.","title":"Is it possible to add custom textures to the mesh map? For example, a furniture piece I scanned is of material Oak color, can I change it to a glossy silver color?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-you-clarify-the-underlying-differences-for-a-roomcaptureview-and-a-roomcapturesession-roomcaptureview-seems-like-it-is-a-uiview-subclass-that-handles-all-of-the-underlying-logic-for-scanning-and-creation-of-the-3d-model-for-output-whereas-roomcapturesession-appears-to-be-a-custom-implementation-that-allows-us-to-customize-the-process-what-would-be-the-use-case-for-a-custom-roomcapturesession","text":"RoomCaptureView is a plug and play view that you can use in your app RoomCaptureSession let\u2019s you subscribe to updates using a delegate and gives you finer grain control over the surfaces and objects that RoomPlan detects. RoomCapture session can be used for filtering out surfaces or objects and rendering them as per your liking.","title":"Can you clarify the underlying differences for a RoomCaptureView and a RoomCaptureSession?  RoomCaptureView seems like it is a UIView subclass that handles all of the underlying logic for scanning and creation of the 3D model for output, whereas RoomCaptureSession appears to be a custom implementation that allows us to customize the process.  What would be the use case for a custom RoomCaptureSession?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#im-just-now-trying-to-get-up-to-speed-is-it-possible-to-use-a-room-plan-as-an-anchor-for-future-ar-sessions-a-particular-use-case-would-be-tying-an-ar-experience-to-the-architecture-and-physical-properties-of-a-room","text":"It is an interesting idea but this question may be better suited for tomorrow RoomPlan Q&A. Thank you!","title":"I'm just now trying to get up to speed. Is it possible to use a room plan as an anchor for future AR sessions? A particular use case would be tying an AR experience to the architecture and physical properties of a room."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-a-point-of-creating-new-models-using-this-years-apis-highresolution-with-last-years-photos-should-it-get-me-a-better-model","text":"We added algorithmic improvements to this year's Object Capture API, so you may see improvements to your model if you re-run this year's API on images you already captured last year.","title":"Is there a point of creating new models using this year's APIs (highResolution?) with last year's photos? should it get me a better model?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#perusing-the-room-capture-demo-would-we-have-access-to-the-view-just-like-an-arview-so-we-have-finer-control-over-objects-and-experiences-in-it-looking-at-the-documentation-doesnt-yield-anything-useful","text":"This is not supported using a RoomCaptureView . But you can have fine control using the data only API and creating a custom ARView (or SCNView or MTKView )","title":"Perusing the room capture demo, would we have access to the view just like an ARView, so we have finer control over objects and experiences in it?   Looking at the documentation doesn\u2019t yield anything useful"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#how-can-i-convert-the-usdz-file-generated-by-object-capture-to-a-format-that-i-can-edit-with-blender-such-as-fbx-or-usd","text":"The Object Capture API can directly provide OBJ and USD (ascii) outputs if you provide a folder URL instead of a USDZ filename when requesting the modelFile . The maps will also be placed into this folder along with the OBJ and USD.","title":"How can I convert the .usdz file generated by object capture to a format that I can edit with blender, such as .fbx or .usd?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-arkit-6s-new-sessioncapturehighresolutionframe-intended-for-use-with-an-object-capture-experience-its-not-clear-if-sessioncapturehighresolutionframe-captures-just-the-high-resolution-camera-feed-or-the-camera-feed-with-overlaid-ar-content","text":"The captureHighResolutionFrame can be used by developers creating their own Object Capture app using ARKit to get higher quality images to be used for Object Capture. It captures the high-res camera feed, returning an ARFrame with a high-res image along with other information such as the camera tracking results. Using this API you just get the camera feed though and not the overlaid content in AR Thanks, <@U03J7GBRQ0G>! Does the output file format (I'm not sure if it's a HEIC or PNG, etc.) also include depth/gravity data baked in? You simply get an ARFrame and can access the CVPixelBuffer through that to eventually save it to the format of your choice https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframe Got it! Thanks so much for the helpful information!","title":"Is ARKit 6's new session.captureHighResolutionFrame intended for use with an Object Capture experience?  It's not clear if session.captureHighResolutionFrame captures just the high resolution camera feed, or the camera feed with overlaid AR content."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#does-the-output-usdz-file-from-a-roomplan-session-maintain-some-sort-of-a-model-hierarchy-where-the-object-types-and-not-surface-types-could-be-modifiedremoved-at-a-later-time-or-is-the-usdz-flattened","text":"These are great questions for the Q&A session for RoomPlan tomorrow. There are two choices for you: You can either tune in at 9-11am or 4-6pm Pacific Time. Alternatively, you can also request a 1:1 lab appointment on the developer app for this Thursday. Thank you! Will focus all future questions on Object Capture!","title":"Does the output USDZ file from a RoomPlan session maintain some sort of a model hierarchy where the object types (and not surface types) could be modified/removed at a later time, or is the USDZ flattened?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#if-someone-were-interested-in-filtering-out-objects-from-a-captured-roomplan-would-it-be-advised-to-do-this-in-the-func-capturesession_-session-roomcapturesession-didendwith-data-capturedroomdata-error-error-method-just-prior-to-exporting-to-a-usdz-model-or-at-some-other-time-during-the-capture-process","text":"Another excellent question. It would be great to have you join us at the RoomPlan Q&A session tomorrow where we can cover this and lots of other questions on this topic. It all depends if you\u2019re using RoomCaptureView or not: if you are, there\u2019s no way of filtering out objects from live scan rendering, so you\u2019ll have to do it prior to exporting to a USDZ. If you have your own custom view (using Data Only API), We\u2019d recommend doing it directly in the live view (so that end users understand that objects won\u2019t be part of the final USDZ) That makes sense, thank you very much! :slightly_smiling_face:","title":"If someone were interested in filtering out [.objects] from a captured RoomPlan, would it be advised to do this in the func captureSession(_ session: RoomCaptureSession,                          didEndWith data: CapturedRoomData, error: Error?) method, just prior to exporting to a USDZ model, or at some other time during the capture process?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#as-someone-who-is-new-to-both-those-tools-but-is-working-in-swiftmetal-realm-is-the-api-of-both-of-them-accessible-to-have-the-functionality-added-to-other-applications","text":"Object Capture and Room Plan are both public Swift API\u2019s intended to be used by developers to make their own applications. We have sample code to https://developer.apple.com/documentation/realitykit/taking_pictures_for_3d_object_capture|take pictures for 3D object capture and https://developer.apple.com/documentation/realitykit/creating_a_photogrammetry_command-line_app|creating a photogrammetry command-line app on Mac from last year. This year we add another sample code for https://developer.apple.com/documentation/realitykit/using_object_capture_assets_in_realitykit|using object capture assets in RealityKit . Thanks you","title":"As someone who is new to both those tools, but is working in Swift/Metal realm, Is the API of both of them accessible to have the functionality added to other applications?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#are-there-any-usd-prim-naming-guarantees-for-the-output-room-for-example-could-the-naming-schema-suddenly-change-in-ios-162","text":"There\u2019s no guarantee that the naming schema will remain the same over time, but we do our best to maintain consistency and compatibility over time.","title":"Are there any USD prim naming guarantees for the output room? For example, could the naming schema suddenly change in iOS 16.2?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#are-there-any-hints-about-the-kinds-of-result-degredation-we-can-expect-if-we-use-it-outside-of-residential-settings-eg-if-we-can-live-without-objects-or-high-confidence-levels","text":"Current technology is designed to support interior of residential homes.","title":"Are there any hints about the kinds of result degredation we can expect if we use it outside of residential settings? e.g. if we can live without \"objects\", or high confidence levels..."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#does-roombuilderconfigurationoptionsbeautifyobjects-affect-captured-room-surfaces-or-just-objects","text":"Just objects","title":"Does RoomBuilder.ConfigurationOptions.beautifyObjects affect captured room surfaces, or just objects?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-a-recommended-workflow-for-performing-photogrammetry-to-capture-textures-alongside-roomplan","text":"Great question! We don\u2019t have a recommended workflow for photogrammetry with RoomPlan. It is a great idea for developers to explore!","title":"Is there a recommended workflow for performing photogrammetry to capture textures alongside roomplan?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#should-we-be-able-to-provide-our-own-arsession-to-the-roomcapturesession-i-attempted-to-set-the-arsession-property-with-a-custom-configuration-running-but-room-reconstruction-never-begins-i-noticed-without-the-custom-arsession-the-roomcapturesession-creates-a-roomplanarroomcaptureconfiguration-with-custom-scene-understanding-techniques-keyframe-with-color-if-this-is-a-bug-id-be-happy-to-file-feedback","text":"@Jack: ARSession property in RoomCaptureSession is intended to give you a handle to give developers the ability to customize the rendering detected CapturedRoom in the camera preview. It's not meant to be overwritten.","title":"Should we be able to provide our own ARSession to the RoomCaptureSession? I attempted to set the arSession property with a custom configuration running, but room reconstruction never begins. I noticed without the custom ARSession, the RoomCaptureSession creates a RoomPlan.ARRoomCaptureConfiguration, with custom scene understanding techniques (keyframe with color). If this is a bug, I'd be happy to file feedback."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#should-we-create-a-roombuilder-per-capturedroomdata-or-are-their-advantages-to-reusing-the-same-room-builder-instance","text":"This is an option that can be left up to the developer to implement for what makes sense for the design of their app. There aren\u2019t RoomBuilder specific advantages or disadvantages to reusing the same instance.","title":"Should we create a RoomBuilder per CapturedRoomData, or are their advantages to reusing the same room builder instance?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-scenedepth-depth-maps-be-captured-at-the-same-time-as-as-a-roomplan-scan","text":"We enable sceneDepth in underlying arsession in roomCaptureSession. Therefore yes, you could also access it via roomCaptureSession.arSession Thank you <@U03HWCNKLRF>!","title":"Can sceneDepth depth maps be captured at the same time as as a RoomPlan scan?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-a-way-to-get-the-generated-usdz-model-in-actual-world-size-currently-the-exported-model-comes-out-to-be-very-very-small-in-real-world-scale","text":"This is listed as a known issue in Beta 1","title":"Is there a way to get the generated usdz model in actual world size? Currently the exported model comes out to be very very small in real world scale"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#i-noticed-that-capture-only-identifies-a-single-floor-with-the-configuration-in-the-code-sample-would-it-be-possible-to-combine-multiple-captures-to-capture-multiple-floors-in-one-model-with-the-same-origin-reference","text":"Great question! The current technology is designed to support single room and single floor scan. Combining different scans would be a great direction for developer to explore. Thank you <@U03J9L3CG2X>!","title":"I noticed that capture only identifies a single floor with the configuration in the code sample. Would it be possible to combine multiple captures to capture multiple floors in one model with the same origin reference?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#for-the-photogrammetrysession-api-is-there-any-way-to-geo-reference-the-result-so-that-we-can-place-model-on-a-map-accurately-based-on-photos-capture-location","text":"This could be a good question for the Object Capture Digital Lounge on Friday 10am-12pm.","title":"For the PhotogrammetrySession API, is there any way to geo-reference the result? (so that we can place model on a map accurately based on photos capture location)"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-this-be-used-for-scanning-multiple-rooms-in-a-single-scan","text":"We are currently only supporting a single room scan","title":"Can this be used for scanning multiple rooms in a single scan?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-it-possible-to-real-time-texture-the-roomplan-objects-before-or-after-export","text":"Currently we do not provide texture of 3D objects.","title":"Is it possible to real time texture the RoomPlan objects before or after export?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#not-sure-if-this-was-asked-but-is-it-possible-to-add-textures-to-the-resulting-capturedroom-maybe-by-using-the-data-api-to-create-a-room-that-looks-closer-to-the-real-thing","text":"The CapturedRoom struct itself does not allow adding textures but your custom visualizer can parse the structure to get dimension and locations and add textures.","title":"Not sure if this was asked, but is it possible to add textures to the resulting CapturedRoom (Maybe by using the data API) to create a room that looks closer to the real thing?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-any-recommendations-on-manipulating-roomcapturedata-for-custom-display-like-rendering-as-2d-plans-or-even-using-it-as-an-overlay-on-the-real-world-after-the-fact-imagining-replacing-the-chair-objects-with-photorealistic-chairs-or-fantastical-objects-for-games","text":"Both CapturedRoom and CapturedRoomData are read-only. CapturedRoomData is intended for generating the final room model with RoomBuilder. During capture, you can parse the CapturedRoom structures and add your custom visualization (eg. photorealist chairs, fantastical objects as you mentioned) on the real world.","title":"Is there any recommendations on manipulating RoomCaptureData for custom display? Like rendering as 2D plans, or even using it as an overlay on the real world after the fact? (Imagining replacing the chair objects with photorealistic chairs, or fantastical objects for games)."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#im-just-now-trying-to-get-up-to-speed-is-it-possible-to-use-a-room-plan-as-an-anchor-for-future-ar-sessions-a-particular-use-case-would-be-tying-an-ar-experience-to-the-architecture-and-physical-properties-of-a-room_1","text":"RoomPlan is not an ARAnchor in current design. Thanks for suggestion. We will take into consideration. Any other suggestions, by chance? Could this be reasonably solved with an archived world map, distributed to client devices, either in the app binary or over the network?","title":"I'm just now trying to get up to speed. Is it possible to use a room plan as an anchor for future AR sessions? A particular use case would be tying an AR experience to the architecture and physical properties of a room."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#hi-all-super-stoked-to-watch-the-roomplan-session-yesterday-i-am-quite-new-to-arkit-and-the-usd-filetype-so-forgive-me-if-this-question-is-out-of-scope-for-this-session-what-would-be-the-best-way-to-create-a-2d-floor-plan-image-from-the-outputted-usd-file-on-device-weve-got-ways-we-could-do-this-on-our-servers-but-the-ability-to-process-this-on-device-would-be-amazing","text":"USD file contains 3D dimension and transformation information. Therefore, you could do projection to 2D in any arbitrary angle. For 2D floorplan, simply projection from y coordinate should work. Alex, are the dimensions available directly from the USD file? Is there sample code for this? yes, dimensions are inside usdz file. You could import the exported usdz. I think you could find more info in documentation.","title":"Hi all, super stoked to watch the RoomPlan session yesterday. I am quite new to ARKit and the USD filetype so forgive me if this question is out of scope for this session. What would be the best way to create a 2D floor-plan (image) from the outputted USD file on-device? We've got ways we could do this on our servers, but the ability to process this on device would be amazing."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#would-it-be-possible-to-choose-which-components-to-display-from-roomcaptureview-and-adjust-location-and-sizing-on-screen-like-for-example-only-show-the-mini-map-render-and-adjust-its-location-and-size","text":"The Model (or mini-map as described) is not currently adjustable in the RoomCaptureView , but can be disabled optionally by setting roomCaptureView.isModelEnabled = false and a custom model could be implemented if desired using a custom visualizer. This also seems like a good opportunity to use the data-only version of the API and implement a completely custom visualizer for the application Thank you <@U03HHNW0MRR>!","title":"Would it be possible to choose which components to display from RoomCaptureView and adjust location and sizing on screen? Like for example only show the mini-map render and adjust it's location and size?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-this-api-improving-based-on-usage-ml","text":"No","title":"Is this API improving based on usage (ML?)"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#are-there-any-recommendations-on-converting-the-resulting-usdz-file-to-something-like-an-obj-file-to-be-used-in-blender-the-current-structure-of-a-usd-file-and-folders-of-usda-files-dont-seem-to-import-nicely","text":"The current API does not support OBJ, developers are encouraged to explore using existing tools for conversion.","title":"Are there any recommendations on converting the resulting usdz file to something like an obj file to be used in Blender? The current structure of a usd file and folders of usda files don't seem to import nicely"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#are-there-corrections-being-done-to-fix-close-to-90-angles-to-90-and-parallel-perpendicular-geometries-by-the-api","text":"When user finish scanning and generating final result, the post processing would try to generate output as close to reality as possible.","title":"Are there corrections being done to fix close to 90 angles to 90, and parallel, perpendicular geometries by the API?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#are-we-able-to-capture-armeshanchors-while-running-a-roomplan-capture-session-at-the-same-time","text":"No, you cannot capture ARMeshAnchors while running RoomCaptureSession. Thank you <@U03HAQGCXC7>! You're welcome <@U03K6CZC8LQ>. It would be a cool feature to have the capture be right to the right anchor","title":"Are we able to capture ARMeshAnchors while running a RoomPlan capture session at the same time?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#definitely-would-like-something-slightly-more-general-still-with-rooms-and-objects-but-more-industrial-applicability-even-if-the-confidence-is-low-and-its-more-surfaces-than-objects-with-categories-it-would-be-very-useful-to-us-thanks","text":"Thanks for the question, can you clarify more what is the \"industrial\" application? Do you mean support commercial store scanning? Not just residential rooms with sinks, fireplaces, etc but more like a factory floor or other industrial setup. These may be small rooms with windows and doors, but the objects inside aren't home furnishings. We will have to see how well RoomPlan v1 works when we test it Got it, thanks Steven for the clarification!","title":"Definitely would like something slightly more general, still with rooms and objects, but more \"industrial\" applicability. Even if the confidence is low and it's more surfaces than objects with categories it would be very useful to us. Thanks"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-any-documentation-on-what-the-output-usdz-structure-is-like-i-brought-my-room-plan-usdz-output-into-realitykit-and-looked-at-the-children-and-its-very-well-organized-just-wondering-if-theres-any-documentation-to-know-how-we-might-remove-certain-subcomponents-like-walls-windows-etc","text":"Thank you for trying out USDZ file so quickly! Currently there is no documentation for USDZ structure. Thanks for the feedback and we will take it into consideration Thank you, <@U03HWCN4S0Z>! I\u2019m missing ceilings and floors in the export and the metersPerUnit header detail in all files <@U03J7B27ADD>: We don't support ceilings and floors. Thanks for noticing the missing metersPerUnit header in usd files. We've logged it in our database.","title":"Is there any documentation on what the output USDZ structure is like?  I brought my Room Plan USDZ output into RealityKit and looked at the children, and it\u2019s very well organized.  Just wondering if there\u2019s any documentation to know how we might remove certain subcomponents (like walls, windows, etc)."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#thank-you-so-much-for-taking-the-time-to-answer-our-questions-this-is-incredibly-helpful","text":"We\u2019re more than happy to see developers, such as yourself, are interested in RoomPlan, and would do our best possible to support your adoption of it in your project.","title":"Thank you so much for taking the time to answer our questions!! This is incredibly helpful"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#how-are-dimensions-mapped-in-the-in-the-dimensions-simd_float3-for-capturedroomobject-and-capturedroomsurface-do-these-values-represent-height-width-and-depth-if-so-in-which-order","text":"<@U03K6CZC8LQ>: The dimension value order is width, height, and length.","title":"How are dimensions mapped in the in the dimensions simd_float3 for CapturedRoom.Object and CapturedRoom.Surface? Do these values represent height, width, and depth? If so in which order?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#how-can-we-get-a-scanned-object-taglabel-chairfireplace","text":"<@U03JTPRQ66Q>: Scanned object's label is a enum typed property named category in CapturedRoom.Object","title":"How can we get a scanned object tag/label (chair/fireplace...)"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-any-guidance-on-how-floors-are-calculatedrendered-in-a-room-plan-capture-does-the-floor-type-exist-or-in-the-rendered-capturedroom-is-the-output-adding-a-general-plane-as-the-floor","text":"We currently don\u2019t support floor type in the framework. Developers are encouraged to explore custom visualizations! Thank you, <@U03HWCN4S0Z>! Thanks for the question","title":"Is there any guidance on how \"floors\" are calculated/rendered in a Room Plan capture?  Does the .floor type exist, or in the rendered CapturedRoom, is the output adding a general plane as the floor?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-custom-object-classifications-be-implemented-for-roomplan-other-than-the-default-object-recognition-what-level-of-ml-integration-can-it-support","text":"the current classification model in the pipeline is not a module. It cannot be easily replaced by an external model.","title":"Can custom object classifications be implemented for RoomPlan other than the default object recognition? What level of ML integration can it support?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-the-accuracy-of-the-roomplan-the-same-level-of-accuracy-as-the-lidar-scan-aside-from-the-known-issue-with-the-scale-being-100-times-off-how-much-difference-is-it-from-real-scale-when-scaled-back-appropriately-when-its-exported-from-usdz-to-fbx-or-obj","text":"We are constantly working on improving the accuracy of the RoomPlan results. RoomPlan is intended for simplified parametric representation of the room. It does not have the same level of accuracy as LiDAR scans. We do have a lot to improve to support applications that have high accuracy requirement.","title":"is the accuracy of the RoomPlan the same level of accuracy as the LiDAR scan? aside from the known issue with the scale being 100 times off, how much difference is it from real scale when scaled back appropriately when it's exported from USDZ to FBX or OBJ."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#why-isnt-the-object-capture-api-available-on-ios","text":"Please ask during the Object Capture Digital Lounge on Friday June 10th from 10am-12pm PST, or ask in one of the Object Capture Labs. This Digital Lounge is currently for RoomPlan.","title":"Why isn't the object capture API available on iOS?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#aside-from-the-recommendation-of-a-30-x-30-room-maximum-is-there-an-imposed-maximum-area-size-limit-with-how-roomplan-can-capture-new-objects-and-surfaces","text":"No, there is nothing that will stop you from scanning a large area While you will be able to scan larger areas, there is still impact that will happen with drift and device heat for larger areas makes sense, thank you <@U03HHNW0MRR>!","title":"Aside from the recommendation of a 30\u2019 x 30\u2019 room maximum, is there an imposed maximum area size limit with how RoomPlan can capture new objects and surfaces?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-we-storeexport-the-original-mesh-while-using-the-roomplan-api-alongside-the-roomplan-capturedroom-also-can-roomplan-be-run-concurrently-while-also-capturing-a-mesh-with-lidar","text":"<@U03J98VF2K1>: RoomPlan doesn't export mesh data. You'll have to run reconstruction mesh while RoomCaptureSession isn't active. Would we still be able to extract the depth map, color image, color camera position, lidar camera position? <@U03J98VF2K1>: Yes, you can still assign yourself as a ARSessionDelegate and get those ARKit's information. Awesome thank you :slightly_smiling_face: I\u2019m currently able to read the color camera position, is there lidar camera position information as well? <@U03HAQGCXC7> Additionally, which camera position is being returned when doing a Room Scan with lidar? <@U03J98VF2K1>: The camera position is from the wide angled lens. Is that the only camera position we can receive? No lidar camera position information is being exposed? <@U03J98VF2K1>: Yes, to the best of my knowledge, you only get the position from the camera that you are seeing on the screen. Since this is intrinsically ARKit information, you could confirm it in ARKit channel. Ah thanks, I\u2019ll give that a try tomorrow :slightly_smiling_face:","title":"Can we store/export the original mesh while using the RoomPlan API alongside the RoomPlan CapturedRoom? Also, can RoomPlan be run concurrently while also capturing a mesh with Lidar?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-it-possible-to-get-the-underlying-mesh-information-during-or-after-a-roomplan-scan","text":"It\u2019s not possible to get underlying mesh information while RoomCaptureSession is active. Thank you :slightly_smiling_face:","title":"Is it possible to get the underlying mesh information during or after a RoomPlan scan?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#i-played-around-with-the-configurations-during-a-roomplan-namely-roombuilder-what-does-beautifyobjects-actually-do-i-exported-two-captured-rooms-with-the-option-on-and-off-the-results-looked-the-same","text":".beautifyObjects on vs off is to align chairs along with table. Thanks! yes... if you try a scene with chairs around a table (like a dining set), you can see the difference.. the beautified chairs is shorter than table, and evenly distributed and align along table. Ah that\u2019s why I didn\u2019t see any difference, I only had one chair and it was pushed into my desk","title":"I played around with the configurations during a RoomPlan, namely RoomBuilder. What does .beautifyObjects actually do? I exported two captured rooms with the option on and off. The results looked the same."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-it-possible-to-leverage-the-object-detection-data-separately-from-the-room-measurements-walls-doors-windows-we-have-an-app-that-uses-arkit-lidar-to-capture-precise-measurements-but-would-like-to-augment-that-with-roomplan","text":"<@U03K881J7DX>: Object detection results are presented in the array property objects in CapturedRoom struct. You can just fish them out separately.","title":"Is it possible to leverage the object detection data separately from the room measurements (walls, doors, windows)? We have an app that uses ArKit + LiDAR to capture precise measurements, but would like to augment that with RoomPlan."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#ive-see-that-yall-recommend-a-custom-visualizer-would-it-be-possible-to-add-ar-objects-at-the-same-time-the-user-is-taking-measurements-say-real-time-substituting-chairs-beds-walls-etc","text":"Yeah, using a custom visualizer it would totally be possible to substitute objects. A great opportunity for developers to explore! I probably wouldn't be able to use RealityKit to place those objects with my custom visualizer correct? Because it would be busy with the RoomPlan? Correct, the custom visualizer would replace RoomCaptureView and utilize the data only API to perform the visualizations.","title":"I\u2019ve see that y\u2019all recommend a custom visualizer. Would it be possible to add AR objects at the same time the user is taking measurements? Say, real-time substituting chairs, beds, walls, etc?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-an-app-access-the-arsessioncurrentframe-property-on-the-roomcapturesessionarsession-for-instance-to-obtain-the-rgb-frame-depth-or-camera-pose-as-usual-if-so-is-there-any-way-to-obtain-the-latest-frame-when-available-other-than-just-polling-at-60hz-since-the-roomcapturesession-owns-the-arsessiondelegate-which-is-usually-how-the-latest-frame-would-be-obtained","text":"<@U03HVDET8S2>: Yes, you can still get all of ARKit's information through ARSessionDelegate. RoomCaptureSession doesn't prevent client from becoming the delegate. Oh, nice! I just assumed it did -- thanks!","title":"Can an app access the ARSession.currentFrame property on the RoomCaptureSession.arSession (for instance to obtain the RGB frame, depth, or camera pose) as usual? If so, is there any way to obtain the latest frame when available other than just... polling at 60Hz? Since the RoomCaptureSession owns the ARSessionDelegate, which is usually how the latest frame would be obtained."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#do-you-plan-on-supporting-additional-ceiling-types-beyond-flat-sloped-peaked-traycoffered","text":"Good question! We only support flat ceiling. It would be a great direction for developers to explore. +1 to this. My ceiling is sloped about 1\" over 10' to allow draining in a common direction. I\u2019m sure it would confuse any attempt to clean up angles to 90\u00b0, etc. Thanks for the feedback. We will take it into consideration","title":"Do you plan on supporting additional ceiling types beyond flat? Sloped, peaked, tray/coffered?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-you-please-let-us-know-if-this-roomplan-is-built-on-top-of-arkit-planes-mesh","text":"Room Plan is built upon various ARKit technologies Can I assume that it using ARKit planes information? No, sorry we can not comment on implementation details and they are also subject to change","title":"Can you please let us know if this RoomPlan is built on top of ARKit planes mesh?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#what-is-the-smallest-wall-surface-length-that-is-supported-were-had-difficulty-capturing-walls-that-less-than-1-foot-in-length-not-height","text":"We don't set an explicit limit on the length of the smallest walls. But thanks for the feedback. We will take this into consideration.","title":"What is the smallest wall surface length that is supported? We're had difficulty capturing walls that less than 1 foot in length (not height)."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#are-the-roomplan-models-as-accurate-as-the-armeshanchors-or-is-accuracy-being-sacrificed-for-clean-geometry","text":"We haven\u2019t compared the accuracy between RoomPlan models with ARMeshAnchors. A RoomPlan model is intended to be a simplified parametric representation of the room. We are trying to make the final model to be as close to the real world as possible. But there are some abstractions or simplifications such as representing wall/window/door/opening as planes without thickness. Thank you <@U03HWCN4S0Z>!","title":"Are the RoomPlan models as accurate as the ARMeshAnchors or is accuracy being sacrificed for clean geometry?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-any-documentation-for-extracting-wall-dimensions-from-the-usdz-file-from-within-swift-on-ios-i-can-import-the-usdz-into-xcode-and-easily-see-any-manipulate-the-data-but-im-not-seeing-how-to-access-that-programmatically","text":"Using RealityKit you can load the USDZ as an Entity and then perform evaluations on it. Please note that there is a known issue with Beta 1 on the scale of dimensions in a USDZ file. https://developer.apple.com/documentation/realitykit/loading-entities-from-a-file Additionally, you are able to obtain the dimension values directly from the CapturedRoom surfaces and objects without needing to export and load the USDZ Thank you, are there any particular resources within RealityKit that I should start looking into? Yeah that link above provides documentation on how to load a USDZ file as an Entity, and then you can walk through the child Entities to find your desired objects and surfaces. Here is the documentation on an Entity https://developer.apple.com/documentation/realitykit/entity Perfect, thank you!","title":"Is there any documentation for extracting wall dimensions from the USDZ file from within swift on iOS? I can import the USDZ into Xcode and easily see any manipulate the data, but I\u2019m not seeing how to access that programmatically."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#as-might-be-a-common-use-case-do-you-have-any-guidance-for-how-one-might-implement-a-measurement-tool-to-measure-from-one-point-to-another-i-presume-using-a-raycast-to-determine-those-points-but-not-sure-where-id-go-from-there-to-measure-in-meters-in-feet-the-distance-between-two-points","text":"Good question! Looks like you already have some ideas. Feel free to explore the possibilities.","title":"As might be a common use case, do you have any guidance for how one might implement a \"measurement\" tool to measure from one point to another (I presume using a raycast to determine those points, but not sure where I'd go from there to measure in meters in feet the distance between two points)?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-the-arsession-provided-by-roomcapturesession-be-supplied-to-the-new-nearbyinteraction-nisessionsetarsession-this-would-be-useful-to-say-localize-a-uwb-beacon-within-a-room-being-scanned","text":"<@U03HVDET8S2>: You can certainly supply the ARSession provided by RoomCaptureSession to other object. The intention is for users to get existing ARKit supplied information out of it. But if the ARSession is used to run ARConfiguration, RoomCaptureSession will stop automatically. That makes sense; just wanted to clarify. Thanks again <@U03HAQGCXC7>","title":"Can the ARSession provided by RoomCaptureSession be supplied to the new NearbyInteraction NISession.setARSession? This would be useful to, say, localize a UWB beacon within a room being scanned."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#would-it-be-possible-to-apply-textures-to-the-capturedroom","text":"Great question! That will be a good use case for data-only API with custom visualization. I have turned off different things in the configurations, like instructions. Is that what data-only API means or is that something else? Oh, sorry for the confusion. The data-only API is mainly referring to RoomCaptureSession and RoomCaptureSessionDelegate protocol, with which you can get CapturedRoom during the scan and apply textures yourself. For final model, you will also get CapturedRoom from RoomBuilder.capturedRoom(from:) to apply textures. Ah that makes sense. thanks :slightly_smiling_face: So when applying the textures, is that the same as grabbing an ARFrame and aligning to some timestamp to apply?","title":"Would it be possible to apply textures to the CapturedRoom?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-a-3d-processing-library-coming-to-ios-any-time-soon-make-meshes-watertight-is-that-anything-you-guys-have-on-your-radar","text":"Thanks, we don\u2019t discuss future releases of Apple products. But we\u2019d love your feedback and suggestion. Please file your feedback here to get it into our system. https://developer.apple.com/bug-reporting/ I am aware. Will file this to put it on your teams radar Thank you!","title":"Is there a 3D processing library coming to iOS any time soon (make meshes watertight). Is that anything you guys have on your radar?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#on-full-reconstruction-displacement-maps-always-turn-out-red-they-are-about-100mb-large-but-contain-no-useable-data-i-checked-all-the-different-color-channels","text":"In the full detail level we provide the following texture maps: diffuse, normal, AO, roughness, displacement . Displacement is useful for some use cases. In case you do not need some of the texture maps you could try to output uncompressed USDA and individual texture maps in a folder (by providing a folder URL). That makes it easier to choose a subset of the texture maps. Thanks <@U03J7GGBZDW>. We would actually like to use displacement for rendering with NIRA but the displacement map we get from Full Quality doesn't have any useable information. The displacement map is a single channel output. We can look at your data if you share it via feature request. Yeah I thought it is in the red channel but checked all channels and couldn't find any output. We filed a bug for it FB10124152","title":"On Full Reconstruction Displacement maps always turn out Red. They are about 100MB large but contain no useable data. I checked all the different Color channels"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#can-we-use-a-bounding-box-on-the-pointcloud-to-pass-to-photogrammetrysession-before-starting-mesh-reconstruction-to-cut-down-on-time-for-reconstruction","text":"The .bounds Request should be used to get the initial reconstruction bounding box. This can be used in an GUI iterative workflow as shown in the WWDC 21 Object Capture video to then send the desired reconstruction volume box for the reconstruction to use by adding it into the Geometry property of the Request . Note that this year we added OrientedBoundingBox to allow more general reconstruction boxes (not axis-aligned). We recommend to use the .preview Detail along with .bounds for this, but it should also be possible to use the point cloud as well. The key is that the volume specified in Geometry is relative to the initial coordinate system defined by the .bounds Request . Intermediate results in the session are cached, so subsequent requests should be faster than the initial ones as well.","title":"Can we use a Bounding Box on the PointCloud to pass to PhotogrammetrySession before starting mesh reconstruction to cut down on time for reconstruction?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#for-the-photogrammetrysession-api-is-there-a-way-to-geo-reference-the-results-so-that-we-can-put-generated-3d-models-on-a-map-especially-with-drone-captured-imagery","text":"We do not provide automatic geo-referencing functionality in the API. If this is a feature you would like to see, please submit to us a request in the Feedback Assistant. Alright! Will submit a feedback! You could just get the location using existing APIs though, right? https://developer.apple.com/documentation/corelocation Well you could get the locations of the images yes, but not the generated model right? I was thinking of capturing the location of the device as it captures the room. Obviously, GPS inside doesn't always work well... Sorry, I was focused on RoomPlan. You're looking at ObjectCapture :) Yeah, the last time I played with PhotogrammetrySession, I used a bunch of drone imagery of a building, and the results were great but sadly no option to geo reference it. My team used to do a lot with drone capture - using drone images to create accurate 3D roof measurements. I'd be happy to see if anyone has ideas that could help short of re-creating what we did. I don't think they want us sharing contact info on here, but you can look me up on LinkedIn - Jeff Lewis w/Verisk Drone imagery is a great use case for photogrammetry.:+1::skin-tone-2: <@U03K881J7DX> I work in a local startup that works on photogrammetry hence my interest in this subject. Don\u2019t use LinkedIn much but I think I found your profile I have used a drone for photogrammetry that includes GPS and altitude in it's metadata. The resulting models are always reconstructed high in the Y axis as if it is using the altitude data. Does the API use altitude? This also seems to suggest that the APA can use GPS data to aid in reconstruction, but not return it. https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata Btw <@U03JSLCQU04> you might want to use the workflow feature for your question as the Apple Engineers may not be monitoring this thread Done, thank you <@U03J21CNQ1G>","title":"For the PhotogrammetrySession API, is there a way to Geo-reference the results? So that we can put generated 3d models on a map, especially with drone captured imagery."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#hello-all-im-checking-the-photogrammetrysessionprocessrequest-new-doc-and-it-seems-that-now-we-can-use-it-on-ios-devices-httpsdeveloperapplecomdocumentationrealitykitphotogrammetrysessionprocessrequestschangeslatest_majorhttpsdeveloperapplecomdocumentationrealitykitphotogrammetrysessionprocessrequestschangeslatest_major-will-i-be-able-to-use-object-capture-on-an-ipad-is-there-some-limitations-like-needed-m1-thanks","text":"This is an error in the documentation that will be addressed. Object Capture is only available on macOS.","title":"Hello all! I'm checking the PhotogrammetrySession.process(request:) new doc and it seems that now we can use it on iOS devices (https://developer.apple.com/documentation/realitykit/photogrammetrysession/process(requests:)?changes=latest_major).|https://developer.apple.com/documentation/realitykit/photogrammetrysession/process(requests:)?changes=latest_major). Will I be able to use Object Capture on an iPad? Is there some limitations like needed M1? Thanks!!!"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#id-love-to-create-both-images-for-object-capture-and-an-arreferenceobject-so-that-i-can-later-show-virtual-content-registered-on-both-the-real-object-and-the-usdz-from-object-capture-after-all-both-give-me-access-to-a-raw-point-cloud-while-there-doesnt-seem-to-be-an-official-api-for-such-a-use-case-i-was-wondering-if-you-have-anything-you-can-point-me-to-do-register-these-point-clouds-are-there-any-assumptions-i-can-make-eg-on-the-origin-of-the-created-usdz-model","text":"It is an interesting idea. The two point clouds are different data structures and come from different algorithms. We do not provide currently algorithms for point cloud alignment but there are a several open source packages for this. We are happy about feedback, please submit also a feature request for it. Thanks! Does anyone knows particular algorithms or search queries that could point me in the right direction? FB10160265","title":"I'd love to create both images for Object Capture and an ARReferenceObject so that I can later show virtual content registered on both the real object and the USDZ from Object Capture. After all, both give me access to a raw point cloud.  While there doesn't seem to be an official API for such a use-case I was wondering if you have anything you can point me to do register these point clouds. Are there any assumptions I can make, e.g. on the origin of the created USDZ model?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#on-full-reconstruction-and-large-image-data-sets-300-images-we-experienced-timeouts-and-failed-jobs-and-when-a-job-finished-it-sometimes-produced-no-albedo-texture-is-it-possible-the-gpu-runs-out-of-memory-and-cannot-produce-the-high-resolution-texture-8k","text":"It is difficult to diagnose and depends also on the other workload which you have on the GPU. Please provide feedback with your data. That will allow us to look into this. FB10124598","title":"On Full Reconstruction and large image data sets (300 images+) we experienced timeouts and failed jobs and when a job finished it sometimes produced no Albedo Texture     - is it possible the gpu runs out of memory and cannot produce the high resolution texture (8K)"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#ive-noticed-when-i-set-my-phone-down-on-the-table-with-the-camera-facing-down-didendwith-gets-called-automatically-is-there-a-way-to-prevent-this-from-happening","text":"Since RoomCaptureSession relies on ARSession , this cannot be prevented since if tracking is failing, didEnd will automatically be called. However, you can take action based on the error provided in the didEnd delegate.","title":"I've noticed when I set my phone down on the table with the camera facing down, didEndWith gets called automatically. Is there a way to prevent this from happening?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#there-is-an-option-to-add-exif-metadata-to-samples-in-object-capture-like-focal-length-gps-position-etc-but-there-is-no-example-or-documentation-on-how-this-data-should-be-formatted-and-no-feedback-from-the-api-that-this-data-is-successfully-being-used-is-it-possible-to-provide-an-example-of-this","text":"ImageIO is a great framework to read and write image metadata, including EXIF metadata. Please see https://developer.apple.com/documentation/imageio Thank you for the advice! I may need to clarify better. If I am creating a photogrammetry sample manually with CVPixelBuffers, there is an option to add metadata as a [String : any] dictionary. Should the metadata be a float, int, string? https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata That is good feedback for our documentation, thanks","title":"There is an option to add EXIF metadata to samples in object capture (like focal length, GPS position, etc), but there is no example or documentation on how this data should be formatted, and no feedback from the API that this data is successfully being used. Is it possible to provide an example of this?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#i-am-using-the-photogrammetry-cli-application-called-hello-photogrammetry-that-you-have-in-your-documentation-it-all-starts-well-by-producing-messages-like-the-following-to-indicate-progress-2022-06-09-223024675274-0500-hellophotogrammetry423402622322-hellophotogrammetry-progressrequest-modelfileurl-fileusersmerficiusdownloadstestfoodfood-high-heic-meatusdz-detail-realityfoundationphotogrammetrysessionrequestdetailfull-geometry-nil-0013281-at-a-good-constant-rate-but-then-it-stops-the-progress-and-starts-spamming-this-kind-of-message-2022-06-09-224341325189-0500-hellophotogrammetry423402663624-vpa-info-plugin-is-intel-avd_id-1080020-avd_apicreate0x24c2bdf2c-after-a-while-it-resumes-to-the-first-kind-of-message-and-then-goes-back-to-the-second-message-then-to-the-first-message-etc-is-this-normal-behavior","text":"Thanks for the feedback. Please provide data (images, logs, machine info etc.) to developer forum https://developer.apple.com/bug-reporting/ or Feedback Assistant https://feedbackassistant.apple.com for us to look into.","title":"I am using the photogrammetry cli application called \"Hello Photogrammetry\" that you have in your documentation. It all starts well by producing messages like the following to indicate progress \"2022-06-09 22:30:24.675274-0500 HelloPhotogrammetry[42340:2622322] [HelloPhotogrammetry] Progress(request = modelFile(url: file:///Users/Merficius/Downloads/Test/Food/food-high-heic-meat.usdz, detail: RealityFoundation.PhotogrammetrySession.Request.Detail.full, geometry: nil) = 0.013281\" at a good constant rate, but then it stops the progress and starts spamming this kind of message \"2022-06-09 22:43:41.325189-0500 HelloPhotogrammetry[42340:2663624] VPA info: plugin is INTEL, AVD_id = 1080020, AVD_api.Create:0x24c2bdf2c\", after a while it resumes to the first kind of message and then goes back to the second message, then to the first message, etc. Is this normal behavior?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-a-way-to-feed-a-folder-of-images-and-masks-to-the-photogrammetry-api-or-will-i-need-to-instantiate-each-image-and-mask-individually-i-know-i-can-provide-a-folder-i-just-wasnt-sure-if-there-is-a-way-for-the-api-to-know-which-images-are-masks","text":"The PhotogrammetrySession when provided with a folder will assume all image files in the folder are RGB images of the object to be reconstructed, so you cannot mix them all in one folder if you use the folder input init. There is currently not a way to specify masks using the folder input to a session. To provide masks, you will need to use the custom input PhotogrammetrySession that takes a lazy sequence of PhotogrammetrySample objects that you create yourself in your app. You will have to load the images yourself in this case and they can be stored whereever you wish, in the same folder, in a database, etc. Got it, thank you, <@U03HL4SJY5S>!","title":"Is there a way to feed a folder of images and masks to the Photogrammetry API, or will I need to instantiate each image and mask individually?  I know I can provide a folder, I just wasn\u2019t sure if there is a way for the API to know which images are masks."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#repostedclarified-from-threads-this-seems-to-suggest-that-gps-metadata-can-be-added-to-a-sample-to-aid-in-reconstruction-httpsdeveloperapplecomdocumentationrealitykitphotogrammetrysamplemetadatahttpsdeveloperapplecomdocumentationrealitykitphotogrammetrysamplemetadata-does-it-help-with-the-scale-and-altitude-of-a-reconstruction-the-dictionary-key-should-be-a-string-but-what-data-type-should-the-value-be-added-in-float-double-int","text":"Yes providing GPS information is useful and can help with the reconstruction, especially for drone use cases. We use the same formatting as ImageIO.","title":"(Reposted/clarified from threads) This seems to suggest that GPS metadata can be added to a sample to aid in reconstruction. https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata|https://developer.apple.com/documentation/realitykit/photogrammetrysample/metadata Does it help with the scale and altitude of a reconstruction? The dictionary key should be a string, but what data type should the value be added in? Float, double, int?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-a-way-to-force-the-api-to-use-a-specific-gpu","text":"The API does not support to specify GPUs. If you have this feature request in your use case, please consider filing a request in Feedback Assistant https://feedbackassistant.apple.com or developer forum https://developer.apple.com/bug-reporting/","title":"Is there a way to force the API to use a specific GPU?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#if-object-masking-is-enabled-when-processing-a-photogrammetry-model-but-there-are-no-object-masks-provided-does-the-algorithm-create-its-own-mask","text":"yes if this value is enabled, but the samples don\u2019t contain object masks, Object Capture attempts to automatically create a mask algorithmically. If it\u2019s unable to create a mask, Object Capture reverts to reconstructing the object using the entire image. More information can be found in the documentation here: https://developer.apple.com/documentation/realitykit/photogrammetrysession/configuration-swift.struct/isobjectmaskingenabled Got it, thank you <@U03J7GGBZDW> <@U03HL4SJY5S>!","title":"If \"object masking\" is enabled when processing a Photogrammetry model, but there are no object masks provided, does the algorithm create its own mask?"},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#our-app-captures-rooms-we-use-scenekit-to-draw-lines-representing-wall-edges-we-want-to-stay-with-our-wall-capture-method-but-auto-capturing-everything-else-is-enticing-ive-plugged-in-a-roomcapturesession-into-the-app-i-want-to-use-point-data-to-draw-lines-with-scnnodes-but-when-the-callbacks-get-calledeg-didupdate-there-does-not-seem-to-be-any-useful-point-data-for-a-a-door-even-with-a-high-confidence-level-in-the-didupdate-method-when-i-look-inside-the-capturedroom-im-seeing-0-values-for-completededges-polygoncorners-and-polygonedges-is-there-a-particular-callback-method-i-should-be-using-it-looks-like-didupdate-and-didchange-are-the-main-methods-that-get-called-but-im-not-seeing-any-valuable-point-data-in-them","text":"You can use transform and dimensions paremeters to draw lines. The 4 corners can be inferred from those 2 parameters: the first column of the transform is the \"right\" vector, and second is the \"up\" vector. The fourth column is the position of the wall/door/opening/window/object Combining those unit vectors with the dimensions vector will give you the corners","title":"Our App captures rooms. We use SceneKit to draw lines representing Wall edges. We want to stay with our wall capture method. But auto capturing everything else is enticing. I\u2019ve plugged in a RoomCaptureSession into the app. I want to use point data to draw lines with SCNNodes. But when the callbacks get called(e.g. didUpdate), there does not seem to be any useful point data for a a door. Even with a high confidence level, in the didUpdate method when I look inside the CapturedRoom, I\u2019m seeing 0 values for CompletedEdges, PolygonCorners and PolygonEdges. Is there a particular callBack method I should be using? It looks like didUpdate and didChange are the main methods that get called. But I\u2019m not seeing any valuable point data in them."},{"location":"wwdc22/object-capture-and-room-plan-lounge.html#is-there-still-a-1000-image-hard-limit-in-photogrammetry-sessions-previously-the-api-would-stop-processing-images-after-1000-regardless-of-their-resolution-or-how-much-memory-was-in-the-system-i-have-tests-both-2-megapixel-up-to-12-megapixel-datasets-on-intel-m1-and-m1-max-chips","text":"Thanks for the feedback. What is the desired upper limit for your use case? It would be great if you can provide more info about your use case if the current set-up is limiting, at Feedback Assistant https://feedbackassistant.apple.com or developer forum https://developer.apple.com/bug-reporting/ .","title":"Is there still a 1000 image hard limit in photogrammetry sessions? Previously the API would stop processing images after 1000 regardless of their resolution, or how much memory was in the system. I have tests both 2 megapixel up to 12 megapixel datasets, on Intel, M1, and M1 Max chips."},{"location":"wwdc22/photos-camera-lounge.html","text":"photos-camera-lounge QAs by FeeTiki Hello, I pretty new at video in general and I was wondering where to get started with video processing, specifically how do I rewrite multiple videos next to each other for side by side 3D? Thank you! Would you like to record a video with two video tracks? Composite two together? Blend two together? Producing a stereo effect? Give us a little more info. I\u2019d like to take already recoded videos (left and right) and position them side by side not overlapping, like in a VR headset. AVMutableComposition should do the trick. You can use that to composite two movies together and write a new one. It would have an extra wide video track. Great, thank you! Just to piggyback on <@U03JBMMB10A> question, What are the examples out there of doing composition with multiple video tracks, I know a little about layer instructions, but is there any examples of using the frames directly? Have you checked out AVMulticamPiP? https://developer.apple.com/documentation/avfoundation/capture_setup/avmulticampip_capturing_from_multiple_cameras This composites front and rear camera into a single video track. Can there be more than one iPhone connected via Continuity Camera? Only one active stream at a time, but multiple iPhones can be discovered. So you can switch between them, but only one can be streaming actively at a time. I would like to add this as a feature request. We have limited bandwidth over WiFi. 1080p video ain't cheap. :slightly_smiling_face: Will it utilize usb if plugged in? Yes it will. USB has bandwidth constraints too. :slightly_smiling_face: Hi Brad, I have a dumb question. I still use UIKit for using the camera with AVFoundation. Did I miss it that you can make a camera app in SwiftUI? Yes, you can use SwiftUI with AVFoundation. See this sample app we just released for Continuity Camera that shows this: https://developer.apple.com/documentation/avfoundation/capture_setup/supporting_continuity_camera_in_your_macos_app?language=objc Thanks. So, this is new this year? You have been able to use SwiftUI with AVFoundation since SwiftUI's introduction. But this is our first sample released showing SwiftUI with AVFoundation By using UIViewRepresentable , you can use an AVCaptureVideoPreviewLayer in SwiftUI The sample app shows this But that is not native SwiftUI, is it? That is creating a UIView in SwitftUI. I do that for MapKit as well. Actually, a UIView Layer Hi Paul, just chiming in cause I had a heck of a time figuring this out, but totally doable going back to first SwiftUI release (although I require iOS 14+ for my app because of an issue encountered in 13 related to this, can't remember what it was though Edit: I think it was related to viewfinder not stretching to bounds properly ). Have yet to look at the sample mentioned above, but solution is probably similar. At a high level you have your normal CameraViewController. You then create a UIViewControllerRepresentable, lets call it CameraVCSwiftUIView, with a CameraViewController as a member. This CameraVCSwiftUIView is now usable in SwiftUI as your CameraViewController / video preview :slightly_smiling_face: Correct, there is not a native SwiftUI solution for AVCaptureVideoPreviewLayer Check out CameraPreview.swift in the sample app, ContinuityCam Hi Edward, I have this now in my app. The app was originally build in 2015 based on UIKit. Now that I am adding new functionalities and the integration with the Apple watch, I'm also starting to move some code to SwiftUI. Currently, I wrapped the CameraViewController in a UIViewControllerRepresentable. I was, however, more curious to learn, when the AVFoundation camera would natively work in SwiftUI. I know how to build a camera app and how to integrate UIKit code into SwiftUI. That is not my problem. Hi Nikolas, that is too bad, but also gives me time to focus on other things. Thanks. My guess is it probably won't be brought to SwiftUI, just cause of the architecture -- how would one handle pipeline config, state changes, etc in a way that makes sense in a SwiftUI context? (I'm new to SwiftUI so I could be missing something) I think the problem is the AVCaptureVideoPReviewLayer that is used to capture the image/video. The rest is pretty straight forward. Many schools use iPads as cameras. Would be cool if this could also be made to use with iPad. True. iPad form factor is a little unwieldy as a Continuity Camera, as it needs to be facing away from you to use the rear facing cameras. We're just supporting iPhones for now. Imagine the device as an external camera. So many use cases in creative areas. And all the sensors of the iPhone/iPad as external sensors to be used from the Mac. Yep, Continuity Camera is very powerful. Hello, I've noticed the viewfinder gets stretched at the left and bottom edges when LivePhoto is enabled, and AVCaptureDevice.DeviceType is set to .builtInUltraWideCamera. Is this a known issue, and is there a workaround? Thank you for your time! Hi Edward, are you only seeing this when LivePhoto is enabled? Or do you always see this when using the .buildInUltraWideCamera? If it's the latter then I suspect it's simply the distortion of this ultra wide angle camera lens that you're seeing. But you should be seeing that in all 4 corners of the image. Thank you for your reply! Happens only when LivePhoto is enabled. See image below that doesn't look expected. Could you file a bug report with that screenshot and repro steps? Is this only happening with iOS 16? I believe it started in iOS 15, but can't be sure. This is the bug report that was filed a while back :slightly_smiling_face: FB9983221 thanks, I just found the report. It's with our team. The screenshot you sent here is very informative, I've attached it to the bug report. Thank you so much!! I've had a couple of users report this issue too! Haven't been able to replicate it myself Its there on the 12 Pro, but not on the 13 Pro as far as I can tell. Been there through most of iOS 15, and is still there on 15.4.1 Easy question for you! Is UIImagePickerController still the best way to allow people to take photos inside your app, if you're not building a completely camera interface? (The camera mode isn't deprecated, just the photoLibrary one has been?) I'd say UIImagePickerController is the easiest way to make a quick video or photo. Whether it's the best will depend on the feature set you need. It's a canned view, so you have little work to do. Late reply, but you have it exactly right \u2014 UIImagePickerController with the camera source for capture, and PHPicker for selecting existing assets is the recommended combination here. Awesome thanks! Looking forward to a future WWDC where there\u2019s an all new camera API to replace UIImagePickerController :smirk: Please do file feedback with any particular use cases / camera features / etc. that you'd like to see in a new API and feel free to drop the FB # here! I might just have to compose all my thoughts on that! FB10136910: Modern camera API desired to replace UIImagePickerController What is the best way to write a live video stream into a file (via AVAssetWriter) with custom FPS (e.g 18) but in the same time updating AVCaptureVideoPreviewLayer with 30 FPS? On macOS, you can set your frame rate on individual connections, so you can have one frame rate going to preview and one going to video data output. On iOS, you can't. It's just one frame rate dictated by the AVCaptureDevice, so if you want a lower frame rate to your asset writer, you'll need to do your own frame dropping in your VDO. In iOS 16 it now seems possible to capture high resolution photos during an ARKit session. What camera properties can and can not be configured during an active ARKit session? Can we: - lock/set the exposure, white balance, and control the focus distance? - enable/disable multi-exposure fusion and tone mapping? - capture images with 8bit depth? - capture Pro RAW images (presumably not)? You'll need to check with the ARKit team on what features they support. I don't believe you can do any of the things you've mentioned here. They do provide access to the AVCaptureDevice now https://developer.apple.com/documentation/arkit/arconfiguration/3930045-configurablecapturedeviceforprim?changes=latest_minor But yes, I guess it is more a question for the ARKit team If they're providing direct access to AVCaptureDevice, you can certainly set properties on it directly, but I don't know how that will interact with the algorithms they're running. The Continuity Camera's desk view is really interesting and amazing! How does it work? It uses the ultrawide lens of your iPhone. The field of view is wide enough to capture both your head and the desk in front of you. The desk view is warped and flipped with some pretty tricky distortion correction to make it appear rectilinear / flat, as if the camera were overhead instead of in front of it. You'll want to watch the session \"Bringing Continuity Camera to your app\" Already available! Thank you very much! Will definitely watch it. I'm trying to build an app for iPadOS to provide a portable video streaming setup, and wonder if there is a way to make a USB UVC camera appear in AVCaptureDevice.DiscoverySession ? I would have to say no since we don't support UVC camera on iPadOS Thank you ^^ FB10074848 for potential future enhancements :pray: Thanks, Tian! How to speed up session start time and prevent drop of first frames? For example Snapchat works very fast (almost immediately), pausing and restarting are invisible and have no glitches. Even system Camera, Instagram, Facebook, Apple's demo app spend some time to configure the session and draw the first frame. Creating the session with just the necessary outputs, e.g. only AVCaptureVideoDataOutput helps speed up the session startup. You can also try configuring everything upfront in the app user flow so the startup time is less noticeable to users. Is there a performance difference between wrapping any single output configuration into beginConfiguration and commitConfiguration and using only one begin-commit operation to batch outputs configurations? Batching all your configurations into one begin-commit is more efficient. beginConfiguration 's documentation also states: > You can nest beginConfiguration and commitConfiguration pairs, and the system applies the changes when you call the outermost commit. This may be helpful if you want to take care of different configuration aspects in different methods See: https://developer.apple.com/documentation/avfoundation/avcapturesession/1389174-beginconfiguration?language=objc Thanks Is it possible to efficiently write the AVDepthData from the cameras (front and back) using AVAssetWriter? There is no API support in AVAssetWriter for writing depth at this time, though it is supported in the QuickTime spec as an auxiliary video track. You can read up on it in the official QuickTime spec, and a good example of this spec in action is Cinematic Video captured in http://Camera.app|Camera.app . Thanks, is that the system Camera app on the iPhone 13 Pros? Yes, correct. Is it publicly available in the quicktime spec? I can\u2019t see the https://twitter.com/jankais3r/status/1442466943697489923?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1442466943697489923%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fnofilmschool.com%2Fdepth-map-new-iphone-cinematic-mode|identifiers listed from the cinematic video in the https://developer.apple.com/library/archive/documentation/QuickTime/QTFF/Metadata/Metadata.html#//apple_ref/doc/uid/TP40000939-CH1-SW37|spec here Is there a way to use Continuity Camera on iPadOS rather than macOS? Specifically, could the iPhone camera be used as the camera in an app like FaceTime or Zoom? Hi Brandon, Continuity Camera is only available on macOS currently, so iPad apps won't be able to see these new cameras. Thanks for the Q&A! I notice there may be a session on this tomorrow, but is there a way to continue running an AVCaptureSession in slide over and split view? :grin: Yes! In tomorrow's session we describe new API that you can use to allow your app to use the camera while multitasking There are 2 new properties on AVCaptureSession: isMultitaskingCameraAccessSupported and isMultitaskingCameraAccessEnabled The session refers to this article which you can read in advance of the session being released: https://developer.apple.com/documentation/avkit/accessing_the_camera_while_multitasking?language=objc This is awesome! Thanks so much for developing this and the detailed doc :grin: Do the updates for iPads with M1 to support custom USB drivers with DriverKit mean that I can use external cameras and other capture devices (like an Elgato UBS capture card, for example) with my application through AVFoundation? Currently only internal cameras on the iPad are supported in AVFoundation. Okay, I\u2019ve got FB9948623 filed as an enhancement request for this for tracking purposes. Did I understand this correctly? You will be able to capture high res stills while still seeing the live video feed? speaking of Continuity Camera, yes? That's correct. Stills can be taken while doing live video. Awesome! Can you please also add the depth info to the stream? And iPhone telemetry? (Orientation, position, velocity) Tell us more about this. You can already add your own telemetry as metadata in timed metadata tracks in QuickTime movies using AVCaptureMetadataInput. We do support streaming depth as AVDepthData objects in AVCaptureDepthDataOutput. We do not currently support writing depth to movies via asset writer or AVCaptureMovieFileOutput though. For a few releases now, I've been \"warming up\" my AVAssetWriter as soon as my app launches, then cancelling the writing session before actually starting to write. If I don't do this, I end up with black frames at the beginning of the AVAssetWriter. Is this the suggested step to take or is this a different issue I'm facing? Hrm... it would seem like this would happen if the movie also has audio that starts at an earlier time. Have you looked at the movie for edits. Thanks, <@U03HHA1DV9D>. In my scenario, that is not the case, as I am not recording audio. I'm usually recording very short clips (1-3 seconds), but can almost consistently replicate the issue, if I don't \"warm up\" the AVAssetWriter in advance. I can file a bug report on it if it's not a common/known topic! I haven't heard of this before. What exactly are you doing to warm up the assetwriter? (Before I forget, if you do file a bug report please include one of the clips.) I am instantiating an AVAssetWriter as soon as the app launches, starting a session at .zero , then adding a delay of 0.5 seconds, and then calling .cancelWriting() on the session. If I do that once, typically as soon as the app launches, I can negate the issue altogether. I got the suggestion from https://stackoverflow.com/questions/44135223/record-video-with-avassetwriter-first-frames-are-black|this post, though it feels like a workaround, for sure. The advice in that link sounds good. I think the confusion is around starting at zero but not actually writing with the first frame. Did you ever measure how long the period of black frames was? It's typically brief, though that's a great troubleshooting step to take. I can definitely measure the time; maybe it's consistent and I can use that to troubleshoot. Maybe I'm misunderstanding, but should I not be starting at .zero ? I had thought that was writing the timecode to start at 00:00:00, but perhaps I'm misunderstanding here. If you end up looking at the presentation times given by media samples read back from a movie file, you'll find that the timeline of every movie file starts at zero. The presentation times of the incoming samples are shifted. I'm not super-expert with AVAssetWriter, but I have the feeling that when you set the session to start at zero you are nullifying this normal adjustment. We are using the camera to take photos for photogrammetric reconstruction. What are some tips and best practices for getting the best results (in terms of configuring the camera and capture settings)? In this use case we want to use the camera as a light measuring instruments. So we do not want any tone mapping, and a predictable mapping from linear intensities (photon counts) to recorded pixel values, so that we can invert it and get back to linear intensities. Pro RAW seems potentially ideal(?), but is only supported on a small percentage of devices, and would be too large a format for us to send to servers for reconstruction. Hi! This is a deep question. Capturing Pro RAW (or Bayer RAW) is your best option to get access to non-tone mapped data, but as you mention the formats supporting these are large, and Pro RAW is not supported on all devices. I encourage you to try -[AVCaptureDevice globalToneMappingEnabled] to at least avoid local tone mapping. Thank you! But is that not just for global (non-local) tone mapping, judging by the name? I looked at the documentation and I think I understand. Turning this on disables local tonemapping. Yes, it will give you data that is globally tone mapped. Currently it\u2019s not possible capture linear data, or something that can be converted into linear data. But if I use an sRGB color space, could I not then use the known sRGB formula to get back to linear intensities? Are you interesting in streaming data? Or are there other unknown global tone mapping operators happening? No, I am interested in discrete images taken during an ARKit session. Yes, there\u2019s a scene dependent global tone mapping applied And no way to get that mapping I assume? Correct. :pensive: It would be good if you could file a feedback request with your use case. Will do. Thank you for the insights. Where available Pro RAW should be exactly what I want, right? (Except for file size) This is a common request. Please drop my name in the feedback request. Yes. Pro RAW should give you want you want. I think we actually had a nice lab conversation last year. Nice to meet you again! You too! Quite a different topic last year IIRC <@U03HR90DADU> Yes, last year we talked about minimizing camera shake blur and latency (after the shutter press) while taking photos for a computer vision application. How can I get the Mac webcam in a Catalyst application? Using the iOS code, the camera looks cropped. This is probably because your app is declaring that it supports the portait mode, so we have to crop in on the camera frames to make them fill your app's preview. If you support landscape orientation, we should give you an uncropped view. Will look in to that. Thanks (: great work!! I'm using .photo sessionPreset to capture fullRes photos. But at the same time I want to record video via AVAssetWriter in 1920x1080 (without any session's reconfiguration). How can I configure AVCaptureVideoDataOutput to get from the delegate sampleBuffers with correct sizes (1920x1080)? I know about automaticallyConfiguresOutputBufferDimensions and deliversPreviewSizedOutputBuffers properties. But their combinations produces 1284x1712 or 3024x4032 outputs. In tomorrow's session \"Discover advancements in iOS camera capture\" we go over how to do this in iOS 16. You can set the width and height in the videoSettings property on an AVCaptureVideoDataOutput to receive custom dimensions. See: https://developer.apple.com/videos/play/wwdc2022/110429 You can specify the width and height width kCVPixelBufferWidthKey and kCVPixelBufferHeightKey , respectively <@U03HHA1D44F> Am I correct that kCVPixelBufferWidthKey and kCVPixelBufferHeightKey don't work on iOS < 16? correct Thanks However, it's important to note that you cannot receive 1920x1080 when using the photo session preset as that resolution is 4:3, often 4032x3024. When specifying the width and height in the AVCaptureVideoDataOutput's videoSettings. The aspect ratio must match the aspect ratio of the source camera's format So you could request 1920x1440, which is 4:3, when using the photo session preset T\u00f4 use my camera app on Sideview on iPad, I need special entitlements? Anyone can get it or is a bit restrict? Before iOS 16, yes, there's an entitlement granting process that requires filling out a form. In iOS 16 and later, there's API to opt in for multitasking support (sideview/slideover). Nice. Thanks!! There's a session dropping tomorrow that you'll want to watch called \"Discover advancements in iOS camera capture: Depth, focus, and multitasking\" Going to check it. I found that I have to access inputNode of AVAudioEngine before starting the engine to be able to use it. This means the permission dialog will be presented before the engine is running and also the recording indication in the status bar is present. Is there any way to start using the input node later, when the engine is already running. I found I get mostly sample rate mismatch errors if I don't access the input node beforehand. I was not able to find a combination, any tips on how to solve this? This is a great question to ask over in the <#C03H9J5AW4U|>. We don't have any AVAudioEngine experts in the room right now. iPhone can be free from the Mac or need to be \u201cmounted\u201d on the top? Can I walk with the Phone for example? The specific placement, orientation of the phone is required for automatic switching. But yes, you can use Continuity Camera when walking with your phone. Thanks Karen FYI: We actually show a demo of exactly that in the What's new in Create ML session which dropped today. If your phone is not in a stand, it will still be offered in the list of cameras, but it won't be designated at the systemPreferredCamera until you place it in a stand. By the way, it doesn't need to be clipped to the top of a macbook. It can be in any stand of your choosing, as long as it's landscape, stationary, screen off, and close by to the mac. Go it. The stand needs to have some chip? Or only checks for proximity and orientation? Nice. Great to know We check phone's proximity and orientation. No chip in stands :slightly_smiling_face: <@U03DJTBMHFF>, Create ML demonstrated Wombat? I don't see Wombat in this video https://developer.apple.com/videos/play/wwdc2022/110332/ Is it possible to connect multiple iOS devices to the same mac, and use continuity camera to capture multiple video streams simultaneously? It doesn't support streaming multiple continuity cameras at the same time right now. But you can have multiple iOS devices around and switch between devices. would love the ability to stream multiple devices at once, we're doing this at https://detail.co%F0%9F%99%82|https://detail.co :slightly_smiling_face: Neat! Will Managed AppleIDs work with this feature? Despite Continuity in full NOT being a feature supported with MAIDs? Seems there is a slight lip sync issue when Karen is using Continuity Camera, so which mic is used from iPhone or from the Mac? Is it configurable through API? The audio in the session is from filming. You can use either built-in mic or Continuity Camera mic with the Continuity Camera. The Continuity mic can be found with both AVAudioSession and AVCapture APIs. And the A/V sync is good! :slightly_smiling_face: Whether you use a built in mic or the iPhone's microphone, we take care of it for you. I see, thanks :slightly_smiling_face: Continuity features (so far at least) have not been supported for Managed AppleIDs - does the Camera Continuity adhere to the same restriction? Yes, it has the same restrictions as other continuity features. :disappointed: Thank you Do continuity camera sample buffer timestamps account for network delay? If you use a continuity camera and a macOS connected USB microphone, will the timing between them match up? Yes, it's all covered! The buffer timestamps from continuity camera will reflect capture time on the phone converted to mac's timeline. Great, thanks Karen! Is there any lossy compression/encoding applied to the iOS camera stream that would result in lower quality recordings compared to recording on-iOS device? Technically there's a lossy compression applied during transport. But it's sufficiently high bit rate that we think you won't notice quality loss. Does Continuity Camera allow access to the LiDaR scanner as well? Can it be used to support ARKit effects on Mac? Thanks! Currently we only support the wide angle RGB camera (the one with the best quality!). We do internally switch to the Ultrawide camera when Center Stage is enabled. And DeskView also uses the Ultrawide camera. There is currently no access to the telephoto lens (if present) or LiDAR camera (if present). Ah, alrighty, understood! Thanks for the quick response Brad. I\u2019m so excited for this feature, it was something that I didn\u2019t even realize how much I needed. If you ever do get telephoto/LiDAR access going that would be huge in my opinion. :wink: Thanks so much to yourself and team! Can we leverage the increase-memory-limit entitlement within an image & video processing app? Can we give Core Image access to more memory for processing and caching? We are actively working on this. Feel free to also file an enhancement request if you need a specific need addressed. Is it possible to write a CoreImage filter for compute, rather than render? I'd like to perform compute operations on the CVPixelBuffer of a depth map, which I can represent as a CIImage. Hi, CoreImage does use Metal compute pipeline states to perform the chained CIKernel operations you apply to the image. When you encode renders with CI, it dispatches compute commands instead of render commands. When I have tried this my values get mutated between what is returned in my metal code and what is rendered. Are there settings we need to change in the CIContext to prevent things like premultiplied alpha, and any other settings that might change / clamp / remap the outputs? These might be helpful https://developer.apple.com/documentation/coreimage/cirenderdestination/2875443-alphamode?language=objc https://developer.apple.com/documentation/coreimage/cirenderdestination/2875451-clamped?language=objc Also if you want, you can tell CI to disable color management using the the context option kCIContextWorkingColorSpace : [NSNull null] but use that with caution. In addition to what Baljit mentioned, mutated pixel values could be due to color management. CVPixelBuffers often are tagged with color space metadata and CI by default would color manage the input data to the CIContext's 'working space' and then convert to the output destination's color space when the image is rendered. This is on by default since it is usually desirable. But you do have the option to disable color management if you choose to. In the case of the depth map I think you'd want the data to remain linear. So either you can turn off color management as David suggested above, or set the render destination's color space to match the input. Thank you all :slightly_smiling_face: I will investigate these options Is there much performance overhead in rendering this (e.g. to bytes) from a CIContext to inspect the values, vs creating a metal buffer and inspecting them there? And does this change if I am using other CIFilter s such as masks I don't think we offer an API to render to a MTLBuffer. If you're comparing rendering to bytes vs rendering to texture, I think the difference should be fairly minimal on devices with unified memory architecture. Synchronization between the GPU memory and system memory is required in both cases so I think it should be fairly close. Thanks There is a render to metal texture https://developer.apple.com/documentation/coreimage/cicontext/1438026-render|function . I wondered if it would be quicker than to bytes We have CIFilter.personSegmentation() . Is there a way to generate the other segmentation mattes (skin, hair) from AVPhotoCapture on demand using CIImages? Please file an enhancement request for that. That said some Jpeg/Heic/ProRaw images already contain \"aux images\" for other segmentations. You can get at these either though ImageIO or CoreImage APIs Thanks, I will do. I was hoping to be able to apply this to video so can\u2019t access those file types Does the person segmentation filter return the same results as a VNGeneratePersonSegmentationRequest? I just learned they added a filter as well It seems to do from my experiments The quality parameters give the same sized buffers as results <@U03JG9JF529> Interesting, I\u2019ll have to give that a try! Is there any chance of seeing a CIFilter for film grain, that\u2019s more natural looking than the CIRandonGenerator? Please file an enhancement request. That would be great to add. Will do! You may also want to try the CIDither filter. I\u2019ll have a look at combining that with the random noise maybe, though I'm hoping to filter a live camera feed, hopefully it'll be performant enough. (Also CIDither doesn't seem to be listed on the Core Image Filter Reference page) I'm trying to blur the background of a photo, similar to what Portrait Mode does, using a matte of the person. What's a good approach to filling the area inside the subject so that there's no halo around the person when blurring the image? The simplest approach is to: \u2022 make a CIImage from the image where the foreground pixels are made transparent \u2022 blur that image \u2022 device that image by its alpha channel \u2022 use SourceOverCompositing to put the foreground image over the above result. <@U03HB0AV6S3> thanks for the reply! By \u201cdevice that image by its alpha channel\u201d, do you mean making the transparent pixels opaque? That sounds like a good approach, I\u2019ll give that a try I meant to type \"devide that image by its alpha channel\" Oh of course! Thanks! WIth a custom CIKernel like this: float4 alpha_denorm (sample_t i) { if (i.a &gt; 0.001) return float4(i.rgb / i.a, 1.0); return i; } Since CIFilters aren't thread safe, what would be the recommended way to structure a render pipeline if I want to use Swift concurrency? Especially when one wants to initialize the filter once and reuse it for multiple renders. It is safe to render CIImage output of CIFilter on mulitple threads (and will be concurrent if you use different instances of CIContext) Thanks! Yes but initializing a filter from a raw file takes quite a while, so I would like to reuse the first instance when adjusting. I assume the filter will always have to be on the same thread? If I recall correctly, the CIRawFilter implements the copy method. That should be faster. Yes I tried the copy method but renders became unfortunately corrupt when using the copied version. We would appreciate a feedback/bug report for that I will file a report. Good to know that the copy method should work. Would love to further discuss different options during a lab session. Which CI filters would you recommend running in linear space? Most CIFilters are designed to work in linear space. The notable exceptions are the the Blend filters and Compositing filters. These can work in linear space by may artists are more familiar using them in a perceptual colorspace. (eg. CIScreenBlendMode and others) Thanks! Would the recommendation be to run most filters in the linearSpaceFilter with a RAW file? And change the context working space to linear when working with non-raws? I would (with some exceptions) use linear for raw and non raw image. Feel free to schedule a Lab to discuss further. I wasn't aware that I could use the linearSpaceFilter for non raw's. Will try that! A lab has already been requested :slightly_smiling_face: Can I use CIAreaAverage to get the average color except transparent areas? Yes the CIAreaAverage filter is essentially an alpha-weighted average. Oh, alpha-weighted average! Thanks! Is there a good approach to color matching two photos with core image? Such as when superimposing a person on a different background, so the photo's hue, saturation, maybe contrast and brightness, somewhat match the new background You might consider using the Reduction filters such as CIAreaAverage or CIAreaHistogram to collect some stats on the two images. You can then use this stats to make one better match the other. Is the LiDAR-based depth data capable of being embedded in a HEIC, similar to how disparity-based depth data is? Hi Brandon, yes. Depth generated with the AVCaptureDeviceTypeBuiltInLiDARDepthCamera can be embedded in HEIC just like depth generated from other cameras. Is it possible to get the raw point cloud representation from the LiDAR camera through AVFoundation, rather than an ARKit/Metal-based session? Unfortunately no. The closest thing to it would be to turn off depthDataFiltering on the AVCaptureDepthDataOutput and then get depth data objects which will contain the full field of view of the wide camera / LiDAR camera, but will have holes (NaN pixels) where LiDAR spots were unavailable. Thanks, <@U03HXTBNYBC>! There's something very fun and magical about capturing point clouds lol, and am always curious of the technologies that allow for this. Appreciate your help and hard work on this new technology! It's point clouds all the way down. :nerd_face: To <@U03J20E7UBV>\u2019s last comment, it really does help to understand that AVFoundation's LiDARCam is really a marriage between time of flight technology and machine learning \"hallucinated\" depth \u2014 guided by the LiDAR spots. It just wouldn't be that useful with the LiDAR hardware alone. Together, they really advance the state of the art for depth data acquisition on iOS products. They give you great video quality, high resolution depth, and real-world scale, all in one package. They're also faced away from you, unlike the TrueDepth camera, so now you've got real-world scale options in both directions. Thanks, <@U03HXTBNYBC>! It's always been super cool to be able to get depth data from our devices, but having that added accuracy and high-resolution depth, alongside now higher capture quality, is really huge! The multitasking camera features seem geared towards video chat apps. Are you expecting pro camera apps to adopt this, and will the native camera app adopt this behaviour? Multitasking Camera Access is often used by video chat apps. But it can be used by other apps. The built in Camera app requires itself to be full screen. But you can have a Slide Over app on top of it to multitask. Like opening Notes on top See for example this screenshot from the video. This is Apple's built in Camera app Got it! Thank you. Hi! I've watched the video to live text interaction and i was wondering, if the quick actions can be customized to add own functions? Excellent question. The quick actions are all driven by built-in data detectors. There is no opportunity for extending that or customizing those at this time. That said, we'd love to hear what kind of customizations you were hoping to provide. Thanks for the answer. It would be great to just add functions like \u201cApply for the textfield\u201d or \u201cGenerate new case/entry\u201d and the recognized text can just be added to own textfields or used as titles. Will we ever see a public API for using the hardware volume buttons to capture photos? The current.. workarounds are less than ideal :unamused: Magic eightball says: \"Outlook uncertain\". Thanks for the feedback, Ben. We're asked about that a lot. Have you filed a request through Feedback Assistant? If I have it wasn't recently! But I brought it up in a lab yesterday, so I'll submit one now to hopefully give a little boost in priority There\u2019s an API to get tagged faces out of a photo (at least the region within the photo where there\u2019s a face, not sure about the persons name..). I thought there might be a way to manually tag faces that the auto Apple tagging hadn\u2019t picked up, but I couldn\u2019t find a way to do this, is it possible? Something you\u2019d consider adding to either the api or the photos app if not? Using the ImageIO APIs <ImageIO/CGImageProperties.h> you can load up an image on disk and inspect its metadata. Detected faces are held within the kCGImagePropertyExifAuxDictionary 's @\"Regions\". Is that what you had in mind by \"tagging\", or something else? That's probably where I found the data, but it didn't seem possible to add to it and update it so the photos app would show those additions? This is about AVCaptureDevice. Will there be a public API to check on the lens power of the camera? This would be exceedingly useful if the device is a telephoto camera (that is, is it 2.0x optical power, or 2.5x or 3.0x, or some future power?) This would be great for any camera app where the user manually selects the lens to use (no virtual device) as well as set manual shutter speed & ISO. Thanks! Seconded! This is definitely more awkward than it should be Hi Eric, Ben. could you clarify what you mean with \"lens power\"? Are you referring to the focal length / native zoom? Hi Ben. What I mean is, say I don't know if the user has the iPhone 12 Pro that has the 2.5x telephoto, or the iPhone 13 Pro that has the 3.0x tele. It would be wonderful to look at one get variable to see if it's a 2.5x or 3.0x, instead of doing the cumbersome task of determining that the user does have say an iPhone 12 The -[AVCaptureDevice virtualDeviceSwitchOverVideoZoomFactors] property tells the relative zoom factors between the cameras. So if you're looking to build a UI around these switch-over zoom factors, then you'll want to use that. In order to know if the telephoto camera is 2\u00d7, 2.5\u00d7 or 3\u00d7 (and display it to the user), you have to get the video field of view, then work out how that angle translates to a focal length, and how that focal length gets generalised I did check that variable and it reads empty brackets if there is no virtual device. I'm having the user manually select whether to use the wide lens or the telephoto, not rely on a virtual camera to do that. This is how I handle it in my app You can programmatically pick the virtual camera device and then query that property. It's a static property on the device in question, so it does not change depending on whether the device is connected to a session or not. (I feel like we may be trying to solve slightly different problems, apologies for hijacking!) Also note that the zoom factor of the widest FOV constituent device is always defined as 1x. If you want to build something similar to http://Camera.app|Camera.app where the wide-angle camera is defined as 1x then you'll want to divide the zoom factors by the wide-angle camera's switch-over zoom factor. One more note, on iPhone X, the telephoto camera's switch-over zoom factor is actually 1.8x (despite http://Camera.app|Camera.app showing 2x). We didn't want to confuse users with too much technical detail, so we use 2x in the UI. Eric, Ben, did this help answer your question(s)? Querying the virtual camera is probably cleaner than what I'm doing, but it would still be nicer to just have it as a property on each camera device. In my case I'm using the individual cameras to enable raw, manual WB, etc Hi Jozef, thanks for the answers. But in my app users not only have a zoom control, they also have push buttons to manually select what lens to use. So for the iPhone 11 Pro I present three buttons: 0.5x, 1x, and 2.0x. For the iPhone 13 Pro I present these: 0.5x, 1x, and 3.0x. Right now I do a roundabout way to know whether to call the button that connects to the telephoto camera a 2.0x or 3.0x. I believe the API I mentioned solves both of these problems. Just query for it on the Triple Camera device and you have all the zoom factors you need. Ben, the problem is that these zoom factors are relative to each other, so they only make sense in the context of a virtual camera. Note that you get different numbers from the Triple Camera than from the Dual Camera because the baseline is different. On the Triple Camera the wide-angle camera is typically 2x while on the Dual Camera it's 1x. This is why this is a property on the virtual cameras and not on the physical ones. Hi Josef, the key thing is that property is just for the virtual cameras. I'm using only the physical cameras, and I just ask for a get variable that is valid only for the physical devices. I understand, but you can still use the virtual camera's property for this. I propose that such a variable would read nothing if a virtual device is used to avoid confusing those who use the virtual camera. I'm sorry, I don't quite understand what you mean. Could you clarify? I suppose I can load the virtual camera, read the property, then discard the device and load the physical device.... Yes, that's exactly what I was proposing. And for what it's worth, all AVCaptureDevices get instantiated at the same time. So there's really no such thing as loading/discarding them. I have my project open, I'll give it a try, thanks! Sure thing! The issue is not knowing what the zoom factors are relative to each, but that the user understands 0.5x, 1x, 3x as shorthand to refer to the physical cameras because they're used in Apple\u2019s marketing in this way If you want to use the wide-angle camera as the 1x, then you can just query this property on the Dual Camera. Either that, or you can query it on the Triple Camera and divide all of them by the wide-angle camera's switch-over zoom factor. Both options should yield the same result. The switch-over zoom factors are always relative to the widest FOV camera in the virtual device. So for the Triple Camera the ultra-wide camera is the base (1x), while for the Dual Camera the wide-angle one is the base (1x). Sounds like, <@U03HVE4BEBY>, you should file a feedbackassistant request for us to provide an API that maps the actual zoom factors to \"marketing friendly\" zoom factors that match Apple's built in camera app. I think using the virtual cam zoom factors is definitely cleaner than my approach of using the field of view, so I'll switch to that as you've suggested <@U03HR8XVATG> but also yes I'll file a feedback to be able to check for a quick and basic \"label\" <@U03HXTBNYBC> Thank you both! Hi Brad, it sounds like the solution I had in mind in starting this thread. You expressed it perfectly, I'll file a feedback myself too, thanks! And Ben, thanks for your input! I appreciate all of you talking this through and answering my questions w/o me having to ask :smile: My thanks to the Camera team for a couple of great digital lounges this week! I just wanted to provide the team with the number of a new Feedback Assistant request, FB10137353, that addresses what was discussed in this long thread. It\u2019s the proposed API to map a \u2018marketing friendly\u2019 zoom factor for a physical AVCaptureDevice. It has the solution that was well voiced by Brad Ford in the wrap up to this discussion. Again, many thanks! How would you recommend to detect a blurry image? The image can be of any resolution allowed by a camera, or may be imported into Photos. Live photos and videos are excluded. Hi, Kiril. I think that Core ML may be your friend here. There are many models floating around in research circles like this one: https://arxiv.org/abs/2010.07936 Thanks! Using coremltools, you could convert the model to Core ML format and then use it inside the Vision framework to get your predictions of image quality. If you want to dive into this a bit further with Core ML Engineering, there are lab appointments available tomorrow. Got it Thank you very much If you need get least blurry image from a video / sequence of images, you can compute laplacian (MPSImageLaplacian), and than compute maximum using MPSImageStatisticsMinAndMax. Then, image with the smallest resulting value should be least blurry. You can try to get a treshold, below which you can consider image blurry, but it may fail in some cases, e.g. when you take a picture of a wall with no obvious edges. You could also do this in Core Image by taking a CIAreaHistogram of a CIGaborGradients . Or perhaps CIAreaAverage of CIGaborGradients which would be faster. I would recommend using Gabor over Laplacian because even a blurry image will contain noise. Gabor (a 5x5 convolution) is less affected by noise than Laplacian (a 3x3 convolution). In fact you my want to consider a 7x7 Gabor. in the Creating Camera Extensions session, you said that CMIO Extensions can be video sinks, but these sinks don't have an AVCapture interface, we have to use the C APIs. Which APIs are these? I cannot find any examples of using a DAL plug-in this way. We have implemented DAL plug-ins as virtual cameras, but we use them through their AVCapture interfaces. From my app, I can find the CMIObject corresponding to my Camera Extension, (it is of class 'aplg', isExtension is true, and it has the correct bundle ID). But how do I go from that to making an API call to feed frames from my app to the extension? Hi Stuart. The C APIs to which I referred are the CoreMediaIO interfaces called by apps (rather than extension / plug in developers). CMIOHardware.h, CMIOHardwareDevice.h, CMIOHardwareStream.h CMIOHardwareSystem introduces the concept of a \"System\" object ( kCMIOObjectSystemObject ) that can be queried for its devices ( kCMIOHardwarePropertyDevices ). Once you've got the list of devices, you can query them for various properties, such as unique ID, or streams. You can, for instance, see if it has streams on its output scope (output streams instead of input streams). You push sample buffers to output streams, and these are outputted to a video deck or some other external piece of hardware that accepts a stream of video. It may help your mental model, <@U03JDS776JH>, to know that the AVCapture APIs on macOS are clients of the CMIOHardware APIs. They wrap the functionality in a more modern, friendly, swift/obj-c compatible way, and they hide some of the complexity one takes on when dealing with the CoreMediaIO APIs directly. To be clear, you don't need to search for your DAL plugin (the plugin object), unless there's a property that you want to set on the plug-in itself. For instance, if you had some property that's global to your entire extension, and all devices it vends. Typically, you'd just ask the system object for the list of CMIODeviceIDs, and then iterate through them, searching for the characteristics that fit your criteria. ok. So I query the system object for its kCMIOHardwarePropertyDevices, query those objects for their kCMIODevicePropertyStreams, querty those objects for their kCMIOStreamPropertyDirection etc. But I\u2019m unclear what the API is to push buffers to the stream. Also, \u201cinput\u201d and \u201coutput\u201d are from the point of view of the extension itself, right? AVCaptureDevice APIs hide CMIODeviceIDs that only contain output streams. You don't need to query them for their direction. That property is there for legacy stuff like DV cameras, which can be input or output streams, depending on what mode you've set the camera to be in (playback mode or recording mode). Just search for streams on the output scope rather than the input scope. That will give you the exclusively sink streams. that\u2019s good to know (output==sink)! But still, what\u2019s the API to push a frame to CMIOObject? You push frames to a CMIOStream. with CMIOStreamCopyBufferQueue to get the queue, then I push somethings onto that queue? And why would I want queueAlteredProc? Call CMIOStreamCopyBufferQueue to get its simple queue. You can add frames to that queue when you're ready. (If it's an input stream, you'd listen for when the queue was altered and dequeue the new frame or frames). There are also some other handy stream properties: kCMIOStreamPropertyOutputBufferUnderrunCount, kCMIOStreamPropertyOutputBufferRepeatCount, kCMIOStreamPropertyOutputBufferQueueSize, kCMIOStreamPropertyOutputBuffersRequiredForStartup, kCMIOStreamPropertyOutputBuffersNeededForThrottledPlayback, and kCMIOStreamPropertyFirstOutputPresentationTimeStamp. Also: kCMIOStreamPropertyScheduledOutputNotificationProc thank you this all extremely helpful. I have to run to a lab now but I posted another question on a slightly different topic. I hope to see you at a lab tomorrow If we want to do something like OBS where we have an application that renders video frames and passes them to a DAL plugin for people to select as a virtual webcam, is the recommended approach to create both a source and sink stream, and route frames in the extension from the sink to the source. Then in the app send frames to the sink? That's one approach. In the CMIOExtensions video (available now! and....watch party in 10 minutes), I describe a different technique where you can run physical streams from other cameras right within your CMIOExtension. We call this case \"creative camera\". There are currently 3 frameworks for image processing: Accelerate, CoreImage filters, and Metal. For example, if we take a \"black & white conversion\" of the image: we can use Accelerate to accomplish it, a CIFilter (e.g. CIColorMonochrome ), or create a Metal shader. What are your recommendations for using each framework? E.g. in which cases each framework is preferable to others, or should be avoided? And is it OK mixing them, or should that (as a general rule) be avoided? As you say, there are several ways to achieve the same \"monochrome\" result. The best will likely depend on what else you want to do and other factors. For example if you want to combine monochrome with other effects you want to pick a strategy that that avoids switching back and forth. Switching say between CPU and GPU will introduce stalls and increase memory. Another thing to consider is if you need to deal with imaged that are large sizes or with HDR color spaces. In that case a higher lave framework will have that built in. I guess the main reason for me to look for CI alternatives was that conversion from CIImage (with several filters) to CGImage or to JPEG representation of the image took a very long time (5-10 sec) and a lot of memory (for larger images especially). Is that generally a point where you'd say: yes, in those cases switch to Accelerate or Metal, or could it be related to a particular (possibly incorrect) use of CIFilters? You might want to try going directly from CIImage to JPEG using API like JPEGRepresentationOfImage:colorSpace:options: . A time of 5 to 10 seconds seems very slow. How big is the image? raw image taken on iphone 10 for example For a RAW image using CoreImage is the fastest/easiest way to produce a demosaiced and denoised result. This is non trivial but 5 to 10 seconds seems slow to me. Here are some suggestions: \u2022 use the CIRAWFilter API to have more control (for example you can set .scaleFactor if you just want a half sized image. \u2022 create the CIContext with the option kCIContextCacheIntermediates set to @NO to minimize memory usage. Great suggestions, thanks. I wasn't using either - hence probably the timing and memory. FYI: there's another question/discussion about CIRAWFilter in this channel today. Is there a way to get a transform from original frame to a stabilized one? We have custom AR content overlaying video, and wanted to preview non-stabilized video for less latency, and write cinematic stabilization to a disk. However, as we need to render AR stuff over video, and we don't want to run pipeline twice, and with transforms we could just transform what we rendered once to a stabilized frame. This is an interesting one! There's currently no API to perform video stabilization but only expose stabilization transform without applying it onto frames. Could you file a feature request for this? So there is a way to expose stabilization transform? (Without applying stabilization) Sorry for asking, but could you direct me where should I file this request? Hi Viacheslav, exposing stabilization transform is currently not supported. You can file feature request here http://feedbackassistant.apple.com|feedbackassistant.apple.com . <@U03HY66772A> are you familiar with the new feature in iOS 16 which allows you to run multiple video data outputs? You can run one with no stabilization for preview, and run a separate one with stabilization for writing to disk. This would require running the pipeline twice, but you could potentially run a lower quality algorithm for the realtime feed, and a higher quality version of it for the one being written to disk. Sadly, we are running on the edge of performance targets, and rendering twise would mean waiting for video to process after the fact for some time for our users But you definetly could run two connections with different stabilization options before? At least for preview and for writing on disk. (I think that\u2019s how Apple Camera app works) Question about CIRAWFilter - does scaleFactor do the same thing as in CIFilter with RAW options? let ciImage = CIImage(data: self.imageData)! let ciFilter = CIFilter(imageData: self.imageData, options: [ .scaleFactor: 0.25 ])! let ciRawFilter = CIRAWFilter(imageData: self.imageData, identifierHint: \"com.adobe.raw-image\")! ciRawFilter.scaleFactor = 0.25 print(\"CIImage\", ciImage.extent) print(\"CIFilter\", ciFilter.outputImage?.extent) print(\"CIRAWFilter\", ciRawFilter.outputImage?.extent) Prints CIImage (0.0, 0.0, 5952.0, 3968.0) CIFilter Optional((0.0, 0.0, 1488.0, 992.0)) CIRAWFilter Optional((0.0, 0.0, 5952.0, 3968.0)) See that the CIFilter version scaled the extent but the CIRAWFilter did not. More importantly, is scaling here an effective way to improve performance of RAW editing? Any other options or things to keep in mind for performance specifically? Big yes. Using the .scaleFactor on the CIRAWFilter class will give a big benefit in performance. It reduces the amount of memory needed as well and reduces the amount of noise reduction that is needed. Asking the filter for .nativeSize will always give the full size of the asset. Asking the filter for .outputImage.extent should give the scaled size (assuming that you set .scaleFactor before) Please file a bug report on the ciRawFilter.scaleFactor / ciRawFilter.outputImage?.extent issue. That should give you the smaller extent. Thanks <@U03HB0AV6S3>! Will do. You might try toggling one of the other properties to see if that helps. Is there any workaround to scale using CIRAWFilter? Or have to fall back to CIFilter for now? Try setting .orientation to a different value and then back again? That property also alters ciRawFilter.outputImage?.extent No luck. Setting ciRawFilter.orientation = .left swapped the dimensions but still at full size. I tried both orders of orientation, scaleFactor If we find a workaround we will note it in your bug report. much appreciated! I\u2019ll file that asap I was told the resolution of LiDAR depth data from ARKit had been for accuracy down to around the size of a table leg, or accuracy within a few inches. With AVFoundation having greater depth resolution, is there a kind of rule-of-thumb in terms of accuracy down to set distance, or down an object size, like a tooth? Hi Rob, unfortunately we don\u2019t have a hard and fast rule for how small the objects can be at a given distance, but in general the AVFoundation LiDAR Depth Camera can detect finer details \u2014 for example the edge of table will be straighter/sharper than ARKit due to the increase in pixel resolution. A tooth at 5m from the camera is probably too small. That makes sense, thank you. Just so I\u2019m understanding, that\u2019s 5 meters or 5 millimeters? That's in meters. You're welcome! Hello! I understand this one is a bit tricky, but what does the magic 8-ball say about .builtInTripleCamera one day supporting ExposureMode.custom and manual focus? :innocent: Hah! The problem with supporting custom exposure modes is that each constituent camera in the virtual camera has a different min and max iso and shutter speed. Apps written for the past ~10 years are not prepared for min and max ranges to suddenly jump or shift. We would certainly crash a lot of apps if we did this. Also, zoom level is not the only factor that might cause the virtual camera to switch from one constituent camera to another. For instance, on the iPhone 13 Pro, the ultrawide lens is preferred up close because it has such a small minimum focus distance. So, we'd need to figure out a way to have a lot of properties jump \u2014 and jump atomically \u2014 when the constituent camera shifts. That's the reason custom exposure isn't supported on the triple camera. Appreciate the detailed answer! Totally get it, logistically its messy. My immediate thought is to expose only common ranges supported across all lenses. As the developer of ProShot I'd gladly take that range hit, and then find my own creative way to visualize it to users in the UI :slightly_smiling_face: At any rate, I really appreciate all the amazing work that's been going into these APIs, so much good stuff to work with! Thank you all I've noticed that new version of my Camera Extension on macOS 12.3 aren't actually used until I restart, although 'systemextensionsctl list' tells me it is loaded and the old version is unloaded. Is this intended behavior? <@U03JDS776JH>, in the sample project I built in the CMIO Extensions video (viewing party in half an hour!), the extension is loaded and usable immediately. No need to reboot. Currently, uninstalling it does not take effect right away \u2014 you have to reboot before the extension is unloaded for all apps, and systemextensionsctl tells you that. I verified the above behaviors on macOS 12.3, so I'm not sure why you'd be seeing a difference. Perhaps file a bug at feedbackassistant. do changes to the CMIOExtension take effect before rebooting? I tried a few weeks ago and found that uninstalling, reinstalling, and rebooting sometimes wasn't enough to update the extension That should do it every time, Finn. (uninstall, reboot, reinstall). If you want to be completely neurotic, you could bump the version number of your extension. You need to make sure that your installer code is actually telling the system to update or replace the previous one (if you're installing over an old one). it might be a caching issue. I bump the build number and change the name of the virtual device every time I make a change. Photo Booth will display the name of the build 84 after I\u2019ve succesfully activated build 85, I only see build 85 after a restart. This is with SIP turned off and the hosting app not in /Applications. Can we download the sample project? Is it possible to use camera directly in app extensions, especially in the keyboard extension? Or at least a Live text, but through our own UI? An iOS NSExtension? The only iOS NSExtension that supports camera access are iMessage Extensions Can confirm iMessage extensions work with camera with one important bug with the plist permission entries (I think was fixed last year). Which bug is that? Involving the NSCameraUsageDescription entry in the Info.plist? <@U03HHA1D44F> But should it be possible to use Live Text then, without directly getting access to the Camera? P.S. for now, I constantly receive the following error: Assertion failure in -[UIKeyboardCameraSession _keyboardCameraPreparationDidComplete] Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'Keyboard Camera is being used without remote keyboards enabled What API do you call to get Live Text in the keyboard? I am not sure how the Keyboard Camera is implemented but if it tries to use the camera within your process, that makes sense why you are seeing that error <@U03HHA1D44F> Live Text API has only two requirements to be called: 1. Have a UIResponder object 2. Have a UIKeyInput object And I was trying to make my UIInputViewController as both of them while returning UITextDocumentProxy properties to satisfy the UIKeyInput protocol. It looked pretty valid from the design view. And the code I\u2019ve been trying to run is close to the similar: class KeyboardViewController: UIInputViewController { override func viewDidLoad() { super.viewDidLoad() liveTextStraightButton.addAction( UIAction.captureTextFromCamera( responder: self, identifier: .paste ), for: .primaryActionTriggered ) } } extension KeyboardViewController: UIKeyInput { var hasText: Bool { textDocumentProxy.hasText } func insertText(_ text: String) { textDocumentProxy.insertText(text) } func deleteBackward() { textDocumentProxy.deleteBackward() } } <@U03HHA1D44F> hey again, Couldn\u2019t get to the lab today regarding this question. Would be grateful for some attention to the feedback ticket regarding that issue. It has a test project attached, which helps reproduce the issue: > FB10022377 Live Text: UIAction.captureTextFromCamera not working in keyboard extension Hi <@U03JER2C7MX>, I've brought FB10022377 to the attention of a colleague who is more familiar with this API to take a look. Thank you for including the test project in the radar! Wow, those are fantastic news, thank you! :scream::green_heart: We've shipped a \"modern\" Core Media IO system extension in our app. However, we never could figure out how to connect to an XPC service defined in the extension's CMIOExtensionMachServiceName \u2014 and a DTS ticket didn't get us far either! Is it intended that we can communicate with our extension via the XPC using the mach service? If not, what are your recommended methods of communicating with the extension when we're delivering frames over that connection? CMIOExtensions work a little differently from Endpoint or Network extensions. CMIOExtensions are run a a role user in a background process. That's why you're not able to find them with the named mach service. Keep watching. I describe a method to talk to your extension through custom properties. <@U03HXTBNYBC> Thanks for the reply. Would custom properties be appropriate for delivering frame buffers in terms of latency/etc? Ideally we\u2019d use the old technique of delivering IOSurface buffers via IPC. No, custom properties would be for smaller state, such as strings or NSData. For frames, you should use IOSurface backed buffers. Is it possible to deliver such buffers to our CMIO system extension from our parent app? Is the idea here that your app is capturing or generating real-time video and you want to get that to other camera-sourcing apps? Yes. One way to do this is to present 2 devices in your extension. One that presents an output or sink stream, and one that presents an input or source stream. You push frames to the sink stream device, and then in the extension, re-publish it through the camera. <@U03HHA1DV9D> Yup! Our app connects to DSLR/mirrorless cameras over network or USB and lets them be used as webcams even if they\u2019re not UVC devices (which most aren\u2019t). For various reasons, it makes sense for that connection and processing to be going on in the parent app. <@U03HXTBNYBC> Ok great, thanks for the idea. We have a Feedback open (FB10026784) assuming the XPC thing is a bug \u2014 perhaps that should be changed to a documentation request clarifying what the mach service key is for. As of right now, the mach service name is required so that CMIO's registerassistantservice recognizes the extension as a CMIO extension. But it's not currently used as a named /known XPC service that an app connects to. Thanks for the FB. Are audio CMIOExtensions nearly identical to the sample video one, where I would just send audio CMSampleBuffers to the CMIOExtensionStream? We do not recommend shipping audio drivers as CMIOExtensions. Most apps won't work with them. Stick to writing an Audio driver using HAL apis if you're delivering a pure audio driver. Audio samples are sometimes interesting to deliver from a CMIOExtension if, for instance, your camera delivers a muxed stream. Like the built in iOS Screen Capture DAL plugin, you can present your stream as a muxxed media type, and then deliver video or audio samples once you're running. I expect that shipping a HAL driver with my app would make App Store distribution difficult. It\u2019d be nice to have something like a Camera Extension that works for audio. Hmm okay. We have a HAL that we use to act as a virtual microphone that we can send any audio to (similar to BlackHole), and from this tweet: https://twitter.com/KhaosT/status/1497711090183987204?s=20&t=QwkmE9VvspUaWLmXO3-tyA I assumed it would be possible to convert to a CMIOExtension for distribution on the app store. You can certainly present it as a CMIOExtension. But most audio apps won't find it. They talk to the HAL, not the DAL. How about FaceTime? Would that see the CMIOExtension as a \"microphone\"? Unfortunately no. In this case we need a replacement for the HALs as well! one of the use cases is \"providing pre-rendered content\". But that content is likely to be large and on-disk as a file. But I thought Camera Extensions don't have access to the file system? You're free to use pre-rendered content that resides in your extension's own sandbox. CMIOExtensions don't have general file system access though. They have a more locked down sandbox that regular apps. yes, I just re-watched that bit of your video. So I could send a huge parameter set (perhaps slowly) as a property, and the extension could cache that in its own container as a file. Can I pre-populate an extension\u2019s container at build time? This is a problem for our use case, too. One suggestion we got from DTS was to set up two devices from the extension - one as the virtual camera, and one as an output device. Then, the app should be able to stream media from the App\u2019s sandbox (and user files) to the output device, which then would route them to the extension. I haven\u2019t tried that out yet, to see how well it works. yes, Brad mentioned that idea in a lab yesterday. In principle you can wrap any data up in a video buffer and send it, so you could even use a video queue if the property interface is too slow. If extensions were easier to develop and debug I\u2019d be happy to put more intelligence in the extension, but at present I have to restart every time I rev my extension. So for me, for now, the extension if just a conduit for frames created elsewhere. <@U03JDS776JH> What do you mean by \"build time\"? Can I pre-populate an extension\u2019s container at build time? I'm definitely hearing that rebooting to uninstall is a pain point. Camera Extensions can communicate over Firewire (!), USB, Bluetooth... but PCIe (Thunderbolt) is not mentioned. Will that be enabled? PCIe access would currently mean you need to resort to the legacy IOVideoFamily kernel interface. Kernel access is deprecated. But unfortunately we don't have a replacement for IOVideoFamily yet (sort of a peer to DEXT but with access to PCI hardware). I didn't mention PCI hardware because of the kext necessity. It's technically possible to use a CMIOExtension in conjunction with IOVideoFamily \u2014 so it will work. Just know that if at all possible to keep your code out of the kernel, you should. I'm sure the CoreOS team would love to hear your feedback about what you can and can't do with DEXTs. :slightly_smiling_face: if my Camera Extension already vends to any application, and it has some custom controls accessible over USB (either in a UVC Extension Unit or via HID), why would I use a DEXT to communicate with it, when I can use the HID Manager or user-space USB methods to communicate with the device? You don't have to use a DEXT. You're free to use other services. HID should be fine. we\u2019ve always had to jump through a small hoop to associate an AVCaptureDevice (which is actually a UVC device) with the HID device on the same USB device which provides us with additional control and status. We have to parse the uniqueID, which contains the USB location ID. That\u2019s messy. Right. We recognized that some 3rd parties are probably encoding bits of the vendor id / product id into their plugin's unique identifier. That's why we didn't force all unique IDs to be GUID style strings in the new extension world. You can implement a legacyID if you need to maintain backward compatibility. I\u2019m talking about Apple\u2019s UVC driver - it creates a unique ID for a USB AVCCaptureDevice which is concatenation of vid, pid and location ID. Even when we wholeheartedly adopt Camera Extensions, we\u2019ll still have to correlate HID devices with UVC devices, and there\u2019s no public API to do so. (FB6146541) Right. Apple's UVC Extension is a client that uses the legacyID feature, so apps such as yours (I suspect!) won't break. The Camera Extension is awesome! Can we have multiple instances of it, say, to send different content from separate documents to the host application? Thank you! We're excited about them. The team has worked really hard to produce a modern replacement. We want everyone to adopt them at lightning speed. :zap: You can only have one instance of your CMIOExtension loaded at a time. But it can vend more than one device. Or, your installer app could install more than one CMIOExtension. Sample code that covers everything would be helpful. :slightly_smiling_face: Hopefully the Xcode walk through in the video was helpful. The CMIOExtension template is really a great starting point. It builds a fully functional camera device / stream, so just studying the code produced by the template is a great start. Thanks for the walkthrough. Unfortunately, the video is actually too small to read the code you put in there. It would still be great to have the project as Sample Code. Just because it comes to mind: We currently have a DAL for Video and a corresponding HAL for Audio. Both currently have to be installed by the user separately from the app. The Camera Extension being a part of the app is a great relief. Do you know what corresponding API/Extension we can use for sound so that we do not have to install the HAL component? That is a great question. I don't know if it's possible to install a HAL driver through an app from the app store. Would you mind asking that question over at the <#C03H9J5AW4U|> slack channel? As I mentioned in a previous thread, you can certainly deliver audio through your CMIOExtension \u2014 it just won't get picked up by general purpose audio recording apps, who only query the HAL APIs to find HAL devices. An app like QuickTimePlayerX would be able to record from it though (assuming you've got a \"muxx\" device). Not sure if that works in apps like Zoom where you have a separate selector for audio devices. I asked in the audio-and-video-lounge but they seem to be off duty now. I hope they still see the question Typically when an application publishes a list of audio sources they are HAL devices. If your CMIO Extension delivers audio with its video, it is considered a \"muxed\" CMIO stream. The original CMIO muxed streams were from DV and HDV cameras. Taking audio from muxed stream is not usually handled by applications. Unless the application is also capturing video from the device. Audio in this case does not come from CoreAudio, but instead stays in the realm of AVCapture APIs. An app could use an AVCaptureAudioDataOutput to get access to the audio from a muxed stream, or use an AVCaptureMovieFileOutput to record it. Thanks for the session! I was wondering what would be the best way to transmit an app's view to its Camera Extension? So far I've tried hooking up an XPC Service to pass IOSurface data, but unsure how/where to connect in the extension Sure seems like a lot of you are trying to hook up an XPC service to pass frames to your extension. That's currently not working due to the fact that CMIOExtensions are run as a background process under a role user account, so they have no connection to the front user / aqua session. I have been nagging Brad with questions about exactly this! I\u2019ve even got it to work, but the user experience isn\u2019t ideal. One means of doing it is to do the fake output device -> fake camera device dance in your extension. Find the fake output device in your app and push frames to it. Then in your extension, republish those frames as a camera stream. I think we\u2019re all trying to do it because it worked in the old DAL plugin system :stuck_out_tongue: Yup lot of speculation and attempts before the official docs came out :smile: As I now understand it: initialize a fake output device within the app to consume frames. Then find that device within the extension and republish, probably from within CMIOExtensionDeviceSource ? Right. Or the ProviderSource, depending on how you want to separate your code. Not a question, but thank you for building this. I'm really happy to have a modern/secure approach for building this kind of plugin, and even happier to be able to distribute it on the App Store! From the whole team, THANK YOU! I will second that. I\u2019ve wanted this for a very long time!:100: We're excited about it too. That's why we've developed such an aggressive deprecation plan for DAL plugins. The whole system will be more safe and secure once DAL plugins are a thing of the past. does that mean that the fake output device/virtual camera output \u201cdance\u201d approach you mentioned will become impossible? Because the extension is still built on top of DAL\u2026 I applaud this as the DALs really sucked. But I hope you will provide all features necessary to replace them before you remove them. CMIOExtensions are part of the CoreMediaIO framework, but we consider them distinct from the DAL Plugin interfaces (which are deprecated). We are not planning on removing the ability to use CMIOExtensions as output devices. Hi Brad! Core Media IO seems very cool. I'm curious, is there or might there every be the capability to capture high quality stills via these plugins? Or do they only provide video streams? Thinking about the use case of a tethered mirrorless camera Yes! We'd like to support high res stills very soon. The Continuity Camera that Craig talked about in the WWDC keynote is capable of producing high res (12 MP) stills and delivering them to an AVCapturePhotoOutput. Definitely want 3rd party extensions to be able to do likewise. Awesome! The new iCloud shared photo library looks awesome! Will the content of the shared library be accessible from the PhotoKit API? (e.g. fetching PHAssets) Yes! All Shared assets would be accessible via the existing PHAsset fetches Great! Is there some way to tell apart shared assets from non-shared? I didn\u2019t see any API changes in the new SDK but thought those might not be in since the shared library isn\u2019t in seed 1. There is no API to differentiate shared vs. non-shared. From an API perspective, they would appear to all be part of the same local library As always, we would welcome use cases for specific API additions via Feedback requests (The more detail the better). Missed you during the lab slot yesterday <@U03HMDFMVNK> :wink: Haha, sorry I got caught up watching videos. :sweat_smile: Not exactly sure yet what I might want to do with shared vs. non-shared, but once I have a chance to play around with a seed with the shared library UI, that will give me more ideas and will file feedback accordingly. Can I get access to the smart filters present in Photos app via PHAsset.fetchAssets(with:) ? You can fetch the various Media smart albums and favorites (for example) by using the API fetchAssetCollectionsWithType:subtype:options: with the appropriate smart album subtype And then use the returned asset collection as the input to fetchAssets(in:options:) For example: PHAssetCollection.fetchAssetCollections(with: .smartAlbum, subtype: .smartAlbumFavorites, options: options) Yeah, I knew that option:) My question was more in the direction of filtering photos by keyword. For a bit more of a context: we create a clothing try-on app, and we would greatly benefit from showing our users photos that contain people. There are no smart albums for that, as far as i know:( I see, yes, that's correct- there is no API currently to fetch based on people. If you haven't already submitted an enhancement feedback request please do and provide the specifics of your desired use case. Though it doesn't address your request specifically, you might be able to make use of the PHPicker to give the user a way to select specific assets based on search and filter for keywords To elaborate, PHPickerViewController now supports https://developer.apple.com/documentation/photokit/phpickerfilter|additional filter APIs (e.g. screenshots filter). Users can also search for terms like \u201cPeople\u201d in the picker search bar. Could you achieve same result by getting faces smart albums? I don\u2019t know whether it returns anything if user hadn\u2019t marked anyone in his library. Search supports unnamed person Is there a way to query the \"People\" library/collection within the user's Photo library? People information (e.g. the names associated and groupings) are not provided via PhotoKit I encourage you to file a Feedback with your specific use case in mind. Or if you\u2019ve already filed one, please paste it here! FB9163331 :slightly_smiling_face: UIImagePickerViewController has the ability to crop and resize a selected image, but PHPickerViewController does not (unless it was added in iOS 16 and I missed it?). Since picking photos using UIImagePickerViewController is being deprecated, what is the correct way to now do this? Would we need a custom overlay? allowsEditing to enable crop is indeed only available on UIImagePickerController . If you want to support any kind of editing, including crop, you'd need to combine PHPicker + your own UI. Ah I see, is there a reason why it wasn't included in PHPickerViewController? I think part of it is the on-demand nature of the file provider; also each app can have its own requirements on crop parameters (aspect ratios, minimum sizes, etc.) That makes sense, it does appear to only allow a square crop. Thank you for your answer! Yes as Greg mentioned, PHPickerViewController allows you to fetch asset data after the picker is dismissed. You can implement your own edit/crop UI separately based on your own use case. Thank you! Really wish there was a default allowsEditing capability in PHPicker! Please file a feedback request! <@U03JYF8GT7A> Will do. The default square crop is perfect for things like quickly adjusting profile pics in profile setup flows. Bonus tip for any feedback is to also include some use cases / reasons / user impact as well. It really helps to include the context of your app in particular! Thanks Greg! Any difference if I submit it in my \u201cPersonal\u201d account vs my LLC which is a member of the Apple Developer Program? I don't think it matters, but maybe the developer program one would be more appropriate if it's dev/SDK kind of feedback. Is it possible yet to read (and write) the \"caption\" metadata property of a user's photo from their library? That is not possible today. I encourage you to file a Feedback with your specific use case in mind. Or if you\u2019ve already filed one, please paste it here! I havent filed a feedback for this, but I will do. Thanks for the suggestion :slightly_smiling_face: I did! :raised_hand: Def have a use case for this too. FB8244665 For what it's worth: If you are using the picker API and get an exported image, and that image had a caption, it should be in the IPTC metadata within the file Ooo it\u2019s in the metadata alongside EXIF etc. I haven\u2019t noticed IPTC before I\u2019ll have to investigate. That would solve my use case fr! Thank you I am working on an app to edit the metadata of photos in the iCloud Library, using Image I/O. After editing a RAW photo, it is no longer possible to edit that photo as a RAW image on any RAW photo editor (Darkroom, for example). My guess is that when we have to save the image as JPEG for the PHAssetChangeRequest, editors only see the PHAsset as JPEG instead of raw. Is there a way to edit a RAW photo\u2019s metadata but not remove the ability to edit it as RAW afterwards? If your goal is primarily to be able to edit the metadata - then your assessment is correct, any adjustment must be submitted in the form of a JPEG, which would produce a different file. The only way I could think of to change attributes of the original asset - would be to create a new asset with the updated original data, and delete the old one. Additionally, feel free to submit a feedback request with some more context and what your requirements are. It'd be useful to know which specific metadata (if any) you're thinking about writing, too! Yep, that is the only workaround I could think. I will try it, see how it goes but if I have any limitations I will file a feedback. <@U03HU4PCSET> I allow users to edit a subset of all metadata tags, but this specific scenario affects especially pro photographers who want to mainly edit IPTC tags. If their workflow is edit metadata first and then edit their RAW photo, then is when we get into trouble because they can\u2019t edit the photo as RAW no more. I wish we weren\u2019t limited to only writing JPEG images to PHContentEditingOutput\u2019s renderedContentURL Those are great reasons, thank you <@U03J4CQQR9A>! Adding that context when submitting a feedback request would be really great. The Photos app treats assets as \"digital negatives\" and doesn't modify them in place (an edited image is an additional resource, for example). So I think right now, <@U03HU4PD371>'s suggestion of adding a new asset (and deleting the old one) would be the way to do this (though it's likely not a great user experience). You may want to talk in the Core Graphics lab tomorrow to the ImageIO team, to see if writing specific RAW file formats are supported Sadly I can\u2019t make it to tomorrow\u2019s lab. If I have trouble in the future writing RAW files would a Technical Support Incident (TSI) be an appropriate channel to get help? Does the new ability to \"lock\" the Hidden and Recently Deleted albums have any effect on being able to change the hidden state of a photo or delete a photo using PhotoKit? Your app will still be able to hide photos however it won\u2019t be able to fetch the hidden photos back (if the user has chosen to keep the setting on which requires authentication for the hidden and recently deleted albums). Your app will still be able to attempt to delete photos and the user will be prompted to accept the deletion like in previous releases. Elaborating a bit, this means your app won\u2019t be able to unhide photos as it doesn\u2019t have access to hidden photos. I see, so if the hidden album is locked, then those hidden assets just won\u2019t be included in any fetch requests that would normally return them? correct Im building an app that tries to recognise and identify people in a photo. I believe the current Image framework is able to identify faces with great success, but has there been any progress in the framework that can help us \"recognize\" faces? (obvious use case here is to find and associate people to an identified face in a photo) If by \u201cthe framework\u201d you\u2019re referring to PhotoKit, there isn\u2019t any mechanism to try and associate images with the People that are tagged / named within the Photos app / Photo Library. I\u2019m not familiar enough with the Vision framework to know if there are clustering APIs which aid in you doing the face recognition all within your app though. Is that more what you were getting at? yeah I think possibly what Im needing to do is extract identified faces then build some ML model to further recognise familiar faces. Probably then the wrong channel for this. :slightly_smiling_face: thanks! <@U03K0BHRMNC> Are you in the machine-learning-lounge as well? There is an active Q&A going on with the Vision team right now. We probably have the right experts there that can give you suggestions of what to pursue. i will hop over there... thanks! Hey! So Im currently needing to get the file extension of a PHAsset from the user's photo library. At the moment the way Ive been directed to follow is to request requestContentEditingInput of the asset, and then get the extension from the fullSizeImageURL value. But im currently occasionally getting a [ImageManager] Media resource request failed to return valid data or url with error: Error with this request and I cant understand why. so maybe 2 questions... 1) is this the ideal/suggested way to understand if an asset is a PNG/JPEG/HEIC, and 2) what would possibly be the reason why I would get this error sometimes? And how do I \"recover\" from it so I can get the extension of the file reliably? Thanks! to be a little clearer, the error code i got with this was code=3164 . Ive seen some suggestions that its possibly because the asset is in iCloud, but Im fetching the asset first and then calling this request. So it seems hard to think that its iCloud related.. but maybe it is? The best way to determine the kind of file for an original image would be to use PHAssetResourceManager -- ie, get the list of resources for an asset, then isolate the one with PHAssetResourceTypePhoto, and call: -[PHAssetResource uniformTypeIdentifier] and the above method doesn't require use of the network ah ok, that seems helpful and surprised I missed it earlier.. Thanks! :blush: Is there a way to determine if your app has edited a given PHAsset? For example if I edit a photo in my app and then edit it in a different app, is there a way to know that my app has edited it? I'm wondering because I'd like to build a filter to only show photos my app has edited (as opposed to all edited photos). Given a PHAsset, if you wanted to know if your app owns the top level adjustment, then you might inspect the data in the PHAssetResourceTypeAdjustmentData resource. If another app has placed an adjustment on top of yours, you could tell by also inspecting data from PHAssetResourceTypeAdjustmentBasePhoto . You might inspect the value from -[PHAdjustmentData formatIdentifier] and see if it matches your app's bundle Another approach would be to keep track of edited assets' PHCloudIdentifier on the app side. So PHAsset.adjustmentFormatIdentifier does tell you the id of the last applied edit, though you can edit it in another app and then at that point you can\u2019t detect your app has edited it. Unless PHAssetResourceTypeAdjustmentBasePhoto does this but I don\u2019t understand how :thinking_face: as that provides an unaltered version of its photo asset for use in for use in reconstructing recent edits. if you're looking for \"ever edited with my app\" That\u2019s indeed what I want - filter out all photos except those that my app has edited even if they\u2019ve edited it afterwards in another app :slightly_smiling_face: I understand PHCloudIdentifier is kind of a heavy operation to convert to local identifiers, likely much too inefficient to loop over the entire photo library (or even as they\u2019re scrolling to show an \u201cedited\u201d indicator on the thumbnail). If a resource is present with the type PHAssetResourceTypeAdjustmentBasePhoto , it is an indication that an adjustment is applied to an asset, on top of another adjustment which didn't originate from the Photos app. Using requestDataForAssetResource... passing in the resource should give you back enough info to determine if the earlier adjustment is yours. 'Ever' edited with your app might require additional state -- as once as user reverts all adjustments, you would no longer be able to depend on adjustment data to answer that question. Ah yah, sorry \u201cever edited\u201d is not quite desired, just that there is an adjustment to that asset that my app performed. If they wipe out all edits, I don\u2019t want to indicate my app has edited it. Gotcha, yah that would do the trick if I needed to know that per photo asynchronously. For this use case it\u2019d need to be synchronous and efficient. I believe this data should be on disk already -- (in most cases does not require a download from network), it still might work for your use case, depending on how many assets you need to run this check against. (It probably is not efficient enough to run in a loop against a large collection of assets - but perhaps efficient enough to run once for all assets, then run incrementally based on changes to the library) Gotcha thanks! My gut is that wouldn\u2019t quite accomplish what I need. I previously filed FB5415719 about this but doesn\u2019t include a lot of details for what I intend to use this for which I imagine would be helpful. Would yall prefer a new bug report or I can send a reply on this one to provide additional deets? :slightly_smiling_face: <@U03HMCT187R> to make it efficient for scrolling, you can convert the PHCloudIdentifiers of edited photos to local identifiers, and fetch them all with +[PHAsset fetchAssetsWithLocalIdentifiers:options:] and keep that fetchResult around then in the cell, you can check if that asset has been edited via [fetchResult containsObject:assetInCell] , behind the scenes containsObject is very efficient and fast You can reply here with comments and I can add them to the FB. I think providing more context on how many assets you'd like to run the check on, the use case in the app, etc. would be helpful. Oh okay that approach makes sense! Though I can think of some drawbacks that doesn\u2019t really work for my use case. When the user edits a photo I\u2019d need to persist the cloud identifier for that asset, and sync that list of identifiers to all their devices, otherwise it wouldn\u2019t know about the edited photos from different devices. I also support editing photos from inside the Photos app, so need to use App Groups to try to update that in a shared location (which I actually don\u2019t think is possible because you don\u2019t get a PHAsset). If they go revert a photo in the Photos app, now my app thinks that photo is edited by my app but it no longer is. I\u2019ll compose a message detailing my use case. Thank you so much!! :hugging_face: I have an app (DateStamper) that loads up the user\u2019s photo library and allows them to scroll through a collection view of thumbnails, very similar to the Photos app. My app can edit photos as well (in the app and in a Photos editing extension). I would like to show a little \u201cedited\u201d indicator on each photo thumbnail if my app has edited that photo, more precisely, if that asset currently has adjustments that were performed by my app. To be super clear, if the user reverts edits to the photo, I do not want the \u201cedited\u201d indicator to show up anymore. I\u2019d also potentially like to provide a way for them to filter the photos to only show photos that my app edited in the collection view. I believe this may not quite be possible to achieve for this use case. You can determine if a PHAsset has been edited via PHAssetResource.assetResources(for: asset).contains(where: { $0.type == .adjustmentData }) (and there\u2019s new API in iOS 16 for PHAsset.hasAdjustments ) but this means any app could have edited it. I want to know if my app edited it. You can determine if your app edited a PHAsset by checking if PHAsset.adjustmentFormatIdentifier is one of your app\u2019s identifiers but this only gives you the id of the last edit performed. For example if you edit a photo in my app, then edit it in the Photos app or another third party app, you can no longer detect that your app has edited the photo. You could keep track of all the assets your app has edited via PHCloudIdentifier but this has some problems. The Photos editing extension doesn\u2019t have access to that, photos can be edited on other devices, and photos can be reverted outside of your app. Trying to track this manually would not accurately represent photos that are currently edited by your app. You could use PHAssetResourceManager to fetch the adjustment data and examine it to see if there\u2019s any created by your app, but this wouldn\u2019t really work for this use case, it would only really work in async contexts likely limited to a single photo at a time as opposed to a filter applied to all photos in the library. Thanks <@U03HU4PD371>! When you add this to the feedback will that show up as additional info to me? I\u2019d like to have that recorded for my future viewing too. I can go into follow-up with that as additional information if needed. I've added the discussion thread today to the radar, <@U03DJTBMHFF> would Jordan be able to see any additional info on his end? Is there a way to use PHFetchOptions to find only JPEG+RAW pairs and then split them or discard one half of the pair? There isn't any API for fetch options to choose assets based on JPEG+RAW original resources. And there's no way to remove an original resource from an asset, you'd have to create a new asset from one or more of the resources. Please do file a feedback to request support for searching based on R+J resource if that would still be useful for you In general, R+J Assets use the PHAssetResourceTypeAlternatePhoto resource to store one of the pairs. This tied with looking at the uniformTypeIdentifier would allow you to identify them. Unfortunately this will be slower than a fetchOption supported way. Ok thanks! My use case is basically trying to clean up a library and discard unnecessary RAWs so I'll look into the uniformTypeIdentifier option. In my testing, the modificationDate property of PHAsset gets updated every time a photo is viewed in the photos app (and other apps), even if no edits were made - it seems to behave more like a last viewed date. Is this intended behavior? Yes this is expected, not specifically to reflect that the asset was viewed, but the modificationDate will reflect changes due to internal bookkeeping and processing. If you expected or would better be able to use a modification date property that reflects only changes to properties exposed on PHAsset I'd encourage you to submit a feedback request with your specific use case Thanks for the clarification! With iCloud shared photo library, it sounds like PhotoKit basically shows just one system library like normal, which might happen to have shared assets if the user has the shared library enabled. So would that mean for example, if they then disabled the shared library on their system, that things like change observers and the new change history API would get notified of a whole bunch of deletions, since the shared assets would no longer be appearing in the library? Basically yes you\u2019re right. When a user disables Shared Library on their system, they get a choice of either keeping all shared assets or just the assets they\u2019ve contributed into the Shared Library. Based on the choice they make here, there could be a bunch of deletions appearing in the local system library Are CloudIdentifiers globally unique? Would two users ever have the same CloudIdentifier? The short answer is YES - they are unique. The more complete answer reflects what you get back from the lookup APIs when looking up a local identifier based on a PHCloudIdentifier : 1. In a set up where the user has enabled iCloud Photo Library the cloud identifier will be able to uniquely distinguish the asset and it's local identifier even when there are assets with the same image/video data 2. In the case where the photo library is not associated with an iCloud account (in iCloud Photo Library), it is possible for a cloud identifier to resolve to more than one local asset when looking up the local identifier via localIdentifierMappings(for:) - in that case the local identifier mapping result will indicate the error code: PHPhotosErrorMultipleIdentifiersFound Thanks! ... and you can get the set of potential matches for that cloud identifier via the error user info via the key: PHLocalIdentifiersErrorKey Is there a precalculated hash for a PHAsset to help check for duplicates? I've been using a Perceptual Hash of the image's thumbnail and having success, but was wondering if there was a better way. There is not any sort of API around a pre-calculated hash for a given PHAsset. A perceptual hash of the thumbnail is a reasonable approach. Combining that with metadata information and asset resource information would be a good way of checking for duplicates across the library. Also check out VNGenerateImageFeaturePrintRequest in the Vision framework. It does something similar in concept to a perceptual hash. Thanks!! I've found that if I request an image thumbnail using PHImageManager passing the .original option for the imageVersion, then if that asset hasn't had its original image downloaded from iCloud yet, it returns an error. The only workaround I've found is to manually request the original photo resource via PHAssetResourceManager and create the thumbnail myself. Is there any way to force PHImageManager to download the asset so it can create the thumbnail in this case, or is the current behavior how it's intended to work? Do you have networkAccessAllowed set in the request options? Yes also - in your case do you specifically need the original file? if your goal is to get a thumbnail, usually 'current' is better and more efficient I would not expect an error to be returned in the case you describe -- if you have a case that repro's, pls file a feedback and we can look further. Yes, in my case I\u2019m displaying a preview of what something will look like when I copy a photo elsewhere, and I have an option in my UI to copy the unmodified original instead of the current one, so I\u2019d want to show a thumbnail reflecting the version that\u2019s going to be copied. To be honest I encountered this one a while back and haven\u2019t checked on more recent versions of macOS, so I\u2019ll check the behavior again to see if it\u2019s changed. Just seemed like odd behavior and wanted to check to make sure I wasn\u2019t missing anything. I see - yes in the case of an adjusted asset that makes sense. If the asset is not adjusted you'll get better performance using 'current'. Odd indeed - pls include all the options you're using to set up the request, and all the logs/error details you're getting, as well as a sample asset that produces the issue. Is there a way manually create depth data for existing images? I'm looking to build something where I can basically \"paint in\" the depth information into images that are maybe not shot on iPhone. Not sure if we have the appropriate people here right now, but I will reach out. From the photokit perspective the data would need to be in the original image, so a new PHAsset would have to be created with the appropriate depth information in the original image. I\u2019ll leave it up to the camera folks to chime in on the depth creation part. Check out Vision\u2019s segmentation API: https://betterprogramming.pub/new-in-ios-15-vision-person-segmentation-5c031a2f3822 This sounds similar to what the Focos app does Cool, I'll check out all your suggestions. Thanks everyone! :raised_hands: There are some other machine-learning models out there that estimate depth based solely on an image. One of them, FCRN-DepthPrediction , is available as a Core ML model in our model gallery. https://developer.apple.com/machine-learning/models/ Using a model like this along with the Vision framework you may be able to achieve exactly what you're looking for. Monocular depth estimation is super cool, the best results I've gotten are with the model \u201cMiDaS\u201d. I was able to convert it to coreml using CoreML tools and use it in my Swift Student Challenge submission (with some quantizing to reduce the size). They also have a newer transformer model, which I haven't been able to convert to CoreML using the builtin tools, but I'd like to try again in the future. In the new iCloud Shared Album (w your family), will photos shared to you be located on the your Camera Roll ( smartAlbum/smartAlbumUserLibrary )? Follow up: will all of those photos be locally available in all devices of the family or will they be \u201ciCloud dehydrated\u201d and the user or an app needs to do an explicit download? Yes! From a PhotoKit API POV, all Shared assets that are inserted via the iCloud Shared Photo Library feature would appear as normal assets and would be shown in all the usual places (Photos tab, For You tab, Albums tab) and be available through all the usual fetches Bonus followup question: The same per-device policy applies here based on what they have setup in Settings > Photos > [Optimize Storage OR Download and Keep Originals] on that device. Are there any APIs for editing videos shot in Cinematic mode? There are not any at this point for editing the cinematic-ness of the video, but the video can be edited in the same way that any other video can be edited. Thanks for the reply! I know you can't comment on future plans but I hope to see this feature in the future. Thanks for the feedback and interest!","title":"photos camera"},{"location":"wwdc22/photos-camera-lounge.html#photos-camera-lounge-qas","text":"","title":"photos-camera-lounge QAs"},{"location":"wwdc22/photos-camera-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/photos-camera-lounge.html#hello-i-pretty-new-at-video-in-general-and-i-was-wondering-where-to-get-started-with-video-processing-specifically-how-do-i-rewrite-multiple-videos-next-to-each-other-for-side-by-side-3d-thank-you","text":"Would you like to record a video with two video tracks? Composite two together? Blend two together? Producing a stereo effect? Give us a little more info. I\u2019d like to take already recoded videos (left and right) and position them side by side not overlapping, like in a VR headset. AVMutableComposition should do the trick. You can use that to composite two movies together and write a new one. It would have an extra wide video track. Great, thank you! Just to piggyback on <@U03JBMMB10A> question, What are the examples out there of doing composition with multiple video tracks, I know a little about layer instructions, but is there any examples of using the frames directly? Have you checked out AVMulticamPiP? https://developer.apple.com/documentation/avfoundation/capture_setup/avmulticampip_capturing_from_multiple_cameras This composites front and rear camera into a single video track.","title":"Hello,  I pretty new at video in general and I was wondering where to get started with video processing, specifically how do I rewrite multiple videos next to each other for side by side 3D?  Thank you!"},{"location":"wwdc22/photos-camera-lounge.html#can-there-be-more-than-one-iphone-connected-via-continuity-camera","text":"Only one active stream at a time, but multiple iPhones can be discovered. So you can switch between them, but only one can be streaming actively at a time. I would like to add this as a feature request. We have limited bandwidth over WiFi. 1080p video ain't cheap. :slightly_smiling_face: Will it utilize usb if plugged in? Yes it will. USB has bandwidth constraints too. :slightly_smiling_face:","title":"Can there be more than one iPhone connected via Continuity Camera?"},{"location":"wwdc22/photos-camera-lounge.html#hi-brad-i-have-a-dumb-question-i-still-use-uikit-for-using-the-camera-with-avfoundation-did-i-miss-it-that-you-can-make-a-camera-app-in-swiftui","text":"Yes, you can use SwiftUI with AVFoundation. See this sample app we just released for Continuity Camera that shows this: https://developer.apple.com/documentation/avfoundation/capture_setup/supporting_continuity_camera_in_your_macos_app?language=objc Thanks. So, this is new this year? You have been able to use SwiftUI with AVFoundation since SwiftUI's introduction. But this is our first sample released showing SwiftUI with AVFoundation By using UIViewRepresentable , you can use an AVCaptureVideoPreviewLayer in SwiftUI The sample app shows this But that is not native SwiftUI, is it? That is creating a UIView in SwitftUI. I do that for MapKit as well. Actually, a UIView Layer Hi Paul, just chiming in cause I had a heck of a time figuring this out, but totally doable going back to first SwiftUI release (although I require iOS 14+ for my app because of an issue encountered in 13 related to this, can't remember what it was though Edit: I think it was related to viewfinder not stretching to bounds properly ). Have yet to look at the sample mentioned above, but solution is probably similar. At a high level you have your normal CameraViewController. You then create a UIViewControllerRepresentable, lets call it CameraVCSwiftUIView, with a CameraViewController as a member. This CameraVCSwiftUIView is now usable in SwiftUI as your CameraViewController / video preview :slightly_smiling_face: Correct, there is not a native SwiftUI solution for AVCaptureVideoPreviewLayer Check out CameraPreview.swift in the sample app, ContinuityCam Hi Edward, I have this now in my app. The app was originally build in 2015 based on UIKit. Now that I am adding new functionalities and the integration with the Apple watch, I'm also starting to move some code to SwiftUI. Currently, I wrapped the CameraViewController in a UIViewControllerRepresentable. I was, however, more curious to learn, when the AVFoundation camera would natively work in SwiftUI. I know how to build a camera app and how to integrate UIKit code into SwiftUI. That is not my problem. Hi Nikolas, that is too bad, but also gives me time to focus on other things. Thanks. My guess is it probably won't be brought to SwiftUI, just cause of the architecture -- how would one handle pipeline config, state changes, etc in a way that makes sense in a SwiftUI context? (I'm new to SwiftUI so I could be missing something) I think the problem is the AVCaptureVideoPReviewLayer that is used to capture the image/video. The rest is pretty straight forward.","title":"Hi Brad, I have a dumb question. I still use UIKit for using the camera with AVFoundation. Did I miss it that you can make a camera app in SwiftUI?"},{"location":"wwdc22/photos-camera-lounge.html#many-schools-use-ipads-as-cameras-would-be-cool-if-this-could-also-be-made-to-use-with-ipad","text":"True. iPad form factor is a little unwieldy as a Continuity Camera, as it needs to be facing away from you to use the rear facing cameras. We're just supporting iPhones for now. Imagine the device as an external camera. So many use cases in creative areas. And all the sensors of the iPhone/iPad as external sensors to be used from the Mac. Yep, Continuity Camera is very powerful.","title":"Many schools use iPads as cameras. Would be cool if this could also be made to use with iPad."},{"location":"wwdc22/photos-camera-lounge.html#hello-ive-noticed-the-viewfinder-gets-stretched-at-the-left-and-bottom-edges-when-livephoto-is-enabled-and-avcapturedevicedevicetype-is-set-to-builtinultrawidecamera-is-this-a-known-issue-and-is-there-a-workaround-thank-you-for-your-time","text":"Hi Edward, are you only seeing this when LivePhoto is enabled? Or do you always see this when using the .buildInUltraWideCamera? If it's the latter then I suspect it's simply the distortion of this ultra wide angle camera lens that you're seeing. But you should be seeing that in all 4 corners of the image. Thank you for your reply! Happens only when LivePhoto is enabled. See image below that doesn't look expected. Could you file a bug report with that screenshot and repro steps? Is this only happening with iOS 16? I believe it started in iOS 15, but can't be sure. This is the bug report that was filed a while back :slightly_smiling_face: FB9983221 thanks, I just found the report. It's with our team. The screenshot you sent here is very informative, I've attached it to the bug report. Thank you so much!! I've had a couple of users report this issue too! Haven't been able to replicate it myself Its there on the 12 Pro, but not on the 13 Pro as far as I can tell. Been there through most of iOS 15, and is still there on 15.4.1","title":"Hello,  I've noticed the viewfinder gets stretched at the left and bottom edges when LivePhoto is enabled, and AVCaptureDevice.DeviceType is set to .builtInUltraWideCamera. Is this a known issue, and is there a workaround?  Thank you for your time!"},{"location":"wwdc22/photos-camera-lounge.html#easy-question-for-you-is-uiimagepickercontroller-still-the-best-way-to-allow-people-to-take-photos-inside-your-app-if-youre-not-building-a-completely-camera-interface-the-camera-mode-isnt-deprecated-just-the-photolibrary-one-has-been","text":"I'd say UIImagePickerController is the easiest way to make a quick video or photo. Whether it's the best will depend on the feature set you need. It's a canned view, so you have little work to do. Late reply, but you have it exactly right \u2014 UIImagePickerController with the camera source for capture, and PHPicker for selecting existing assets is the recommended combination here. Awesome thanks! Looking forward to a future WWDC where there\u2019s an all new camera API to replace UIImagePickerController :smirk: Please do file feedback with any particular use cases / camera features / etc. that you'd like to see in a new API and feel free to drop the FB # here! I might just have to compose all my thoughts on that! FB10136910: Modern camera API desired to replace UIImagePickerController","title":"Easy question for you! Is UIImagePickerController still the best way to allow people to take photos inside your app, if you're not building a completely camera interface? (The camera mode isn't deprecated, just the photoLibrary one has been?)"},{"location":"wwdc22/photos-camera-lounge.html#what-is-the-best-way-to-write-a-live-video-stream-into-a-file-via-avassetwriter-with-custom-fps-eg-18-but-in-the-same-time-updating-avcapturevideopreviewlayer-with-30-fps","text":"On macOS, you can set your frame rate on individual connections, so you can have one frame rate going to preview and one going to video data output. On iOS, you can't. It's just one frame rate dictated by the AVCaptureDevice, so if you want a lower frame rate to your asset writer, you'll need to do your own frame dropping in your VDO.","title":"What is the best way to write a live video stream into a file (via AVAssetWriter) with custom FPS (e.g 18) but in the same time updating AVCaptureVideoPreviewLayer with 30 FPS?"},{"location":"wwdc22/photos-camera-lounge.html#in-ios-16-it-now-seems-possible-to-capture-high-resolution-photos-during-an-arkit-session-what-camera-properties-can-and-can-not-be-configured-during-an-active-arkit-session-can-we-lockset-the-exposure-white-balance-and-control-the-focus-distance-enabledisable-multi-exposure-fusion-and-tone-mapping-capture-images-with-8bit-depth-capture-pro-raw-images-presumably-not","text":"You'll need to check with the ARKit team on what features they support. I don't believe you can do any of the things you've mentioned here. They do provide access to the AVCaptureDevice now https://developer.apple.com/documentation/arkit/arconfiguration/3930045-configurablecapturedeviceforprim?changes=latest_minor But yes, I guess it is more a question for the ARKit team If they're providing direct access to AVCaptureDevice, you can certainly set properties on it directly, but I don't know how that will interact with the algorithms they're running.","title":"In iOS 16 it now seems possible to capture high resolution photos during an ARKit session.  What camera properties can and can not be configured during an active ARKit session?  Can we:  - lock/set the exposure, white balance, and control the focus distance?  - enable/disable multi-exposure fusion and tone mapping?  - capture images with  8bit depth? - capture Pro RAW images (presumably not)?"},{"location":"wwdc22/photos-camera-lounge.html#the-continuity-cameras-desk-view-is-really-interesting-and-amazing-how-does-it-work","text":"It uses the ultrawide lens of your iPhone. The field of view is wide enough to capture both your head and the desk in front of you. The desk view is warped and flipped with some pretty tricky distortion correction to make it appear rectilinear / flat, as if the camera were overhead instead of in front of it. You'll want to watch the session \"Bringing Continuity Camera to your app\" Already available! Thank you very much! Will definitely watch it.","title":"The Continuity Camera's desk view is really interesting and amazing! How does it work?"},{"location":"wwdc22/photos-camera-lounge.html#im-trying-to-build-an-app-for-ipados-to-provide-a-portable-video-streaming-setup-and-wonder-if-there-is-a-way-to-make-a-usb-uvc-camera-appear-in-avcapturedevicediscoverysession","text":"I would have to say no since we don't support UVC camera on iPadOS Thank you ^^ FB10074848 for potential future enhancements :pray: Thanks, Tian!","title":"I'm trying to build an app for iPadOS to provide a portable video streaming setup, and wonder if there is a way to make a USB UVC camera appear in AVCaptureDevice.DiscoverySession?"},{"location":"wwdc22/photos-camera-lounge.html#how-to-speed-up-session-start-time-and-prevent-drop-of-first-frames-for-example-snapchat-works-very-fast-almost-immediately-pausing-and-restarting-are-invisible-and-have-no-glitches-even-system-camera-instagram-facebook-apples-demo-app-spend-some-time-to-configure-the-session-and-draw-the-first-frame","text":"Creating the session with just the necessary outputs, e.g. only AVCaptureVideoDataOutput helps speed up the session startup. You can also try configuring everything upfront in the app user flow so the startup time is less noticeable to users. Is there a performance difference between wrapping any single output configuration into beginConfiguration and commitConfiguration and using only one begin-commit operation to batch outputs configurations? Batching all your configurations into one begin-commit is more efficient. beginConfiguration 's documentation also states: > You can nest beginConfiguration and commitConfiguration pairs, and the system applies the changes when you call the outermost commit. This may be helpful if you want to take care of different configuration aspects in different methods See: https://developer.apple.com/documentation/avfoundation/avcapturesession/1389174-beginconfiguration?language=objc Thanks","title":"How to speed up session start time and prevent drop of first frames? For example Snapchat works very fast (almost immediately), pausing and restarting are invisible and have no glitches. Even system Camera, Instagram, Facebook, Apple's demo app spend some time to configure the session and draw the first frame."},{"location":"wwdc22/photos-camera-lounge.html#is-it-possible-to-efficiently-write-the-avdepthdata-from-the-cameras-front-and-back-using-avassetwriter","text":"There is no API support in AVAssetWriter for writing depth at this time, though it is supported in the QuickTime spec as an auxiliary video track. You can read up on it in the official QuickTime spec, and a good example of this spec in action is Cinematic Video captured in http://Camera.app|Camera.app . Thanks, is that the system Camera app on the iPhone 13 Pros? Yes, correct. Is it publicly available in the quicktime spec? I can\u2019t see the https://twitter.com/jankais3r/status/1442466943697489923?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1442466943697489923%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fnofilmschool.com%2Fdepth-map-new-iphone-cinematic-mode|identifiers listed from the cinematic video in the https://developer.apple.com/library/archive/documentation/QuickTime/QTFF/Metadata/Metadata.html#//apple_ref/doc/uid/TP40000939-CH1-SW37|spec here","title":"Is it possible to efficiently write the AVDepthData from the cameras (front and back) using AVAssetWriter?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-way-to-use-continuity-camera-on-ipados-rather-than-macos-specifically-could-the-iphone-camera-be-used-as-the-camera-in-an-app-like-facetime-or-zoom","text":"Hi Brandon, Continuity Camera is only available on macOS currently, so iPad apps won't be able to see these new cameras.","title":"Is there a way to use Continuity Camera on iPadOS rather than macOS?  Specifically, could the iPhone camera be used as the camera in an app like FaceTime or Zoom?"},{"location":"wwdc22/photos-camera-lounge.html#thanks-for-the-qa-i-notice-there-may-be-a-session-on-this-tomorrow-but-is-there-a-way-to-continue-running-an-avcapturesession-in-slide-over-and-split-view-grin","text":"Yes! In tomorrow's session we describe new API that you can use to allow your app to use the camera while multitasking There are 2 new properties on AVCaptureSession: isMultitaskingCameraAccessSupported and isMultitaskingCameraAccessEnabled The session refers to this article which you can read in advance of the session being released: https://developer.apple.com/documentation/avkit/accessing_the_camera_while_multitasking?language=objc This is awesome! Thanks so much for developing this and the detailed doc :grin:","title":"Thanks for the Q&amp;A!  I notice there may be a session on this tomorrow, but is there a way to continue running an AVCaptureSession in slide over and split view? :grin:"},{"location":"wwdc22/photos-camera-lounge.html#do-the-updates-for-ipads-with-m1-to-support-custom-usb-drivers-with-driverkit-mean-that-i-can-use-external-cameras-and-other-capture-devices-like-an-elgato-ubs-capture-card-for-example-with-my-application-through-avfoundation","text":"Currently only internal cameras on the iPad are supported in AVFoundation. Okay, I\u2019ve got FB9948623 filed as an enhancement request for this for tracking purposes.","title":"Do the updates for iPads with M1 to support custom USB drivers with DriverKit mean that I can use external cameras and other capture devices (like an Elgato UBS capture card, for example) with my application through AVFoundation?"},{"location":"wwdc22/photos-camera-lounge.html#did-i-understand-this-correctly-you-will-be-able-to-capture-high-res-stills-while-still-seeing-the-live-video-feed","text":"speaking of Continuity Camera, yes? That's correct. Stills can be taken while doing live video. Awesome!","title":"Did I understand this correctly? You will be able to capture high res stills while still seeing the live video feed?"},{"location":"wwdc22/photos-camera-lounge.html#can-you-please-also-add-the-depth-info-to-the-stream-and-iphone-telemetry-orientation-position-velocity","text":"Tell us more about this. You can already add your own telemetry as metadata in timed metadata tracks in QuickTime movies using AVCaptureMetadataInput. We do support streaming depth as AVDepthData objects in AVCaptureDepthDataOutput. We do not currently support writing depth to movies via asset writer or AVCaptureMovieFileOutput though.","title":"Can you please also add the depth info to the stream? And iPhone telemetry? (Orientation, position, velocity)"},{"location":"wwdc22/photos-camera-lounge.html#for-a-few-releases-now-ive-been-warming-up-my-avassetwriter-as-soon-as-my-app-launches-then-cancelling-the-writing-session-before-actually-starting-to-write-if-i-dont-do-this-i-end-up-with-black-frames-at-the-beginning-of-the-avassetwriter-is-this-the-suggested-step-to-take-or-is-this-a-different-issue-im-facing","text":"Hrm... it would seem like this would happen if the movie also has audio that starts at an earlier time. Have you looked at the movie for edits. Thanks, <@U03HHA1DV9D>. In my scenario, that is not the case, as I am not recording audio. I'm usually recording very short clips (1-3 seconds), but can almost consistently replicate the issue, if I don't \"warm up\" the AVAssetWriter in advance. I can file a bug report on it if it's not a common/known topic! I haven't heard of this before. What exactly are you doing to warm up the assetwriter? (Before I forget, if you do file a bug report please include one of the clips.) I am instantiating an AVAssetWriter as soon as the app launches, starting a session at .zero , then adding a delay of 0.5 seconds, and then calling .cancelWriting() on the session. If I do that once, typically as soon as the app launches, I can negate the issue altogether. I got the suggestion from https://stackoverflow.com/questions/44135223/record-video-with-avassetwriter-first-frames-are-black|this post, though it feels like a workaround, for sure. The advice in that link sounds good. I think the confusion is around starting at zero but not actually writing with the first frame. Did you ever measure how long the period of black frames was? It's typically brief, though that's a great troubleshooting step to take. I can definitely measure the time; maybe it's consistent and I can use that to troubleshoot. Maybe I'm misunderstanding, but should I not be starting at .zero ? I had thought that was writing the timecode to start at 00:00:00, but perhaps I'm misunderstanding here. If you end up looking at the presentation times given by media samples read back from a movie file, you'll find that the timeline of every movie file starts at zero. The presentation times of the incoming samples are shifted. I'm not super-expert with AVAssetWriter, but I have the feeling that when you set the session to start at zero you are nullifying this normal adjustment.","title":"For a few releases now, I've been \"warming up\" my AVAssetWriter as soon as my app launches, then cancelling the writing session before actually starting to write.  If I don't do this, I end up with black frames at the beginning of the AVAssetWriter.  Is this the suggested step to take or is this a different issue I'm facing?"},{"location":"wwdc22/photos-camera-lounge.html#we-are-using-the-camera-to-take-photos-for-photogrammetric-reconstruction-what-are-some-tips-and-best-practices-for-getting-the-best-results-in-terms-of-configuring-the-camera-and-capture-settings-in-this-use-case-we-want-to-use-the-camera-as-a-light-measuring-instruments-so-we-do-not-want-any-tone-mapping-and-a-predictable-mapping-from-linear-intensities-photon-counts-to-recorded-pixel-values-so-that-we-can-invert-it-and-get-back-to-linear-intensities-pro-raw-seems-potentially-ideal-but-is-only-supported-on-a-small-percentage-of-devices-and-would-be-too-large-a-format-for-us-to-send-to-servers-for-reconstruction","text":"Hi! This is a deep question. Capturing Pro RAW (or Bayer RAW) is your best option to get access to non-tone mapped data, but as you mention the formats supporting these are large, and Pro RAW is not supported on all devices. I encourage you to try -[AVCaptureDevice globalToneMappingEnabled] to at least avoid local tone mapping. Thank you! But is that not just for global (non-local) tone mapping, judging by the name? I looked at the documentation and I think I understand. Turning this on disables local tonemapping. Yes, it will give you data that is globally tone mapped. Currently it\u2019s not possible capture linear data, or something that can be converted into linear data. But if I use an sRGB color space, could I not then use the known sRGB formula to get back to linear intensities? Are you interesting in streaming data? Or are there other unknown global tone mapping operators happening? No, I am interested in discrete images taken during an ARKit session. Yes, there\u2019s a scene dependent global tone mapping applied And no way to get that mapping I assume? Correct. :pensive: It would be good if you could file a feedback request with your use case. Will do. Thank you for the insights. Where available Pro RAW should be exactly what I want, right? (Except for file size) This is a common request. Please drop my name in the feedback request. Yes. Pro RAW should give you want you want. I think we actually had a nice lab conversation last year. Nice to meet you again! You too! Quite a different topic last year IIRC <@U03HR90DADU> Yes, last year we talked about minimizing camera shake blur and latency (after the shutter press) while taking photos for a computer vision application.","title":"We are using the camera to take photos for photogrammetric reconstruction. What are some tips and best practices for getting the best results (in terms of configuring the camera and capture settings)? In this use case we want to use the camera as a light measuring instruments. So we do not want any tone mapping, and a predictable mapping from linear intensities (photon counts) to recorded pixel values, so that we can invert it and get back to linear intensities.  Pro RAW seems potentially ideal(?), but is only supported on a small percentage of devices, and would be too large a format for us to send to servers for reconstruction."},{"location":"wwdc22/photos-camera-lounge.html#how-can-i-get-the-mac-webcam-in-a-catalyst-application-using-the-ios-code-the-camera-looks-cropped","text":"This is probably because your app is declaring that it supports the portait mode, so we have to crop in on the camera frames to make them fill your app's preview. If you support landscape orientation, we should give you an uncropped view. Will look in to that. Thanks (: great work!!","title":"How can I get the Mac webcam in a Catalyst application? Using the iOS code, the camera looks cropped."},{"location":"wwdc22/photos-camera-lounge.html#im-using-photo-sessionpreset-to-capture-fullres-photos-but-at-the-same-time-i-want-to-record-video-via-avassetwriter-in-1920x1080-without-any-sessions-reconfiguration-how-can-i-configure-avcapturevideodataoutput-to-get-from-the-delegate-samplebuffers-with-correct-sizes-1920x1080-i-know-about-automaticallyconfiguresoutputbufferdimensions-and-deliverspreviewsizedoutputbuffers-properties-but-their-combinations-produces-1284x1712-or-3024x4032-outputs","text":"In tomorrow's session \"Discover advancements in iOS camera capture\" we go over how to do this in iOS 16. You can set the width and height in the videoSettings property on an AVCaptureVideoDataOutput to receive custom dimensions. See: https://developer.apple.com/videos/play/wwdc2022/110429 You can specify the width and height width kCVPixelBufferWidthKey and kCVPixelBufferHeightKey , respectively <@U03HHA1D44F> Am I correct that kCVPixelBufferWidthKey and kCVPixelBufferHeightKey don't work on iOS < 16? correct Thanks However, it's important to note that you cannot receive 1920x1080 when using the photo session preset as that resolution is 4:3, often 4032x3024. When specifying the width and height in the AVCaptureVideoDataOutput's videoSettings. The aspect ratio must match the aspect ratio of the source camera's format So you could request 1920x1440, which is 4:3, when using the photo session preset","title":"I'm using .photo sessionPreset to capture fullRes photos. But at the same time I want to record video via AVAssetWriter in 1920x1080 (without any session's reconfiguration). How can I configure AVCaptureVideoDataOutput to get from the delegate sampleBuffers with correct sizes (1920x1080)? I know about automaticallyConfiguresOutputBufferDimensions and deliversPreviewSizedOutputBuffers properties. But their combinations produces 1284x1712 or 3024x4032 outputs."},{"location":"wwdc22/photos-camera-lounge.html#to-use-my-camera-app-on-sideview-on-ipad-i-need-special-entitlements-anyone-can-get-it-or-is-a-bit-restrict","text":"Before iOS 16, yes, there's an entitlement granting process that requires filling out a form. In iOS 16 and later, there's API to opt in for multitasking support (sideview/slideover). Nice. Thanks!! There's a session dropping tomorrow that you'll want to watch called \"Discover advancements in iOS camera capture: Depth, focus, and multitasking\" Going to check it.","title":"T\u00f4 use my camera app on Sideview on iPad, I need special entitlements? Anyone can get it or is a bit restrict?"},{"location":"wwdc22/photos-camera-lounge.html#i-found-that-i-have-to-access-inputnode-of-avaudioengine-before-starting-the-engine-to-be-able-to-use-it-this-means-the-permission-dialog-will-be-presented-before-the-engine-is-running-and-also-the-recording-indication-in-the-status-bar-is-present-is-there-any-way-to-start-using-the-input-node-later-when-the-engine-is-already-running-i-found-i-get-mostly-sample-rate-mismatch-errors-if-i-dont-access-the-input-node-beforehand-i-was-not-able-to-find-a-combination-any-tips-on-how-to-solve-this","text":"This is a great question to ask over in the <#C03H9J5AW4U|>. We don't have any AVAudioEngine experts in the room right now.","title":"I found that I have to access inputNode of AVAudioEngine before starting the engine to be able to use it. This means the permission dialog will be presented before the engine is running and also the recording indication in the status bar is present. Is there any way to start using the input node later, when the engine is already running. I found I get mostly sample rate mismatch errors if I don't access the input node beforehand. I was not able to find a combination, any tips on how to solve this?"},{"location":"wwdc22/photos-camera-lounge.html#iphone-can-be-free-from-the-mac-or-need-to-be-mounted-on-the-top-can-i-walk-with-the-phone-for-example","text":"The specific placement, orientation of the phone is required for automatic switching. But yes, you can use Continuity Camera when walking with your phone. Thanks Karen FYI: We actually show a demo of exactly that in the What's new in Create ML session which dropped today. If your phone is not in a stand, it will still be offered in the list of cameras, but it won't be designated at the systemPreferredCamera until you place it in a stand. By the way, it doesn't need to be clipped to the top of a macbook. It can be in any stand of your choosing, as long as it's landscape, stationary, screen off, and close by to the mac. Go it. The stand needs to have some chip? Or only checks for proximity and orientation? Nice. Great to know We check phone's proximity and orientation. No chip in stands :slightly_smiling_face: <@U03DJTBMHFF>, Create ML demonstrated Wombat? I don't see Wombat in this video https://developer.apple.com/videos/play/wwdc2022/110332/","title":"iPhone can be free from the Mac or need to be \u201cmounted\u201d on the top? Can I walk with the Phone for example?"},{"location":"wwdc22/photos-camera-lounge.html#is-it-possible-to-connect-multiple-ios-devices-to-the-same-mac-and-use-continuity-camera-to-capture-multiple-video-streams-simultaneously","text":"It doesn't support streaming multiple continuity cameras at the same time right now. But you can have multiple iOS devices around and switch between devices. would love the ability to stream multiple devices at once, we're doing this at https://detail.co%F0%9F%99%82|https://detail.co :slightly_smiling_face: Neat! Will Managed AppleIDs work with this feature? Despite Continuity in full NOT being a feature supported with MAIDs?","title":"Is it possible to connect multiple iOS devices to the same mac, and use continuity camera to capture multiple video streams simultaneously?"},{"location":"wwdc22/photos-camera-lounge.html#seems-there-is-a-slight-lip-sync-issue-when-karen-is-using-continuity-camera-so-which-mic-is-used-from-iphone-or-from-the-mac-is-it-configurable-through-api","text":"The audio in the session is from filming. You can use either built-in mic or Continuity Camera mic with the Continuity Camera. The Continuity mic can be found with both AVAudioSession and AVCapture APIs. And the A/V sync is good! :slightly_smiling_face: Whether you use a built in mic or the iPhone's microphone, we take care of it for you. I see, thanks :slightly_smiling_face:","title":"Seems there is a slight lip sync issue when Karen is using Continuity Camera, so which mic is used from iPhone or from the Mac? Is it configurable through API?"},{"location":"wwdc22/photos-camera-lounge.html#continuity-features-so-far-at-least-have-not-been-supported-for-managed-appleids-does-the-camera-continuity-adhere-to-the-same-restriction","text":"Yes, it has the same restrictions as other continuity features. :disappointed: Thank you","title":"Continuity features (so far at least) have not been supported for Managed AppleIDs - does the Camera Continuity adhere to the same restriction?"},{"location":"wwdc22/photos-camera-lounge.html#do-continuity-camera-sample-buffer-timestamps-account-for-network-delay-if-you-use-a-continuity-camera-and-a-macos-connected-usb-microphone-will-the-timing-between-them-match-up","text":"Yes, it's all covered! The buffer timestamps from continuity camera will reflect capture time on the phone converted to mac's timeline. Great, thanks Karen!","title":"Do continuity camera sample buffer timestamps account for network delay? If you use a continuity camera and a macOS connected USB microphone, will the timing between them match up?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-any-lossy-compressionencoding-applied-to-the-ios-camera-stream-that-would-result-in-lower-quality-recordings-compared-to-recording-on-ios-device","text":"Technically there's a lossy compression applied during transport. But it's sufficiently high bit rate that we think you won't notice quality loss.","title":"Is there any lossy compression/encoding applied to the iOS camera stream that would result in lower quality recordings compared to recording on-iOS device?"},{"location":"wwdc22/photos-camera-lounge.html#does-continuity-camera-allow-access-to-the-lidar-scanner-as-well-can-it-be-used-to-support-arkit-effects-on-mac-thanks","text":"Currently we only support the wide angle RGB camera (the one with the best quality!). We do internally switch to the Ultrawide camera when Center Stage is enabled. And DeskView also uses the Ultrawide camera. There is currently no access to the telephoto lens (if present) or LiDAR camera (if present). Ah, alrighty, understood! Thanks for the quick response Brad. I\u2019m so excited for this feature, it was something that I didn\u2019t even realize how much I needed. If you ever do get telephoto/LiDAR access going that would be huge in my opinion. :wink: Thanks so much to yourself and team!","title":"Does Continuity Camera allow access to the LiDaR scanner as well? Can it be used to support ARKit effects on Mac?  Thanks!"},{"location":"wwdc22/photos-camera-lounge.html#can-we-leverage-the-increase-memory-limit-entitlement-within-an-image-video-processing-app-can-we-give-core-image-access-to-more-memory-for-processing-and-caching","text":"We are actively working on this. Feel free to also file an enhancement request if you need a specific need addressed.","title":"Can we leverage the increase-memory-limit entitlement within an image &amp; video processing app? Can we give Core Image access to more memory for processing and caching?"},{"location":"wwdc22/photos-camera-lounge.html#is-it-possible-to-write-a-coreimage-filter-for-compute-rather-than-render-id-like-to-perform-compute-operations-on-the-cvpixelbuffer-of-a-depth-map-which-i-can-represent-as-a-ciimage","text":"Hi, CoreImage does use Metal compute pipeline states to perform the chained CIKernel operations you apply to the image. When you encode renders with CI, it dispatches compute commands instead of render commands. When I have tried this my values get mutated between what is returned in my metal code and what is rendered. Are there settings we need to change in the CIContext to prevent things like premultiplied alpha, and any other settings that might change / clamp / remap the outputs? These might be helpful https://developer.apple.com/documentation/coreimage/cirenderdestination/2875443-alphamode?language=objc https://developer.apple.com/documentation/coreimage/cirenderdestination/2875451-clamped?language=objc Also if you want, you can tell CI to disable color management using the the context option kCIContextWorkingColorSpace : [NSNull null] but use that with caution. In addition to what Baljit mentioned, mutated pixel values could be due to color management. CVPixelBuffers often are tagged with color space metadata and CI by default would color manage the input data to the CIContext's 'working space' and then convert to the output destination's color space when the image is rendered. This is on by default since it is usually desirable. But you do have the option to disable color management if you choose to. In the case of the depth map I think you'd want the data to remain linear. So either you can turn off color management as David suggested above, or set the render destination's color space to match the input. Thank you all :slightly_smiling_face: I will investigate these options Is there much performance overhead in rendering this (e.g. to bytes) from a CIContext to inspect the values, vs creating a metal buffer and inspecting them there? And does this change if I am using other CIFilter s such as masks I don't think we offer an API to render to a MTLBuffer. If you're comparing rendering to bytes vs rendering to texture, I think the difference should be fairly minimal on devices with unified memory architecture. Synchronization between the GPU memory and system memory is required in both cases so I think it should be fairly close. Thanks There is a render to metal texture https://developer.apple.com/documentation/coreimage/cicontext/1438026-render|function . I wondered if it would be quicker than to bytes","title":"Is it possible to write a CoreImage filter for compute, rather than render? I'd like to perform compute operations on the CVPixelBuffer of a depth map, which I can represent as a CIImage."},{"location":"wwdc22/photos-camera-lounge.html#we-have-cifilterpersonsegmentation-is-there-a-way-to-generate-the-other-segmentation-mattes-skin-hair-from-avphotocapture-on-demand-using-ciimages","text":"Please file an enhancement request for that. That said some Jpeg/Heic/ProRaw images already contain \"aux images\" for other segmentations. You can get at these either though ImageIO or CoreImage APIs Thanks, I will do. I was hoping to be able to apply this to video so can\u2019t access those file types Does the person segmentation filter return the same results as a VNGeneratePersonSegmentationRequest? I just learned they added a filter as well It seems to do from my experiments The quality parameters give the same sized buffers as results <@U03JG9JF529> Interesting, I\u2019ll have to give that a try!","title":"We have CIFilter.personSegmentation(). Is there a way to generate the other segmentation mattes (skin, hair) from AVPhotoCapture on demand using CIImages?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-any-chance-of-seeing-a-cifilter-for-film-grain-thats-more-natural-looking-than-the-cirandongenerator","text":"Please file an enhancement request. That would be great to add. Will do! You may also want to try the CIDither filter. I\u2019ll have a look at combining that with the random noise maybe, though I'm hoping to filter a live camera feed, hopefully it'll be performant enough. (Also CIDither doesn't seem to be listed on the Core Image Filter Reference page)","title":"Is there any chance of seeing a CIFilter for film grain, that\u2019s more natural looking than the CIRandonGenerator?"},{"location":"wwdc22/photos-camera-lounge.html#im-trying-to-blur-the-background-of-a-photo-similar-to-what-portrait-mode-does-using-a-matte-of-the-person-whats-a-good-approach-to-filling-the-area-inside-the-subject-so-that-theres-no-halo-around-the-person-when-blurring-the-image","text":"The simplest approach is to: \u2022 make a CIImage from the image where the foreground pixels are made transparent \u2022 blur that image \u2022 device that image by its alpha channel \u2022 use SourceOverCompositing to put the foreground image over the above result. <@U03HB0AV6S3> thanks for the reply! By \u201cdevice that image by its alpha channel\u201d, do you mean making the transparent pixels opaque? That sounds like a good approach, I\u2019ll give that a try I meant to type \"devide that image by its alpha channel\" Oh of course! Thanks! WIth a custom CIKernel like this: float4 alpha_denorm (sample_t i) { if (i.a &gt; 0.001) return float4(i.rgb / i.a, 1.0); return i; }","title":"I'm trying to blur the background of a photo, similar to what Portrait Mode does, using a matte of the person. What's a good approach to filling the area inside the subject so that there's no halo around the person when blurring the image?"},{"location":"wwdc22/photos-camera-lounge.html#since-cifilters-arent-thread-safe-what-would-be-the-recommended-way-to-structure-a-render-pipeline-if-i-want-to-use-swift-concurrency-especially-when-one-wants-to-initialize-the-filter-once-and-reuse-it-for-multiple-renders","text":"It is safe to render CIImage output of CIFilter on mulitple threads (and will be concurrent if you use different instances of CIContext) Thanks! Yes but initializing a filter from a raw file takes quite a while, so I would like to reuse the first instance when adjusting. I assume the filter will always have to be on the same thread? If I recall correctly, the CIRawFilter implements the copy method. That should be faster. Yes I tried the copy method but renders became unfortunately corrupt when using the copied version. We would appreciate a feedback/bug report for that I will file a report. Good to know that the copy method should work. Would love to further discuss different options during a lab session.","title":"Since CIFilters aren't thread safe, what would be the recommended way to structure a render pipeline if I want to use Swift concurrency? Especially when one wants to initialize the filter once and reuse it for multiple renders."},{"location":"wwdc22/photos-camera-lounge.html#which-ci-filters-would-you-recommend-running-in-linear-space","text":"Most CIFilters are designed to work in linear space. The notable exceptions are the the Blend filters and Compositing filters. These can work in linear space by may artists are more familiar using them in a perceptual colorspace. (eg. CIScreenBlendMode and others) Thanks! Would the recommendation be to run most filters in the linearSpaceFilter with a RAW file? And change the context working space to linear when working with non-raws? I would (with some exceptions) use linear for raw and non raw image. Feel free to schedule a Lab to discuss further. I wasn't aware that I could use the linearSpaceFilter for non raw's. Will try that! A lab has already been requested :slightly_smiling_face:","title":"Which CI filters would you recommend running in linear space?"},{"location":"wwdc22/photos-camera-lounge.html#can-i-use-ciareaaverage-to-get-the-average-color-except-transparent-areas","text":"Yes the CIAreaAverage filter is essentially an alpha-weighted average. Oh, alpha-weighted average! Thanks!","title":"Can I use CIAreaAverage to get the average color except transparent areas?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-good-approach-to-color-matching-two-photos-with-core-image-such-as-when-superimposing-a-person-on-a-different-background-so-the-photos-hue-saturation-maybe-contrast-and-brightness-somewhat-match-the-new-background","text":"You might consider using the Reduction filters such as CIAreaAverage or CIAreaHistogram to collect some stats on the two images. You can then use this stats to make one better match the other.","title":"Is there a good approach to color matching two photos with core image? Such as when superimposing a person on a different background, so the photo's hue, saturation, maybe contrast and brightness, somewhat match the new background"},{"location":"wwdc22/photos-camera-lounge.html#is-the-lidar-based-depth-data-capable-of-being-embedded-in-a-heic-similar-to-how-disparity-based-depth-data-is","text":"Hi Brandon, yes. Depth generated with the AVCaptureDeviceTypeBuiltInLiDARDepthCamera can be embedded in HEIC just like depth generated from other cameras.","title":"Is the LiDAR-based depth data capable of being embedded in a HEIC, similar to how disparity-based depth data is?"},{"location":"wwdc22/photos-camera-lounge.html#is-it-possible-to-get-the-raw-point-cloud-representation-from-the-lidar-camera-through-avfoundation-rather-than-an-arkitmetal-based-session","text":"Unfortunately no. The closest thing to it would be to turn off depthDataFiltering on the AVCaptureDepthDataOutput and then get depth data objects which will contain the full field of view of the wide camera / LiDAR camera, but will have holes (NaN pixels) where LiDAR spots were unavailable. Thanks, <@U03HXTBNYBC>! There's something very fun and magical about capturing point clouds lol, and am always curious of the technologies that allow for this. Appreciate your help and hard work on this new technology! It's point clouds all the way down. :nerd_face: To <@U03J20E7UBV>\u2019s last comment, it really does help to understand that AVFoundation's LiDARCam is really a marriage between time of flight technology and machine learning \"hallucinated\" depth \u2014 guided by the LiDAR spots. It just wouldn't be that useful with the LiDAR hardware alone. Together, they really advance the state of the art for depth data acquisition on iOS products. They give you great video quality, high resolution depth, and real-world scale, all in one package. They're also faced away from you, unlike the TrueDepth camera, so now you've got real-world scale options in both directions. Thanks, <@U03HXTBNYBC>! It's always been super cool to be able to get depth data from our devices, but having that added accuracy and high-resolution depth, alongside now higher capture quality, is really huge!","title":"Is it possible to get the raw point cloud representation from the LiDAR camera through AVFoundation, rather than an ARKit/Metal-based session?"},{"location":"wwdc22/photos-camera-lounge.html#the-multitasking-camera-features-seem-geared-towards-video-chat-apps-are-you-expecting-pro-camera-apps-to-adopt-this-and-will-the-native-camera-app-adopt-this-behaviour","text":"Multitasking Camera Access is often used by video chat apps. But it can be used by other apps. The built in Camera app requires itself to be full screen. But you can have a Slide Over app on top of it to multitask. Like opening Notes on top See for example this screenshot from the video. This is Apple's built in Camera app Got it! Thank you.","title":"The multitasking camera features seem geared towards video chat apps. Are you expecting pro camera apps to adopt this, and will the native camera app adopt this behaviour?"},{"location":"wwdc22/photos-camera-lounge.html#hi-ive-watched-the-video-to-live-text-interaction-and-i-was-wondering-if-the-quick-actions-can-be-customized-to-add-own-functions","text":"Excellent question. The quick actions are all driven by built-in data detectors. There is no opportunity for extending that or customizing those at this time. That said, we'd love to hear what kind of customizations you were hoping to provide. Thanks for the answer. It would be great to just add functions like \u201cApply for the textfield\u201d or \u201cGenerate new case/entry\u201d and the recognized text can just be added to own textfields or used as titles.","title":"Hi! I've watched the video to live text interaction and i was wondering, if the quick actions can be customized to add own functions?"},{"location":"wwdc22/photos-camera-lounge.html#will-we-ever-see-a-public-api-for-using-the-hardware-volume-buttons-to-capture-photos-the-current-workarounds-are-less-than-ideal-unamused","text":"Magic eightball says: \"Outlook uncertain\". Thanks for the feedback, Ben. We're asked about that a lot. Have you filed a request through Feedback Assistant? If I have it wasn't recently! But I brought it up in a lab yesterday, so I'll submit one now to hopefully give a little boost in priority","title":"Will we ever see a public API for using the hardware volume buttons to capture photos? The current.. workarounds are less than ideal :unamused:"},{"location":"wwdc22/photos-camera-lounge.html#theres-an-api-to-get-tagged-faces-out-of-a-photo-at-least-the-region-within-the-photo-where-theres-a-face-not-sure-about-the-persons-name-i-thought-there-might-be-a-way-to-manually-tag-faces-that-the-auto-apple-tagging-hadnt-picked-up-but-i-couldnt-find-a-way-to-do-this-is-it-possible-something-youd-consider-adding-to-either-the-api-or-the-photos-app-if-not","text":"Using the ImageIO APIs <ImageIO/CGImageProperties.h> you can load up an image on disk and inspect its metadata. Detected faces are held within the kCGImagePropertyExifAuxDictionary 's @\"Regions\". Is that what you had in mind by \"tagging\", or something else? That's probably where I found the data, but it didn't seem possible to add to it and update it so the photos app would show those additions?","title":"There\u2019s an API to get tagged faces out of a photo (at least the region within the photo where there\u2019s a face, not sure about the persons name..). I thought there might be a way to manually tag faces that the auto Apple tagging hadn\u2019t picked up, but I couldn\u2019t find a way to do this, is it possible? Something you\u2019d consider adding to either the api or the photos app if not?"},{"location":"wwdc22/photos-camera-lounge.html#this-is-about-avcapturedevice-will-there-be-a-public-api-to-check-on-the-lens-power-of-the-camera-this-would-be-exceedingly-useful-if-the-device-is-a-telephoto-camera-that-is-is-it-20x-optical-power-or-25x-or-30x-or-some-future-power-this-would-be-great-for-any-camera-app-where-the-user-manually-selects-the-lens-to-use-no-virtual-device-as-well-as-set-manual-shutter-speed-iso-thanks","text":"Seconded! This is definitely more awkward than it should be Hi Eric, Ben. could you clarify what you mean with \"lens power\"? Are you referring to the focal length / native zoom? Hi Ben. What I mean is, say I don't know if the user has the iPhone 12 Pro that has the 2.5x telephoto, or the iPhone 13 Pro that has the 3.0x tele. It would be wonderful to look at one get variable to see if it's a 2.5x or 3.0x, instead of doing the cumbersome task of determining that the user does have say an iPhone 12 The -[AVCaptureDevice virtualDeviceSwitchOverVideoZoomFactors] property tells the relative zoom factors between the cameras. So if you're looking to build a UI around these switch-over zoom factors, then you'll want to use that. In order to know if the telephoto camera is 2\u00d7, 2.5\u00d7 or 3\u00d7 (and display it to the user), you have to get the video field of view, then work out how that angle translates to a focal length, and how that focal length gets generalised I did check that variable and it reads empty brackets if there is no virtual device. I'm having the user manually select whether to use the wide lens or the telephoto, not rely on a virtual camera to do that. This is how I handle it in my app You can programmatically pick the virtual camera device and then query that property. It's a static property on the device in question, so it does not change depending on whether the device is connected to a session or not. (I feel like we may be trying to solve slightly different problems, apologies for hijacking!) Also note that the zoom factor of the widest FOV constituent device is always defined as 1x. If you want to build something similar to http://Camera.app|Camera.app where the wide-angle camera is defined as 1x then you'll want to divide the zoom factors by the wide-angle camera's switch-over zoom factor. One more note, on iPhone X, the telephoto camera's switch-over zoom factor is actually 1.8x (despite http://Camera.app|Camera.app showing 2x). We didn't want to confuse users with too much technical detail, so we use 2x in the UI. Eric, Ben, did this help answer your question(s)? Querying the virtual camera is probably cleaner than what I'm doing, but it would still be nicer to just have it as a property on each camera device. In my case I'm using the individual cameras to enable raw, manual WB, etc Hi Jozef, thanks for the answers. But in my app users not only have a zoom control, they also have push buttons to manually select what lens to use. So for the iPhone 11 Pro I present three buttons: 0.5x, 1x, and 2.0x. For the iPhone 13 Pro I present these: 0.5x, 1x, and 3.0x. Right now I do a roundabout way to know whether to call the button that connects to the telephoto camera a 2.0x or 3.0x. I believe the API I mentioned solves both of these problems. Just query for it on the Triple Camera device and you have all the zoom factors you need. Ben, the problem is that these zoom factors are relative to each other, so they only make sense in the context of a virtual camera. Note that you get different numbers from the Triple Camera than from the Dual Camera because the baseline is different. On the Triple Camera the wide-angle camera is typically 2x while on the Dual Camera it's 1x. This is why this is a property on the virtual cameras and not on the physical ones. Hi Josef, the key thing is that property is just for the virtual cameras. I'm using only the physical cameras, and I just ask for a get variable that is valid only for the physical devices. I understand, but you can still use the virtual camera's property for this. I propose that such a variable would read nothing if a virtual device is used to avoid confusing those who use the virtual camera. I'm sorry, I don't quite understand what you mean. Could you clarify? I suppose I can load the virtual camera, read the property, then discard the device and load the physical device.... Yes, that's exactly what I was proposing. And for what it's worth, all AVCaptureDevices get instantiated at the same time. So there's really no such thing as loading/discarding them. I have my project open, I'll give it a try, thanks! Sure thing! The issue is not knowing what the zoom factors are relative to each, but that the user understands 0.5x, 1x, 3x as shorthand to refer to the physical cameras because they're used in Apple\u2019s marketing in this way If you want to use the wide-angle camera as the 1x, then you can just query this property on the Dual Camera. Either that, or you can query it on the Triple Camera and divide all of them by the wide-angle camera's switch-over zoom factor. Both options should yield the same result. The switch-over zoom factors are always relative to the widest FOV camera in the virtual device. So for the Triple Camera the ultra-wide camera is the base (1x), while for the Dual Camera the wide-angle one is the base (1x). Sounds like, <@U03HVE4BEBY>, you should file a feedbackassistant request for us to provide an API that maps the actual zoom factors to \"marketing friendly\" zoom factors that match Apple's built in camera app. I think using the virtual cam zoom factors is definitely cleaner than my approach of using the field of view, so I'll switch to that as you've suggested <@U03HR8XVATG> but also yes I'll file a feedback to be able to check for a quick and basic \"label\" <@U03HXTBNYBC> Thank you both! Hi Brad, it sounds like the solution I had in mind in starting this thread. You expressed it perfectly, I'll file a feedback myself too, thanks! And Ben, thanks for your input! I appreciate all of you talking this through and answering my questions w/o me having to ask :smile: My thanks to the Camera team for a couple of great digital lounges this week! I just wanted to provide the team with the number of a new Feedback Assistant request, FB10137353, that addresses what was discussed in this long thread. It\u2019s the proposed API to map a \u2018marketing friendly\u2019 zoom factor for a physical AVCaptureDevice. It has the solution that was well voiced by Brad Ford in the wrap up to this discussion. Again, many thanks!","title":"This is about AVCaptureDevice.  Will there be a public API to check on the lens power of the camera?  This would be exceedingly useful if the device is a telephoto camera (that is, is it 2.0x optical power, or 2.5x or 3.0x, or some future power?)  This would be great for any camera app where the user manually selects the lens to use (no virtual device)  as well as set manual shutter speed &amp; ISO.  Thanks!"},{"location":"wwdc22/photos-camera-lounge.html#how-would-you-recommend-to-detect-a-blurry-image-the-image-can-be-of-any-resolution-allowed-by-a-camera-or-may-be-imported-into-photos-live-photos-and-videos-are-excluded","text":"Hi, Kiril. I think that Core ML may be your friend here. There are many models floating around in research circles like this one: https://arxiv.org/abs/2010.07936 Thanks! Using coremltools, you could convert the model to Core ML format and then use it inside the Vision framework to get your predictions of image quality. If you want to dive into this a bit further with Core ML Engineering, there are lab appointments available tomorrow. Got it Thank you very much If you need get least blurry image from a video / sequence of images, you can compute laplacian (MPSImageLaplacian), and than compute maximum using MPSImageStatisticsMinAndMax. Then, image with the smallest resulting value should be least blurry. You can try to get a treshold, below which you can consider image blurry, but it may fail in some cases, e.g. when you take a picture of a wall with no obvious edges. You could also do this in Core Image by taking a CIAreaHistogram of a CIGaborGradients . Or perhaps CIAreaAverage of CIGaborGradients which would be faster. I would recommend using Gabor over Laplacian because even a blurry image will contain noise. Gabor (a 5x5 convolution) is less affected by noise than Laplacian (a 3x3 convolution). In fact you my want to consider a 7x7 Gabor.","title":"How would you recommend to detect a blurry image? The image can be of any resolution allowed by a camera, or may be imported into Photos. Live photos and videos are excluded."},{"location":"wwdc22/photos-camera-lounge.html#in-the-creating-camera-extensions-session-you-said-that-cmio-extensions-can-be-video-sinks-but-these-sinks-dont-have-an-avcapture-interface-we-have-to-use-the-c-apis-which-apis-are-these-i-cannot-find-any-examples-of-using-a-dal-plug-in-this-way-we-have-implemented-dal-plug-ins-as-virtual-cameras-but-we-use-them-through-their-avcapture-interfaces-from-my-app-i-can-find-the-cmiobject-corresponding-to-my-camera-extension-it-is-of-class-aplg-isextension-is-true-and-it-has-the-correct-bundle-id-but-how-do-i-go-from-that-to-making-an-api-call-to-feed-frames-from-my-app-to-the-extension","text":"Hi Stuart. The C APIs to which I referred are the CoreMediaIO interfaces called by apps (rather than extension / plug in developers). CMIOHardware.h, CMIOHardwareDevice.h, CMIOHardwareStream.h CMIOHardwareSystem introduces the concept of a \"System\" object ( kCMIOObjectSystemObject ) that can be queried for its devices ( kCMIOHardwarePropertyDevices ). Once you've got the list of devices, you can query them for various properties, such as unique ID, or streams. You can, for instance, see if it has streams on its output scope (output streams instead of input streams). You push sample buffers to output streams, and these are outputted to a video deck or some other external piece of hardware that accepts a stream of video. It may help your mental model, <@U03JDS776JH>, to know that the AVCapture APIs on macOS are clients of the CMIOHardware APIs. They wrap the functionality in a more modern, friendly, swift/obj-c compatible way, and they hide some of the complexity one takes on when dealing with the CoreMediaIO APIs directly. To be clear, you don't need to search for your DAL plugin (the plugin object), unless there's a property that you want to set on the plug-in itself. For instance, if you had some property that's global to your entire extension, and all devices it vends. Typically, you'd just ask the system object for the list of CMIODeviceIDs, and then iterate through them, searching for the characteristics that fit your criteria. ok. So I query the system object for its kCMIOHardwarePropertyDevices, query those objects for their kCMIODevicePropertyStreams, querty those objects for their kCMIOStreamPropertyDirection etc. But I\u2019m unclear what the API is to push buffers to the stream. Also, \u201cinput\u201d and \u201coutput\u201d are from the point of view of the extension itself, right? AVCaptureDevice APIs hide CMIODeviceIDs that only contain output streams. You don't need to query them for their direction. That property is there for legacy stuff like DV cameras, which can be input or output streams, depending on what mode you've set the camera to be in (playback mode or recording mode). Just search for streams on the output scope rather than the input scope. That will give you the exclusively sink streams. that\u2019s good to know (output==sink)! But still, what\u2019s the API to push a frame to CMIOObject? You push frames to a CMIOStream. with CMIOStreamCopyBufferQueue to get the queue, then I push somethings onto that queue? And why would I want queueAlteredProc? Call CMIOStreamCopyBufferQueue to get its simple queue. You can add frames to that queue when you're ready. (If it's an input stream, you'd listen for when the queue was altered and dequeue the new frame or frames). There are also some other handy stream properties: kCMIOStreamPropertyOutputBufferUnderrunCount, kCMIOStreamPropertyOutputBufferRepeatCount, kCMIOStreamPropertyOutputBufferQueueSize, kCMIOStreamPropertyOutputBuffersRequiredForStartup, kCMIOStreamPropertyOutputBuffersNeededForThrottledPlayback, and kCMIOStreamPropertyFirstOutputPresentationTimeStamp. Also: kCMIOStreamPropertyScheduledOutputNotificationProc thank you this all extremely helpful. I have to run to a lab now but I posted another question on a slightly different topic. I hope to see you at a lab tomorrow If we want to do something like OBS where we have an application that renders video frames and passes them to a DAL plugin for people to select as a virtual webcam, is the recommended approach to create both a source and sink stream, and route frames in the extension from the sink to the source. Then in the app send frames to the sink? That's one approach. In the CMIOExtensions video (available now! and....watch party in 10 minutes), I describe a different technique where you can run physical streams from other cameras right within your CMIOExtension. We call this case \"creative camera\".","title":"in the Creating Camera Extensions session, you said that CMIO Extensions can be video sinks, but these sinks don't have an AVCapture interface, we have to use the C APIs. Which APIs are these? I cannot find any examples of using a DAL plug-in this way. We have implemented DAL plug-ins as virtual cameras, but we use them through their AVCapture interfaces. From my app, I can find the CMIObject corresponding to my Camera Extension, (it is of class 'aplg', isExtension is true, and it has the correct bundle ID). But how do I go from that to making an API call to feed frames from my app to the extension?"},{"location":"wwdc22/photos-camera-lounge.html#there-are-currently-3-frameworks-for-image-processing-accelerate-coreimage-filters-and-metal-for-example-if-we-take-a-black-white-conversion-of-the-image-we-can-use-accelerate-to-accomplish-it-a-cifilter-eg-cicolormonochrome-or-create-a-metal-shader-what-are-your-recommendations-for-using-each-framework-eg-in-which-cases-each-framework-is-preferable-to-others-or-should-be-avoided-and-is-it-ok-mixing-them-or-should-that-as-a-general-rule-be-avoided","text":"As you say, there are several ways to achieve the same \"monochrome\" result. The best will likely depend on what else you want to do and other factors. For example if you want to combine monochrome with other effects you want to pick a strategy that that avoids switching back and forth. Switching say between CPU and GPU will introduce stalls and increase memory. Another thing to consider is if you need to deal with imaged that are large sizes or with HDR color spaces. In that case a higher lave framework will have that built in. I guess the main reason for me to look for CI alternatives was that conversion from CIImage (with several filters) to CGImage or to JPEG representation of the image took a very long time (5-10 sec) and a lot of memory (for larger images especially). Is that generally a point where you'd say: yes, in those cases switch to Accelerate or Metal, or could it be related to a particular (possibly incorrect) use of CIFilters? You might want to try going directly from CIImage to JPEG using API like JPEGRepresentationOfImage:colorSpace:options: . A time of 5 to 10 seconds seems very slow. How big is the image? raw image taken on iphone 10 for example For a RAW image using CoreImage is the fastest/easiest way to produce a demosaiced and denoised result. This is non trivial but 5 to 10 seconds seems slow to me. Here are some suggestions: \u2022 use the CIRAWFilter API to have more control (for example you can set .scaleFactor if you just want a half sized image. \u2022 create the CIContext with the option kCIContextCacheIntermediates set to @NO to minimize memory usage. Great suggestions, thanks. I wasn't using either - hence probably the timing and memory. FYI: there's another question/discussion about CIRAWFilter in this channel today.","title":"There are currently 3 frameworks for image processing: Accelerate, CoreImage filters, and Metal. For example, if we take a \"black &amp; white conversion\" of the image: we can use Accelerate to accomplish it, a CIFilter (e.g. CIColorMonochrome), or create a Metal shader.  What are your recommendations for using each framework? E.g. in which cases each framework is preferable to others, or should be avoided? And is it OK mixing them, or should that (as a general rule) be avoided?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-way-to-get-a-transform-from-original-frame-to-a-stabilized-one-we-have-custom-ar-content-overlaying-video-and-wanted-to-preview-non-stabilized-video-for-less-latency-and-write-cinematic-stabilization-to-a-disk-however-as-we-need-to-render-ar-stuff-over-video-and-we-dont-want-to-run-pipeline-twice-and-with-transforms-we-could-just-transform-what-we-rendered-once-to-a-stabilized-frame","text":"This is an interesting one! There's currently no API to perform video stabilization but only expose stabilization transform without applying it onto frames. Could you file a feature request for this? So there is a way to expose stabilization transform? (Without applying stabilization) Sorry for asking, but could you direct me where should I file this request? Hi Viacheslav, exposing stabilization transform is currently not supported. You can file feature request here http://feedbackassistant.apple.com|feedbackassistant.apple.com . <@U03HY66772A> are you familiar with the new feature in iOS 16 which allows you to run multiple video data outputs? You can run one with no stabilization for preview, and run a separate one with stabilization for writing to disk. This would require running the pipeline twice, but you could potentially run a lower quality algorithm for the realtime feed, and a higher quality version of it for the one being written to disk. Sadly, we are running on the edge of performance targets, and rendering twise would mean waiting for video to process after the fact for some time for our users But you definetly could run two connections with different stabilization options before? At least for preview and for writing on disk. (I think that\u2019s how Apple Camera app works)","title":"Is there a way to get a transform from original frame to a stabilized one? We have custom AR content overlaying video, and wanted to preview non-stabilized video for less latency, and write cinematic stabilization to a disk. However, as we need to render AR stuff over video, and we don't want to run pipeline twice, and with transforms we could just transform what we rendered once to a stabilized frame."},{"location":"wwdc22/photos-camera-lounge.html#question-about-cirawfilter-does-scalefactor-do-the-same-thing-as-in-cifilter-with-raw-options-let-ciimage-ciimagedata-selfimagedata-let-cifilter-cifilterimagedata-selfimagedata-options-scalefactor-025-let-cirawfilter-cirawfilterimagedata-selfimagedata-identifierhint-comadoberaw-image-cirawfilterscalefactor-025-printciimage-ciimageextent-printcifilter-cifilteroutputimageextent-printcirawfilter-cirawfilteroutputimageextent-prints-ciimage-00-00-59520-39680-cifilter-optional00-00-14880-9920-cirawfilter-optional00-00-59520-39680-see-that-the-cifilter-version-scaled-the-extent-but-the-cirawfilter-did-not-more-importantly-is-scaling-here-an-effective-way-to-improve-performance-of-raw-editing-any-other-options-or-things-to-keep-in-mind-for-performance-specifically","text":"Big yes. Using the .scaleFactor on the CIRAWFilter class will give a big benefit in performance. It reduces the amount of memory needed as well and reduces the amount of noise reduction that is needed. Asking the filter for .nativeSize will always give the full size of the asset. Asking the filter for .outputImage.extent should give the scaled size (assuming that you set .scaleFactor before) Please file a bug report on the ciRawFilter.scaleFactor / ciRawFilter.outputImage?.extent issue. That should give you the smaller extent. Thanks <@U03HB0AV6S3>! Will do. You might try toggling one of the other properties to see if that helps. Is there any workaround to scale using CIRAWFilter? Or have to fall back to CIFilter for now? Try setting .orientation to a different value and then back again? That property also alters ciRawFilter.outputImage?.extent No luck. Setting ciRawFilter.orientation = .left swapped the dimensions but still at full size. I tried both orders of orientation, scaleFactor If we find a workaround we will note it in your bug report. much appreciated! I\u2019ll file that asap","title":"Question about CIRAWFilter - does scaleFactor do the same thing as in CIFilter with RAW options?   let ciImage = CIImage(data: self.imageData)! let ciFilter = CIFilter(imageData: self.imageData, options: [ .scaleFactor: 0.25 ])! let ciRawFilter = CIRAWFilter(imageData: self.imageData, identifierHint: \"com.adobe.raw-image\")! ciRawFilter.scaleFactor = 0.25 print(\"CIImage\", ciImage.extent) print(\"CIFilter\", ciFilter.outputImage?.extent) print(\"CIRAWFilter\", ciRawFilter.outputImage?.extent) Prints CIImage (0.0, 0.0, 5952.0, 3968.0) CIFilter Optional((0.0, 0.0, 1488.0, 992.0)) CIRAWFilter Optional((0.0, 0.0, 5952.0, 3968.0))  See that the CIFilter version scaled the extent but the CIRAWFilter did not.   More importantly, is scaling here an effective way to improve performance of RAW editing? Any other options or things to keep in mind for performance specifically?"},{"location":"wwdc22/photos-camera-lounge.html#i-was-told-the-resolution-of-lidar-depth-data-from-arkit-had-been-for-accuracy-down-to-around-the-size-of-a-table-leg-or-accuracy-within-a-few-inches-with-avfoundation-having-greater-depth-resolution-is-there-a-kind-of-rule-of-thumb-in-terms-of-accuracy-down-to-set-distance-or-down-an-object-size-like-a-tooth","text":"Hi Rob, unfortunately we don\u2019t have a hard and fast rule for how small the objects can be at a given distance, but in general the AVFoundation LiDAR Depth Camera can detect finer details \u2014 for example the edge of table will be straighter/sharper than ARKit due to the increase in pixel resolution. A tooth at 5m from the camera is probably too small. That makes sense, thank you. Just so I\u2019m understanding, that\u2019s 5 meters or 5 millimeters? That's in meters. You're welcome!","title":"I was told the resolution of LiDAR depth data from ARKit had been for accuracy down to around the size of a table leg, or accuracy within a few inches. With AVFoundation having greater depth resolution, is there a kind of rule-of-thumb in terms of accuracy down to set distance, or down an object size, like a tooth?"},{"location":"wwdc22/photos-camera-lounge.html#hello-i-understand-this-one-is-a-bit-tricky-but-what-does-the-magic-8-ball-say-about-builtintriplecamera-one-day-supporting-exposuremodecustom-and-manual-focus-innocent","text":"Hah! The problem with supporting custom exposure modes is that each constituent camera in the virtual camera has a different min and max iso and shutter speed. Apps written for the past ~10 years are not prepared for min and max ranges to suddenly jump or shift. We would certainly crash a lot of apps if we did this. Also, zoom level is not the only factor that might cause the virtual camera to switch from one constituent camera to another. For instance, on the iPhone 13 Pro, the ultrawide lens is preferred up close because it has such a small minimum focus distance. So, we'd need to figure out a way to have a lot of properties jump \u2014 and jump atomically \u2014 when the constituent camera shifts. That's the reason custom exposure isn't supported on the triple camera. Appreciate the detailed answer! Totally get it, logistically its messy. My immediate thought is to expose only common ranges supported across all lenses. As the developer of ProShot I'd gladly take that range hit, and then find my own creative way to visualize it to users in the UI :slightly_smiling_face: At any rate, I really appreciate all the amazing work that's been going into these APIs, so much good stuff to work with! Thank you all","title":"Hello! I understand this one is a bit tricky, but what does the magic 8-ball say about .builtInTripleCamera one day supporting ExposureMode.custom and manual focus? :innocent:"},{"location":"wwdc22/photos-camera-lounge.html#ive-noticed-that-new-version-of-my-camera-extension-on-macos-123-arent-actually-used-until-i-restart-although-systemextensionsctl-list-tells-me-it-is-loaded-and-the-old-version-is-unloaded-is-this-intended-behavior","text":"<@U03JDS776JH>, in the sample project I built in the CMIO Extensions video (viewing party in half an hour!), the extension is loaded and usable immediately. No need to reboot. Currently, uninstalling it does not take effect right away \u2014 you have to reboot before the extension is unloaded for all apps, and systemextensionsctl tells you that. I verified the above behaviors on macOS 12.3, so I'm not sure why you'd be seeing a difference. Perhaps file a bug at feedbackassistant. do changes to the CMIOExtension take effect before rebooting? I tried a few weeks ago and found that uninstalling, reinstalling, and rebooting sometimes wasn't enough to update the extension That should do it every time, Finn. (uninstall, reboot, reinstall). If you want to be completely neurotic, you could bump the version number of your extension. You need to make sure that your installer code is actually telling the system to update or replace the previous one (if you're installing over an old one). it might be a caching issue. I bump the build number and change the name of the virtual device every time I make a change. Photo Booth will display the name of the build 84 after I\u2019ve succesfully activated build 85, I only see build 85 after a restart. This is with SIP turned off and the hosting app not in /Applications. Can we download the sample project?","title":"I've noticed that new version of my Camera Extension on macOS 12.3 aren't actually used until I restart, although 'systemextensionsctl list' tells me it is loaded and the old version is unloaded. Is this intended behavior?"},{"location":"wwdc22/photos-camera-lounge.html#is-it-possible-to-use-camera-directly-in-app-extensions-especially-in-the-keyboard-extension-or-at-least-a-live-text-but-through-our-own-ui","text":"An iOS NSExtension? The only iOS NSExtension that supports camera access are iMessage Extensions Can confirm iMessage extensions work with camera with one important bug with the plist permission entries (I think was fixed last year). Which bug is that? Involving the NSCameraUsageDescription entry in the Info.plist? <@U03HHA1D44F> But should it be possible to use Live Text then, without directly getting access to the Camera? P.S. for now, I constantly receive the following error: Assertion failure in -[UIKeyboardCameraSession _keyboardCameraPreparationDidComplete] Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'Keyboard Camera is being used without remote keyboards enabled What API do you call to get Live Text in the keyboard? I am not sure how the Keyboard Camera is implemented but if it tries to use the camera within your process, that makes sense why you are seeing that error <@U03HHA1D44F> Live Text API has only two requirements to be called: 1. Have a UIResponder object 2. Have a UIKeyInput object And I was trying to make my UIInputViewController as both of them while returning UITextDocumentProxy properties to satisfy the UIKeyInput protocol. It looked pretty valid from the design view. And the code I\u2019ve been trying to run is close to the similar: class KeyboardViewController: UIInputViewController { override func viewDidLoad() { super.viewDidLoad() liveTextStraightButton.addAction( UIAction.captureTextFromCamera( responder: self, identifier: .paste ), for: .primaryActionTriggered ) } } extension KeyboardViewController: UIKeyInput { var hasText: Bool { textDocumentProxy.hasText } func insertText(_ text: String) { textDocumentProxy.insertText(text) } func deleteBackward() { textDocumentProxy.deleteBackward() } } <@U03HHA1D44F> hey again, Couldn\u2019t get to the lab today regarding this question. Would be grateful for some attention to the feedback ticket regarding that issue. It has a test project attached, which helps reproduce the issue: > FB10022377 Live Text: UIAction.captureTextFromCamera not working in keyboard extension Hi <@U03JER2C7MX>, I've brought FB10022377 to the attention of a colleague who is more familiar with this API to take a look. Thank you for including the test project in the radar! Wow, those are fantastic news, thank you! :scream::green_heart:","title":"Is it possible to use camera directly in app extensions, especially in the keyboard extension? Or at least a Live text, but through our own UI?"},{"location":"wwdc22/photos-camera-lounge.html#weve-shipped-a-modern-core-media-io-system-extension-in-our-app-however-we-never-could-figure-out-how-to-connect-to-an-xpc-service-defined-in-the-extensions-cmioextensionmachservicename-and-a-dts-ticket-didnt-get-us-far-either-is-it-intended-that-we-can-communicate-with-our-extension-via-the-xpc-using-the-mach-service-if-not-what-are-your-recommended-methods-of-communicating-with-the-extension-when-were-delivering-frames-over-that-connection","text":"CMIOExtensions work a little differently from Endpoint or Network extensions. CMIOExtensions are run a a role user in a background process. That's why you're not able to find them with the named mach service. Keep watching. I describe a method to talk to your extension through custom properties. <@U03HXTBNYBC> Thanks for the reply. Would custom properties be appropriate for delivering frame buffers in terms of latency/etc? Ideally we\u2019d use the old technique of delivering IOSurface buffers via IPC. No, custom properties would be for smaller state, such as strings or NSData. For frames, you should use IOSurface backed buffers. Is it possible to deliver such buffers to our CMIO system extension from our parent app? Is the idea here that your app is capturing or generating real-time video and you want to get that to other camera-sourcing apps? Yes. One way to do this is to present 2 devices in your extension. One that presents an output or sink stream, and one that presents an input or source stream. You push frames to the sink stream device, and then in the extension, re-publish it through the camera. <@U03HHA1DV9D> Yup! Our app connects to DSLR/mirrorless cameras over network or USB and lets them be used as webcams even if they\u2019re not UVC devices (which most aren\u2019t). For various reasons, it makes sense for that connection and processing to be going on in the parent app. <@U03HXTBNYBC> Ok great, thanks for the idea. We have a Feedback open (FB10026784) assuming the XPC thing is a bug \u2014 perhaps that should be changed to a documentation request clarifying what the mach service key is for. As of right now, the mach service name is required so that CMIO's registerassistantservice recognizes the extension as a CMIO extension. But it's not currently used as a named /known XPC service that an app connects to. Thanks for the FB.","title":"We've shipped a \"modern\" Core Media IO system extension in our app. However, we never could figure out how to connect to an XPC service defined in the extension's CMIOExtensionMachServiceName \u2014 and a DTS ticket didn't get us far either! Is it intended that we can communicate with our extension via the XPC using the mach service? If not, what are your recommended methods of communicating with the extension when we're delivering frames over that connection?"},{"location":"wwdc22/photos-camera-lounge.html#are-audio-cmioextensions-nearly-identical-to-the-sample-video-one-where-i-would-just-send-audio-cmsamplebuffers-to-the-cmioextensionstream","text":"We do not recommend shipping audio drivers as CMIOExtensions. Most apps won't work with them. Stick to writing an Audio driver using HAL apis if you're delivering a pure audio driver. Audio samples are sometimes interesting to deliver from a CMIOExtension if, for instance, your camera delivers a muxed stream. Like the built in iOS Screen Capture DAL plugin, you can present your stream as a muxxed media type, and then deliver video or audio samples once you're running. I expect that shipping a HAL driver with my app would make App Store distribution difficult. It\u2019d be nice to have something like a Camera Extension that works for audio. Hmm okay. We have a HAL that we use to act as a virtual microphone that we can send any audio to (similar to BlackHole), and from this tweet: https://twitter.com/KhaosT/status/1497711090183987204?s=20&t=QwkmE9VvspUaWLmXO3-tyA I assumed it would be possible to convert to a CMIOExtension for distribution on the app store. You can certainly present it as a CMIOExtension. But most audio apps won't find it. They talk to the HAL, not the DAL. How about FaceTime? Would that see the CMIOExtension as a \"microphone\"? Unfortunately no. In this case we need a replacement for the HALs as well!","title":"Are audio CMIOExtensions nearly identical to the sample video one, where I would just send audio CMSampleBuffers to the CMIOExtensionStream?"},{"location":"wwdc22/photos-camera-lounge.html#one-of-the-use-cases-is-providing-pre-rendered-content-but-that-content-is-likely-to-be-large-and-on-disk-as-a-file-but-i-thought-camera-extensions-dont-have-access-to-the-file-system","text":"You're free to use pre-rendered content that resides in your extension's own sandbox. CMIOExtensions don't have general file system access though. They have a more locked down sandbox that regular apps. yes, I just re-watched that bit of your video. So I could send a huge parameter set (perhaps slowly) as a property, and the extension could cache that in its own container as a file. Can I pre-populate an extension\u2019s container at build time? This is a problem for our use case, too. One suggestion we got from DTS was to set up two devices from the extension - one as the virtual camera, and one as an output device. Then, the app should be able to stream media from the App\u2019s sandbox (and user files) to the output device, which then would route them to the extension. I haven\u2019t tried that out yet, to see how well it works. yes, Brad mentioned that idea in a lab yesterday. In principle you can wrap any data up in a video buffer and send it, so you could even use a video queue if the property interface is too slow. If extensions were easier to develop and debug I\u2019d be happy to put more intelligence in the extension, but at present I have to restart every time I rev my extension. So for me, for now, the extension if just a conduit for frames created elsewhere. <@U03JDS776JH> What do you mean by \"build time\"? Can I pre-populate an extension\u2019s container at build time? I'm definitely hearing that rebooting to uninstall is a pain point.","title":"one of the use cases is \"providing pre-rendered content\". But that content is likely to be large and on-disk as a file. But I thought Camera Extensions don't have access to the file system?"},{"location":"wwdc22/photos-camera-lounge.html#camera-extensions-can-communicate-over-firewire-usb-bluetooth-but-pcie-thunderbolt-is-not-mentioned-will-that-be-enabled","text":"PCIe access would currently mean you need to resort to the legacy IOVideoFamily kernel interface. Kernel access is deprecated. But unfortunately we don't have a replacement for IOVideoFamily yet (sort of a peer to DEXT but with access to PCI hardware). I didn't mention PCI hardware because of the kext necessity. It's technically possible to use a CMIOExtension in conjunction with IOVideoFamily \u2014 so it will work. Just know that if at all possible to keep your code out of the kernel, you should. I'm sure the CoreOS team would love to hear your feedback about what you can and can't do with DEXTs. :slightly_smiling_face:","title":"Camera Extensions can communicate over Firewire (!), USB, Bluetooth... but PCIe (Thunderbolt) is not mentioned. Will that be enabled?"},{"location":"wwdc22/photos-camera-lounge.html#if-my-camera-extension-already-vends-to-any-application-and-it-has-some-custom-controls-accessible-over-usb-either-in-a-uvc-extension-unit-or-via-hid-why-would-i-use-a-dext-to-communicate-with-it-when-i-can-use-the-hid-manager-or-user-space-usb-methods-to-communicate-with-the-device","text":"You don't have to use a DEXT. You're free to use other services. HID should be fine. we\u2019ve always had to jump through a small hoop to associate an AVCaptureDevice (which is actually a UVC device) with the HID device on the same USB device which provides us with additional control and status. We have to parse the uniqueID, which contains the USB location ID. That\u2019s messy. Right. We recognized that some 3rd parties are probably encoding bits of the vendor id / product id into their plugin's unique identifier. That's why we didn't force all unique IDs to be GUID style strings in the new extension world. You can implement a legacyID if you need to maintain backward compatibility. I\u2019m talking about Apple\u2019s UVC driver - it creates a unique ID for a USB AVCCaptureDevice which is concatenation of vid, pid and location ID. Even when we wholeheartedly adopt Camera Extensions, we\u2019ll still have to correlate HID devices with UVC devices, and there\u2019s no public API to do so. (FB6146541) Right. Apple's UVC Extension is a client that uses the legacyID feature, so apps such as yours (I suspect!) won't break.","title":"if my Camera Extension already vends to any application, and it has some custom controls accessible over USB (either in a UVC Extension Unit or via HID), why would I use a DEXT to communicate with it, when I can use the HID Manager or user-space USB methods to communicate with the device?"},{"location":"wwdc22/photos-camera-lounge.html#the-camera-extension-is-awesome-can-we-have-multiple-instances-of-it-say-to-send-different-content-from-separate-documents-to-the-host-application","text":"Thank you! We're excited about them. The team has worked really hard to produce a modern replacement. We want everyone to adopt them at lightning speed. :zap: You can only have one instance of your CMIOExtension loaded at a time. But it can vend more than one device. Or, your installer app could install more than one CMIOExtension. Sample code that covers everything would be helpful. :slightly_smiling_face: Hopefully the Xcode walk through in the video was helpful. The CMIOExtension template is really a great starting point. It builds a fully functional camera device / stream, so just studying the code produced by the template is a great start. Thanks for the walkthrough. Unfortunately, the video is actually too small to read the code you put in there. It would still be great to have the project as Sample Code.","title":"The Camera Extension is awesome! Can we have multiple instances of it, say, to send different content from separate documents to the host application?"},{"location":"wwdc22/photos-camera-lounge.html#just-because-it-comes-to-mind-we-currently-have-a-dal-for-video-and-a-corresponding-hal-for-audio-both-currently-have-to-be-installed-by-the-user-separately-from-the-app-the-camera-extension-being-a-part-of-the-app-is-a-great-relief-do-you-know-what-corresponding-apiextension-we-can-use-for-sound-so-that-we-do-not-have-to-install-the-hal-component","text":"That is a great question. I don't know if it's possible to install a HAL driver through an app from the app store. Would you mind asking that question over at the <#C03H9J5AW4U|> slack channel? As I mentioned in a previous thread, you can certainly deliver audio through your CMIOExtension \u2014 it just won't get picked up by general purpose audio recording apps, who only query the HAL APIs to find HAL devices. An app like QuickTimePlayerX would be able to record from it though (assuming you've got a \"muxx\" device). Not sure if that works in apps like Zoom where you have a separate selector for audio devices. I asked in the audio-and-video-lounge but they seem to be off duty now. I hope they still see the question Typically when an application publishes a list of audio sources they are HAL devices. If your CMIO Extension delivers audio with its video, it is considered a \"muxed\" CMIO stream. The original CMIO muxed streams were from DV and HDV cameras. Taking audio from muxed stream is not usually handled by applications. Unless the application is also capturing video from the device. Audio in this case does not come from CoreAudio, but instead stays in the realm of AVCapture APIs. An app could use an AVCaptureAudioDataOutput to get access to the audio from a muxed stream, or use an AVCaptureMovieFileOutput to record it.","title":"Just because it comes to mind: We currently have a DAL for Video and a corresponding HAL for Audio. Both currently have to be installed by the user separately from the app. The Camera Extension being a part of the app is a great relief. Do you know what corresponding API/Extension we can use for sound so that we do not have to install the HAL component?"},{"location":"wwdc22/photos-camera-lounge.html#thanks-for-the-session-i-was-wondering-what-would-be-the-best-way-to-transmit-an-apps-view-to-its-camera-extension-so-far-ive-tried-hooking-up-an-xpc-service-to-pass-iosurface-data-but-unsure-howwhere-to-connect-in-the-extension","text":"Sure seems like a lot of you are trying to hook up an XPC service to pass frames to your extension. That's currently not working due to the fact that CMIOExtensions are run as a background process under a role user account, so they have no connection to the front user / aqua session. I have been nagging Brad with questions about exactly this! I\u2019ve even got it to work, but the user experience isn\u2019t ideal. One means of doing it is to do the fake output device -> fake camera device dance in your extension. Find the fake output device in your app and push frames to it. Then in your extension, republish those frames as a camera stream. I think we\u2019re all trying to do it because it worked in the old DAL plugin system :stuck_out_tongue: Yup lot of speculation and attempts before the official docs came out :smile: As I now understand it: initialize a fake output device within the app to consume frames. Then find that device within the extension and republish, probably from within CMIOExtensionDeviceSource ? Right. Or the ProviderSource, depending on how you want to separate your code.","title":"Thanks for the session! I was wondering what would be the best way to transmit an app's view to its Camera Extension? So far I've tried hooking up an XPC Service to pass IOSurface data, but unsure how/where to connect in the extension"},{"location":"wwdc22/photos-camera-lounge.html#not-a-question-but-thank-you-for-building-this-im-really-happy-to-have-a-modernsecure-approach-for-building-this-kind-of-plugin-and-even-happier-to-be-able-to-distribute-it-on-the-app-store","text":"From the whole team, THANK YOU! I will second that. I\u2019ve wanted this for a very long time!:100: We're excited about it too. That's why we've developed such an aggressive deprecation plan for DAL plugins. The whole system will be more safe and secure once DAL plugins are a thing of the past. does that mean that the fake output device/virtual camera output \u201cdance\u201d approach you mentioned will become impossible? Because the extension is still built on top of DAL\u2026 I applaud this as the DALs really sucked. But I hope you will provide all features necessary to replace them before you remove them. CMIOExtensions are part of the CoreMediaIO framework, but we consider them distinct from the DAL Plugin interfaces (which are deprecated). We are not planning on removing the ability to use CMIOExtensions as output devices.","title":"Not a question, but thank you for building this. I'm really happy to have a modern/secure approach for building this kind of plugin, and even happier to be able to distribute it on the App Store!"},{"location":"wwdc22/photos-camera-lounge.html#hi-brad-core-media-io-seems-very-cool-im-curious-is-there-or-might-there-every-be-the-capability-to-capture-high-quality-stills-via-these-plugins-or-do-they-only-provide-video-streams-thinking-about-the-use-case-of-a-tethered-mirrorless-camera","text":"Yes! We'd like to support high res stills very soon. The Continuity Camera that Craig talked about in the WWDC keynote is capable of producing high res (12 MP) stills and delivering them to an AVCapturePhotoOutput. Definitely want 3rd party extensions to be able to do likewise. Awesome!","title":"Hi Brad! Core Media IO seems very cool. I'm curious, is there or might there every be the capability to capture high quality stills via these plugins? Or do they only provide video streams? Thinking about the use case of a tethered mirrorless camera"},{"location":"wwdc22/photos-camera-lounge.html#the-new-icloud-shared-photo-library-looks-awesome-will-the-content-of-the-shared-library-be-accessible-from-the-photokit-api-eg-fetching-phassets","text":"Yes! All Shared assets would be accessible via the existing PHAsset fetches Great! Is there some way to tell apart shared assets from non-shared? I didn\u2019t see any API changes in the new SDK but thought those might not be in since the shared library isn\u2019t in seed 1. There is no API to differentiate shared vs. non-shared. From an API perspective, they would appear to all be part of the same local library As always, we would welcome use cases for specific API additions via Feedback requests (The more detail the better). Missed you during the lab slot yesterday <@U03HMDFMVNK> :wink: Haha, sorry I got caught up watching videos. :sweat_smile: Not exactly sure yet what I might want to do with shared vs. non-shared, but once I have a chance to play around with a seed with the shared library UI, that will give me more ideas and will file feedback accordingly.","title":"The new iCloud shared photo library looks awesome! Will the content of the shared library be accessible from the PhotoKit API? (e.g. fetching PHAssets)"},{"location":"wwdc22/photos-camera-lounge.html#can-i-get-access-to-the-smart-filters-present-in-photos-app-via-phassetfetchassetswith","text":"You can fetch the various Media smart albums and favorites (for example) by using the API fetchAssetCollectionsWithType:subtype:options: with the appropriate smart album subtype And then use the returned asset collection as the input to fetchAssets(in:options:) For example: PHAssetCollection.fetchAssetCollections(with: .smartAlbum, subtype: .smartAlbumFavorites, options: options) Yeah, I knew that option:) My question was more in the direction of filtering photos by keyword. For a bit more of a context: we create a clothing try-on app, and we would greatly benefit from showing our users photos that contain people. There are no smart albums for that, as far as i know:( I see, yes, that's correct- there is no API currently to fetch based on people. If you haven't already submitted an enhancement feedback request please do and provide the specifics of your desired use case. Though it doesn't address your request specifically, you might be able to make use of the PHPicker to give the user a way to select specific assets based on search and filter for keywords To elaborate, PHPickerViewController now supports https://developer.apple.com/documentation/photokit/phpickerfilter|additional filter APIs (e.g. screenshots filter). Users can also search for terms like \u201cPeople\u201d in the picker search bar. Could you achieve same result by getting faces smart albums? I don\u2019t know whether it returns anything if user hadn\u2019t marked anyone in his library. Search supports unnamed person","title":"Can I get access to the smart filters present in Photos app via PHAsset.fetchAssets(with:)?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-way-to-query-the-people-librarycollection-within-the-users-photo-library","text":"People information (e.g. the names associated and groupings) are not provided via PhotoKit I encourage you to file a Feedback with your specific use case in mind. Or if you\u2019ve already filed one, please paste it here! FB9163331 :slightly_smiling_face:","title":"Is there a way to query the \"People\" library/collection within the user's Photo library?"},{"location":"wwdc22/photos-camera-lounge.html#uiimagepickerviewcontroller-has-the-ability-to-crop-and-resize-a-selected-image-but-phpickerviewcontroller-does-not-unless-it-was-added-in-ios-16-and-i-missed-it-since-picking-photos-using-uiimagepickerviewcontroller-is-being-deprecated-what-is-the-correct-way-to-now-do-this-would-we-need-a-custom-overlay","text":"allowsEditing to enable crop is indeed only available on UIImagePickerController . If you want to support any kind of editing, including crop, you'd need to combine PHPicker + your own UI. Ah I see, is there a reason why it wasn't included in PHPickerViewController? I think part of it is the on-demand nature of the file provider; also each app can have its own requirements on crop parameters (aspect ratios, minimum sizes, etc.) That makes sense, it does appear to only allow a square crop. Thank you for your answer! Yes as Greg mentioned, PHPickerViewController allows you to fetch asset data after the picker is dismissed. You can implement your own edit/crop UI separately based on your own use case. Thank you! Really wish there was a default allowsEditing capability in PHPicker! Please file a feedback request! <@U03JYF8GT7A> Will do. The default square crop is perfect for things like quickly adjusting profile pics in profile setup flows. Bonus tip for any feedback is to also include some use cases / reasons / user impact as well. It really helps to include the context of your app in particular! Thanks Greg! Any difference if I submit it in my \u201cPersonal\u201d account vs my LLC which is a member of the Apple Developer Program? I don't think it matters, but maybe the developer program one would be more appropriate if it's dev/SDK kind of feedback.","title":"UIImagePickerViewController has the ability to crop and resize a selected image, but PHPickerViewController does not (unless it was added in iOS 16 and I missed it?). Since picking photos using UIImagePickerViewController is being deprecated, what is the correct way to now do this? Would we need a custom overlay?"},{"location":"wwdc22/photos-camera-lounge.html#is-it-possible-yet-to-read-and-write-the-caption-metadata-property-of-a-users-photo-from-their-library","text":"That is not possible today. I encourage you to file a Feedback with your specific use case in mind. Or if you\u2019ve already filed one, please paste it here! I havent filed a feedback for this, but I will do. Thanks for the suggestion :slightly_smiling_face: I did! :raised_hand: Def have a use case for this too. FB8244665 For what it's worth: If you are using the picker API and get an exported image, and that image had a caption, it should be in the IPTC metadata within the file Ooo it\u2019s in the metadata alongside EXIF etc. I haven\u2019t noticed IPTC before I\u2019ll have to investigate. That would solve my use case fr! Thank you","title":"Is it possible yet to read (and write) the \"caption\" metadata property of a user's photo from their library?"},{"location":"wwdc22/photos-camera-lounge.html#i-am-working-on-an-app-to-edit-the-metadata-of-photos-in-the-icloud-library-using-image-io-after-editing-a-raw-photo-it-is-no-longer-possible-to-edit-that-photo-as-a-raw-image-on-any-raw-photo-editor-darkroom-for-example-my-guess-is-that-when-we-have-to-save-the-image-as-jpeg-for-the-phassetchangerequest-editors-only-see-the-phasset-as-jpeg-instead-of-raw-is-there-a-way-to-edit-a-raw-photos-metadata-but-not-remove-the-ability-to-edit-it-as-raw-afterwards","text":"If your goal is primarily to be able to edit the metadata - then your assessment is correct, any adjustment must be submitted in the form of a JPEG, which would produce a different file. The only way I could think of to change attributes of the original asset - would be to create a new asset with the updated original data, and delete the old one. Additionally, feel free to submit a feedback request with some more context and what your requirements are. It'd be useful to know which specific metadata (if any) you're thinking about writing, too! Yep, that is the only workaround I could think. I will try it, see how it goes but if I have any limitations I will file a feedback. <@U03HU4PCSET> I allow users to edit a subset of all metadata tags, but this specific scenario affects especially pro photographers who want to mainly edit IPTC tags. If their workflow is edit metadata first and then edit their RAW photo, then is when we get into trouble because they can\u2019t edit the photo as RAW no more. I wish we weren\u2019t limited to only writing JPEG images to PHContentEditingOutput\u2019s renderedContentURL Those are great reasons, thank you <@U03J4CQQR9A>! Adding that context when submitting a feedback request would be really great. The Photos app treats assets as \"digital negatives\" and doesn't modify them in place (an edited image is an additional resource, for example). So I think right now, <@U03HU4PD371>'s suggestion of adding a new asset (and deleting the old one) would be the way to do this (though it's likely not a great user experience). You may want to talk in the Core Graphics lab tomorrow to the ImageIO team, to see if writing specific RAW file formats are supported Sadly I can\u2019t make it to tomorrow\u2019s lab. If I have trouble in the future writing RAW files would a Technical Support Incident (TSI) be an appropriate channel to get help?","title":"I am working on an app to edit the metadata of photos in the iCloud Library, using Image I/O. After editing a RAW photo, it is no longer possible to edit that photo as a RAW image on any RAW photo editor (Darkroom, for example). My guess is that when we have to save the image as JPEG for the PHAssetChangeRequest, editors only see the PHAsset as JPEG instead of raw. Is there a way to edit a RAW photo\u2019s metadata but not remove the ability to edit it as RAW afterwards?"},{"location":"wwdc22/photos-camera-lounge.html#does-the-new-ability-to-lock-the-hidden-and-recently-deleted-albums-have-any-effect-on-being-able-to-change-the-hidden-state-of-a-photo-or-delete-a-photo-using-photokit","text":"Your app will still be able to hide photos however it won\u2019t be able to fetch the hidden photos back (if the user has chosen to keep the setting on which requires authentication for the hidden and recently deleted albums). Your app will still be able to attempt to delete photos and the user will be prompted to accept the deletion like in previous releases. Elaborating a bit, this means your app won\u2019t be able to unhide photos as it doesn\u2019t have access to hidden photos. I see, so if the hidden album is locked, then those hidden assets just won\u2019t be included in any fetch requests that would normally return them? correct","title":"Does the new ability to \"lock\" the Hidden and Recently Deleted albums have any effect on being able to change the hidden state of a photo or delete a photo using PhotoKit?"},{"location":"wwdc22/photos-camera-lounge.html#im-building-an-app-that-tries-to-recognise-and-identify-people-in-a-photo-i-believe-the-current-image-framework-is-able-to-identify-faces-with-great-success-but-has-there-been-any-progress-in-the-framework-that-can-help-us-recognize-faces-obvious-use-case-here-is-to-find-and-associate-people-to-an-identified-face-in-a-photo","text":"If by \u201cthe framework\u201d you\u2019re referring to PhotoKit, there isn\u2019t any mechanism to try and associate images with the People that are tagged / named within the Photos app / Photo Library. I\u2019m not familiar enough with the Vision framework to know if there are clustering APIs which aid in you doing the face recognition all within your app though. Is that more what you were getting at? yeah I think possibly what Im needing to do is extract identified faces then build some ML model to further recognise familiar faces. Probably then the wrong channel for this. :slightly_smiling_face: thanks! <@U03K0BHRMNC> Are you in the machine-learning-lounge as well? There is an active Q&A going on with the Vision team right now. We probably have the right experts there that can give you suggestions of what to pursue. i will hop over there... thanks!","title":"Im building an app that tries to recognise and identify people in a photo. I believe the current Image framework is able to identify faces with great success, but has there been any progress in the framework that can help us \"recognize\" faces? (obvious use case here is to find and associate people to an identified face in a photo)"},{"location":"wwdc22/photos-camera-lounge.html#hey-so-im-currently-needing-to-get-the-file-extension-of-a-phasset-from-the-users-photo-library-at-the-moment-the-way-ive-been-directed-to-follow-is-to-request-requestcontenteditinginput-of-the-asset-and-then-get-the-extension-from-the-fullsizeimageurl-value-but-im-currently-occasionally-getting-a-imagemanager-media-resource-request-failed-to-return-valid-data-or-url-with-error-error-with-this-request-and-i-cant-understand-why-so-maybe-2-questions-1-is-this-the-idealsuggested-way-to-understand-if-an-asset-is-a-pngjpegheic-and-2-what-would-possibly-be-the-reason-why-i-would-get-this-error-sometimes-and-how-do-i-recover-from-it-so-i-can-get-the-extension-of-the-file-reliably-thanks","text":"to be a little clearer, the error code i got with this was code=3164 . Ive seen some suggestions that its possibly because the asset is in iCloud, but Im fetching the asset first and then calling this request. So it seems hard to think that its iCloud related.. but maybe it is? The best way to determine the kind of file for an original image would be to use PHAssetResourceManager -- ie, get the list of resources for an asset, then isolate the one with PHAssetResourceTypePhoto, and call: -[PHAssetResource uniformTypeIdentifier] and the above method doesn't require use of the network ah ok, that seems helpful and surprised I missed it earlier.. Thanks! :blush:","title":"Hey! So Im currently needing to get the file extension of a PHAsset from the user's photo library. At the moment the way Ive been directed to follow is to request requestContentEditingInput of the asset, and then get the extension from the fullSizeImageURL value. But im currently occasionally getting a [ImageManager] Media resource request failed to return valid data or url with error: Error with this request and I cant understand why.   so maybe 2 questions...  1) is this the ideal/suggested way to understand if an asset is a PNG/JPEG/HEIC, and  2) what would possibly be the reason why I would get this error sometimes? And how do I \"recover\" from it so I can get the extension of the file reliably? Thanks!"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-way-to-determine-if-your-app-has-edited-a-given-phasset-for-example-if-i-edit-a-photo-in-my-app-and-then-edit-it-in-a-different-app-is-there-a-way-to-know-that-my-app-has-edited-it-im-wondering-because-id-like-to-build-a-filter-to-only-show-photos-my-app-has-edited-as-opposed-to-all-edited-photos","text":"Given a PHAsset, if you wanted to know if your app owns the top level adjustment, then you might inspect the data in the PHAssetResourceTypeAdjustmentData resource. If another app has placed an adjustment on top of yours, you could tell by also inspecting data from PHAssetResourceTypeAdjustmentBasePhoto . You might inspect the value from -[PHAdjustmentData formatIdentifier] and see if it matches your app's bundle Another approach would be to keep track of edited assets' PHCloudIdentifier on the app side. So PHAsset.adjustmentFormatIdentifier does tell you the id of the last applied edit, though you can edit it in another app and then at that point you can\u2019t detect your app has edited it. Unless PHAssetResourceTypeAdjustmentBasePhoto does this but I don\u2019t understand how :thinking_face: as that provides an unaltered version of its photo asset for use in for use in reconstructing recent edits. if you're looking for \"ever edited with my app\" That\u2019s indeed what I want - filter out all photos except those that my app has edited even if they\u2019ve edited it afterwards in another app :slightly_smiling_face: I understand PHCloudIdentifier is kind of a heavy operation to convert to local identifiers, likely much too inefficient to loop over the entire photo library (or even as they\u2019re scrolling to show an \u201cedited\u201d indicator on the thumbnail). If a resource is present with the type PHAssetResourceTypeAdjustmentBasePhoto , it is an indication that an adjustment is applied to an asset, on top of another adjustment which didn't originate from the Photos app. Using requestDataForAssetResource... passing in the resource should give you back enough info to determine if the earlier adjustment is yours. 'Ever' edited with your app might require additional state -- as once as user reverts all adjustments, you would no longer be able to depend on adjustment data to answer that question. Ah yah, sorry \u201cever edited\u201d is not quite desired, just that there is an adjustment to that asset that my app performed. If they wipe out all edits, I don\u2019t want to indicate my app has edited it. Gotcha, yah that would do the trick if I needed to know that per photo asynchronously. For this use case it\u2019d need to be synchronous and efficient. I believe this data should be on disk already -- (in most cases does not require a download from network), it still might work for your use case, depending on how many assets you need to run this check against. (It probably is not efficient enough to run in a loop against a large collection of assets - but perhaps efficient enough to run once for all assets, then run incrementally based on changes to the library) Gotcha thanks! My gut is that wouldn\u2019t quite accomplish what I need. I previously filed FB5415719 about this but doesn\u2019t include a lot of details for what I intend to use this for which I imagine would be helpful. Would yall prefer a new bug report or I can send a reply on this one to provide additional deets? :slightly_smiling_face: <@U03HMCT187R> to make it efficient for scrolling, you can convert the PHCloudIdentifiers of edited photos to local identifiers, and fetch them all with +[PHAsset fetchAssetsWithLocalIdentifiers:options:] and keep that fetchResult around then in the cell, you can check if that asset has been edited via [fetchResult containsObject:assetInCell] , behind the scenes containsObject is very efficient and fast You can reply here with comments and I can add them to the FB. I think providing more context on how many assets you'd like to run the check on, the use case in the app, etc. would be helpful. Oh okay that approach makes sense! Though I can think of some drawbacks that doesn\u2019t really work for my use case. When the user edits a photo I\u2019d need to persist the cloud identifier for that asset, and sync that list of identifiers to all their devices, otherwise it wouldn\u2019t know about the edited photos from different devices. I also support editing photos from inside the Photos app, so need to use App Groups to try to update that in a shared location (which I actually don\u2019t think is possible because you don\u2019t get a PHAsset). If they go revert a photo in the Photos app, now my app thinks that photo is edited by my app but it no longer is. I\u2019ll compose a message detailing my use case. Thank you so much!! :hugging_face: I have an app (DateStamper) that loads up the user\u2019s photo library and allows them to scroll through a collection view of thumbnails, very similar to the Photos app. My app can edit photos as well (in the app and in a Photos editing extension). I would like to show a little \u201cedited\u201d indicator on each photo thumbnail if my app has edited that photo, more precisely, if that asset currently has adjustments that were performed by my app. To be super clear, if the user reverts edits to the photo, I do not want the \u201cedited\u201d indicator to show up anymore. I\u2019d also potentially like to provide a way for them to filter the photos to only show photos that my app edited in the collection view. I believe this may not quite be possible to achieve for this use case. You can determine if a PHAsset has been edited via PHAssetResource.assetResources(for: asset).contains(where: { $0.type == .adjustmentData }) (and there\u2019s new API in iOS 16 for PHAsset.hasAdjustments ) but this means any app could have edited it. I want to know if my app edited it. You can determine if your app edited a PHAsset by checking if PHAsset.adjustmentFormatIdentifier is one of your app\u2019s identifiers but this only gives you the id of the last edit performed. For example if you edit a photo in my app, then edit it in the Photos app or another third party app, you can no longer detect that your app has edited the photo. You could keep track of all the assets your app has edited via PHCloudIdentifier but this has some problems. The Photos editing extension doesn\u2019t have access to that, photos can be edited on other devices, and photos can be reverted outside of your app. Trying to track this manually would not accurately represent photos that are currently edited by your app. You could use PHAssetResourceManager to fetch the adjustment data and examine it to see if there\u2019s any created by your app, but this wouldn\u2019t really work for this use case, it would only really work in async contexts likely limited to a single photo at a time as opposed to a filter applied to all photos in the library. Thanks <@U03HU4PD371>! When you add this to the feedback will that show up as additional info to me? I\u2019d like to have that recorded for my future viewing too. I can go into follow-up with that as additional information if needed. I've added the discussion thread today to the radar, <@U03DJTBMHFF> would Jordan be able to see any additional info on his end?","title":"Is there a way to determine if your app has edited a given PHAsset? For example if I edit a photo in my app and then edit it in a different app, is there a way to know that my app has edited it? I'm wondering because I'd like to build a filter to only show photos my app has edited (as opposed to all edited photos)."},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-way-to-use-phfetchoptions-to-find-only-jpegraw-pairs-and-then-split-them-or-discard-one-half-of-the-pair","text":"There isn't any API for fetch options to choose assets based on JPEG+RAW original resources. And there's no way to remove an original resource from an asset, you'd have to create a new asset from one or more of the resources. Please do file a feedback to request support for searching based on R+J resource if that would still be useful for you In general, R+J Assets use the PHAssetResourceTypeAlternatePhoto resource to store one of the pairs. This tied with looking at the uniformTypeIdentifier would allow you to identify them. Unfortunately this will be slower than a fetchOption supported way. Ok thanks! My use case is basically trying to clean up a library and discard unnecessary RAWs so I'll look into the uniformTypeIdentifier option.","title":"Is there a way to use PHFetchOptions to find only JPEG+RAW pairs and then split them or discard one half of the pair?"},{"location":"wwdc22/photos-camera-lounge.html#in-my-testing-the-modificationdate-property-of-phasset-gets-updated-every-time-a-photo-is-viewed-in-the-photos-app-and-other-apps-even-if-no-edits-were-made-it-seems-to-behave-more-like-a-last-viewed-date-is-this-intended-behavior","text":"Yes this is expected, not specifically to reflect that the asset was viewed, but the modificationDate will reflect changes due to internal bookkeeping and processing. If you expected or would better be able to use a modification date property that reflects only changes to properties exposed on PHAsset I'd encourage you to submit a feedback request with your specific use case Thanks for the clarification!","title":"In my testing, the modificationDate property of PHAsset gets updated every time a photo is viewed in the photos app (and other apps), even if no edits were made - it seems to behave more like a last viewed date. Is this intended behavior?"},{"location":"wwdc22/photos-camera-lounge.html#with-icloud-shared-photo-library-it-sounds-like-photokit-basically-shows-just-one-system-library-like-normal-which-might-happen-to-have-shared-assets-if-the-user-has-the-shared-library-enabled-so-would-that-mean-for-example-if-they-then-disabled-the-shared-library-on-their-system-that-things-like-change-observers-and-the-new-change-history-api-would-get-notified-of-a-whole-bunch-of-deletions-since-the-shared-assets-would-no-longer-be-appearing-in-the-library","text":"Basically yes you\u2019re right. When a user disables Shared Library on their system, they get a choice of either keeping all shared assets or just the assets they\u2019ve contributed into the Shared Library. Based on the choice they make here, there could be a bunch of deletions appearing in the local system library","title":"With iCloud shared photo library, it sounds like PhotoKit basically shows just one system library like normal, which might happen to have shared assets if the user has the shared library enabled. So would that mean for example, if they then disabled the shared library on their system, that things like change observers and the new change history API would get notified of a whole bunch of deletions, since the shared assets would no longer be appearing in the library?"},{"location":"wwdc22/photos-camera-lounge.html#are-cloudidentifiers-globally-unique-would-two-users-ever-have-the-same-cloudidentifier","text":"The short answer is YES - they are unique. The more complete answer reflects what you get back from the lookup APIs when looking up a local identifier based on a PHCloudIdentifier : 1. In a set up where the user has enabled iCloud Photo Library the cloud identifier will be able to uniquely distinguish the asset and it's local identifier even when there are assets with the same image/video data 2. In the case where the photo library is not associated with an iCloud account (in iCloud Photo Library), it is possible for a cloud identifier to resolve to more than one local asset when looking up the local identifier via localIdentifierMappings(for:) - in that case the local identifier mapping result will indicate the error code: PHPhotosErrorMultipleIdentifiersFound Thanks! ... and you can get the set of potential matches for that cloud identifier via the error user info via the key: PHLocalIdentifiersErrorKey","title":"Are CloudIdentifiers globally unique? Would two users ever have the same CloudIdentifier?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-precalculated-hash-for-a-phasset-to-help-check-for-duplicates-ive-been-using-a-perceptual-hash-of-the-images-thumbnail-and-having-success-but-was-wondering-if-there-was-a-better-way","text":"There is not any sort of API around a pre-calculated hash for a given PHAsset. A perceptual hash of the thumbnail is a reasonable approach. Combining that with metadata information and asset resource information would be a good way of checking for duplicates across the library. Also check out VNGenerateImageFeaturePrintRequest in the Vision framework. It does something similar in concept to a perceptual hash. Thanks!!","title":"Is there a precalculated hash for a PHAsset to help check for duplicates? I've been using a Perceptual Hash of the image's thumbnail and having success, but was wondering if there was a better way."},{"location":"wwdc22/photos-camera-lounge.html#ive-found-that-if-i-request-an-image-thumbnail-using-phimagemanager-passing-the-original-option-for-the-imageversion-then-if-that-asset-hasnt-had-its-original-image-downloaded-from-icloud-yet-it-returns-an-error-the-only-workaround-ive-found-is-to-manually-request-the-original-photo-resource-via-phassetresourcemanager-and-create-the-thumbnail-myself-is-there-any-way-to-force-phimagemanager-to-download-the-asset-so-it-can-create-the-thumbnail-in-this-case-or-is-the-current-behavior-how-its-intended-to-work","text":"Do you have networkAccessAllowed set in the request options? Yes also - in your case do you specifically need the original file? if your goal is to get a thumbnail, usually 'current' is better and more efficient I would not expect an error to be returned in the case you describe -- if you have a case that repro's, pls file a feedback and we can look further. Yes, in my case I\u2019m displaying a preview of what something will look like when I copy a photo elsewhere, and I have an option in my UI to copy the unmodified original instead of the current one, so I\u2019d want to show a thumbnail reflecting the version that\u2019s going to be copied. To be honest I encountered this one a while back and haven\u2019t checked on more recent versions of macOS, so I\u2019ll check the behavior again to see if it\u2019s changed. Just seemed like odd behavior and wanted to check to make sure I wasn\u2019t missing anything. I see - yes in the case of an adjusted asset that makes sense. If the asset is not adjusted you'll get better performance using 'current'. Odd indeed - pls include all the options you're using to set up the request, and all the logs/error details you're getting, as well as a sample asset that produces the issue.","title":"I've found that if I request an image thumbnail using PHImageManager passing the .original option for the imageVersion, then if that asset hasn't had its original image downloaded from iCloud yet, it returns an error. The only workaround I've found is to manually request the original photo resource via PHAssetResourceManager and create the thumbnail myself. Is there any way to force PHImageManager to download the asset so it can create the thumbnail in this case, or is the current behavior how it's intended to work?"},{"location":"wwdc22/photos-camera-lounge.html#is-there-a-way-manually-create-depth-data-for-existing-images-im-looking-to-build-something-where-i-can-basically-paint-in-the-depth-information-into-images-that-are-maybe-not-shot-on-iphone","text":"Not sure if we have the appropriate people here right now, but I will reach out. From the photokit perspective the data would need to be in the original image, so a new PHAsset would have to be created with the appropriate depth information in the original image. I\u2019ll leave it up to the camera folks to chime in on the depth creation part. Check out Vision\u2019s segmentation API: https://betterprogramming.pub/new-in-ios-15-vision-person-segmentation-5c031a2f3822 This sounds similar to what the Focos app does Cool, I'll check out all your suggestions. Thanks everyone! :raised_hands: There are some other machine-learning models out there that estimate depth based solely on an image. One of them, FCRN-DepthPrediction , is available as a Core ML model in our model gallery. https://developer.apple.com/machine-learning/models/ Using a model like this along with the Vision framework you may be able to achieve exactly what you're looking for. Monocular depth estimation is super cool, the best results I've gotten are with the model \u201cMiDaS\u201d. I was able to convert it to coreml using CoreML tools and use it in my Swift Student Challenge submission (with some quantizing to reduce the size). They also have a newer transformer model, which I haven't been able to convert to CoreML using the builtin tools, but I'd like to try again in the future.","title":"Is there a way manually create depth data for existing images? I'm looking to build something where I can basically \"paint in\" the depth information into images that are maybe not shot on iPhone."},{"location":"wwdc22/photos-camera-lounge.html#in-the-new-icloud-shared-album-w-your-family-will-photos-shared-to-you-be-located-on-the-your-camera-roll-smartalbumsmartalbumuserlibrary-follow-up-will-all-of-those-photos-be-locally-available-in-all-devices-of-the-family-or-will-they-be-icloud-dehydrated-and-the-user-or-an-app-needs-to-do-an-explicit-download","text":"Yes! From a PhotoKit API POV, all Shared assets that are inserted via the iCloud Shared Photo Library feature would appear as normal assets and would be shown in all the usual places (Photos tab, For You tab, Albums tab) and be available through all the usual fetches Bonus followup question: The same per-device policy applies here based on what they have setup in Settings > Photos > [Optimize Storage OR Download and Keep Originals] on that device.","title":"In the new iCloud Shared Album (w your family), will photos shared to you be located on the your Camera Roll (smartAlbum/smartAlbumUserLibrary)? Follow up: will all of those photos be locally available in all devices of the family or will they be \u201ciCloud dehydrated\u201d and the user or an app needs to do an explicit download?"},{"location":"wwdc22/photos-camera-lounge.html#are-there-any-apis-for-editing-videos-shot-in-cinematic-mode","text":"There are not any at this point for editing the cinematic-ness of the video, but the video can be edited in the same way that any other video can be edited. Thanks for the reply! I know you can't comment on future plans but I hope to see this feature in the future. Thanks for the feedback and interest!","title":"Are there any APIs for editing videos shot in Cinematic mode?"},{"location":"wwdc22/swiftui-lounge.html","text":"swiftui-lounge QAs by FeeTiki Great session and thank you for all the new features! But, most importantly, was the cake as nice as it looked!?!?!?!? :D Yup! It was chocolate, and delicious. We had some deleted scenes with the sliced cake, but we don't release WWDC Bloopers and Deleted Scenes unfortunately Thanks for so many great improvements! Are any of these changes back-deployable to previous OSes (e.g., like the new Section initialisers were last year) Not this year! Last year we had some nice syntactic refinements that we were able to back deploy, but many of the features this year require fundamental new support in the OS. What is the best venue to ask SwiftUI questions during the year? Swift Forums is amazingly useful for Swift language and library questions, with many of the implementers frequenting it, but it is focussed solely on the Swift language itself. Is there any comparable place where y'all converse during the year? The developer forums are a great place to ask questions! We monitor them throughout the year and reply when we can, and so do other members of the community! I\u2019ve queued up a year\u2019s worth of questions for my 30 minutes in the lab :slightly_smiling_face: but this week is a bit overwhelming with all the new stuff too to make much of a dent. What I've appreciated more about the Apple Developer Forums is that engineers who work on the relevant frameworks will come in with their expertise. There's also full-year lab-style code-level assistance from https://developer.apple.com/support/technical/|Developer Technical Support to facilitate your SwiftUI questions. Don\u2019t be worried that you\u2019ll \u201cwaste\u201d your DTS contacts! When DTS confirms that the issue raised is a bug and a bug is filed by the developer, would it make sense if DTS can mark it as a verified bug so that it gets the attention of the relevant team? DTS commonly elevates that feedback to the engineering team. We have a fantastic partnership with DTS. They\u2019re just the best! Apple Developer Forums have lots of unanswered questions, especially the complex ones / edge cases. Also, there\u2019s sadly little engagement from the community as a whole, so if an Apple engineer misses the question, you\u2019re out of luck. Not sure I've ever seen a good answer on the Apple Developer Forums but I've seen my own questions asked w/o any answers many times. The results there are so low that I've never asked a question there myself. It doesn't seem useful. You\u2019ll never get an answer if you don\u2019t ask. :slightly_smiling_face: With graphics and custom layouts being added, when should we use native elements vs custom styling? How far should custom designs go? It entirely depends on your use case! We ultimately trust your sense of design on this one since only you know what\u2019s right for your app. To give some general guidance though: Take native elements as far as you can, and if you find you need further customization beyond that to finely polish the experience, then don\u2019t be afraid to use it! One quick disclaimer: It can be easy to drop to custom layouts prematurely. Stacks and Grids are incredibly powerful already, so be sure you can\u2019t get what you need with them before you use the new layout protocol. (edited) I saw that VStack and HStack conform to Layout. Does that mean using an if-else to switch between the two with the same content will now animate the change? Yes! To animate between layouts you'll need to switch between the layouts using AnyLayout for example: let layout = isVertical ? AnyLayout(VStack()) : AnyLayout(HStack()) layout { Text(\"Hi\") Text(\"World\") } Check out the Compose custom layouts with SwiftUI talk tomorrow for more details! Looking forward to it. Thank you! This sounds great! Looking forward to getting rid of some UIStackViews These kinds of examples will be in the code snippets soon too! Both that table layout code, and the grid morphing to a scattered layout will be included Hi great stuff so far. Thank you! I was wondering if there are any updates, or opinionated best practices when it comes to large scaled applications using SwiftUI in terms of state management and dependency management/injection. Thanks :v: SwiftUI is not opinionated on a specific architecture for state management. We offer the building blocks to build whatever best fit your use case. It might be good to sign up for a lab so that we can discuss the specific of your use case and give you targeted advices I tried signing up for a lab and was denied. It seems the demand is too high for what you guys are able to provide. That is an absolute shame You can also try the DTS Open Hours labs, and I can help you there as well. That's the one I signed up for... and was denied There are DTS labs throughout the week, so you may have missed the first set of appointments. There are still many more openings, though! It would be great to capture your use case in Feedback Assistant before the labs as well, so we can get started quickly or can follow up after the conference, if needed. I was hoping to have some type of lab each day. Having requested that one and then denied, precluded me from requesting any other lab for the day. It's frustrating. I don't know whether I'll get a single lab in this week at this rate. I'll try and use Feedback Assistant like you said, but I don't know about using it in this way... I've only ever used it to send bug reports and feature suggestions. This would be entirely different. I'm new to WWDC this year, and so far this has been a soured experience Thanks for the great session! Talking about Charts, will it be possible to add a gradient color filling the area underneath the line in a line chart? It is! I made one like this while putting the talk together, I can't remember exactly how I did it. I may have used an AreaMark that fills the area under the line, and they also stack to make some really cool area charts When you're making those kinds of charts, using Color.clear as a part of your gradient makes the Chart look a lot neater in light and dark appearances IMO Thanks a lot! This will enable me to migrate from my custom made charts to SwiftCharts in my existing app! :heart_eyes: I have to say, it's extremely fun to see how quickly you can make custom-rolled charts from various apps in Swift Charts. Like a speed run, only with accessibility, dynamic type, and localization built in There are several sessions about SwiftCharts as well! Definitely check them out. Where's this SwiftUI party at? And don't tell me the cake is a lie.. The cake was baked locally in San Francisco, and we even piped the SwiftUI bird on there ourselves! Using the new .frosting(.blue) modifier That is not real ^ The cake is, in fact, not a lie. :birthday: <@U03HL00QL68> made it! And the SwiftUI party is everywhere that you\u2019re using SwiftUI. Celebrate with us. :partying_face: Can we file feedback for a .frosting() modifier? Maybe it could wrap PencilKit or something. And make sure it tastes delicious, too. File that along with an enhancement request for a replicator. I need iOS to replicate the cake on demand. :smile: Is it an Apple cake? The cake will be sold in exactly 0 Apple Stores Does Grid replace VGrid? LazyVGrid and LazyHGrid arrange their children lazily and so are great fits for large amounts of content within a scroll view Grid requires all of its children be loaded up front and because of that has some powerful features that the lazy grids do not Yeah, check out the Composing custom layouts with SwiftUI talk tomorrow for more info Is there any new control in SwiftUI like the NSSegmentedControl used under tables to add / delete a row? This is a common pattern in macOS applications and the current SwiftUI segmented control does not fit to this use case. Good news: There\u2019s not a new control, but an old one :smile: Last year we introduced ControlGroup that enables building these kinds of controls (which you might have used NSSegmentedControl for with AppKit). You can create one with Buttons, Toggles, and more! Nice, didn\u2019t know about this one. Will try it out. Thanks! :thumbsup: What is the difference between .onChange and .onReceive modifier.? \".onReceive is like a combination of .onAppear and .onChange\", is this the complete and accurate picture? onReceive is specifically to subscribe to Combine\u2019s Publisher types and produce a side effect. onChange is used to produce a side effect when a property of your view changes. For example you can use that to produce a side effect when the scene phase in the environment changes. Okay, thank you for the explanation! If I use both .onReceive and .onChange on a Published property, will there be a difference in behaviour and is one recommended over the other in this case? It really depends on your use case: Is the value equatable? Is it a lightweight event? Does your view have constraints to adhere to, such as de-duping, debouncing, exponential backoff, etc. Curious if there's updates related to UI testing in SwiftUI apps? Or should I be thinking more in terms of testing the model layer that drives the declarative UI? Our recommendation is still to thoroughly test your model layer. In my experience, the best SwiftUI apps move their logic into model code, using SwiftUI as a declarative mapping from that model state to views. Then your tests of the model layer are effectively tests of the resulting UI as well :slightly_smiling_face: Is it possible enumerate a NavigationPath or replace certain elements? It\u2019s not currently possible to enumerate a NavigationPath . Because it\u2019s type-erased, the elements could only be exposed as any Hashable so aren\u2019t directly useful. Generally, if the set of things that can be added to a path form a closed set, which would be the case if you can usefully enumerate it, I\u2019d recommend wrapping your presented values in an enum with associated types. I\u2019m wondering if having a mutable collection of enum values might be generally more useful than using a NavigationPath. Snap! Then use an array of that enum instead of NavigationPath. Sounds like that could work better and might fit nicely with some form of Coordinator-esque style pattern. Thanks! Terrific updates all around! When animating the Font on a Text, when can we expect the font to smoothly interpolate instead of crossfade? Generally changing weights and sizes of the same font will interpolate, but not changing fonts Is it possible to set default focus on TextField when it appeared first time? Without workarounds with delay? Yeah, checkout the new defaultFocus modifier! :smile: https://developer.apple.com/documentation/swiftui/view/defaultfocus(_:_:priority:) Does the new NavigationSplitView preserve state when we switch to a new navigation destination like tab bar in UIKit, or do we need to roll our state restoration, like what we had to deal with using the old TabView? If you want to switch to a TabView in narrow size classes, you\u2019ll want to design a navigation model type that provides selection information for the NavigationSplitView in regular size classes and a different projection of the same information to a TabView that\u2019s shown in narrow size classes. I\u2019d love Feedback with specific use cases. I\u2019m very interested in making this easier. Does Transferable also support asynchronous behavior like NSFilePromiseProvider, e.g. if you do not have an file URL yet (e.g. cause a file first needs to be downloaded from a web server, but only when the drag operation ends) Definitely, the FileRepresentation transfer representation does what you\u2019re describing. <@U03HW7P2WTB>\u2019s amazing talk \u201cMeet Transferable\u201d will be out with more detail tomorrow ( June 8 ) :smile: What is the correct way to implement invalidation when conforming some type to DynamicProperty ? Should hosting another builtin DynamicProperty enough? Are these wrapped properties like @State or @ObservedObject guaranteed to work as expected in DynamicProperty ? Dynamic properties are fully composable, so any other dynamic properties you use inside of their declarations will be picked up on and invalidated properly. That means supporting invalidation as you would expect is often just a matter of utilizing the support already built into other dynamic properties, like @State How can I hide the disclosure indicator for a NavigationLink within a List? For example, if I was wanting to implement the built-in Reminders app with the grid icons in the List header. Hi, Christian. The disclosure indicator isn\u2019t currently customizable. With the new NavigationStack and path binding, you can use a regular Button and append your value directly to the path. I\u2019d love a Feedback about your use case for hiding the indicator though! Ooh. Appending directly to the path is gonna be super helpful. Yep, I think appending the path is the best solution. Thanks <@U03HW7P0HQR>. This is an example of the use case, mimicking the Reminders app. Noticing the disclosure indicators at for All and Today . Submitted as FB10082608 Hello everyone! I'm amazed by the new text animations, can we animate text with any font? Does the font need to respect some criteria? Thank you! Yes, you can use any font. Thanks a lot! However, you can't go from one font to another or animate from one font family to another. For example, italic to non-italic will not animate Clear thanks, how about variable fonts that can change weight? I mean if I use static fonts (that have one file for bold, one for medium etc\u2026) I suppose I cannot either change weight becas *because it would be another font family, right? Correct, I wouldn't expect that to ~work~ animate Hi, is there anything different visually, to the user, between a NavigationView and a NavigationStack? They\u2019re both structural components, but they do behave a bit differently. Can you elaborate in what way they behave differently? NavigationView renders differently depending on how many views you pass to it and what navigationViewStyle you set on it. NavigationStack takes a single root view and always renders as a stack. Gotcha, thanks for the clarification :) When opening the Control Center via the Menu Bar Extra, the button remains highlighted while the Control Center is visible. Is there a way to replicate this using the new MenuBarExtra API? Hi - thanks for the question! I think this is likely just a bug with the framework at the moment, and that it should behave similar to Control Center by default. If you could please file a feedback for this, I'd appreciate it. Thanks for your response. Honestly, I have not tried the new API yet, I just saw the behavior in a WWDC session video. I will try it once I have installed the beta and will file a feedback if necessary. Excellent, thanks! <@U03HHJH9C66> FB10086096 Is it possible to reorder items and insert into an outline group? SwiftUI doesn\u2019t have much in the way of automatic support for that, I\u2019m afraid. I\u2019d love a Feedback with details of your specific use case. We did expose public API for disclosureGroupStyle this year, which might be worth investigating. The use case would be a sidebar that the user can organize on their own. I was thinking this could be a normal list, but there a limits with cross-section item moves that I end up tripping over. Makes sense. With the current API, I\u2019d use a view model that vends the items as if they are a flat array, then style the items based on their apparent depth. That would let you use onMove or the awesome new editing support on List to update your view model. Yeah, I think biggest trick there is figuring out that \u201coffset 20\u201d is actually \u201coffset 2 of parent 4\u201d or something like that. Thanks for your input. It's not the first time I've read this solution, and since there's nothing new in this area for 2022, I think I may need to take the plunge and go for it. Pages and Numbers are both document-based apps which have a beautiful onboarding experience. How do I build such an onboarding experience for my document-based app using SwiftUI? Hi - this is a great question! While we do not currently have a high-level concept in SwiftUI for this type of flow, it may be possible to combine the new Window scene together with the new OpenDocumentAction and NewDocumentAction to create it. You would need to define the Window as the primary (first) scene in your app, however. It's possible there may be some drawbacks to this approach, so I'd love a feedback for an enhancement in this area. Thanks for the idea. I'll try! I currently use NavigationView in my app with NavigationLink to go from parent view to child destination view. It works fine for what I want. Do I need to migrate to using NavigationStack in my case, especially if I\u2019m just linking to destination views and not needing to link by value? NavigationView is only soft-deprecated and so will continue to work fine in all its existing use cases with no compiler warnings. If writing new code, however, we recommend you start using NavigationStack . Ok. So NavigationStack with a NavigationLink to a destination view will work the same for me? Yup! ViewThatFits has some strange effects when the size is animated. Is it intended to work with animation or should it be avoided? ViewThatFits should support animations, could you file a feedback for this one and share right here? ( hi <@U03HL00QL68> :wave: ) Will do Hi Christian! FB10082380 On iPadOS in a NavigationView, Is it possible to switch between two column and three column layout depending on what is selected in my sidebar? It isn\u2019t possible to do that directly. Often those style of UIs can be built by swapping the view in the detail area between a single root view and an HStack with two-elements. That is, instead of conceiving of it as a two or three column NavigationSplitView , think of it as a two-column one, where the detail column itself has one or two columns. Great, that's exactly what I'm doing now. It's working fine, just a little extra work to coordinate with my compact view. Thanks! Awesome! I\u2019m glad you found a solution. Is there a better way to prevent DynamicProperty from invalidating a view\u2019s body based on different criteria? I am currently doing this with a private ObservableObject backing that manages its objectWillSend calls, which seems to works well but also feels like I am doing some backflips (the context is being able to scope in on specific changes on an ObservableObject model for performance reasons). There is no direct way to prevent DynamicProperty from updating. What you are doing is a way to do it. If the purpose is to manage ObservableObject invalidation I would suggest consider refactoring your model into multiple model. Keep in mind that you can also implement your own objectWillChange Publisher. Makes sense. Thanks! Previously in NavigationView on iPad, the detail view would be displayed with the ForEach list being collapsed in a sidebar. With the new NavigationSplitView, can I use a modifier to not collapse the ForEach list? Thanks for the question. NavigationSplitView takes a columnVisibility binding. You can set it to .all to reveal all the columns programmatically! Great, thank you! Is it possible to use SwiftUI Navigation for iOS 15? How can we transition to it on the existing project? To learn how to transition your existing projects to the new navigation types, see this handy migration guide: https://developer.apple.com/documentation/swiftui/migrating-to-new-navigation-types Annnnd add to reading list. Thanks! Hi Curt, in your talk you used objectWillChangeSequence but I can't find it in the new Xcode beta, where is it? Thanks Hi, Malcolm! That\u2019s a method that I implemented on my NavigationModel but didn\u2019t fit into the slides. Ok thanks It\u2019s in the Copy Code for the talk accessible in the Developer app though I think you can use for await _ in store.objectWillChange.values in a similar way? That\u2019s close, Christian! You have to buffer the values too. var objectWillChangeSequence: AsyncPublisher&lt;Publishers.Buffer&lt;ObservableObjectPublisher&gt;&gt; { objectWillChange .buffer(size: 1, prefetch: .byRequest, whenFull: .dropOldest) .values } Great thanks, for some reason the copy codes aren't appearing for me Bummer. There\u2019s also a sample app with similar code: https://developer.apple.com/documentation/swiftui/bringing_robust_navigation_structure_to_your_swiftui_app The sample app is a little more complex than what I shared in the talk. <@U03HW7P0HQR> Instead of objectWillChangeSequence would it be possible to use onChange(of: perform) to track any selection changes and update @SceneStorage data variable? Yep. That approach will work too. I used task in the talk so the restore and save code was in one block, but splitting them into separate onAppear and onChange should also work. How does ViewThatFits decide if a view fits? I find it a bit strange that Text with a line limit of 1 still \"fits\" even if it's truncated for example. ViewThatFits consults the view's ideal frame size for both dimensions. The latter behavior isn't intended. A feedback # and keeping an eye on later seeds is a good strategy here Adding a fixedSize(horizontal: true, vertical: false) seems to fix the issue but also seems unnecessary. I\u2019ll file a feedback FB10082577 2 for Ryan :slightly_smiling_face: Does Swift Charts have a way to highlight values on a line chart for a specific X position? There's a Swift Charts Q&A at 11am PT on Thu. Please ask there! In the meantime, you can always try drawing an additional mark for the highlight. Its possible to modify the .foregroundStyle to have a different color per X position, it depends on what kind of highlight you want to call out. It might be possible to do an annotation as well but the charts team will be the experts! The Build a Productivity App for Apple Watch talk, coming tomorrow, has an example of exactly that! https://developer.apple.com/wwdc22/10133 You can use PointMark (possibly with annotations) to highlight values on a line, such as: Chart { // Create the line. ForEach(data) { LineMark( x: .value(\"X\", $0.x), y: .value(\"Y\", $0.y) ) } // Highlight a point. PointMark( x: .value(\"Highlight X\", 10), y: .value(\"Highlight Y\", 50) ) .foregroundStyle(highlightColor) // You can also add textual annotations to the point: .annotation(position: .top) { Text(\"Highlight\") } } I am sorry for not being clear. What I want is having a vertical line when I tap on a specific X position so that all datapoints on that X position are highlighted. You would decompose the problem into 1) using a chart proxy to get the x position for a tap and 2) highlighting the data points at that x. The lollipop tooltip we have in the Swift Charts sample app may be a starting point to look at. Is there a way to access DragSession from DropDelegate (like UIKit)? Hi Harry, this API is not available. However, we'd be interested in learning more about your use case here. Could you please explain your scenario and the expected behavior in https://feedbackassistant.apple.com|Feedback Assistant for the SwiftUI team to consider for a future release? Will do One primary use is to determine if the drag started from the same app (either same view or from another scene) The other reason is to use that to pass data (where NSItemProvider works but is async and may be out of order) This could be semi-related, but at the moment if a drag begins and ends without any movement (i.e. you let your finger go immediately after the drag was activated) there\u2019s no communication back to the app. So, for example, if the drag trigger activates a specific UI state, then when the drag is released, it won\u2019t be de-activated because it considers the drag to still be (incorrectly) active. Yes, I think both of these use cases should be captured as feedback. Providing context like this is very helpful in prioritizing new features. Thank you, both how can i get simething similar to autolayout priorities, of being able to have 2 views in a stack equal but only up to a certain width You can pass any kind of information you like to an adopter of Layout . So that \"certain width\" could be passed to the instance of the Layout itself. And the layout priority can be provided via the subviews proxy of Layout . In the swiftinterface file there is discussion of this: /// Views have layout values that you set with view modifiers. /// Layout containers can choose to condition their behavior accordingly. /// For example, a built-in ``HStack`` allocates space to its subviews based /// in part on the priorities that you set with the ``View/layoutPriority(_:)`` /// view modifier. Your layout container accesses this value for a subview by /// reading the proxy's ``LayoutSubview/priority`` property. You can see the rendered version of that text in the docs under the heading \u201cAccess Layout Values\u201d: https://developer.apple.com/documentation/swiftui/layout Thinking about this more.... Reasoning about this in terms of autolayout might make things tricker than they need to be, especially for 2 views. For instance you could instead use a custom LayoutValueKey to say that View A should take 70% of the proposed size, and View B should take 30%. Rough sample code: MyCustomLayout(widthThreshold: 300.0) { viewA.oversizedWidthPercentage(0.7) viewB.oversizedWidthPercentage(0.3) } i was thinking something more like breaking \"constraints\" due to certain conditions based (if possible) Is there something similar for simple Views? E.g. a view which calculates its required height based on the available width. What's the best practice to solve something like this? What's the intended use for DynamicProperty 's update() method? It seems most of the time the DynamicProperty 's members are @State ful objects managing their own state and using update() with them requires a dispatch async (to the next runloop cycle). You\u2019re definitely correct that the state management dynamic properties do is usually handled using their own sub-dynamic properties, and custom logic! The update method is called directly before the corresponding view\u2019s body is called, and is more a place for any logic that needs to happen before body runs to occur. So, for things that I assume are not stateful in nature, just derivates of the state? That tends to be the case. That said, there may be some examples of property wrappers that use information about when the view is rendered to drive stateful systems. When debugging, I\u2019ll sometimes use a DynamicProperty with an Int wrappedValue that increments whenever the surrounding view\u2019s body is drawn, for example :slightly_smiling_face: That\u2019s a neat trick! Thanks for sharing and for the answer! Of course! Thanks for the question! Can we set the NavigationPath from onContinueUserActivity and onOpenURL? Hi - great question! This is certainly something you can do, and is a good example of how you can support something like deep linking, for example, by parsing the URL provided to onOpenURL and using it to construct your navigation path. Is there a way to show a preview of a view when the user taps and holds on a NavigationLink (similar to how a website is previewed when holding on a link in Safari)? I tried using the contextMenu modifier, but it only seems to render Button views. NavigationLink( ... ) .contextMenu { ViewIWantToPreview() // is not previewed Button { ... } // other options show correctly } Yes, use the new contextMenu overload that accepts a preview view. NavigationLink( ... ) .contextMenu { Button { ... } } preview: { ViewIWantToPreview() } Wow, handy. Nice work! Thank you, this seems to work quite well! This was one of the things I was stuck trying to figure out for a while while making my Swift Student Challenge submission (and ultimately never figured out :sweat_smile:) Here\u2019s some more detail on that new modifier: https://developer.apple.com/documentation/swiftui/view/contextmenu(menuitems:preview:) Can I have the user tap the preview to open another view (the \u201cfull\u201d view) as well? I'm playing around with it in Xcode previews and it seems like tapping the preview just closes the context menu. No, it's not currently supported\u2014 please file a feedback request. Alright, done! FB10083104 Thanks for the SwiftUI improvements this year by the way, they've been great! Hello! Thank you for amazing sessions! I am curious is it possible to navigate from SwiftUI view to UIViewController. Is it possible to pop out from the SwiftUI view back to UIViewController? I\u2019m glad you\u2019re enjoying the sessions! You can push a UIViewRepresentable onto a NavigationStack , and use that to wrap a UIView . That works with the view-destination NavigationLink s inside a NavigationView as well for previous releases. Hello all, i believe this is the channel for this question. I\u2019m wanting to get your take on how SwiftUI\u2019s data flow can change how we architect our apps? It seems as if, with EnvironmentObjects, ObservableObjects and possibly more, they change where you get your data from, which could cause variations in, let\u2019s say MVVM. What do you guys think is a good way to make use of these property wrappers and could it change how we architect things? Thanks! The data flow primitives we provide in SwiftUI were designed to be agnostic to the way you design your model. Think of them less as parts of a specific design pattern and more as tools to allow you to hook your model (however you think it would be best designed) into SwiftUI. If you want more information on this, I recommend you check out the fantastic talk by my colleagues, Raj, and Luca: https://developer.apple.com/videos/play/wwdc2020/10040/|Data Essentials in SwiftUI Thanks for the response, that\u2019s an excellent way of putting it. It\u2019s definitely difficult explaining this to an Android developer who relies on older \u201carchitecture\u201d or design patterns. This will probably help a lot. I\u2019m not sure if this would be good to add here, but i created some SwiftUI pattern i think could be beneficial for my team and it leverages wrappers. The thing is, it\u2019s difficult convincing others that it could work since this is how SwiftUI handles data flow. I will definitely check out this video though, this is what i was envisioning too, maybe you can provide some feedback if any: Providing feedback on specific designs is hard to do in this venue, but definitely sign up for a lab! They tend to do better for long-form discussions / questions. Great! I will sign up for the SwiftUI Lab thanks. <@U03HKVDCL7N> Is there any session explaining how to debug SwiftUI views (techniques such as Self._printChanges() )? Sometimes I notice @96 has changed being printed but wasn\u2019t sure which dependency was responsible. I suspect it was @FecthRequest but wasn\u2019t sure There is the SwiftUI Lab which i think would be best fit for any SwiftUI in depth question. You can check it out here: https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/upcoming?q=SwiftUI Fingers crossed it gets accepted, i have a lot of feedback IDs and doubts :slightly_smiling_face: What is the best way to resign (text field) focus from some distant view in the view hierarchy. scrollDismissesKeyboard is a great step in the direction I need, but I'd like to programmatically trigger that same behavior, for example, on some button tap. For example, looking at ways to replace this code: UIApplication.shared .sendAction(#selector(UIResponder.resignFirstResponder), to: nil, from: nil, for: nil) // as an action to perform on a view: extension View { func resignFocus() { UIApplication.shared.sendAction(...) } } You can do this with the Focus State API: https://developer.apple.com/documentation/swiftui/focusstate You want to bind a focus state property to your text field using View.focused(_:equals:) , and then set the binding's value to nil / false from your button action as a way to programmatically resign focus and dismiss the keyboard when the bound text field has focus. Making the action available to distant views is a matter of arranging your app's data flow appropriately. There's no single answer, but for example, you could declare your focus state property on your scene's root view and pass a reset action down to any descendant views that need it. Or if the action is created by a leaf view, you can use the preferences system to make the action available to ancestors. Preferences is an interesting idea. Trying to make the focusstate \u201cglobally accessible\u201d was challenging. I\u2019ll give these suggestions a shot! Thanks. Not sure if this was already asked before since it's such a common question. But, what's the recommended way to use a @ViewBuilder for custom components: calling it right away in the init() and storing the view, or calling it later inside the body and storing the view builder itself? We\u2019d generally recommend resolving it right away and storing the view Thank you, much appreciated! Storing the view will have better performance than storing a closure (potentially avoiding allocations). Of course, if you need to dynamically resolve a view from a closure (such as if it takes parameters), then storing the closure is also fine! Is this the same for Layout? The example code for AnyLayout shows creating a layout in the body instead of in the View. You can now (last year?) also declare the viewbuilder as just a property. In SwiftUI 1, you had to manually implement the init, but no longer needed. Best (IMHO) to use synthesized init when you can. struct MyView&lt;Content: View&gt;: View { @ViewBuilder var content: Content } (I believe is the correct code, coding from memory) Yes you can attach @ViewBuilder to properties! You can actually support it on both view and closure properties, depending on whichever you need: struct MyView&lt;Content: View&gt;: View { @ViewBuilder var content: Content // or @ViewBuilder var content: (Int) -&gt; Content } > Is this the same for Layout? The example code for AnyLayout shows creating a layout in the body instead of in the View. Apologies, not sure I fully understand the question. Just in case: creating views in body is fine, we\u2019re specifically talking about stored properties , and storing views versus closures for creating views. Sorry not sure if this is not the right place to ask this. When a view uses ScrollViewReader and the parent view\u2019s state changes, the the child view gets reloaded and the scroll position in the child view changes. This happens only when scroll(to:) has been executed. I have filed a feedback FB10037533 Talking about this, can you confirm it\u2019s best for performance to have the @ViewBuilder be a computed (lazy ?) var than a function in my View struct whenever possible? BTW there is a 3rd-party article explains why the view is preferable than the closure https://rensbr.eu/blog/swiftui-escaping-closures/ (If you don\u2019t mind posting 3rd-party article) When using the Layout protocol, is it possible to resolve an alignment guide for the layout container, not a subview, to align the subviews relative to the container correctly? For instance, a Layout that proposes size (100, 50) would resolve HorizontalAlignment.center to 50 (if unmodified). If not, what is the correct way to do this? You may be able to use the alignmentGuide modifier to define how an alignment guide for a view will resolve My goal is to get the CGFloat value from the container view for its own alignment guides in the placeSubviews function. (Specifically, to do a layout similar to VStack where the children align in the left middle or right) Well, you get the bounds of the region that you are laying out inside of, so you can know the midpoint in each dimension, for example You can also set explicit alignment guides for the layout container by implementing either or both of the explicit alignment methods: \u2022 https://developer.apple.com/documentation/swiftui/layout/explicitalignment(of:in:proposal:subviews:cache:)-8ofeu \u2022 https://developer.apple.com/documentation/swiftui/layout/explicitalignment(of:in:proposal:subviews:cache:)-3iqmu Ok, thank you. I also thought maybe using the largest subviews\u2019 guides as the base, then it would work with custom AlignmentIDs What happens to a subview if you don't call .place() on it in placeSubviews in a Layout? What happens if you call it twice? If you call the method more than once for a subview, the last call takes precedence. If you don\u2019t call this method for a subview, the subview appears at the center of its layout container and uses the layout container\u2019s size proposal. I found this function a bit strange as I was expecting it to return an array of placements instead of running a place function. Is it more efficient the way it\u2019s set currently set up? Actually, one more follow-up. Is there a way to simply not place a view or have it hidden? Imagine a coverflow style view where you only want to show a small set of subviews. You interact with the subviews from within the layout protocol via the subview proxies that you get as input. This lets you do things like propose a size or place the view, but not perform arbitrary operations on the view, or apply arbitrary modifiers. If there\u2019s something specific that you\u2019d like to do but can\u2019t, please do file a feedback with your use case. When is onAppear/onDisappear called? How do they compare to the UIKit equivalents of did/willAppear? hi, the framework makes no guarantees on the specific timeframes on when these methods are called, but you can be sure that onAppear will always be called before the view is visible to the user and onDisappear will never be called while the view is on screen. We have recently updated the documentation for these methods online to clarify these details; see https://developer.apple.com/documentation/SwiftUI/AnyView/onAppear(perform:) and https://developer.apple.com/documentation/SwiftUI/AnyView/onDisappear(perform:) in terms of a comparison to UIKit, there is not a direct parallel. The UIKit methods are invoked specifically around when views become visible/not visible to the user, whereas the SwiftUI calls are tied more to when the views are constructed/torn down, rather than visually presented. Love the new toolbarBackground(_:in:) and toolbarColorScheme(_:in:) modifiers in iOS 16! Is there any way to have them take effect always? They seem to only make a difference when content scrolls up beneath the navigation toolbar and using something like .toolbar(.visible, in: .navigationBar) doesn't seem to make a difference. (My basic goal here is to control the navigation bar background color and the status bar style, so if there's a better way to do that please let me know!). Hi - thanks for the question. .toolbarBackground(.visible) should work for this. If you are not seeing that, please file a feedback Okay! Once I file the feedback I'll drop the # here FB10084072 Is it possible to have a presented view that switches between the popover/sheet presentation styles depending on the size class (i.e., sheet in compact, popover in regular)? Yes! You can do this by creating a custom view modifier that uses looks at the UserInterfaceSizeClass . Custom modifiers are great for conditional combinations of other modifiers, views, etc. and this is a perfect use case :slightly_smiling_face: Brilliant, thank you! We have an example of this use case in the https://developer.apple.com/documentation/swiftui/bringing_multiple_windows_to_your_swiftui_app|BookClub sample app , in ProgressEditor.swift . Doing it this way, the popover/sheet disappears when the size class changes from regular to compact (though not the other way around) due to window resizing on iPad. Does this seem like a framework bug or something I've overlooked? FWIK it's a good practice to have state changes the deeper in the view hierarchy as you can get, in order to avoid redundant redraws. Are there any best practices to achieve that besides breaking views down to tiny elements and composing them together? The right way to structure your state varies by app. I\u2019m interested in understanding your specific use case. Are you asking this when using @State or ObservableObject ? Both actually. My use case involves a ViewModel which has @Published variables, together with diff element which is kind of a half drawer that involves @State for it's open/closed state. For State, I would generally recommend defining it at the level where you are going to use it. For your ViewModel it could be useful to break your model into multiple ObservableObject that are specific to a specific functionality (like a cell, or a single screen). But I would avoid prematurely breaking you model into very small pieces. SwiftUI does diff your view and will only re-render if it detects that something has changed. \u2022 Everything suggested in the Demystify SwiftUI Session from a year or two back is definitely useful \u2022 Keeping Identifier s separate from values is recommended generally (but not a hard and fast rule), especially in Navigation APIs. Are there any types of macos apps or interfaces where you would still recommend using appkit rather than swiftui? I'm yet to invest the considerable amount of time learning swiftui and I keep reading mentions of people saying it's still a mixed bag for macos development, so I don't want to potentially waste time. Thanks! Good question! Across all platforms, we\u2019d recommend comparing your needs to what SwiftUI provides (so no hard rules/recommendations) \u2014 and keeping in mind that you can adopt SwiftUI incrementally. Within Apple\u2019s own apps on macOS, we\u2019re ourselves using the full spectrum of approaches. From just a specific view/views in an app, e.g. in Mail, iWork, Keychain Access; to an entire portion of the UI or a new feature, e.g. in Notes, Photos, Xcode; and all the way to the majority of an application, e.g. Control Center, Font Book, System Settings. But in the end, I\u2019d recommend starting with a part you\u2019re comfortable with and building up from there! You should look at SwiftUI as another tool in your toolset in enabling you to build the best apps you can. Great answer. Thanks. When using the new MenuBarExtra in window style is it possible to control the width and maximum height of the content window? Hi - great question. The window size should be derived from the content provided to it, though we do impose a minimum and maximum on the resulting size. If you are seeing unexpected behavior in this area, please do file a feedback with a sample to reproduce it, and we can take a look. I experience a pretty large minimum width and a pretty small maximum height. I'll file a feedback with a project. Can text animate fluidly when using matchedGeometryEffect and one is set to a fixedSize(horizontal: true) and the other is not? Hi Bardia, I'd expect that to work, especially because text transitions work well when the text is changing layouts (multiple lines/alignments). We realize that it can be a little tricky to know how far exactly you can push those beautiful text transitions. Have you tried this out and seen otherwise? Ah I can reply now :slightly_smiling_face: Sorry for the late response (it was very late in London last night). Yep, I\u2019ve tried and have logged FB9156668 including code/clip. Basically it animates perfectly back-and-forth when both text are visible on a single line. However, when the one text goes over two lines \u2013 due to larger text size for example \u2013 and me allowing it to (via fixedSize(vertical: true)), this causes the animation in either direction to not be fluid (best demonstrated in the video). I think that feedback is a different one you've filed. That one looks Text Avoidance related Whoops! FB10101271 Hello, is it possible to present a popover as popover on iPhone, like it's the case with iOS? On iPhone, the .popover(\u2026) modifier always presents as a sheet. There isn\u2019t currently a way to customize that behavior. I\u2019d love a Feedback with details of your use case though! <@U03HW7P0HQR> On a similar note, confirmationDialog for iOS presents an alert, iPadOS presents a popup, macOS presents an alert. Is it possible to make it present an action sheet for iOS? Please file a feedback with your request! (and feel free to paste that feedback in here once it\u2019s filed) If a 3rd party solution is ok, I made a library: https://github.com/aheze/Popovers Hello! Talking about the Date Picker with multiple dates, can they allow to choose date ranges (like when you book a flight), so with just two dates possible and the highlight spanning over all of the days in between? Hi Cristina, currently, MultiDatePicker only supports non-contiguous date selection. So range selection is not supported at the moment. Speaking of booking flights... https://developer.apple.com/documentation/weatherkit/fetching_weather_forecasts_with_weatherkit|FlightPlanner sample app now available. Would be great to have, maybe also with a two months view! If you think you can consider something like this in the future I could file a feedback Yes please do! We'd highly appreciate that > Speaking of booking flights... https://developer.apple.com/documentation/weatherkit/fetching_weather_forecasts_with_weatherkit|FlightPlanner sample app now available. Great! I\u2019m downloading it, thanks! When creating a MenuBarExtra , is it possible to create something that runs on its own, separate from the parent app? I have an app that users can save data to by means of various app extensions. If the app is closed, however, those changes don't get synced to CloudKit. I was thinking of having a menu bar app that would essentially be my workaround: always listening, always active. Can a MenuBarExtra do that for me? Hi - you can certainly define a SwiftUI App with only a MenuBarExtra Scene . If your app should not show a Dock icon, that will require some changes to the Info.plist file, which should be noted in the documentation for MenuBarExtra See https://developer.apple.com/documentation/swiftui/menubarextra Posting that bit of documentation here as well: > For apps that only show in the menu bar, a common behavior is for the app to not display its icon in either the Dock or the application switcher. To enable this behavior, set the LSUIElement flag in your app\u2019s Info.plist file to true . Good to know. So it sounds like I would want to create a separate, MenuBarExtra -only app. Can I bundle that with my Mac app on the App Store? (Wrong lounge, I know\u2026) Hmm, I'm afraid I do not know the answer to that App Store question. That\u2019s alright. I had to ask anyway. Do MenuBarExtra\u2019s support other SwiftUI features, like keyboard shortcuts, so I could invoke it or an action it has via a (global?) shortcut? Are Deep Links possible in SwiftUI? If so, does it work well? Are there any limitations vs what is possible in UIKit? Check out the SwiftUI Cookbook for Navigation session and the What's New in SwiftUI session for 2 examples of deep links They are indeed possible, and we think they work pretty well :wink: Of course, there's a lot of routing to consider with any app \u2014 for instance if your deep link is behind some authentication token, you'll need to do the extra work there. The general idea is that a deep link is certain destinations presented \u2014 in order \u2014 on a navigation stack, and with this years' new APIs you can have full control over what is on the navigation stack. What architecture is recommended for Enterprise apps when using SwiftUI? For instance: Coordinator Pattern, Dependency Injection Containers, etc. Thanks for the question! One of the strengths of SwiftUI as a framework is that it works well with a variety of architectures. We might be opinionated about API design :slightly_smiling_face:, but we want you to have the flexibility to design the data model that works best for your app. The particular data model of any individual app depends on the domain of that app. For example, a car rental app might have a very different architecture from a news reader app. Can you recommend a website that provides various examples of architecture using SwiftUI? One challenge I\u2019ve come across is that the coordinator pattern doesn\u2019t translate well to SwiftUI. You will often have NavigationLink s in your views, whereas the coordinator pattern wants to separate that logic from the view itself. But this years NavigationStack is going to make new coordinator-like patterns much easier to implement. I\u2019m curious to experiment with that this week. Is it possible in general to use TimelineView as a \"global clock\" that can be synced by all the devices (that use the app) in order to display/show something at the same exact time? (if so, what precision should be expect?) While TimelineView can be used to update views at a specific point in time, it isn\u2019t designed specifically for cross-device synchronization. You may be able to use it for that kind of use case, but it wouldn\u2019t provide any better precision than using a Timer or other scheduling mechanism That presentation was simply beautiful. I am awestruck at how SwiftUI has accelerated this year. Congrats to the team that built it, and thank you. Thanks so much! We listen closely to community feedback. We may not always respond directly to the Feedback you file \u2014 there\u2019s a lot of it :slightly_smiling_face: \u2014 but it really helps us prioritize the work. We love seeing all the great things you all are building! S0 - AnyLayout could be user selectable (if selection changed @State)? Yes! As Paul showed in the video any State or property can drive conditional logic to choose between different Layouts using AnyLayout That is very powerful! Gets rid of all sorts of Case and If views :grinning: Can a column span partial columns in Grid? i.e. If I have 8 items in a three column Grid, I want the last row to have two cells that are 1\u00bd columns each. The column spanning uses an integer, but you could try doubling the number of columns you're using Hmm\u2026 that could work, thanks! Using integer multiples works really well. If you want one view to be 3/5 of the width and the other 2/5 you just use 3 and 2 on each (assuming they each take all available space) Can I use custom layout to create a hierarchical tree map/view of rectangular boxes of data items connected with lines? That\u2019s a fascinating question, Andrew! Layout could readily handle positioning the boxes. The lines would be the interesting part. Sounds like a good challenge :slightly_smiling_face: I'm just not sure of any other way to do this currently for an app view I want to do. You might experiment with using the layout values to route information. You could also experiment with using Canvas to do the drawing, though that\u2019s a low-level approach that wouldn\u2019t give you accessibility out of the box. Check out the accessibilityRepresentation modifier for that. Any ideas on how to \u201croute\u201d? Ultimately, that\u2019s \u201cjust\u201d math. There are a few open source frameworks for diagram drawing. You might be able to see what algorithms they are using. Maybe alternate row of boxes & row of custom Paths? Use the layout info from the boxes to feed the custom Paths. In fact you can do it using SwiftUI 2021. http://objc.io|objc.io made a series of video to demo how to draw tree image using pure SwiftUI (sorry for 3rd-party resource) https://talk.objc.io/episodes/S01E293-advanced-alignment-part-3 Do LayoutValueKey values travel up through nested Layouts? Also, how are they different from PreferenceKey? LayoutValueKeys only travel up to the nearest Layout container OK, good to know One reason for this is for Layout to facilitate building of encapsulations that can be composed without having unanticipated effects on the other views that surround it Do all system stacks implement the Layout protocol for use in AnyLayout? Is this documented anywhere? The goal is for as many system provided layouts to conform to the Layout protocol as possible, including the stacks. You can find what protocols a type conforms to in the developer documentation Yup, scroll down on this page: https://developer.apple.com/documentation/swiftui/hstack Thanks! What examples of custom layouts do you think a new layout protocol is a best choice for? Flow layouts are great for it. There\u2019s a good example in the Food Truck sample app. Whatever you can imagine :upside_down_face: In the State of the Union we mentioned Flow and Radial layouts as good examples A custom layout can be a good fit if there's something super custom in your app I am thinking of a dynamic layout case for seating order around tables. User could create a new Table, specify if the table is rectangular, oval or circular, and specify the number of guests at each side. We could then get a layout for the table as specified, and the user can fill in the guests. In code terms, this could mean that the specific layout should be created dynamically. Would something like this be possible? It's an even better fit for building a generic layout container that you can reuse across your app (like Flow referenced above) Or if you like bee hives\u2026 https://twitter.com/ryanlintott/status/1534367196251574272 Even really subtle, nuanced layouts can bring a little extra level of polish \u2014 plus be composed together! For higher level features like the new form style on macOS we\u2019re using several small custom layouts to get it to perfectly match what the design specs described Jan, that sort of thing is definitely possible! In fact, in the what's new in SwiftUI talk, they show an example of a layout which is very similar to what you\u2019re describing to make a seating chart for SwiftUI\u2019s birthday party! Thanks <@U03HKVDCL7N>, that's great! Yea I saw that, I just hope we can let user specify the dimensions/shape of the table that talk is what got me going on the idea :slightly_smiling_face: Is there a way to build a Layout that would effect each child view with ViewModifiers? Let's say I always want to style the first view one way and the second another. LayoutValueKey allows you to pass information from your view hierarchy along to layout, but please note that layout can't use this information to make style changes to the view, only to determine its layout. The information is passed through to layout through layout proxies, so the original view isn't accessible from the layout protocol. I guess what I\u2019m looking for is a way to apply modifiers to a Group that would only apply to specific subview indexes. It would only need to apply in one direction. I guess it\u2019s time to write another Feedback. Are all the new layout features available on non-iOS platforms? watchOS, macOS, tvOS... Yes, there are :tada: Excellent, thank you! :partying_face: looking forward to the multiplatform targets with this one And just for reference, you can always check the availability of a bit of API right at the top of the page. For example, for Layout : Thank you <@U03HELSV45B>! I didn't know the new stuff was already documented and 'out there' :blush: I'm noticing a huge hit to scrolling performance when I add a contextMenu to cells in my lazyvstack. When I profiled the app, it showed that buttons in the contextMenu was what was taking so long to render. Does a context menu have to render on creation of a view? Is there any way to create a lazy context menu? Hi - we believe that this issue was fixed in a recent software update. If you are still seeing this, could you please file a feedback? Yes for sure. Thanks! Also <@U03HHJH9C66> what is the best category for SwiftUI feedback? Developer tools, or iOS & iPadOS? <@U03HHJH9C66> i noticed that contextMenu on rows are not updated when the row is updated (for example update the text or symbol based on an action taken). Is this fixed now? Why would anyone use observable object instead of state object? You might want to use @ObservedObject if the lifecycle of your model is not owned by your view. An example is if you are using UIHostingConfiguration and you have your model owned by UIKit in a UIViewController and just passed to a SwiftUI cell. Also @Published properties in ObservableObjects use Combine publishers, which can have their own basket of use cases too Also, in general you want to use @StateObject in a parent view, and if you pass the same ObservableObject instance to child views you can use just @ObservedObject . ViewModifier question: What\u2019s the difference between a custom ViewModifier (without DynamicProperty) that uses some built-in modifiers in body(content:), and a custom View extension func that just use those built-in modifiers? Similarly, what\u2019s the difference between a custom ViewModifier with some DynamicProperty and a custom View with some DynamicProperty (also has a @ViewBuilder content property to receive content to modify) ? I think two have the same render result and behavior. Because of the way a ViewModifier is expressed, the engine knows it's not changing the content passed in and can apply performance optimizations (compared to just an extension on View) A bit off topic to Grids / Layouts but not to Customization - Is there any way in SwiftUI to create multiple Windows (or Window like things) on Mac (and maybe iPad) or Pages on iOS? Hi - thanks for the question! You can certainly compose the different Scene types available in your App to create the functionality you'd like. For instance a WindowGroup and a Window or DocumentGroup and a WindowGroup , etc. hey, I was gonna ask that question later today :joy: Right now we need to use UIKit lifecycle if we want to show HUD that overlays the entire app. For example: https://www.fivestars.blog/articles/swiftui-windows/ it would be really useful to be able to do this without relying on SceneDelegate! I have the same question as well, what would be the recommended way to do what <@U03JMMN8659> says? I thought about using the overlay modifier on the root view of the app but what if there are presented views? Something like what is in that blog post is likely worth a feedback for an enhancement request. Thanks! Hi there! This is not really related to the Layouts video we just watched, but rather a question that I had a while ago. The video was really clear. Thanks for the amazing explanations! Moving on to the question, dan we use SwiftUI to create a numeric text field? I was looking for something that allows typing Int numbers. In my case, I had to work to with 4 digit numbers, and a stepper wasn't the best choice. Instead, I went for a TextField that checks if it can convert the input string to an Int when a button is clicked. Is there a better way to achieve this? Check out the TextField initializers that take a formatter. Just noticed a typo, I meant can instead of dan:sweat_smile: Thank you! Really appreciate it. If this is for iOS see also the keyboardType(_:) modifier. This was for the Swift Student Challenge, and I ran my app both in the iPad Simulator and as a Mac app using Mac Catalyst. Thanks! Is there a way to use a capacity Gauge outside of the lock screen? Yes! Gauge is usable outside of the Lock Screen, and also has a whole host of new styles! I would take a look at the linearCapacity, accessoryLinearCapacity, and accessoryCircularCapacity styles. Please note though that gauge styles prefixed with \u201caccessory\u201d are really intended to appear in contexts like the Lock Screen / widgets, or some similar design context within your app proper, so please give careful consideration to where you use them. I implemented the new searchable APIS yesterday. One issue I have is that when suggestions are presented, it completely blocks out the entire search view. I'm wanting to implement something similar to the Photos search feature where you can see a small box at the top with search suggestions, while also display existing results. Is there a way to accomplish this with searchable? If you\u2019d like search suggestions to be rendered inline your results, you should consider them as part of your results and not provide any suggestion views to the searchable modifier. Note though that in iOS 16.0, for a regular width size class, a search bar that is displayed in the trailing navigation bar will display suggestions in a menu rather than over the main content like on macOS. Take a look at the SearchSuggestionsPlacement type and searchSuggestions(_:in:) modifier for customizing your UI around this behavior. I am using three pickers in a row to allow me to enter hours/minutes/seconds. However, the tap targets for the pickers are all off by some factor, such that if I try to scroll to the right of the middle of a picker, it actually scrolls the picker to the right instead. Is this a known issue? Is there a workaround? Hey Micheal, could you please file a radar fro this issue? That would be greatly appreciated. Will do - thank you! :grinning: Is this a wheel picker? If so, been there, done that, it's a known bug, but we found a workaround. Lemme know ViewThatFits feels very convenient but I'm concerned about making copies of the same content for each layout option. I assume a custom layout would be required to achieve this? ViewThatFits is a convenience that might not be appropriate if you need something with animated transitions, or in general something with more complex behavior. Can the new Charts API allow for scrolling? similar to the health app's charts Sorry, late answer forgot to hit send: Yes, charts should behave just like any other view in this regard. though you may have to set explicit frames on the chart to specify how big (wide? tall?) you want it to be We just couldn\u2019t wait till the next Q&A, Ok I promise we are done now!!! :nerd_face: Hey <@U03J21CNQ1G> I just realized the health app chart does scrolling that keeps the y-axis stationary, that specifically isn't supported, but we've received a few requests about this, but few feedbacks. So if you file a feedback for the Charts team, we'd appreciate it Hey! Is it possible to customise the font on pickers and menu this release? I\u2019ve had to create my own menu (quite a workload) to workaround this styling limitation. Could you confirm which platform you\u2019re seeing this issue with? The font should be customizable on macOS but is not currently on iOS. We\u2019d love to hear more about your use case and please do file a feedback with more details so that we can take it into consideration for the future. Yep, it\u2019s on iOS that I\u2019m referring to. I will, okay! The use case is basically wanting to create a bespoke experience that involves using a custom font (Avenir Next). It\u2019s inconsistent/less cohesive if controls such as menus use a different font. i see. yes, please do file a feedback with a sample app if you can, and feel free to paste the number here once you do so that we can send it along to the right place! How does Layout interact with other view modifiers or constructs that can provide layout data, like offset, geometry reader, anchor preferences, etc? (For context I've got a card game-type view that currently lays out cards in columns all powered by GeometryReader and anchor preferences, because I need specific frame data when a user performs a drag gesture to drag a card over a region of the screen to perform logic. I'm wondering if it's possible to re-implement it in Layout, but I'm unclear about if it's possible to get at the anchor preference frame data.) A layout is only able to position its direct subviews. It can be superior to GeometryReader because it's able to do that while also computing a sizeThatFits. If you're trying to arrange things that are crossing parent-child boundaries, though, anchor preferences will likely still be necessary. A Layout that contained and positioned all of my card subviews would therefore know their positions - and I imagine I could put those in the cache. I guess I'm wondering if there's a way my layout can provide that information out to my model layer when a drag gesture finishes? More generally - can a Layout relay its layout information externally in some way? Or is that not possible? What is recommended way to style navigation bar in SwiftUI? Check out the .toolbarBackground modifier, new in iOS 16 and aligned releases. What about iOS 14, 15 is it still UINavigationBar appearance? Sorry. The new API is not able to be back deployed. Does .toolbarBackground apply to only that view or any view pushed into navigation stack as well? I believe it applies to the current view, so you can change the behavior at different levels of the hierarchy. Awesome, is it safe to assume it takes care of animations too? For example can I set an opaque background on one view and a transparent one on the detail view? <@U03HW7P0HQR> by the way, I\u2019d like to use this opportunity to thank you for the awesome cookbook presentation and thank the team for all the goodies this year. :innocent: There\u2019s also toolbarColorScheme if you need to control the text appearance. Finally, you can use a toolbar item in the principal placement to replace the default navigation bar title with an arbitrary SwiftUI view. Does it apply to both large style and inline style? How do I actually apply Variable Color to an SF Symbol along with the percentage I want highlighted vs dimmed? Is there a SwiftUI modifier for this? You can make a variable color symbol with: Image(systemName: \"wifi\", variableValue: signalStrength) It\u2019s part of initializing the image, not a separate modifier you apply. Thanks Jacob!! Hello! I am trying to create a new custom Layout. I would like to know how to access LayoutValueKeys from within a Layout. The documentation for the new layout API is fantastically written and should have all of the information you need to get up and running! It explains it far better than I could in a short question answer. You can check out the docs for LayoutValueKey here. It even has a walkthrough that shows you how to create, set, and retrieve keys! https://developer.apple.com/documentation/swiftui/layoutvaluekey/ Thanks! Yeah, it definitely is fantastically written! Amazing work! Fairly new to Swift & SwiftUI. Curious if there are any changes to Core Data/SwiftUI integration this year. Using ManagedObjects with SwiftUI seems to results in View that know a lot about Models or Models that have a mix of business logic and view logic which can be confusing. Any suggested best practices for using Managed Objects with SwiftUI? I tried writing value type wrappers to do MVVM but that breaks down with relationships. This is a topic that deserves its own WWDC :joy: I really, really recommend Paul Hudson's videos on this. He helped me a lot with understanding the relationships between SwiftUI and Core Data. Especially important if you plan to use SectionedFetchRequest or generic FetchRequest :zap: There is still a lot of improvement to make Core Data more SwiftUI-esque, here's hoping for WWDC23 :innocent: Hi Tom, Great question. There were no additions to SwiftUI in regards to Core Data. However, we do have our https://developer.apple.com/documentation/coredata/loading_and_displaying_a_large_data_feed|CoreData https://developer.apple.com/documentation/coredata/loading_and_displaying_a_large_data_feed|Earthquakes https://developer.apple.com/documentation/coredata/loading_and_displaying_a_large_data_feed| sample app and https://github.com/orgs/apple/repositories?q=sample-cloudkit&type=all&language=&sort=|CloudKit sample apps all showing best practices with SwiftUI and background contexts. The former performs batch inserts and merges changes on a background thread, then publishes the changes to SwiftUI on the main thread. One place I am struggling with right now is providing the ability to edit an entity with the idea of \"cancel\" and \"save\". <@U03HELS8GHK> Thank you for sharing Earthquakes example, I\u2019ll definitely check it out. Any chance I can get your attention to FB8545866 pretty please? For a very simple Core Data + List implementation, app crashes whenever an item is deleted from the List. It would be awesome if there was a guideline around more complex implementations. > One place I am struggling with right now is providing the ability to edit an entity with the idea of \"cancel\" and \"save\". This notion of edit propagation is described in the https://developer.apple.com/tutorials/swiftui/working-with-ui-controls|Landmarks app in the SwiftUI Tutorials My current plan is to do MVVM by using classes with Value Semantics instead of actual value types but I've just started down that path. Thanks for all the pointers! <@U03HELS8GHK> I'm not sure we can compare the Landmark app based on Struct with an app using Core Data with objects in contexts. How can we use child contexts with SwiftUI? What would be the best approach to edit entities in a child context? Great follow-up question, Axel. We don't have a sample today detailing this exact scenario, so you'll have to apply the same concepts from the Landmarks app to the merge flow shown in Earthquakes. I'd suggest requesting more samples via Feedback Assistant, though, so we can consider more opportunities to improve our learning resources. To clarify, the Earthquakes sample uses private contexts to perform its batch inserts and merges the changes into the main context. So, that sample is a great place to start. Thanks. I explored a lot these sample projects and took some inspiration. But they don't really cover UI edits made by a user. I'll fill some FB. Heya! I have a macOS app where I am attempting to 'tear off' certain SwiftUI views as floating free form windows. I've looked into some of the WindowGroup API, but it doesn't seem like it supports what I'm looking for. Essentially, I have N views of various sizes that can either be docked into a main window, or float above it. I'm running in to issues like sharing state between the docked version and the windowed one, as there seems to be a difference in the relationship between how a standard SwiftUI update bindings works for windows or views. Given that, what would be some ways that I could go about this method of 'docking / undocking' SwiftUI views, and keeping their state in tact? Thank you! Hi - this is really interesting! I'm not really sure we have any API that entirely fits your concept here, but I'd love to learn more. Can you speak to how you are accomplishing this at the moment? Also a feedback for this with any details you provide would be great! Absolutely! I\u2019m doing this right now in my project at http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat . Posting this now to respond to you, and will follow up real quick with a screenshot and a code snippet. The key classes are all here: https://github.com/tikimcfee/LookAtThat/tree/4dbe4a8f93f5dafd56752433df917352076da664/Interop/Views/FloatableView FloatableView swift basically takes some bindings to an enumerable state type (floating, docked), and then either presents the view directly as a child, or acts as an intermediary \u2018adapter\u2019 between the parent hosting view and a floating window. The window itself ends up as a separate instance of the view, but seems to be more or less functional since it still can participate in shared state updates. The other classes, window delegate and the extensions, help construct and manage the window lifecycle a bit. The two things that are tricky are that the view is still technically bound to an EmptyView that lets the state remain active, and the window is its own \u2018thing\u2019. It seems like there would be a way to sever that connection between the root view that owns the docked version of the view and the window (which is why I was really trying to consider WindowGroups), but I just couldn\u2019t crack this one. And hey, is that feedback from the dev-feedback-app? If so, I\u2019d be happy to share this and our discussion with it. And since I was rude and forgot, thank you for taking your time to talk and take a look! Yep, there should be an app called Feedback Assistant that you can use. Any info you can put in there, like what you have above would certainly be useful. <@U03HHJH9C66> Done! I have a fancy-pants ID too : FB10112377 Why does NavigationLink pre-render the destination when it's being rendered? I am loading Heart Rate data in my destination View and this slows down rendering significantly. What's a better pattern to prevent accidental pre-loading of data? In general we discourage to do any expensive work in the init of a view. SwiftUI expects the creation of views to be cheap. We recommend doing any side effect or data loading with either .task or .onAppear . Even .onAppear ran last time I tried. Currently I am using this workaround from StackOverflow: public struct NavigationLazyView&lt;Content: View&gt;: View { public let build: () -&gt; Content public init(_ build: @autoclosure @escaping () -&gt; Content) { self.build = build } public var body: Content { build() } } Starting loading of data in onAppear causes some time the user has to wait for the heart rate graph. Also not super desireable. Check out the new NavigationStack . Inside that NavigationLink s can present values instead of views. The resulting view can then do work in onAppear or using task . When using UIHostingController, is it better to add it to the view controller hierarchy (with addChildViewController), or could we use its rootView and simply add it as a subView ? You want to always add a UIHostingController as a childViewController . // Add the hosting controller as a child view controller self.addChild(hostingController) self.view.addSubview(hostingController.view) hostingController.didMove(toParent: self) without the view controller relationship things that depend on the UIViewController hierarchy won\u2019t work. Such as UIViewControllerRepresentable There are a few other types as well that depend on this relationship in their underlying representations so its best practice to add the parent/child relationship. I am building a design system implementation where I\u2019m providing reusable components that other teams can simply build their features. I usually start with SwiftUI but this view controller relationship makes things very hard at times, especially if the view in question is complex. I believe there is NSHostingView on the Mac, I wish we had the same on iOS. For the time being, given the requirement, is there any recommendation to fulfill this requirement? For example, can we even use window\u2019s rootViewController as a parent or is there a better way? yes the AppKit underlying infrastructure makes NSHostingView work while UIKit is not able to do so due to the hierarchy requirements. It depends on exactly what you are trying to build, but potentially you could make a reusable view controller that your teams can use instead depending on where the complexity is at. Parenting at the rootViewController would have issues with any navigation calls you might try to make in SwiftUI. Ah got it, thanks for clarifying. It is a little bit overhead to ask the teams to add a child view controller just for a button from the design system. :sweat_smile: And it\u2019s extra challenging if they are adding it to a UIView subclass down in the view hierarchy where they don\u2019t have access to any view controller for example. Should Layout work fine with UIKit-representable views? Yes! To get more control over how UIViewControllerRepresentables size themselves in SwiftUI layout you may want to implement their new sizeThatFits method: https://developer.apple.com/documentation/swiftui/uiviewcontrollerrepresentable/sizethatfits(_:uiviewcontroller:context:)-35p75 With the new MenuBarExtra. Is it possible to get a callback when the content window appears or disappears. I\u2019ve tried onAppear but it\u2019s only called on first appearance. OnDisappear is never called. Hi - thanks for the question. This sounds like it is probably a bug. Could you please file a feedback for this? While I was working with Canvas and rendering images, shapes, and text, I was trying to sort out if there was a way to get the offset for the baseline value from a ResolvedText. I wanted to align the text baseline to a visual element that I was drawing in canvas, and the only path I found to getting sizes of discrete elements was to resolve them and measure them - something akin to: let captionedTextSampleSize: CGSize = context.resolve(Text(label).font(.caption)).measure(in: size) This hands back CGSize, which mostly worked for what I needed, but I'd really like to align to that baseline - and for that I need to determine an offset. Is there any public API to help me do that, or a technique I could use to get said offset that uses other frameworks or libraries? The GraphicsContext.ResolvedText type also has firstBaseline (and lastBaseline ) methods. So you can do something like: let baseline = context.resolve(myText).firstBaseline(in: size) :face_palm: I totally missed that. Thank you Jacob!!!! SwiftUI Mac specific question: When using .contextMenu on a List , how can I find out what was actually selected? When looking at Finder, the selected (=item on which the action should be performed on) is either the selection (1 or more) OR it is the item that has been right clicked on directly. How can I replicate this behavior using SwiftUI? Check out the new context menu API that accepts a forSelectionType parameter. This passes the value of the selection into the modifier for you to act on. List(selection: $selection) { ... } .contextMenu(forSelectionType: MySelection.self) { selection in // check selection } Be sure to match the type of your lists selection to the type you provide to the context menu modifier. https://developer.apple.com/documentation/swiftui/view/contextaction(forselectiontype:action:) That is awesome, thank you! Are there any plans to back port \"smaller\" API changes like this into macOS Monterey and earlier? I\u2019m a teacher but also write iOS/iPadOS apps. I watched Curt\u2019s Cooking/Recipe section and am trying to understand state restoration. I thought Curt said adding SceneStorage and the Task to check if navigationDetail was nil would do it but it doesn\u2019t seem to work. Do you still need to use the onContinueUserActivity and then have each view in the stack hierarchy have a .userActivity to create/describe a NSUserActivity for that view? Hi, David! I was a teacher before I went full-time software engineer. What I shared in the talk and in the Copy Code associated with it should work. Or check out the sample project here: https://developer.apple.com/documentation/swiftui/bringing_robust_navigation_structure_to_your_swiftui_app You shouldn\u2019t need to use user activity directly at all for this. Hi Curt, yes, we corresponded when you were a teacher and then spent your sabbatical at OmniGroup. I\u2019ve been tempted by some Apple positions, but can\u2019t relocate right now. Hmm. I tried running it in simulator. I select Apple Pie, select Fruit Pie Filling, press Shift-Cmd-H. Press the stop button in Xcode, and then relaunch the app and it starts at the initial Categories screen. Am I not testing it correctly? Ah! Yeah, Xcode kills background storage unless you hold it \u201cjust right\u201d. The pattern when working with Xcode, is to (1) background the app in the simulator or on device, (2) wait 10 seconds for the system to do its backgrounding work, (3) kill the app using the Stop button in Xcode. Then when you run the app again, it should use the saved value. Ah, I waited a few seconds, but probably not 10. Also, I did the next best thing to joining Apple myself - one of my students is now there (Jeremy who did the AV Foundation \u201cCreating a more responsive media app\u201d talk this year. Hmm, I waited more than 10 seconds and still doesn\u2019t work. After navigating, I press Shift-Cmd-H to return to home screen, wait 15 seconds, press stop in Xcode, and then relaunch from Simulator and still starts over with the Category list. If I use the Run button in Xcode, it starts over at the \u201cChoose your navigation experience\u201d? I\u2019m using an iPhone simulator (iPhone 13) not an iPad simulator if that makes ay difference. Also, I chose Stack as the navigation experience Hmm. And this is with the sample code project? Yes, downloaded from here: https://developer.apple.com/documentation/swiftui/bringing_robust_navigation_structure_to_your_swiftui_app I\u2019m afraid I have to go with \u201cit works for me\u201d for the moment, and then double check with the developer build. Sorry for the frustration here. The 2020 SwiftUI State Restoration example that uses NSUserActivity seems to work. https://developer.apple.com/documentation/swiftui/restoring_your_app_s_state_with_swiftui Ok, thanks. I can check back in a week to see if the code on the website has been updated. Interesting, scene storage uses the same mechanism under the hood. That 2020 example also has SceneStorage at one spot: in the ContentView it has @SceneStorage(\u201cContentView.selectedProduct) as an optional string. but still uses .onContinueUserActivity Just starting out with SwiftUI. One of the things that is hard to get my head around is the following. In UIKit, I have a label that pulses (grows and shrinks) based on certain events, so I'd typically have an outlet to the label and a method 'pulse'. In SwiftUI, I accomplished the same by having a binding labelState which I change in the containing View. So while UIKit would have an outlet for multiple labels, in SwiftUI I'd have a binding state for each label. Is that typically the correct pattern? That seems like a totally reasonable approach! Generally, when you have some value that you want to change in SwiftUI, you want to make sure that there\u2019s a clear source of truth for that value. That source of truth can either be @State , or a property within an ObservableObject . When you have a fixed number of controls, having a separate piece of state backing each of those controls is completely reasonable. That said, the sources of truth you provide for your state should mirror your UI, so if you dynamically generate your labels from an array of data, you might then want to have an array of data as the source of truth state for your ui components. If you see that your sources of truth for your data mirror the structure of your UI, you\u2019re probably on the right track :slightly_smiling_face: ok thanks. A bit of looking at the problem from a different axis. Using .buttonStyle(MyCustonStyle()) instead of MyCustomButton(...) is more SwiftUI-y. But why should I prefer one over the other (considering MyCustomButton uses MyCustonStyle under the hood)? Hi Peter, We strongly recommend separating semantic controls and styles as it provides a lot more flexibility. For example, using the button style modifier will apply to the whole hierarchy, so if you decide you want specific parts of your app to use a different button style, you can just apply it to that specific hierarchy without changing the control itself. With this approach you also get multi-platform behaviors for free. My app\u2019s design uses kerning on most of its buttons, but I can\u2019t put that inside of my ButtonStyle because kerning is only available on Text , which ButtonStyleConfiguration.Label is not. I have a feedback open suggesting passing kerning through the environment (FB10020695). In the meantime, do you have any suggestions on how to do this without having to separately use .kerning() on every button label? Moved out here :smile: https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654722720194909 My question is about some code in the SwiftUI Cookbook for Navigation session. Around 23 minutes in, Curt uses the task modifier to run an async for loop over the model's objectWillChange publisher. In the loop, he then accesses an @Published property of the model. But a @Published property fires the objectWillChange property before it updates its wrappedValue . Doesn't that mean that when the code accesses navModel.jsonData , it will get the out-of-date model values? Or is there some async magic that guarantees the loop body will not run until later after the objectWillChange publisher has finished publishing and the @Published property has updated its wrappedValue ? Great question! Task schedules its body on the main actor, which is also where the view update happens. So objectWillChange enqueues the body of the for loop, but it\u2019s enqueued behind the update of the data model. What is the advantage of using custom Symbols rather than SVG with template rendering mode? The ability to programmatically change the colors of several layers? Any other reason? Inlining within Text is a nice benefit, but doesn't seem to work with custom multicolor symbols. In addition to the great color support, using a custom symbol also helps it better fit with text by growing with fonts, respecting bold text, and matching baselines. Christopher, if you haven\u2019t already, can you file a feedback about that? I have, also submitted a question here about it FB10001506 Thank you! Would it be possible to create an \u201cinfinite\u201d paged collection view in SwiftUI? Similar to the week view in the calendar app where you can swipe through weeks endlessly. Thanks! On way to achieve that would be to have onAppear on a the individual views that the List is scrolling over and use that load in more data. What if you want the @State data to not grow as the user continues to page? Removing items from the array would offset the data and causes a jarring affect. I would be happy to submit a feature request for something that would allow me to build this, I\u2019m just not sure what that would look like :dizzy_face:\u200d:dizzy: One way you can solve the problem is by keeping only identifiers in the @State so that the storage doesn\u2019t grow too much, and load on the demand the actual data as view are coming on screen. A feedback would be great. You can simply describe your use case, what you are trying to achieve, and what kind of road block you are finding. That sounds interesting, ill give it a shot. Currently using the .page TabViewStyle but have also been playing with UICollectionViews as well. Ill submit a request if that doesn't work out. Thanks Any tips to avoid a List to scroll to top whenever @State gets updated? Like in the calendar example, I\u2019d probably want to be able to scroll in both directions and have the view appear somewhere in the middle. I think ideally a way to detect when scrolling was finished for that TabView case would be great, because the onChange fires before it\u2019s done scrolling SwiftUI on macOS question: Is there a way to position the windows when they are opened? In my app, the position of the next window always depend on the last, offset a bit to the right and down. So they are slowly crawling down the screen, which is not nice. Hi - thanks for the question! The cascading of windows is the default behavior for WindowGroup . We've also added a new Scene modifier - defaultPosition , which allows you to specify an initial position to use absent any previous state for the window. It takes a UnitPoint , so for example, it would be something like defaultPosition(.topTrailing) . Did you have some other behavior in mind? It'd be great to get more info, if so. It would be nice if I could position a new window just beside the already open window, e.g. to the right or left, depending on the available space. The second window is a \u201ccompagnon\u201d to the document, used for editing. Ah, I see. That makes sense given your use case there. Would you mind filing a feedback for an enhancement? So I have currently a DocumentGroup and a WindowGroup . The WindowGroup instance would have a one-to-one relation to an open document. Ok. Please include those details in the feedback, its definitely helpful when we are designing APIs Hi! Hope non-standard behavior out of SwiftUI is your cup of tea, because I have a really \u201cinteresting\u201d question. This happens using SwiftUI in Xcode 13.4 on an iMac running macOS 12.4. When using .datePickerStyle(.compact), and I touch the date to bring up the calendar, the final displayed date changes format depending on the chosen date. \u201cJune 19, 2022\u201d looks ok, but then I get \u201c6/20/22\u201d when I move to the next day. It gets better: after going left to right in the month, and seeing \u201c6/22/22\u201d, when I start moving right to left from June 24, 2022, the output for the 22nd comes out as \u201cJune 22, 2022\u201d. Any ideas would be welcome! Apologies for the weird problem! You're not alone, I noticed this behavior ever since last summer or so. :slightly_smiling_face: REALLY?! Thank you for letting me know I\u2019m not imagining things! Hi Mayra, Thanks for the feedback. This is a known issue :smiley: and we are fixing it! Hooray! Thank you. Will this be in Xcode 14? Oops\u2026 sorry, can\u2019t comment on future work\u2026 What are good tells that I should use implicit vs explicit animations in SwiftUI? I usually default to withAnimation to set up animations based on when a change occurs. That way, whatever UI changes as a result will implicitly be animated. In some cases, View.animation is useful when you have a certain view or view hierarchy you always want to change with an animation, regardless of what triggered it. A good example is the knob of a Switch: it should just always animate when it changes. (And make sure to pass an appropriate value, like .animation(myAnimation, value: mySwitchValue) , so that you only animate when the property you care about changes, and don\u2019t accidentally animate too often.) Thank you for your reply Jacob I\u2019ve been having some difficulties deciding on best approaches especially when creating reusable components with baked in behaviours. This might be stretching it, but this is a gist that lays out my problem: https://gist.github.com/kanevP/a9d54f5c60eddabca000b1a45a59e70b Would love to hear your thoughts on the different animation usages. I have some custom multicolor symbols that I want to inline with Text , e.g. Text(\u201cLook at my pretty symbol \\(Image(\u201cmySymbol).symbolRenderingMode(.multicolor))\u201d But they don\u2019t render as multi color, is this an expected limitation? Can you think of any work arounds that still provide all of the benefits of inlining? It works great for system provided multicolor symbols\u2026 FB10001506 has a sample project. it could be a bug, or it could be some problem with your symbol template. For instance the template must be at least version 2 to support multicolor. Does the SF Symbols app work with it and is it able to display the symbol in multicolor? Yeah, they render properly outside of Text And in the symbols app oh, the problem only occurs when the symbol is used inline with text? Correct Even duplicating a system/apple provided multicolor symbol has the same result, so nothing to do with my template if the symbol is working correctly in other contexts, that sounds like it could just be a bug on our side let me look into it further In a WWDC20 session \"Data Essentials in SwiftUI\" there's a point in the video where the presenter touches on using a single data model that can have multiple ObservableObject projections to target SwiftUI updates to specific views. How can I achieve this? The exact solution really depend on your data model but its can simply be as having a model that contains a bunch of properties where each of these type is an ObservableObject ; or you could have a function that given an identifier return you the ObservableObject that corresponds to that id. What we wanted to get across was that you want and you can scope your ObservableObject to specific screen of functionality of your app. I think I understand, thank you <@U03J7BQQNPJ>! In our app, we can push a view containing a custom view above the navigation bar. We achieved it by using a NavigationView inside a NavigationView (displaying a custom view with the back button hidden). Is it possible to have a custom view above the navigation bar with NavigationStack? Is it possible to use a similar workaround? Nested navigation stacks don\u2019t compose the same way. An inner navigation stack is effectively a no-op; it doesn\u2019t add anything and any path bound to it should be ignored. Are you trying to achieve something like a very tall navigation bar? I\u2019d love a Feedback with details of your use case! Thanks for the follow-up! You can find the use case, a video and a sample project demonstrating our current approach with a NavigationView inside a NavigationView in this radar: https://feedbackassistant.apple.com/feedback/10112679 I would love to achieve the same effect with NavigationStack Forgive me if this question has the same answer as the scrolling chart question, but could we utilize a scrollview and a chart to display a live chart? So new data added is always visible to the user, instead of having to manually scroll it into view. Yes, this should work, but will extra work. You may have to observe the data and manually keep the scroll view positioned at the very end It sounds like you want to use to be able to scroll backwards through an otherwise-live feed, but if that isn't the case, the data plotted in your chart could always be just the last 7 elements of an array or something conceptually similar Yeah, basically keep a horizontal scrollview fixed to the end and allow the user to scroll back to the left (start) of the view. Is there a modifier or additional context to manually position the location of the scrollbar in a scrollview? Have you seen https://developer.apple.com/documentation/swiftui/scrollviewreader|ScrollViewReader s? Ah I have not. This is perfect thanks! <@U03HL00QL68> When I use a ScrollViewReader and the parent state changes the scroll position changes. I have filed a feedback FB10037533 Thank you for the sample project. Definitely a bug! <@U03HL00QL68> thanks a ton!! for confirming that, hope it gets fixed Question regarding the SwiftUI Instrument: Is there a value of avg. duration that no View's body should go above when rendering for avoiding performance issues/hitches ? View\u2019s body properties should be cheap and avoid any potentially expensive work. If there are things you need in your views body, its best to pre-compute that into a piece of state that you can pass your views. Concretely, each frame has a hard deadline of generally 1/60s or 1/120s (depending on the refresh rate of the display) to render its content. Thats 8ms in the fastest case. Thats for the entire screen to render its content though, not just your single view . So if any one view is taking more than a few ms to finish calling body, you\u2019ll run into issues. If you\u2019re using Swift concurrency to move work off the main thread, consider watching this session: https://developer.apple.com/videos/play/wwdc2022/110350/ This is all mostly related to scrolling performance. Other kinds of things like app launch or navigation pushes will typically take longer than multiple frames to render, but those situations have different expectations around frame deadlines. We can now use .foregroundColor(.white.shadow(\u2026)) on SFSymbols. Will this work with custom PNGs/SVGs as well? foregroundStyle s apply only to shapes, image masks, and text. If the Image is configured as a template, then foreground styles should be applied. If the image is non-template, they won't Yes if you set render as to template for your image asset Excellent, thank you! :pray: Thanks Mirko! and thanks for the great question Jan Is it possible to have a customizable toolbar with SwiftUI on iPad? Hey Johan, Yes it is possible :tada: Checkout the \"What's new in SwiftUI\" talk on the Advanced Controls section to have a peak. And make sure to tune in for the \"SwiftUI on iPad: Add toolbars, and documents, and more\" talk tomorrow to dive deeper. :smiley: Note that the same APIs will work on macOS too. SwiftUI Mac specific question: When using NavigationView, is there a way to start the app with a the sidebar already collapsed (or do it programmatically)? I saw this in on of the videos. Bonus question: Can this also be done on macOS Monterey? Hi - thanks for your question. The new NavigationSplitView API provides an initializer which takes a binding indicating the visibility of the columns. This can be used on macOS to indicate that the sidebar should be initially collapsed. There is an example in the documentation here: https://developer.apple.com/documentation/swiftui/navigationsplitview/ Thank you! Hi there, is there any way to get my offscreen nested StateObjects to not be deallocated when scrolled offscreen within a Lazy Stack? The only solution I've found is to \"hoist\" them to the top level of the Lazy Stack, e.g. within the Content of a ForEach, but that feels kinda clunky to have to do in every scenario\u2014wondering if there's another option. I think \"hoisting\" your state into an ancestor is your best bet here As an aside, StateObject is more commonly used with model objects and you generally don't want to tie your model object's lifecycle to the view In the case of a model object it might make sense for an ancestor model object to be maintaining references to these children model objects <@U03HELTEP9T> What guarantees (if any) do we have about the longevity of a StateObject property on a view? Should I try to move view-owned objects to the root App struct and inject them via EnvironmentObject? I've already run into issues when changing device orientation, with my view hierarchy being torn down and recreated, losing any ephemeral state Thanks! Yes in this scenario it's effectively a State , just incidentally modeled as a StateObject for other reasons \u2014 however we were finding that a State was getting reset as well back to the initial value as well in that same scenario. I assume that's expected as well? <@U03JX6SFGMS> ya State would be expected to behave the same as StateObject here since the motivation is to limit memory growth by avoiding preserving state for an unbounded number of (offscreen) views Is there a way on watchOS for SwiftUI to present a modal where the stays bar is hidden? For example, how the keyboard menu in the phone app hides the time. The entered phone number is displayed in the status bar area instead. Thank you! <@U03JJQ3BMB7> if you present a modal that has a confirmation action, the time will be hidden for you automatically, but otherwise, the time will always be there. Ok, thank you. Is it possible to do this with watch kit or is the time always shown? I'm just wondering if it's possible for me to recreate the keyboard menu in the phone app. Thank you! I don't think so. Can you provide a screenshot of the menu on the phone that you are trying to mimic ? I am trying to make a calculator like app, and liked this layout from the phone app. I liked that the input pad keys were large and the entered numbers were in the status bar area if you wanted to do that, you can do it in a modal sheet on watchOS, and you can provide a custom confirmation action as a button in the top right (like the delete button is), and you could provide a navigationTitle for content in the navigation bar, and then provide your own cancellationAction. but the time will only hide if you actually put content in the navigation bar in that spot (via confirmation action placement toolbar) Great thank you! Would you mind pointing me towards the documentation on custom confirmation actions, please? Thank you! https://developer.apple.com/documentation/swiftui/toolbaritem/init(id:placement:showsbydefault:content:) and then tap on the placement link Should describe those That\u2019s awesome! Thank you for your help! :grinning: no problem! feel free to check in back here and @ me to say whether it works or not <@U03HHHXDL03> - Thank you for your help the other day! I got it working! :grinning: That is so cool! I'm happy to hear! Is there a way to have an onChange(of:perform:) Modifier but only call the closure, when the state was changed by the user and not programmatically? onChange executes its closure whenever the value change (that\u2019s why we require the value to be Equatable ). So onChange only know about comparing the value that your are providing. If you want to make that distinction you probably should model that difference in your state. We have the same use case and we\u2019re hooking into the action closure of a button separately for example. I\u2019d love to have a way similar to .valueChanged events from UIKit though. :sweat_smile: Are there any Xcode build settings that might cause SwiftUI previews in a large/existing project to render text incorrectly? I've noticed that Text(\"\\(doubleValue) \\(floatValue)\") will incorrectly render doubles as -NaN in previews/simulators. Doubles renders correctly on device, and floats don't have the issue. Thank you for reporting this issue. Did you file a feedback with this information? If not we would really appreciate if you could do that. I haven't been able to reproduce this in a small, isolated project. The same code works without issue in a Swift Playground or a new Xcode project. What's the best way to file a feedback in this case without uploading my full repository and all of its dependencies? You can start with just the information you posted here. <@U03J7BQQNPJ> thanks, please see FB10112975 It's an issue on both Xcode 13 and Xcode 14 Thank you :man-bowing::skin-tone-3: <@U03J7BQQNPJ> LOL right after I submitted the FB, I found the source of the issue. It was the Xcode scheme's \"Localization debugging: Show non-localized strings\" setting that was apparently clobbering the SwiftUI strings. Unexpected nevertheless\u2014I'll update the FB now. There has been a lot of controversy regarding the ObservedObject(wrappedValue: ) initializer. Is it safe (and encouraged) to use it, or do we have an alternative for it this year? This initializer is fine to use! In fact, the ObservedObject(wrappedValue:) initializer is invoked every time you construct an ObservedObject , even if you don\u2019t explicitly write it yourself. When you write: @ObservedObject var myObservedObject = myModel , The Swift compiler converts that standard property wrapper syntax to a call that looks something like: var _myObservedObject = ObservedObject(wrappedValue: myModel) . The controversy I think you\u2019re referring to is using that initializer explicitly in the initializer for one of your views. Typically, we see people doing this to allow observed objects to be instantiated with specific information. That is also something which we think is fine to do. We recognize that the ergonomics of that case is not ideal (since you have to access the de-sugared property wrapped (in the example I gave, _myObservedObject ), but it\u2019s not at all harmful. What about the State initializer? I think the confusion comes from this comment: /// Don't call this initializer directly. Instead, declare a property /// with the ``State`` attribute, and provide an initial value: I think ObservedObject also has a the initialiser init(initialValue:) is that preferred? The documentation doesn't mention any warning on this initialiser The state initializer worries me a bit more. Not because it\u2019s dangerous \u2014 it\u2019s totally fine to use it yourself (as I mentioned, the normal syntax is just sugar for the fully spelled out case) \u2014 but because I can\u2019t think of as many cases where you need that syntax for @State that aren\u2019t dangerous. Remember that @State is initialized once per lifetime of the whole view, not once per time a view\u2019s initializer is called The views representation will be recreated on demand. That means that if you\u2019re re-initializing the state every time the views init is called, you\u2019re going to be clobbering your own state. So that\u2019s fine to do, but make sure that you\u2019re only using it to set the initial value of a state, and that you\u2019re not resetting your state depending on some initializer value. <@U03HKVDCL7N> I believe same applies for @Binding and co. too? Yup! But for @Binding is it still harmful? Because source of truth is already on a parent view or somewhere up there? Yeah, binding is also one of the cases that would sound alarm bells for me :warning: Yeah fair, I\u2019d still be cautious instead of making assumptions. Very nice tip though, thank you! :heart: As a final note: regarding the \u201cdon\u2019t call this initializer directly,\u201d that\u2019s mostly because as I mentioned, the cases where you \u201cneed\u201d the underlying initializer (for @State and @Binding especially) are few and far between. Most of the time, you\u2019d want to be using the standard property wrapper syntax, so we want to make sure people reading the docs look there first. Is the concern because State variable's lifetime is outside the view's lifetime and initialising the State variable in the view's initialiser would keep creating a new State every time the view get's created? A recent use case where i would have used it was to pass in a value to a structs initializer, where that struct is also @State Interesting. The place I was using it was that I had a @FetchRequest that was based on a relationship to another object, so I was passing the object into init and then creating the fetch request based on it but was keeping that object as State. Maybe I don't really need it to be State. But maybe I shouldn't be creating the FetchRequest wrapper that way either? <@U03J21ZD15Y> I noticed in this case that the FetchRequest is init everytime the View is init. It does not behave like a State or StateObject where the property is tied to the lifetime of the View. So if I use a predicate or sortDescriptors updated by my subview, any update in the super view will reset the predicate/sortDescriptor. So these should be higher in the hierarchy, or based on a State. <@U03J20KFJG3> <@U03J21ZD15Y> same thing here FB9956812. The FetchRequest struct mistakenly contains a var object that it inits every time which breaks SwiftUI's View identity matching so body is called every time. Really hope it is fixed in this beta cycle. @Tom you can init a FetchRequest with params like this: private var fetchRequest: FetchRequest&lt;Appointment&gt; private var appointments: FetchedResults&lt;Appointment&gt; { fetchRequest.wrappedValue } `init(date: Date, userID: String) {` `fetchRequest = FetchRequest(fetchRequest: Appointment.sortedFetchRequest(userId: userId, date: date))` `}` That wrappedValue trick is taken from the SectionedFetchRequest header comments. Is using @EnvironmentObject for dependency injection of entities not directly related to the view state, like a Service (to fetch values from network) or a telemetry logger, considered bad practice? Thinking on a MVVM architecture context. I wouldn't consider it a bad practice Be mindful when using plain @Environment , that if you're passing a struct, any change in value will invalidate any views reading that value from the environment But if you're using @EnvironmentObject with a class that's effectively immutable that shouldn't be a problem In an MVVM context, as @Environment and @EnvironmentObject values are passed into the view, how can we get them out of the view to view model or any other helper type? I wish I could use @Environment and @EnvironmentObject directly in the view model that is associated with the view for example <@U03HELTEP9T> Is there any documentation on Self._printChanges() There was one instance where it said @96 changed I was not sure what it referred to. Does it refer to @FetchRequest ? <@U03J1V9U7U3> it's best to learn the View struct, it is actually a view model so you don't need another view model object in SwiftUI. If you do use them you'll just get the bugs typical of objects that SwiftUI has been designed to eliminate by its use of value type structs. For simple use cases, yes, I agree. I do exactly this in my own app, but it doesn\u2019t scale in a larger team working on a complex project. Especially as we\u2019re adopting SwiftUI incrementally. I have a UIKit app on iOS. I am at the point of needing to decide\u2014do I ship it for macOS with Catalyst, or given I\u2019m doing a major redesign anyway, do I migrate it to be a SwiftUI app. I am wondering\u2014are any of the features/capabilities that I would have in a Catalyst app to make the app more \"Mac like\" that are not available on macOS using a native SwiftUI app? e.g., the \"Bring your iOS app to the Mac\" session showed the ability in Catalyst to enable/disable the traffic light buttons when they were not appropriate for that window. Is this kind of control also available in SwiftUI on macOS? Hi - thanks for the question! With regards to your last part around the window controls (traffic lights) - SwiftUI will enable/disable these depending on the content of the scene. So, for example, a content view with a fixed size, will have the zoom and fullscreen button disabled. I'd also like to point out that on macOS Ventura, SwiftUI App lifecycle windows will be fully flexible by default (they can be resized to the maximum allowed by the screen). This can be opted out with a new Scene modifier - windowResizability . ie: WindowGroup { } .windowResizability(.contentSize) Enabling/disabling of full screen behavior is also related to the maximum size of the window's contents and how that relates to the screen size of the device If there are other aspects that you would like to customize here, a feedback would be appreciated as well. What is the best way to debug issues with the Live Preview when an error isn't thrown / the build doesn't actually fail and the \"Diagnostic\" button isn't available? In some instances when I resume the Preview, a \"Build for Previews\" is launched and then stops shortly after, pausing the preview again In the new Xcode the diagnostic button is now always available in Xcode's menu Editor > Canvas > Diagnostics, so you can access them even when the canvas doesn't show an error banner. Hard to say without the diagnostics (and potentially your project) what the root cause is, but from the symptoms it sounds like maybe your project has a script phase that modifies some of the source as part of the build? If you file a feedback with the diagnostic we might be able to help narrow down where things are going wrong. I noticed a month ago on one of my Views that any TextField I add to my Form (within a VStack) extends the space below the Form. I only noticed as the view contains the Form in question and then an HStack with buttons. The space between the Form and HStack increases with each TextField I add to the Form. I created a stackoverflow question (linked at end) but couldn't figure it out. I got around this by surrounding it in a ScrollView (realized my other forms were fine and that's why) so I continued on. Figured I'd ask here though. :) https://stackoverflow.com/questions/72293879/the-textfields-on-my-swiftui-macos-project-are-making-my-window-height-too-tall|https://stackoverflow.com/questions/72293879/the-textfields-on-my-swiftui-macos-project-are-making-my-window-height-too-tall hi, this does look unexpected. could you file a feedback with a sample project that reproduces it attached? please do paste the number here so that we can make sure it gets routed to the right place Sure. Where do I go to file the feedback again? you can use the feedback assistant on your Mac or iOS device if they are enrolled in the apple developer or public betas, as well as http://feedbackassistant.apple.com|feedbackassistant.apple.com a Mac enrolled in the beta programs should have Feedback Assistant visible in Launchpad; an iOS device should have an icon for it on the Home Screen. On a Mac, you can also open the assistant directly from /System/Library/CoreServices/Applications . Just created it (through XCode). FB10112924 It's uploading the attachments right now on watchOS is it possible to call the keyboard from swiftUI straight from a complication? i think we still have to do a wkinterfacecontrol or go through a textfield. <@U03J7JKA23F> no, unfortunately not with TextField or TextFieldLink. Could you please file a feedback requeset? Is there a recommended best practice for managing StateObjects / ObservableObjects in a primary-detail type setup? I've been commonly using StateObject(wrappedValue:) to inject a stateobject into a detail view, but of course because wrappedValue is an autoclosure it doesn't get updates to the data value that may occur from the primary view. Passing the entire primary StateObject to the detail view can expose more data to the detail view than necessary. And creating and managing an ObservableObject owned by the primary StateObject can be boilerplatey and messy to manage. Any approaches I've overlooked, or tips on how to approach those tradeoffs? My intuition is that for a primary-detail setup, you would want the detail using @ObservedObject (or even @Binding ) instead of @StateObject . @State and @StateObject are defining a source of truth and lifetime for the state/model. You want the lifetime of the state/model to live on beyond changing the selection in the primary. @ObservedObject will listen to changed in the model object and update the view without establishing a new source of truth. > I agree with <@U03HELTEP9T> and would like to add that I read somewhere that using (wrappedValue:) of State/StateObject in this way is discouraged. I will try to find the source. > For your use case, I would use @StateObject in root, then @ObservedObject in detail, or @Binding in detail if you only want to expose something specific Is the recommendation to inject the entire root StateObject into the detail as an ObservableObject, or for the root StateObject to create and maintain an ObservableObject which has the subset of relevant data, and inject that into the detail view? the latter Content view <- State Object, holds the truth Detail view <- ObservedObject/Binding, refers to the truth Thanks. That was my instinct too... I just don't want to manage it. :stuck_out_tongue: But it's probably a good choice. If you wanna take the pro route :sunglasses: you will rely on protocols, and inject instances as needed using a custom property wrapper. The instances will be stored using a custom class (or save yourself trouble and use something like Resolver) With NavigationPath, I've already felt the need to pass it down to child views as a @Binding or @EnvironmentObject so that they can influence the stack programatically. Is that a reasonable approach or am I overlooking something? Passing the binding or passing a navigation model as an environment object are both reasonable approaches with the API we have available today. I hope we can make this more ergonomic. I\u2019d love a Feedback with your use case! I personally like the navigation model approach, since I can easily do other work when the navigation state changes without putting that code in my Views. But either approach works fine. So, a ViewModel (probably observableobject) that dictates navigation? Yeah. That works for the way I think about navigation. Could a WindowGroup value that has no stored dependencies be recreated along the lifecycle of an app? Hi - would you be able to provide some more detail here? I'd like to make sure I understand your question fully. If some value is stored not in a @State in the WindowGroup, for example, let uuid = UUID(). Will it stay the same in this specific case for the lifetime of the app. This is of course a bad practice, but I've seen it quite some time on the internet, and I would like to know if in this specific case, the value could be recreated or not as the app runs. This is more a theoretical question to try to get a better grasp around this specific value lifecycle. In other words, are WindowGroup susceptible of being recreated even if they have no stored dependencies, no @State, etc. I see. I think for this, I would likely recommend storing this in an app-level model, and providing that value to the WindowGroup But are there (private) system events that can recreate the WindowGroup? Mostly a theoretical question. I would always store something safely elsewhere, but I don't know if in this specific case, this is overzealous. I don't think it's being overzealous, if that is what you are concerned about. Thanks! Is it possible to use @State , et. al from within a custom Layout ? Or is there a different way we should parameterize our layouts? I noticed that the protocol doesn't inherit from View , so I wasn't sure if using the property wrappers would work. You can add input parameters to your layout, and you can attach values to particular views using the LayoutValueKey protocol. To store data between calls into your layout, you can use the cache, but that\u2019s only to avoid repeated calculations, and you should be prepared for the cache to go away at any time. Okay, so anything stateful should live in the parent view (and be passed as init args) or in a child view (and be read through a LayoutValueKey ) ? Yes Makes sense, thanks! Is there a newer way to debug a view while it's in preview? Best way to debug a preview now would be to use Xcode's menu item Debug > Attach to Process (or the Process by Pid or Name) and select/use the name of your app. Then update a string literal in that view and it should cause the View's body to get called and hit any breakpoints you've got in that code. Hope that helps! OMG! That is amazing. Awesome thanks! I would like to migrate my (very complex) iMessage app extension to SwiftUI. How do I deal with the MSMessagesAppViewController? Do I need to use some mix of UIHostingController and UIViewRepresentable? You should use the UIHostingController as a child of the MSMessagesAppViewController When implementing path(in: CGRect) of a Shape, are dynamic Property Wrappers assumed offer the correct values, or is this only valid for bodies ? Shapes don\u2019t have the same data flow primitives as views do, so things like state, environment, etc. won\u2019t work, but don\u2019t despair! You can still get the results that I think you might want. Since you\u2019ll eventually be showing your shapes somewhere in a SwiftUI View , you can hoist your data flow up to the SwiftUI View level, then pass down that information about state, environment, etc. down to the shapes. What is the best way to make a TextField wrap? I noticed some new API regarding lineLimits and axes, and wondering if those provide some functionality in this area. TextFields can be initialized with a new axis parameter. If you provide a .vertical axis, then platforms like iOS and macOS will wrap the text of the text field in a scrollable container instead of letting the text expand horizontally. See one of its inits here: https://developer.apple.com/documentation/swiftui/textfield/init(_:text:axis:)-7n1bm Awesome, thanks! These text fields will also respect the line limit modifier. So specifying a lineLimit(3) will let the text field grow vertically up to 3 lines and then it will stop growing and scroll instead. As we get more and more familiar with SwiftUI we use GeometryReader less and less, but what are some tells that there's no way around it? If you want a visual effect like corner radius, opacity, or color to depend on your geometry, GeometryReader is the way to bridge between these two worlds I particularly use GeometryReader a ton to contextualize Gestures (e.g. to see whether the user has dragged all the way to the left or right of a control). Is there a different way? Layout gives you a superior tool for encapsulating layout logic for re-use. And while using GeometryReader you can position children, the GeometryReader can't reflect back any custom sizing behavior. This is one of the reasons it can become unwieldy when used in complex layouts\u2014because the GeometryReader becomes fully flexible. Ben, would the new SpatialTapGesture help? https://developer.apple.com/documentation/swiftui/spatialtapgesture What is the difference between SpatialTapGesture and TapGesture ? :thinking_face: <@U03HELSV45B> Is it possible to understand the extent of the view from SpatialTapGesture ? My main use case is determining where the touch is within a view (ie knowing that a touch is 10% left and 30% from the top) \u2026I'm also squinting at SpatialTapGesture and seeing whether I could hack \"true\" pinch to zoom (with location) using this new API :thinking_face: > What is the difference between SpatialTapGesture and TapGesture ? For SpatialTapGesture the event in onEnded now provides the tap location. Ah, SpatialTapGesture only seems to have onEnded ... FB9489089 and FB9488452 remain! Would you recommend to use @State/@ObservedObject for any cases where where we don't expect the object to mutate? If the values you\u2019re represented are never going to change, there\u2019s no need to use @State or @ObservedObject . I highly recommend just making and using a standard struct . <@U03HKVDCL7N> what about reference types in views? Would let suffice? Should I ever be concerned that as the view gets thrown away and recreated my UISelectionFeedbackGenerator instance (for example) will be continuously re-created as well? Let would not suffice for reference types! A let binding for a reference type just says: \u201cThis will always point to the same instance of the object\u201d. It doesn\u2019t guarantee that that instance is immutable. If you can guarantee that your reference type is completely immutable, then you should be fine, but that can be a hard thing to ensure, so be cautious (and I would highly recommend using structs where you can) Petar, I wouldn\u2019t ever be concerned that your view gets thrown away and recreated. That\u2019s by design. SwiftUI views aren\u2019t linked to their actual UI representation, but rather are lightweight descriptions of views that can be recomputed quickly and easily. You should expect your views to be thrown away and recreated very frequently. Thank you <@U03HKVDCL7N>, and just to hammer to point: Don\u2019t worry not just that the views are getting recreated, but also what instances get recreated with them. Is there a way to enable interaction with the new device variants in previews? No, unfortunately not. The preview is only interactive when viewing it in the Live mode, and all the other modes are static previews Is there a way to make the Live mode preview landscape? yes, use the Device Settings button to change the orientation to be landscape with the new any keyword, will we still need AnyView to type erase views, or can we declare a collection with any View now? will this also have the same performance implications of using type erased views? Ideally you should not use AnyView and over the year ViewBuilder has improved enough that you should be able to eliminate most, if not all, its usage. That said, yes you will still need to use AnyView because you need to actually instantiate that type, and not just use View as a type. any View is just defining an (existential container) box. is it possible to use view builders as function params or property types without Generics? we are using AnyView to present content. there are different types of content each represented by an enum with associated value. This forced us to use AnyView to have concrete types. What would be your suggestion to move away from AnyView in this use case? Thank you :relaxed: If you have finite number of views you are displaying described by an enum could you switch over that enum in body of a view? ViewBuilder does support switch statements I have similar use case. Sometimes I would like to not couple the view which other views it present, but instead use other Flow/Factory logic that shows whats needed depending on the business logic. It forces me to use AnyView . E.g. struct MainView: View { @ObservedObject var viewModel: MainViewModel var channelsViewFactory: (Int?) -&gt; AnyView var body: some View { Text(\"ABC\") channelsViewFactory(viewModel.selectedId) } } ... extension AppDependencies: MainViewFactory { func makeMainView() -&gt; MainView { return MainView( channelsViewFactory: { id in if loggedIn { self.makeChannelsView(for: id) } else { self.makeSignInView() } } } } It really depends on your use case. But most of the time, if you're in the situation where you think you need [AnyView] or [any View] , what you should likely do is invert the view dependency flow and have [AnyDataModel or [any DataModel] instead, then create your views based on the type of data provided at runtime. In \u0141ukasz case, you could introduce generics into MainView (e.g., MainView&lt;ChannelsView: View&gt;: View ) and then create a container view above MainView to encapsulate the generic constraint(s). We can now use preview to instantly see different dynamic type sizes and schemes all at once when using SwiftUI in XCode. Can we also see different devices at the same time? e.g. all iPhones that support iOS16 We do not currently have variations on devices/device types available. We are actively monitoring for what set of variations the community would find the most useful, so please do file enhancement feedback reports with examples of where and how you think a new set of variations would be useful. Thank you <@U03H32HKZ0F>! Previewing the app live on all iPhones (or Apple Watches, or whatever) at once would be absolutely smashing! :zap: String input on WatchOS is quite limiting, because it happens in a non-customizable modal. Are there any tips or guides on how to achieve autofill of a textField on watchOS? <@U03JMMN8659> can you specify the text content type on your text fields? If you specify .username, .email, or .password, on the appropriate textfields, the system will provide the little keychain icons for autofill appropriately. please let me know if this works. In my case it is custom strings. In my app, the user can see a List of mushroom species and search for a specific species using TextField. On iOS, I implemented another List that overlays the screen and shows results satisfying the user-typed value. The user can tap one of the results to complete the string. Not sure how to proceed in watchOS though that sounds like a great use case. could you file a feedback with this info so we can pass it along? yes, that^ And until then .. . Can you create a modal, which has a list, and at the top of the list, you include a TextField, and below the textfield you include list items that are your autocompletions, and if they tap one of those, they get the short cut values, and if they tap the textfield they can enter the raw text? <@U03HW7PE3SM> will do! <@U03HHHXDL03> not quite, since each mushroom species is unique, so the selection depends on the first several characters of a string. For example, typing \"ama\" will bring up mushrooms from the \"Amanita\" genus. Also, on watchOS, once the user taps on the textfield, the entire screen is covered by the modal, so there is no space for dynamic suggestions to come up while the user is typing. Demo of how this works on iOS I wonder if the new quicktype bar in the keyboard in watchOS 9 would be able to autocomplete some of these? oh! you could totally implement this with search have you tried .searchable on watchOS ? I mean, it might not be exactly the same experience, but it would be very close. one minute, lemme look up documentation for it, it was an API we did last year, and I'm trying to page it back in I\u2019d start here: https://developer.apple.com/documentation/swiftui/adding-search-to-your-app lol, you found it before me... distractions are difficult to avoid :slightly_smiling_face: ya give that a shot, using suggestions based on the text, and using results, etc. It would not help in this case, but getting a number keyboard for watchOS could help in other input fields! thank you both <@U03HHHXDL03> and <@U03HELSV45B>! I will definitely take a look. But I'm not sure if searchable is the right solution, because I need to pass the user-typed value anyway, even if no results are found. For example, imagine the user just found a mushroom species that's not in my List! In this case, I allow the user to create a finding using whatever he/she typed (e.g. \"Unknown mushroom\", or \"Some goop\"). If the string is \"\", I also allow the finding to be saved as an \"Unknown species\". Is this something achievable using .searchable? ya, you could have one of the search results always be the literal string that the user provided So the first result would be the literal, and the rest of the results would be actual search results. you might want to talk with the design lab folks about differentiating the literal from the actual results, but you should be able to achieve something like that aha, interesting. And the matching results will somehow pop onto the screen as the user is typing? the user will have to type the first few characters and click the done button in the top right, so not exactly. if searchable is helpful, great, but if you still feel like there's room for something to be better, please file a feedback request. ok, thank you. I will definitely take a look and explore. <@U03HHHXDL03> thank you for your patience and kind support! :heart: may you find only choice mushrooms on your hikes :mushroom: ha! no problem at all. maybe I'll find a new species! My app\u2019s design uses kerning on most of its buttons, but I can\u2019t put that inside of my ButtonStyle because kerning is only available on Text, not ButtonStyleConfiguration.Label. I have a feedback open suggesting passing kerning through the environment (FB10020695). In the meantime, do you have any suggestions on how to do this without using .kerning() separately on every button label nor making a custom button View? The only other thing I can think of is to create a Button wrapper view that requires a Text be passed in as a parameter. Then attach kerning to the Text yourself and pass it to the underlying Button Haha, was hoping to avoid that so we didn\u2019t need to handle other permutations like buttons with text and an icon, etc :slightly_smiling_face: One other avenue I\u2019ve been exploring is a custom Text view that accepts kerning through an Environment value applied by the ButtonStyle, though that has led to needing to reimplement multiple init s from Text. Doesn\u2019t feel too great, either :sweat: Thanks! <@U03JELC631P> the approach I normally take here Is to create a reusable view specifically for the button label rather than an entire button view. This results in code that composes nicely. I still use button styles for styling buttons more generally. Thanks <@U03HT7Z5NAK>, that\u2019s sort of the direction I was headed with the \u201ccustom Text view\u201d idea\u2014think like: Button(action: {}) { MyAppText(\"Hello World\") } and then the ButtonStyle would set kerning in the environment for MyAppText to look at. Still means our team needs to remember to use MyAppText instead of Text , but maybe that\u2019ll be easier to remember than peppering .kerning everywhere :smile: How would one go about creating a Bubble Chart with Swift Charts? You can use the symbolSize(by:) modifier ( https://developer.apple.com/documentation/charts/chartcontent/symbolsize(by:) ) to size the points by data. To make the points look like bubbles, you can use .symbol(Circle().strokeBorder(lineWidth: 1)) so the symbols are drawn as stroked circles. Here is an example: Chart(data) { PointMark( x: .value(\"Wing Length\", $0.wingLength), y: .value(\"Wing Width\", $0.wingWidth) ) .symbolSize(by: .value(\"Weight\", $0.weight)) .symbol(Circle().strokeBorder(lineWidth: 1)) } great, thanks SwiftUI macOS: How can I control how the focus moves around the controls of my view when I press \"tab\"? Currently it jumps from top left to right, then down. But I have two columns in my window and would prefer it go down first on the left side and then again up to the right. Is there some functionality to pre-define the order of the controls? You can use View.focusSection() for this, which is newly available on macOS 13.0. Marking a view as a focus section causes the Tab loop to cycle through all of the section's focusable content as a group before moving on to the next thing in layout order. So, something like this should get you the sort of column-wise navigation you're after: struct ContentView: View { var body: some View { HStack { VStack { FocusableView() FocusableView() } .focusSection() VStack { FocusableView() FocusableView() } .focusSection() } } } Thanks, that's nice! Also, a plug for my colleague Tanu's WWDC21 talk: Direct and reflect focus in SwiftUI https://developer.apple.com/videos/play/wwdc2021/10023/ It goes over some tvOS use cases for View.focusSection() , and also describes how to work with focus programmatically using focus state bindings. With the new MenuBarExtra API on the Mac, if I want the menu item to be persistent after my app quits, do I need to do any extra work (e.g. adding a login item) or does the system automagically make sure it appears after reboot/logout? Hi - this is a great question. You can certainly use MenuBarExtra to create a standalone menu bar app, though you may want to look into the Info.plist changes noted in the documentation: > For apps that only show in the menu bar, a common behavior is for the app to not display its icon in either the Dock or the application switcher. To enable this behavior, set the LSUIElement flag in your app\u2019s Info.plist file to true . If the menu bar app is used in conjunction with a main app, then you'll need to package it together (ie, the \"helper app\" model). Adding a login item sounds like the way to go if you want it to show at login regardless of the user's preference for restoring state. We'd also certainly welcome any feedbacks for enhancement requests in this area as well. Got it, that combined with the new login item APIs this year should make this sort of thing a lot easier. Thanks! <@U03HHJH9C66> Please make MenuBarExtra available in Catalyst so we can use it with HomeKit which is Catalyst only on Mac at present. Is there any specific style guide for SwiftUI ? There are some cases in SwiftUI that I'm not sure what's the best. f.e. var body: some View { myAmazingView } (More readable) or var body: some View { my AmazingView } (A one-liner!) We don\u2019t have any specific style guide! Use whatever you and your team think feels best. Personally, I\u2019m never opposed to writing one liners on one line in Swift though :slightly_smiling_face: The language is so beautifully concise, so I like to embrace that. If your app is always behind an authentication session what is a good approach for blocking the app's content when authentication is required? In UIKit apps it was common to display a separate UIWindow atop your app's main window. Is this still a good way of handling it in a SwiftUI app? Interesting question! Without knowing the details of your app, my inclination would be to try the RedactionReasons API. You can use the .redacted modifier to set a redaction reason. SwiftUI controls will automatically react to that, hiding sensitive data. You can also read the reason from the environment to redact in custom controls. Not the most elegant, but I\u2019ve done something like this if let _ = auth.sessionToken { ContentView() } else { AuthView() } You could also use an overlay modifier at the root of your hierarchy to present a view over the whole app, then adjust that view\u2019s opacity based on the log in state. Yeah, it's not a cookie cutter kind of problem but curious about other ideas Personally, I\u2019d prefer swapping rootViewController of the app\u2019s window in a UIKit environment. I think the closest approach in SwiftUI would be as Andrew shared above. Not sure if this would cover the need in question though. This is my approach currently, but it\u2019s got some drawbacks... I\u2019m interested to know how others are doing. WindowGroup { switch userSessionManager.isLoggedIn { case true: TabViewScreen() // .viewModifiers //(environmentObjects, etc.) case false: LoginScreen() } } Yeah, the main drawback of conditionally showing different content is you kind of destroy the user's current context as you show the LoginScreen() which is why I like the window approach I tried using .fullScreenCover instead to fix that. But that does not guarantee that your sensitive data will always be covered up when the user is logged out mid-session. e.g. when another sheet or fullScreenCover from a subview is already shown, and even when it is dismissed later. .objectWillChange isn\u2019t called again and the login screen does not pop back up. This is actually still a problem with the above approach. So sadly I tend to avoid using sheets and fullScreenCovers for that reason. is it recommended to use switch-case views for e.g. showing different view types in a list or grid instead of using specific types via generics etc.?. are there any better ways? like: List(items) { item switch item.type { case ...: SomeView(item) case : ... } } When you use a switch within a @ViewBuilder closure, you are in fact using the power of generics :wink:, but in most common cases you hopefully don\u2019t have to think about that overtly. In general, using switch in view builders is a great tool! The best approach really case-by-case though, just as when deciding to use a switch versus other control flow in regular imperative code. Are there any specific alternatives to a switch you were considering? Is there a preferred method to allow user interaction with data points in a chart imbedded in a scrollview? Can you provide a little more detail here? How are you trying to interact with the data points in the Chart ? And the scroll view scroll gesture is interfering maybe? My app uses data visualization to allow the user to edit errors. So I would like the user to tap on the portion of the chart they are interested in and link to a List below where that data row is then selected for editing There can be hundreds of data points in the list so finding the correct one to edit can be cumbersome without the touch feature I currently don\u2019t use a scroll view but would like to as this would allow more space for tapping on the data element. This talk https://developer.apple.com/wwdc22/10137 has some examples of adding interaction I think that would be orthogonal to the question of embedding the chart in a scroll view, but it may complicate hit detection depending on how big the chart is It is an iPhone app so I don\u2019t have lots of space. Does branching in a views body (via if-else or switch ) cause the creation of each branch's views? It only creates the views for the branch of the if / switch that\u2019s actually taken. This is why conditional / inert modifiers are so important, too. Highly recommend \u201cDemystify SwiftUI\u201d from WWDC 2021 for more info. That was an awesome session, I definitely need a refresher :sweat_smile: I\u2019ve watched it \u2026 way too many times. :sweat_smile: This topic actually deserves a small section of a sample app. We should be shown how to use ViewState enums, rather than if-else statements within View! :zap: What is the best way to pass down data managers, specifically a media player to subviews? You can pass them just as argument to your subview initializer. If you have multiple subviews or the subview that needs to use that data manager is further down the hierarchy you could use the Environment to pass is down Great. Is there an example of this somewhere? The documentation for EnvironmentKey has code example for it https://developer.apple.com/documentation/swiftui/environmentkey Wonderful, thank you! Is it a known problem that using a custom Layout with multiple subviews doesn't compile? For example, this minor change to the CustomLayoutSample project, adding a second subview to the use of MyEqualWidthHStack, won't compile: struct ButtonStack: View { var body: some View { ViewThatFits { // Choose the first view that fits. MyEqualWidthHStack { // Arrange horizontally if it fits... Buttons() Text(\"hello\") } MyEqualWidthVStack { // ...or vertically, otherwise. Buttons() } } } } Or am I just doing it wrong? FB10113527 That\u2019s a known issue. See the iOS and iPadOS release notes here: https://developer.apple.com/documentation/ios-ipados-release-notes/ios-ipados-16-release-notes You can search for \u201ccustom Layout\u201d How can I change the (text) color of the scales at the sides in a Swift Charts? You can change the color of the axis labels with the .chart{X/Y}Axis modifier, where you can configure the foregroundStyle of AxisValueLabel . Here is an example changing the label color for the Y axis: .chartYAxis { AxisMarks { _ in AxisGridLine() AxisTick() AxisValueLabel().foregroundStyle(Color.red) } } You can change the color of the grid line and tick in a similar way too. Could it be that this currently doesn't work in the developer beta? Because I just tried the following code and it applied the colors on the grid line and tick, but not on the label next to the tick. .chartYAxis { AxisMarks { _ in AxisGridLine() .foregroundStyle(.yellow) AxisTick() .foregroundStyle(.blue) AxisValueLabel() .foregroundStyle(.red) } } Yes, looks like this is a bug in the beta. Feel free to file a bug via Feedback Assistant. As we are incrementally adopting SwiftUI in a UIKit app, are there guidelines for replacing the patterns that we are used to in a UIKit app, like delegate pattern, notifications, target-actions etc? Some of these are obvious but I kind of feel I'm doing something wrong whenever I pass a closure into a view to get a feedback to replicate delegate pattern for example. :sweat_smile: Delegates do a lot of different things in UIKit, and so the way that they should be translated into SwiftUI can vary heavily depending on what they\u2019re being used for in UIKit. In some situations they\u2019ll become closures that you pass around, in others, a Binding or State , sometimes a onChange(of:) , etc. UIKit and SwiftUI have two very different programming models, so often times, concepts don\u2019t map over in a one-to-one way. I\u2019d highly recommend you book a lab appointment so we can chat more about what specific case you\u2019re trying to translate, and where your pain-points are there :slightly_smiling_face: Awesome, will definitely do that. Can you please touch on using Combine for such feedback mechanisms? I also explored passing a subject into a view to observe a feedback but that also felt a bit wrong. I'm working with CoreImage filters, and currently using the CIContext to create a CGImage, which is turned into a UIImage, which initializes a SwiftUI Image. Is there a better or more efficient way? You should be able to avoid the intermediate UIImage and initialize a SwiftUI Image directly with a CGImage omg, how did I not see this? Thank you!! How can I add use an SVG in my SwiftUI app? You can add an SVG in the asset catalog and it can be rendered via a named Image . Checking \u201cpreserve vector content\u201d (or similar) in the asset item preferences will render it from vectors instead of raster. Drag SVG to Assets. In Inspector, check \"Preserve vector data\", and set scale to \"single scale\" Is there any way to load SVG image from file system? Also, an important tip that should be more known: SVG foreground color can be changed dynamically! So you don't need different color versions of your SVG image if you want to change color while your app is running I noticed that when a subview uses a binding to published variable from an ObservableObject, the subview's body will re-execute any time any of the other published variables in the ObservableObject are modified. Even if this modified variable isn't used by the subview at all. Is this a bug or a performance concern? The subview updates based on the objectWillChange publisher, which is a composite of all the published properties. This is by design, but can require fine tuning of models. Different apps can handle this different ways, so this might be an excellent question for a lab. Or even a Developer Technical Support ticket! Thank you! And views are incredibly cheap in SwiftUI! Don\u2019t hesitate to break them up into smaller pieces. And generally avoid side effects in your view\u2019s initializer. Generally, IMO, I wouldn\u2019t expect performance concerns from the body being repeatedly invoked. Yeh pass properties of the object into subviews and then only the views that were given different values should have their body called. However if a view has a FetchRequest its body is called every time because of a long term bug! I currently have a large data set (over 10k datapoints) that I'm working with. Are there any tips on how to improve the graph performance when working with creating line graphs from data sets of that size or larger? Context is cycling related data in a macOS app that is presenting 6 or more in a LazyVGrid. We recommend simplifying the data before rendering it. For your example, you could simplify the line or summarize the data points in small intervals (let\u2019s say you have data for a year, you summarize the data for a day). The added advantage is that you can summarize using mean, min and max and show the full range within the small interval. Sorry if non-Apple people aren't supposed to comment on this, but an Apple resource I found helpful with this kind of thing is using antialiasing filters to resample the data. https://developer.apple.com/documentation/accelerate/resampling_a_signal_with_decimation Thank you for the pointer <@U03JKLW88TZ>. Another useful resource are the number and date bins we released this year: https://developer.apple.com/documentation/charts/datebins and https://developer.apple.com/documentation/charts/numberbins . Is there a way to get AreaMark to work with .chartYScale(domain: .automatic(includesZero: false)) ? As soon as I add an AreaMark it seems to always include 0 in the scale. You could try setting yStart and yEnd for the area, here is an example: Chart(data) { AreaMark( x: .value(\"Date\", $0.date), yStart: .value(\"Start Price\", 100), yEnd: .value(\"Price\", $0.price) ) } .chartYScale(domain: .automatic(includesZero: false)) The automatic option will use nicely rounded numbers. If that's not working for you, you can also set the domain directly like domain: 100 ... 1000 :thinking_face: the first suggestion doesn\u2019t quit work because there is some padding added to the y range. I\u2019ll give a specific range a try next. Just setting a range doesn\u2019t quit work either\u2026 I guess I would need to do both? Umm, could you try turning off roundLowerBound in axis marks? Full code would be like this: Chart(data) { AreaMark( x: .value(\"Date\", $0.date), yStart: .value(\"Start Price\", 100), yEnd: .value(\"Price\", $0.price) ) } .chartYScale(domain: .automatic(includesZero: false)) .chartXAxis { AxisMarks(values: .automatic(roundLowerBound: false)) } roundLowerBound tries to add a round number below the data range, that might be why we are seeing a 40,000 value below the area like in your first screenshot. That works Can Swift Charts be adapted for non-discrete data, like curves, etc today? You can render curves by sampling data points along the function and then rendering that. You can add interpolation to make the line look smooth. Swift Charts cannot directly render functions. Thanks Dominik M for clarifying the point of functions and Swift Charts. <@U03HW7NSMFT> I was wondering if there were some kind of closure-based API to encapsulate the function, with parameters like range and increment Examples There is not but you could sample a few points and then render the line. Is there any sample code or documentation on how to display vector fields with Swift Charts?:upside_down_face: Like this? Yep This screenshot is from the Hello Swift Charts talk. Let me find the code for it. You can use a custom symbol ( Arrow ). Chart(data, id: \\.x) { PointMark(x: .value(\"x\", $0.x), y: .value(\"y\", $0.y)) .symbol(Arrow(angle: CGFloat(angle)) .foregroundStyle(by: .value(\"angle\", angle)) .opacity(0.7) } ... struct Arrow: ChartSymbolShape { let angle: CGFloat let size: CGFloat func path(in rect: CGRect) -&gt; Path { let w = rect.width * size * 0.05 + 0.6 var path = Path() path.move(to: CGPoint(x: 0, y: 1)) path.addLine(to: CGPoint(x: -0.2, y: -0.5)) path.addLine(to: CGPoint(x: 0.2, y: -0.5)) path.closeSubpath() return path.applying(.init(rotationAngle: angle)) .applying(.init(scaleX: w, y: w)) .applying(.init(translationX: rect.midX, y: rect.midY)) } var perceptualUnitRect: CGRect { return CGRect(x: 0, y: 0, width: 1, height: 1) } } This is not the complete code but should give you enough to make the example. Nice! That\u2019s the thing I was looking for, thank you! Alright, I\u2019m getting closer :smile: What\u2019s left is to figure out what the format was used for data in the sample and where angle came from :thinking_face: Very cool. And here we go It is quite delightful to use this framework and the performance is great. Many thanks to the team :pray: Though, documentation seems to be out of sync with reality a bit. For example, this snippet from PointMark does not compile. I\u2019m assuming that the functionality with KeyPaths is not in the first beta yet since I recall similar syntax from the sessions How are large datasets handled, is there automatic sampling or some kind of recommended limit? There is no automatic sampling in Swift Charts. I would expect the \"preferred\" sampling method to vary on a case-by-case basis. I think the best approach is to see what works for you and then sample in a manner that is honest to your data as you run into performance issues. We're always looking to improve the performance of the framework! File feedback if you run into issues. As a general design principle, I would recommend simplifying / aggregating data as it becomes large. It's rarely useful to look at large amounts of raw data on the screen, as you run into the limits of 2d rendering wrt to occlusion, fidelity, and noise resolution. Does that help <@U03HZ4PT2ER>? <@U03H3193G3H> A little, so are you recommending at most one mark per screen point? and would we use GeometryReader to get that info? One point per screen point is probably excessive too (from a legibility perspective, if not a performance one)... it would help to get an idea of what you are trying to achieve & what your data looks like We draw a lot of scientific data, some is discrete, other is calculated/continuous Can you give a rough estimate to what number of marks the Charts will scale? Like a 1000? I\u2019d also like to plot data that comes in every other second in a scroll view, and would probably have to cut off at some point? Our data is available at once, and can have 5000 points. It will be static with no points just the lines cc <@U03HHJH7RJN>! It depends on the machine performance and complexity of the chart. It's usually a good idea to test with your actual data and chart to figure out how many points can be drawn with acceptable user experience. No rough scale? Like 100, 1000, 10k, 100k no problem? On say an iPhone 12 Mini? Based on my experience it's about 1000 to 2000 for smooth animations. Thanks! You may be interested in the date and number bin apis we added this year: https://developer.apple.com/documentation/charts/numberbins https://developer.apple.com/documentation/charts/datebins . You ca use them to group your data and then summarizing the data in each group/bin. <@U03HW7NSMFT> Interesting, we already have histograms, which we handle differently from this, but I will take a look For large data sets scrolling is inevitable. I guess we have to embed some Chart views into something's like a lazyhgrid - assuming that we can get rid of any padding/scaling numbers to get a seamless/infinite chart. Is that doable? <@U03JRNE4KJL> you might be able to do that with the chartOverlay and manual drawing with the axis objects, then handle gestures for scrolling How easy is it to support interactions like pan-to-move and pinch-to-zoom? For pan-to-move, you can use a SwiftUI gesture in conjunction with ChartProxy . From the gesture, you get the the pan distance, and then you can use ChartProxy to figure out the pan distance in the data domain. Then, you can set the domain for the X scale with .chartXScale(domain: start + offset...end + offset) , where you can adjust the offset to pan the chart. Can a similar strategy be used for pinch-to-zoom? Yes, you can use a pinch to zoom gesture (or any other gesture) and hook up the events in a similar way. Without location information (like the new SpatialTapGesture ) it would be a bit unnatural as we wouldn't know where to center the zoom, right? MagnificationGesture doesn't provide location information, unless I've missed something Feel free to file a feedback to SwiftUI for providing location information to gestures. For now I think you can try implement a UIView with UIPinchGestureRecognizer and then wrap the view with UIViewRepresentable so it can be used in SwiftUI. Can we highlight specific data points and dim the rest; similar to what http://Health.app|Health.app does when selecting \u201cmin/max\u201d, \u201clatest\u201d, etc in Heart Rate and other charts? absolutely! An easy way to do this is to store the desired highlighted data's ids in a @State and check against that in your ForEach of Marks . Something along the lines of ForEach(data) { value in BarMark(x: ..., y: ...) .foregroundStyle(value.id == state.highlighted ? Color.red : Color.gray) } Is it possible to use a Chart to display a large amount of data? For example, display a waveform. Yes! https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654798399767779?thread_ts=1654798325.944549&cid=C03HX19UNCQ We generally recommend aggregating very large datasets since there are only so many marks that can be rendered and reasonably read by a user. Swift Charts can process and render a reasonable number of data points. Thank you! Is there multiple axis support? Logarithmic axis support? A Swift Charts chart can only have one axis per x and y. Swift Charts supports logarithmic scales. For example, you can specify a log scale on x with .chartXScale(type: .log) Thanks You can however, have multiple sets of labels / ticks / grid lines by adding multiple AxisMarks they will share the same scale, but might be useful for things like displaying both C and F on the same chart Yeah, we often have requirements to show multiple data sets with varying scales for comparisons Can you put those different sets of marks on opposite sides of the chart? In https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654799167892029 I talked about ways to show multiple metrics as separate charts. Swift Charts is really fun! Thanks for adding this, I'm having a blast! :) I have one question about the axis labels on charts. When I use the chartXAxis modifier to show custom AxisMarks with an array of values (dates in my case), the chart doesn't show the last value on that axis (the last date). Is it possible to turn that on? // Simple Chart .chartXAxis { // data contains two data points with a date. The entire chart only has these two data points AxisMarks(preset: .extended, values: data.map(\\.date)) } Love to hear it <@U03J21TPWPM>! I think I can guess what is going on here. If a label is placed beyond the edge of the axis, (as can be the case with the last value of a list of dates, as it would define the end of your date range) it won't be rendered, as it would run off the edge of the chart. Off the top of my head, solutions may include: adding a bit of \"padding\" in your date range or switching the axis style. does that help? Yes, thanks :slightly_smiling_face: Is there a way to have multiple scales per chart? Let's say I'm doing weather and I want one line with cloud cover (a 0-100% scale) and rainfall per hour (in, say, mm). The x axis on both of those would be time, but the y axes are different. An axis always has one scale. So x and y have one scale. You can have multiple axes to display e.g. temperature in C and F. If you want to show multiple measures, I would recommend multiple charts with aligned x-axes. You can even hide the axis of the upper chart to create a more compact view. Hey Richard, I'm in love with the Swift Charts framework. I like how we can customise the charts to fit our needs. I noticed some layout issues with the Legend, when there are a lot of items to display, or when the names are long. Do you plan to make the legend Layout automatically wrap/use multiple rows when there is not enough space? I noticed the iCloud design on iOS switches from a HStack to a VStack when the device accessibility is set to true. My feedback with more infos: FB10125848 Thank you for your feedback! Adding data to charts and the modifiers always requires this .value(_:) function that requires a label key. What exactly is the purpose of that label key? Is it some kind of identifier? Does the label key in a foregroundStyle have to match one in a LineMark , for example (if referencing the same data)? Thanks! :) The label key is used for generating a good default description for the chart for VoiceOver users and the audio graph. For example, if you have x: .value(\"time\", ...), y: .value(\"sales\", ...) , VoiceOver users will hear something like \"x axis shows time, y axis shows sales\". The label key in foregroundStyle doesn't necessarily need to match LineMark , but it's a good practice to use the same label if the data is the same. Awesome, thanks for the explanation! :slightly_smiling_face: Are there any limitations to the result builder syntax used to add marks? <@U03J21ZK802> can you clarify what you're looking for here? How many entries can we put in there? View builders are typically limited to 10. Would grouping help if there is an arbitrary limit based on the builder? Got it! Right now it's 10. Grouping into ForEach's etc would help! If you have a use case that requires more, please file feedback~ Swift Charts looks very cool and well designed. What were the inspirations for its API design? It feels very similar in many aspects to ggplot2 which itself is apparently based on \"The Grammar of Graphics\" book. Was that an inspiration? Thank you. Its\u2019 great to hear that you enjoy the API. Swift Charts is a grammar based visualization language, which means charts are composed of building blocks (marks and mark properties) instead of picked from a list of chart types (vertical bar chart, bubble chart, waterfall chart etc). The Grammar of Graphics was hugely influential for many visualization APIs like ggplot, D3, Vega and also Swift Charts. Hi, How can we remove or configure the space between the bars (BarMark) ? Is it possible to define the bar width ? Thanks. BarMark has a width parameter that takes a MarkDimension ! It can be a fixed value or a ratio (of the space the bar is allocated). Thanks. When trying, even with ratio = 1 or inset = 0 it seems to always have some space between the bars. <@U03JSSE35GQ> mind sharing the code? and a screenshot (if you're comfortable) No problem. `*import* SwiftUI` `*import* Charts` `*struct* Point {` `*let* month: Date` `*let* sales: Int` `*let* color: Color` `*let* eat: Bool` `*init*(month: Date, sales: Int, color: Color, eat: Bool = *false*) {` `*self*.month = month` `*self*.sales = sales` `*self*.color = color` `*self*.eat = eat` } } `*func* date(year: Int, month: Int, day: Int = 1) -&gt; Date {` Calendar.current.date(from: DateComponents(year: year, month: month, day: day)) ?? Date() } `*struct* ContentView: View {` `*var* body: *some* View {` Chart(data, id: \\.month) { p *`in`* `BarMark(` `x: .value(\"Month\", p.month, unit: .month),` `y: .value(\"Sales\", p.sales),` `width: MarkDimension.ratio(1)` `)` `.foregroundStyle(p.color)` `.annotation(position: .top) {` `*if* p.eat {` Image(systemName: \"seal\") } } .annotation(position: .bottom) { `*if* p.eat {` VStack { Text(\"\") Text(\"23\") } } } } // .chartXAxis { // AxisMarks(values: .stride(by: .month)) { value in // if <http://value.as|value.as>(Date.self)!.isFirstMonthOfQuarter { // AxisGridLine().foregroundStyle(.black) // AxisTick().foregroundStyle(.black) // AxisValueLabel( // format: .dateTime.month(.narrow) // ) // } else { // AxisGridLine() // } // } // } } `*let* data: [Point] = [` Point(month: date(year: 2021, month: 6), sales: 300, color: Color.yellow), Point(month: date(year: 2021, month: 7), sales: 350, color: Color.yellow), Point(month: date(year: 2021, month: 8), sales: 400, color: Color.orange), Point(month: date(year: 2021, month: 9), sales: 450, color: Color.red), Point(month: date(year: 2021, month: 10), sales: 420, color: Color.green), Point(month: date(year: 2021, month: 11), sales: 410, color: Color.green), Point(month: date(year: 2021, month: 12), sales: 390, color: Color.green), Point(month: date(year: 2022, month: 1), sales: 430, color: Color.green), Point(month: date(year: 2022, month: 2), sales: 450, color: Color.green), Point(month: date(year: 2022, month: 3), sales: 500, color: Color.green), Point(month: date(year: 2022, month: 4), sales: 550, color: Color.green), Point(month: date(year: 2022, month: 5), sales: 700, color: Color.green), Point(month: date(year: 2022, month: 6), sales: 800, color: Color.green), Point(month: date(year: 2022, month: 7), sales: 900, color: Color.yellow), Point(month: date(year: 2022, month: 8), sales: 1000, color: Color.orange), Point(month: date(year: 2022, month: 9), sales: 1200, color: Color.red), Point(month: date(year: 2022, month: 10), sales: 1300, color: Color.green), Point(month: date(year: 2022, month: 11), sales: 1310, color: Color.green), Point(month: date(year: 2022, month: 12), sales: 1200, color: Color.green), Point(month: date(year: 2023, month: 1), sales: 1400, color: Color.green), Point(month: date(year: 2023, month: 2), sales: 1600, color: Color.green), Point(month: date(year: 2023, month: 3), sales: 1800, color: Color.green), Point(month: date(year: 2023, month: 4), sales: 2000, color: Color.green), Point(month: date(year: 2023, month: 5), sales: 2200, color: Color.green), Point(month: date(year: 2023, month: 6), sales: 2400, color: Color.green), Point(month: date(year: 2023, month: 7), sales: 2500, color: Color.yellow, eat: `*true*),` Point(month: date(year: 2023, month: 8), sales: 2450, color: Color.orange), Point(month: date(year: 2023, month: 9), sales: 2430, color: Color.red), Point(month: date(year: 2023, month: 10), sales: 2100, color: Color.green), Point(month: date(year: 2023, month: 11), sales: 2000, color: Color.green), Point(month: date(year: 2023, month: 12), sales: 1800, color: Color.green), Point(month: date(year: 2024, month: 1), sales: 1500, color: Color.green), Point(month: date(year: 2024, month: 2), sales: 1300, color: Color.green), Point(month: date(year: 2024, month: 3), sales: 1400, color: Color.green), Point(month: date(year: 2024, month: 4), sales: 1410, color: Color.green), Point(month: date(year: 2024, month: 5), sales: 1450, color: Color.green), Point(month: date(year: 2024, month: 6), sales: 1500, color: Color.green), Point(month: date(year: 2024, month: 7), sales: 1700, color: Color.yellow), Point(month: date(year: 2024, month: 8), sales: 1800, color: Color.orange), Point(month: date(year: 2024, month: 9), sales: 1830, color: Color.red), Point(month: date(year: 2024, month: 10), sales: 1900, color: Color.green), Point(month: date(year: 2024, month: 11), sales: 1950, color: Color.green), Point(month: date(year: 2024, month: 12), sales: 2000, color: Color.green), Point(month: date(year: 2025, month: 1), sales: 2050, color: Color.green), Point(month: date(year: 2025, month: 2), sales: 2100, color: Color.green), Point(month: date(year: 2025, month: 3), sales: 2150, color: Color.green), Point(month: date(year: 2025, month: 4), sales: 1900, color: Color.green), Point(month: date(year: 2025, month: 5), sales: 1850, color: Color.green), Point(month: date(year: 2025, month: 6), sales: 1800, color: Color.yellow), Point(month: date(year: 2025, month: 7), sales: 1700, color: Color.yellow), Point(month: date(year: 2025, month: 8), sales: 1600, color: Color.orange), Point(month: date(year: 2025, month: 9), sales: 1500, color: Color.red), Point(month: date(year: 2025, month: 10), sales: 1400, color: Color.green), Point(month: date(year: 2025, month: 11), sales: 1200, color: Color.green), Point(month: date(year: 2025, month: 12), sales: 1000, color: Color.green), Point(month: date(year: 2026, month: 1), sales: 800, color: Color.green), Point(month: date(year: 2026, month: 2), sales: 700, color: Color.green), Point(month: date(year: 2026, month: 3), sales: 650, color: Color.green), Point(month: date(year: 2026, month: 4), sales: 600, color: Color.green), Point(month: date(year: 2026, month: 5), sales: 500, color: Color.green), Point(month: date(year: 2026, month: 6), sales: 300, color: Color.green), Point(month: date(year: 2026, month: 7), sales: 250, color: Color.yellow), Point(month: date(year: 2026, month: 8), sales: 200, color: Color.orange), Point(month: date(year: 2026, month: 9), sales: 210, color: Color.red), Point(month: date(year: 2026, month: 10), sales: 200, color: Color.green), Point(month: date(year: 2026, month: 11), sales: 220, color: Color.green), Point(month: date(year: 2026, month: 12), sales: 250, color: Color.green), Point(month: date(year: 2027, month: 1), sales: 300, color: Color.green), Point(month: date(year: 2027, month: 2), sales: 400, color: Color.green), Point(month: date(year: 2027, month: 3), sales: 500, color: Color.green), Point(month: date(year: 2027, month: 4), sales: 600, color: Color.green), Point(month: date(year: 2027, month: 5), sales: 800, color: Color.green), Point(month: date(year: 2027, month: 6), sales: 1000, color: Color.green), Point(month: date(year: 2027, month: 7), sales: 1200, color: Color.yellow), Point(month: date(year: 2027, month: 8), sales: 1400, color: Color.orange), Point(month: date(year: 2027, month: 9), sales: 1600, color: Color.red), Point(month: date(year: 2027, month: 10), sales: 1800, color: Color.green), Point(month: date(year: 2027, month: 11), sales: 2000, color: Color.green), Point(month: date(year: 2027, month: 12), sales: 2200, color: Color.green), Point(month: date(year: 2028, month: 1), sales: 2400, color: Color.green), Point(month: date(year: 2028, month: 2), sales: 2450, color: Color.green), Point(month: date(year: 2028, month: 3), sales: 2300, color: Color.green), Point(month: date(year: 2028, month: 4), sales: 2310, color: Color.green), Point(month: date(year: 2028, month: 5), sales: 2320, color: Color.green), Point(month: date(year: 2028, month: 6), sales: 2400, color: Color.green), Point(month: date(year: 2028, month: 7), sales: 2500, color: Color.yellow), Point(month: date(year: 2028, month: 8), sales: 2600, color: Color.orange), Point(month: date(year: 2028, month: 9), sales: 2670, color: Color.red), Point(month: date(year: 2028, month: 10), sales: 2500, color: Color.green), Point(month: date(year: 2028, month: 11), sales: 2300, color: Color.green), Point(month: date(year: 2028, month: 12), sales: 2000, color: Color.green), Point(month: date(year: 2029, month: 1), sales: 1500, color: Color.green), Point(month: date(year: 2029, month: 2), sales: 1000, color: Color.green), Point(month: date(year: 2029, month: 3), sales: 750, color: Color.green), Point(month: date(year: 2029, month: 4), sales: 500, color: Color.green), Point(month: date(year: 2029, month: 5), sales: 2, color: Color.green), ] `*let* averageValue = 137` } `*extension* Date {` `*var* isFirstMonthOfQuarter: Bool {` Calendar.current.component(.month, from: `*self*) % 3 == 1` } } `*struct* ContentView_Previews: PreviewProvider {` *`static`* `*var* previews: *some* View {` ContentView().frame(width: 400, height: 100.0) } } The screenshot is from the SwiftUI preview. cc <@U03HHJH7RJN>! this is a pixel boundary rendering artifact. One solution is to make the width just a tadddd over 1 so they overlap just a bit It's a rendering artifact that happens when the bars doesn't align with pixel boundaries. There are a couple of ways to address it: \u2022 As Halden mentioned, you can make the bars slightly wider (with something like .inset(-1) or .ratio(1.01) ) \u2022 You can also tweak the width of the plot area (with .chartPlotStyle { $0.frame(width: plotAreaWidth) } ) based on the number of bars, so that the bars are aligned exactly with pixel boundaries. This is more complex to do, but will give the best result. Thanks ! :+1: Do you have any suggestions for displaying \"goal progress\" data? Saw in earlier Q&A that radial/rings charts are not supported, but other types could be used. Looking for visualization similar to Health activity rings. Thinking about 3 bars normalized to % of progress toward goal. You could make a chart with three bars one for each category. If you set the scale to have a domain from e.g. 0 to 100, then the length of a bar indicates the progress towards a goal. A horizontal bar chart could work well. Thanks <@U03HW7NSMFT> that's exactly what I was thinking, too. But unsure on upper bound of chart when goal is exceeded. For rings, if my steps goal is 5000 and I actually take 12000, then it would show 240% for that bar. That\u2019s an interesting consideration. You could have a chart that extends beyond 5000 but adds a vertical RuleMark to annotate the goal. Since you may show different metrics (e.g. steps and distance), you may want to normalize them to % of goal so they have the same scale/axis. Thanks again, I'll give this a whirl! Hi everyone! Is there a suggested \"update rate\" that you'd recommend for real-time charts? When trying this out on a simulated iPhone, I quickly approach 90%+ CPU usage if I try to update a line graph more than 40 times/second (For example, when trying to visualize a simulated accelerometer). Humans have a hard time with anything more than 10Hz anyway... We recommend benchmarking your app to find the right balance between the update rate and the amount of data on your line chart. We'd also appreciate feedback through Feedback Assistant for your use case! If I may follow up: if we're to alter the drawing rate so it adds new data points in batches, does Charts have built-in functionality for that, or would we need to set up the Combine publishers manually? I didn't see anything in the documentation, but I understand the docs could also be in beta Honestly at 40Hz I think you're looking at game semantics, i.e. Metal or at least SceneKit Yeah when I previously worked on the project that would use my own pre-\"Swift Charts\" charts. I experimented with rendering the charts using Metal, and that was not a very productive week :sweat_smile: You would need to write a small framework, but SceneKit (or even SpriteKit?) could do a lot of the work But I think you need to isolate your fast-updating surfaces and not drag all of UIKit/SwiftUI into it otherwise the performance will crash We have data that doesn't necessarily progress linearly from one point to the next, for example the x and y values could either increase or decrease from one point to the next. Would those points be linked as if they were points on a path or would it upset the system? I\u2019m not sure I fully got the question but you can either draw points as separate with PointMark or linked with a LineMark . Adding linked points is well supported by Swift Charts. Sounds like \"can I do XY scatter with joined lines\" Ahh. The answer is yes. Use a LineMark . Thanks I am wondering why Pie Charts are visible in promotional material or in the sessions but don't seem to be supported with Swift Charts. I know that Pie Charts can be misleading, but they are still ideal for some cases. We'd appreciate feedback through Feedback Assistant! Will do! FB10138361 FB10138491 thanks <@U03HUM5PDHV>! I have a horizontal bar chart with two Text overlay annotations on each bar, one aligned leading and one aligned trailing. When the bars are short the annotations overlap and eventually truncate. Is there an easy way to hide overlayed annotations when the bars are too short to accommodate their intrinsic widths or should I explore other layout configurations (e.g. one annotation with an HStack containing two Text views spanning the context target region width)? There isn't a way to hide overlayed annotations right now, but feel free to file a feedback :slightly_smiling_face: In this situation, you can explore other layout options as you mentioned. For example, a single annotation with HStack { Text(...); Spacer(); Text(...) } Thanks. Is there a way for external developers to extend Swift Charts and add new chart types that aren't supported out of the box, for example pie charts, or charts with 3D styling? You can create custom marks but they need to be cartesian. So you can make e.g. a candlestick or boxplot. Swift Charts does not support pie charts or 3D charts. See https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654618060787259 for a discussion of alternatives. Thank you. So I would assume a 2D bar graph with a fake 3D effect for the bar marks should be easily doable. But no circular graphs (for now). Yes, I would agree with that. Is there a method to modify or specify the label style of a chart? For instance, my chart includes price averages for home heating oil. I want to clearly display on the label that the Double is in dollar format on the Y axis. At this time, when I attempted a brief demo app it simply made the Y axis label as an Int. <@U03JF5S79RQ> sorry for the late reply! Yes, there are a few options here. You can use AxisMarks(format: FormatStyle) or AxisValueLabel(format: FormatStyle) or AxisValueLabel { Text(\\(..., format: ...)) } :heart: Perfect, thanks! If your units are more verbose, a design alternative is to clarify them in the title or headline of your chart, to save horizontal space on the y axis. At this time I'm tracking price averages per month over the last 12 months, the user will have the ability to toggle between the last 12 and 6 months. The X axis is the Month, the Y axis is the Average Price. the former is likely enough then, makes sense! Can charts be used in the new Preview option to see a live chart in the preview without needing to go to that screen? Such as current screen allows new data entry and the preview shows the updated chart? Also, can previews be used from widgets to preview the chart without loading the app? Can you clarify what you mean by the preview option? Charts can be in any SwiftUI View. contextMenu(menuItems:preview:) Using the contextMenu in a touch and hold scenario for a button that would lead to the view with a graph - can the live graph be shown? or is it only for fixed assets like an image? You can use Swift Charts to draw live data. But does it update for the preview, or when you've loaded the view with the chart? Swift Charts behaves like other Views and updates the same way. So is the contextMenu preview a View in its own right? I\u2019m not familiar with the contextMenu preview but it looks like it might work. Give it a try and if it doesn\u2019t work file a feedback. Okay, thanks Dominik Does Charts support polar coordinates? It does not, but feedback is appreciated!! Will do. This is a great start! Hi, Sorry if this was already asked, but how can I add a text or SF Symbol at the top of a vertical BarMark ? (I did not found the information at https://developer.apple.com/documentation/charts/|https://developer.apple.com/documentation/charts/ ) ? Thanks. You can use the annotation modifier to add an annotation on top of the bar, where the content of the annotation is a SF Symbol image. Here's an example: BarMark(...) .annotation(position: .top) { Image(systemName: \"sfsymbol_name\") } Note that you can also use Text or other views as the content of the annotation. Parfect ! This is amazing how configurable the Charts are. It will save me a lot of time in my application! Thanks. The charts are amazing! I saw the example for the Interval Bar Chart in the docs and it got me thinking I could represent a simple weekly view of a student's course schedule if I flipped the axis. I made a proof of concept and it worked really well! I calculate the duration of a meeting in minutes to create the size of the BarMark using yStart and yEnd. Is this a good way to do this or can it be achieved with just the start date time and end date time? I also had to reverse the chartYScale to get the AM meetings to apear first, not sure if that is expected or not. struct Series: Identifiable { let courseName: String let meetings: [Meeting] var id: String { courseName } } struct Meeting { let startDate: Date let endDate: Date let startMinute: Double //Minutes since 7am let endMinute: Double } Chart(data) { series in ForEach(series.meetings, id: .startDate) { element in BarMark( x: .value(\"Course\", element.startDate, unit: .day), yStart: .value(\"Start Time\", element.startMinute), yEnd: .value(\"End Time\", element.endMinute) ).annotation(position: .overlay, alignment: .top) { VStack { Text(series.courseName).font(.caption) Text(element.startDate.stringValue(dateFormat: .time)).font(.caption) } } } .cornerRadius(8.0) .foregroundStyle(by: .value(\"Course\", series.courseName)) } .chartYAxis { AxisMarks(position: .leading, values: .stride(by: 60)) { axis in AxisTick() AxisGridLine() AxisValueLabel(centered: false) { if axis.index+7 < 13 { Text(\"(axis.index+7) AM\") } else { Text(\"((axis.index+7)-12) PM\") } } } }.chartXAxis { AxisMarks(position: .top, values: daysOfTheWeek) { _ in AxisGridLine() AxisTick() AxisValueLabel(centered: true) } }.chartYScale(domain: .automatic(reversed: true)) <@U03JDV4PQR0> beautiful!! You can use Date here instead of converting to integer minutes and it should go top to bottom by default. cc <@U03HHJH7RJN> Looks great! Maybe try setting the width of the BarMarks to 100%? amazing Use Date as the unit of x? sorry, I was trying to answer your question about the y axis Int and Double will go bottom up (low to high) while dates will go top down (early to late) lmk if that helps / if you have follow-up Qs I tried: yStart: .value(\"Start Time\", element.startDate), yEnd: .value(\"End Time\", element.endDate) But it did not go well :rolling_on_the_floor_laughing: Ok I understand now. I think you're approach is good! (turning it into a number and then applying a reverse and customizing the axis labels). This is a clever solution, I dig it. The problem with the startDate and endDate approach is that its interpreting the dates including the date, so your Y axis became a mirror the X axis. Very cool alternative use of Charts > <@U03H3193G3H> There are times when a student would not have class on Monday or Friday but I want the X axis to include all the weekdays. I passed an array of the dates to AxisMarks(position: .top, values: daysOfTheWeek) but I noticed Friday did not appear. I'm just noticing this too, would you mind filing feedback for this issue? this looks like a bug cc: <@U03J21TPWPM> in case this is the same issue you're running in to and my response to you is incorrect Is it possible to get a reference to the colors automatically assigned to the series by the foregroundStyle? I may want to create my own legend in a vertical format. Unless legends can be formatted vertically? You can try positioning the legend on the trailing edge of the chart using the chartLegend modifier The default color scale is currently as follows: all system colors .systemBlue , .green , .orange , .purple , .red , .teal , .yellow <@U03H3193G3H> Thanks for letting me know :slightly_smiling_face: This looks very similar, yes. In my use case, I could fix it by adding additional data points, since it was a very simple graph and I don't mind the extra points. Just in case it is a bug, I'm gonna send a report with the project, maybe it helps :slightly_smiling_face: thanks <@U03J21TPWPM>! (btw <@U03J21TPWPM> if you don't want to add extra points, you can adjust the chartXScale(domain:) or range manually and achieve the same result.) Ah, that sounds even better :smile:. Thanks! Filed as FB10139046. Thank you <@U03H3193G3H>! Congrats to you and the team on Swift Charts. Thank you <@U03JDV4PQR0>! Awesome chart, hope to see more! Filed mine as FB10139144 <@U03H3193G3H> :slightly_smiling_face: Thanks for everything, this framework is awesome! btw <@U03JDV4PQR0> I have to agree, this is a really cool use of the new charts! I really like it :smile: Can we transform a mark when the user presses and releases it, like we could do with a ButtonStyle . If not, we can probably use an overlay, but can we change the size of marks without changing their values (for example with a scale effect)? Currently we don't have a scaleEffect modifier. You can use something like width: .fixed(hovering ? 100 : 80) in BarMark constructor. This will make the bar wider when hovering is true. Thanks, I think I can make it work then! Hi, I'm curious on how Charts handles time based data e.g. temperature over a day when data can be received at any point of the day, not a set interval apart? Some chart libraries that have previously existed did not handle this and just spaced out data incorrectly. Hi <@U03HMESB695>! Charts handles Date as a continuous variable, so this should be fine. If you do want to treat the data as spaced out evenly (e.g., exactly one measurement per day) you can tell the framework to do so by using the unit parameter in a value Thanks! I definitely don\u2019t want it spaced evenly. Great to hear it just handles it approriately. Hi, I've got a set of data I wish to chart that is discrete states over time, for example on/off states of a light, or an enumeration a door lock's state. (example: https://twitter.com/aaron_pearce/status/1527452719027728384?s=21&t=ux-SjhyOSmO89k0u4Lf_jA).|https://twitter.com/aaron_pearce/status/1527452719027728384?s=21&t=ux-SjhyOSmO89k0u4Lf_jA). How would you recommend tackling this chart? RectangleMarks? Yes. A Rectangle or Bar Mark. Thanks, I\u2019d need to precalculate the xStart and xEnd for this correct? I suppose you have some kind of start/end time already, no? You can pass those to xStart and xEnd. Yeah, I can work those out so its doable! Thanks What's the best way to use ChartRenderer to draw to a CGContext with slightly different styles (e.g. draw shapes as hollow, or dashed lines, etc)? Currently there's no API to draw marks with styles other than what's supported in ShapeStyle . Feel free to file a feature request via Feedback Assistant. <@U03HHJH7RJN> Basically we'd like to replicate our current draw-to-PDF behaviors which favor line art, simple coloring, and optional line dashes for monochrome As distinct from the on-screen styling That sounds very cool! I'll file an internal feature request for this. <@U03HHJH7RJN> So does that mean we can't pass in a ChartView and apply styles just for the rendering? No, currently there isn't a way to configure styling of the renderer itself. You could try an approach like exporting the rendered content as PDF or SVG and apply the styling by transforming the resulting vector graphics. ok, thank you Is it possible to assign a gradient between specific LineMarks? E.g. to show a change in value in temperature from cold to hot? You can draw an AreaMark with .foregroundStyle(gradient) below the two lines. Thanks! Would this change the line or apply an area under them? It will add an area under them, the lines will be intact. Is it possible to apply a gradient direct to the line? More as a function of the temp over time (left to right) on the line itself than just decorative You can use .foregroundStyle(gradient) on the LineMark as well. with a LinearGradient that goes from left to right. That\u2019s perfect! thanks Hi again, Are Charts planned to be available also on iOS 15 ? Thanks. Swift Charts is iOS 16 only since Swift Charts relies on features that are not available in earlier versions. What's the best way to that we can use Swift Charts in a predominately UIKit app? <@U03K1BXG8RG>, Sara has a session on this where she adds a Swift Chart to a UIKit app :slightly_smiling_face: https://developer.apple.com/videos/play/wwdc2022/10072/ oh hi <@U03HL05BUJG> Yes the new UIHostingConfiguration lets you use SwiftUI in cells and UIHostingController has some new ways to track size changes that make it easier to use it in stack views or in embedded view controller! Definitely check out the talk, anywhere I talk about using a SwiftUI view you can use a Chart as well! <@U03HL05BUJG> <@U03H3193G3H> thank you all super excited about this :100: Except the WWDC22 Videos and https://developer.apple.com/documentation/charts|https://developer.apple.com/documentation/charts , which documentation do you recommend ? In the documentation you will also find a sample app that could be quite useful https://developer.apple.com/documentation/charts/visualizing_your_app_s_data Thanks. Is it possible to share one of the axis between 2 charts or have 2 subcharts on the same axis, eg share x-axis have have 2 different y-axies on either side with their own independent domain range. Or have a plot when another plot to its right that shares the y-axis but has its own x-axis (commonly used when drawing a heatmap to show a histogram attached on the side) <@U03HMDSQ9JB> Hope I'm understanding your question correctly. You can set specify two charts to have the same chartScale and chartAxis using the appropriate modifiers, but we don't support dual scales on the same chart When using Swift Package Manager modules, Xcode Previews will crash when you try to Preview resources that are contained within another module. e.g. FeatureModule is trying to preview a SwiftUI LogoView() contained in a DesignSystem module that loads an image from that module. (It works fine when the app is built and run). Is this a known issue that will be fixed? Thank you for the feedback! This is a known issue, and we are actively working on resolving it. Excellent! I\u2019m so happy to hear that. I filed FB10070029 if you want an extra dupe number Are you the same Jeremy that wrote this? https://dev.jeremygale.com/swiftui-how-to-use-custom-fonts-and-images-in-a-swift-package-cl0k9bv52013h6bnvhw76alid :slightly_smiling_face: That\u2019s me :slightly_smiling_face: I was just going to post that, thanks <@U03J4D7EZNY> Very interesting post, I read it this morning :slightly_smiling_face: For anyone else running into this limitation, my blog post above has a workaround for how to keep your previews working until it gets properly fixed in Xcode :slightly_smiling_face: With SwiftUI previews, I get this error specifically for my macOS target (the same view works fine for iOS): LaunchExternalToolError: Failed to build ImportCSVContactsPreviewPage.swift Failed to launch swiftc. ================================== | HumanReadableNSError: NSInvalidArgumentException | | com.apple.dt.PreviewsFoundation.ExceptionError (0): | ==NSLocalizedFailureReason: too many arguments (4170) -- limit is 4096 What does this mean? BTW, I filed FB10004742 already. Thanks for filing the feedback! This is a known issue we're investigating. Thanks! Is there a way to export/save a screenshot directly from Previews? I use screenshots (CMD+SHIFT+4) now and share them with product owners/designers but it'd be super handy to have a button that saves the preview with the bezel. There's no way to do this now, unfortunately -- but that's a good idea! Filing a feedback request would help us keep track of interest in that feature. I've also been doing this a ton lately for sending to coworkers.. and taking screen recording videos Done! FB10134111 Thanks! I created a separate project that I can copy-paste the preview implementation to run on the simulator just to be able to take a screenshot :sweat_smile: Not directly related to previews, but you might be able to automate some of your snapshots using the new ImageRenderer view announced this year: https://developer.apple.com/documentation/swiftui/imagerenderer (Doesn't include the device chrome, of course.) That's neat, I missed that one <@U03H32HJQEB> would it include status bar though? :thinking_face: no, it would just be the contents of the View Is there a way to show the status bar in previews? Actually, you know what might be cool? What if we had a System Share button on the preview that shared an image? Then we could tap it and share the view all in a couple clicks! We could even utilize a such feature for App Store screenshots with the help of multiple variations now. These sounds great. Please file feedback! I just added the System Share to my Feedback that I filed. Is there a way to add a \"Custom variants\" to Xcode Previews? f.e. I think it'd be a good idea to add RTL/LTR variants There is not currently a way to add custom variants, but that's a great idea! Opening a Feedback request will help us track interest in that feature. Please also file individual feedbacks for any variants group (like our existing color scheme, type size and orientation) you think should be included by the system. Same applies for the widget variant groups too! Done! :rocket: FB10139550 My Previews take generally a long time to render, often failing (timeout) and sometimes it's quicker to just launch a run on the simulator. Also, it takes some huge place on disk (40GB+). Any hint to optimise all this and make the Previews quick, light and reactive? If you're experiencing long times to render there might be something specific about your setup. Filing a feedback with a sysdiagnose taken while the preview is taking the long time to render would be very helpful for us to diagnose this. As far as general tips to keep things fast, it helps to break up you views into smaller components where possible. And if your app project itself takes a long time to build, then it could be very helpful to split your Swift UI views out into a separate framework target so they could be built without the burden of the rest of the project. <@U03H32HJQEB> Could you elaborate on separating out the SwiftUI views? I am currently working on an objc app and adding new SwiftUI screens, but the builds can sometimes take a while because the rest of the app is somewhat large. Anything I can do to speed that up without having to copy paste the SwiftUI code to a blank SwiftUI project would be awesome <@U03H32HJQEB> Yes I was just thinking about this last advice \u2014 although perhaps with a Package rather than a framework. (I don't even know objc really so I try to stay out of it) <@U03JEMNCV18> I think the general theme is trying to focus on building as little as possible. If you can build out those new SwiftUI views in a framework / package, you can opt to build just that package rather than all of the app. I've considered doing that, but have never dove into creating a framework or package. But i'm tempted now.. Yup, packages would be fine, too. It's even easier to use them in Xcode now. The larger point is to break out the parts that need Swift UI into their own targets that can be built independently. It might even help to have a separate \"previewing\" app target that you link your Swift UI views into instead of your main app. It all depends on your setup, of course. The tl;dr is that Previews needs to build some target to get its work done. If your whole app target is large and slow to build then it will be the bottleneck for previews that you can investigate. Of course you might find other bugs on our end that slow things down. Please file feedback reports with sysdiagnosen! :) As of Xcode 13, there are some gotchas with SwiftUI Previews and custom build configurations not named Debug or Release. \u2022 SwiftUI Previews are not compiled-out unless the active build configuration is exactly named Release. \u2022 Source files under the Preview Content directory are compiled-out for non-debug builds (whether this is looking for the active build configuration to not-be-named \u201cDebug\u201d or whether it\u2019s based on code optimization levels, I do not know) So, what this means is\u2026 \u2022 If you have a custom build configuration, say a \u201cStaging\u201d or \u201cBeta\u201d or whatever, which is essentially a variant of \u201cRelease\u201d\u2026. \u2022 And if you have SwiftUI Previews in your app\u2026 \u2022 And if those previews are not wrapped in #if DEBUG directives\u2026 \u2022 And if you have source code to support SwiftUI Previews stashed under the \u201cPreview Content\u201d directory\u2026 Then your project will likely fail to build, as the previews will not be compiled out, but they will depend upon development asset code in \u201cPreview Content\u201d that is compiled out. Do you know if this is resolved in Xcode 14? Thanks for the feedback! Right now the recommendation is to limit the Preview Content folder (which is really just a convenience for the DEVELOPMENT_ASSET_PATHS build setting) to assets like asset catalogs, preview data, etc. For code, we recommend you continue to keep it in your normal source paths, but to wrap with #if . Note: the compiler will normally remove any preview providers from the final binary without needing the #if , but the #if is a guarantee it won\u2019t be included. It\u2019s specifically preview data that we are using in the previews. The problem is our Staging target, and the mismatch between what is being compiled out for Previews vs Preview Content. It seems like those two should agree on what is being compiled out. I\u2019ve also had this problem, I made this SO post for it. https://stackoverflow.com/questions/63078880/why-does-archive-fail-if-there-is-code-for-swiftui-previews-in-the-preview-conte I filed FB8969539 for this issue. Yep, that\u2019s exactly it Jeremy. So annoying because we sometime forget to wrap our new views in the DEBUG check. I need to invest in some script / linter to check this, but I was hoping this problem would go away in Xcode 14. If you don\u2019t put any code in Preview Content, then the compiler will usually be able to correctly remove any branches of code used only for previews. But it is code, and that seems very useful. Do you have other recommendations? I think you\u2019re recommending to move that code out of Preview Content and :crossed_fingers: that it is removed from release builds? I\u2019ll give that a shot. Thanks! How would it get removed from the release builds? I don\u2019t know of any other mechanism other than #if DEBUG I think that\u2019s what is being recommended \u2013 move the code out of Preview Content and manually wrap the entire code in an if DEBUG check. But it seems less than ideal since Preview Content is built for exactly this use case. What is compiled out should match between PreviewProvider and Preview Content. Fully agreed. The compiler optimizes by removing non-public symbols that aren\u2019t referenced from any top level code. This will remove most preview code that is not referenced by other public or used types. Just to clarify my understanding, you are saying that it\u2019s not supported use case to have code inside the Preview Contents folder that will be stripped out in Release builds? Note that this contradicts what was implied in WWDC 2020's session \u201cStructure your app for SwiftUI previews\u201d. > \u201cWhat\u2019s great about Development Assets is they apply not only to files like Asset catalogs, but also to code. Let\u2019s look at what\u2019s inside that preview content folder that we just added.\u201d > > By using the navigator, we can look inside and see two Swift files. These Swift files contain code that I only need for development and debugging and testing my app, and I don\u2019t want to include these when my app is actually deployed. https://developer.apple.com/videos/play/wwdc2020/10149/?time=637 \u2022 SwiftUI previews - compiled out of exactly \u201cRelease\u201d \u2022 Preview Content \u2013 NOT compiled out of \u201cDebug\u201d Ultimately it seems odd to me that these two things have different strategies. Everything discussed in this thread is fine but we have to acknowledge that everything we\u2019re currently doing (#if DEBUG) or recommended in this thread is a workaround for the disconnect between the above bullet points. Great thoughts and responses everyone! > Just to clarify my understanding, you are saying that it\u2019s not supported use case to have code inside the Preview Contents folder that will be stripped out in Release builds? It\u2019s not unsupported ( :slightly_smiling_face: ) to put code in Preview Content, but we\u2019ve found it to work best when Preview Content is limited to assets. > Note that this contradicts what was implied in WWDC 2020's session \u201cStructure your app for SwiftUI previews\u201d :upside_down_face: Thanks for the reply <@U03HW8Y0RFB> Are there any new SwiftUI Preview tools for previewing views on devices of varying os versions? For example viewing how a view would look on an iOS 16 device vs iOS 15 that uses the compiler checks Thanks for the question! There\u2019s not currently a variant mode in the canvas for different iOS versions. However, there is a neat solution! If you have a physical iPhone that is running iOS 15, you can use on-device previews to see your simulator preview in the canvas (running iOS 16) and the preview running on device (running iOS 15). And any changes you make will automatically be updated in both places at the same time! If you have other use cases for seeing multiple OS versions, please file a Feedback Request so we can take a look, thanks! Should feedback be under SwiftUI, Xcode, simulator, or something else? Sent - FB10139747 Any of those is fine, it will make it to us, but Xcode is the fast track :slightly_smiling_face: Cool, I picked the right one What's the best practice for working with Core Data and SwiftUI previews, such that we don't end up interacting with the actual store data? Core Data supports pointing to in-memory storage which won't need the file system and would work great in any situation where you don't want to persist the changes permanently, even in Previews. https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/CoreData/PersistentStoreFeatures.html As preview utilizes some generated code (for dynamically updating values etc), I have faced naming clashes a few times in a rather crowded file. And it wasn't obvious at first, I had to dig through logs to figure out some helper types I had were clashing. What would be suggestions to avoid such cases? Is there a guideline that we can refer to? That's interesting. In order to be able to help we'd have to take a look at a concrete example. If you're able to repro the issue in a sample project, could you file a feedback request for us? <@U03HL18CEEQ> thanks for answering, I don\u2019t have any such project at the moment but I\u2019ll sure keep in mind to file a feedback next time. Sometimes the Xcode previews stop working for no apparent reason. The last example of this was it said \"connection interrupted\" or something like that. So, I question is what are some tips for fixing broken previews? BTW: The new previews are great! Love not having to create multiple previews for landscape and portrait. Thanks for the question. We know there are cases where previews don\u2019t work and we\u2019re actively working on addressing them. The best way to help us is to file a Feedback Request and attach the previews diagnostics. Look for an upcoming message about great tips for filing previews bugs to make it easy for us to investigate! Thanks! <@U03HW8Y0RFB> One unrelated issue I encountered today was that the compiler took too much time to compile the code because it was trying to guess the type of a rather simple expression for an human but not for a computer. Perhaps you could handle this specific case by displaying a dedicated message so that the developer precisely knows what the preview is waiting for. Thanks for the feedback <@U03JSSE35GQ>, that\u2019s a great idea. We\u2019ve been tracking a number of issues related to type checking, so we\u2019re on it! :slightly_smiling_face: Views within an extension aren't always exposed to Previews. This will create a compile issue: enum MyNameSpace { } extension MyNameSpace { struct MyContainerView: View { var body: some View { MyChildView() } } struct MyChildView: View { var body: some View {Text(\"MyView\") } } } struct ContentView_Previews: PreviewProvider { static var previews: some View { MyNameSpace.MyContainerView() } } Is this a known issue? I'm hoping it's not expected. :upside_down_face: Thanks for the question! Ha, yes, definitely not expected :slightly_smiling_face: This is a known issue that we are actively looking at. In the meantime, a workaround is to add a typealias inside of the struct that references the other view. For example in your example I added typealias MyChildView = MyNameSpace.MyChildView inside of MyContainerView and the preview renders. Question re the layout inspector inspecting swiftui - curious if there are public/sanctioned ways to introspect a swiftui view for details, similar to what is presented in the details panel (on the right when selecting a view). Hey Tyler, great question! The current recommendation is to attach to the process and use the view debugger. The runtime view debugger does not provide the same level of detail that you get when using the inspector in Previews \u2013 are there specific details that you are looking for in the view debugger? <@U03HHK591AP> Can you suggest how to use background(color.red.ignoreSafeArea()) together with .cornerRadius? It just dont work :( Sorry, I realized that this question wasn\u2019t actually preview-specific until after I posted. Previews are great for eyeballing layout and validating behavior, but I use the inspector to validate specific fonts, frames, etc. Historically we\u2019ve used internal tooling for inspection against UIViews, etc. from tests and interactive inspectors. We\u2019re rethinking this approach as we migrate to SwiftUI, but still curious if there\u2019s a public way to ask an arbitrary view for this info. The Xcode inspector seems to be able to focus on an arbitrary view and dump useful info. Re previews and eyeballing - I take this back. There\u2019s a lot I can check in non-interactive mode. However, there are still other cases in which I\u2019d like to have access to this type of general info outside of the context of previews. In particular, what level of detail are you looking for with the font? i.e. Is it a custom font that you\u2019re trying to validate or are you trying to make sure something is actually using a system font like .heading We use custom fonts, yes. Consider the scenario - we have a large app; currently, I (or say, a designer) can nav to an arbitrary screen, open up our in-process inspector; focus on a view and inspect its attributes. All without connecting via Xcode and attaching. My assumption here is that we\u2019re entering a new world with new tools and introspection like this isn\u2019t straight forward. But was still curious if there was something I\u2019m missing. Gotcha. To answer your question, I don\u2019t believe that level of introspection is available but we\u2019re interested in hearing more about what you\u2019re looking for. Could you please open a feedback request to help us track interest in a feature like that? Absolutely! Thanks. Adding border to views when developing helps debugging/developing your code. Adding borders to every view in the screen can be tiring. Is there a way currently to automatically add borders to all the views in the screen with the press of a button/setting? If you'd like to see the borders of each view in the preview you can use the non-interactive mode then in the menu bar go to Editor > Canvas > Show View Bounds. Additionally, you can toggle \"Show Clipped Views\" in the same menu to show or hide views that extend beyond the edges of the preview. Thank you! :heart: Currently Xcode Previews fail to find the .module bundle (and associated resources) for views in SPM modules. This has a workaround (manually locating the bundle), but adds additional boilerplate code. I'd like to know it's a known issue. I've seen that Previews got a substantial update in Xcode 14 and I was surprised to see this was not addressed. Thank you for the feedback. Yes, we know this is a common issue and are still finding the best path forward to solve it. Thanks for the patience! as far as i rememeber, .module is only generated when a Swift package target has resources. so it might be a better workaround to add an empty file to a package :slightly_smiling_face: https://developer.apple.com/documentation/packagedescription/target/resources :crossed_fingers::skin-tone-3:Thank you for the answer! Is there a way to have the preview window size to be automatically the same as the device used for the preview ? It usually opens wider than necessary on my small screen ;-) There is not currently a way to have the preview canvas match the size of the device being previewed, though it is a known request. Feel free to file a Feedback request to help us track interest in that feature, thanks! OK, Thanks. Not a Question: Thank you for the feature to provide multiple variations, it'll simplify my code dramatically, as I was constantly boiler-plate extending views for light/dark + multiple accessibility sizes to see how it worked out. Huge benefit, mucho gracias!!! Thank you, Joseph. Glad to hear you are enjoying the new variations support. Please file feedbacks with any additional variations you think would be useful! Sorry if this was already asked / answered, but is it possible to view different devices on the same tab in XCode 14? (e.g. iPhone 13, iPhone 13 mini, iPhone 13 Pro Max) Haven't had a chance to test it out yet Just in general, I wanted to say that Xcode Previews are life-changing. I honestly feel that I\u2019m about 10x more productive with the combination of SwiftUI + Xcode Previews. Being able to rapidly iterate on a view to get it just how I want completely changes my approach to things. Now I just dive in, and start coding, and continually refine until it\u2019s perfect. With .xibs, I would give up far before the point of perfection. Is it possible to print to the console while in SwiftUI previews? This is a known issue we're actively investigating. :slightly_smiling_face: <@U03HW8XNUGH> Thank you! I've loved using Preview, and it's the only thing that's preventing me from using it for everything :smile: (Also not necessarily the best solution, but sometimes I end up just adding a debug label attached to state and use that as a print\u2026) :upside_down_face: FWIW, os_log and Logger do work fine in previews and then you can look for those with the Console app like any other syslog output. First, I love that the interactive mode is default \u2014 I think it makes the entire preview much more useful. It seems that the selectable mode no longer works the way I would expect. Is there an explanation somewhere of what I can do in that mode? We have not made any changes to the behavior of the selectable mode. Can you clarify what is behaving differently? Here's a screencast. It's possible that it's due to something in the project. It's behaving differently that I would expect. Ah. Hmm, that does look like a bug. It should be selecting the text / button. If you file a Feedback Request we\u2019d be happy to take a look to see if there\u2019s something specific to your configuration. You can use the Editor > Canvas > Diagnostics option that will give us all the info we need to double check. Thank you! phew! good to know \u2014 will file a radar. Thank you!","title":"swiftui"},{"location":"wwdc22/swiftui-lounge.html#swiftui-lounge-qas","text":"","title":"swiftui-lounge QAs"},{"location":"wwdc22/swiftui-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/swiftui-lounge.html#great-session-and-thank-you-for-all-the-new-features-but-most-importantly-was-the-cake-as-nice-as-it-looked-d","text":"Yup! It was chocolate, and delicious. We had some deleted scenes with the sliced cake, but we don't release WWDC Bloopers and Deleted Scenes unfortunately","title":"Great session and thank you for all the new features! But, most importantly, was the cake as nice as it looked!?!?!?!? :D"},{"location":"wwdc22/swiftui-lounge.html#thanks-for-so-many-great-improvements-are-any-of-these-changes-back-deployable-to-previous-oses-eg-like-the-new-section-initialisers-were-last-year","text":"Not this year! Last year we had some nice syntactic refinements that we were able to back deploy, but many of the features this year require fundamental new support in the OS.","title":"Thanks for so many great improvements!  Are any of these changes back-deployable to previous OSes (e.g., like the new Section initialisers were last year)"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-best-venue-to-ask-swiftui-questions-during-the-year-swift-forums-is-amazingly-useful-for-swift-language-and-library-questions-with-many-of-the-implementers-frequenting-it-but-it-is-focussed-solely-on-the-swift-language-itself-is-there-any-comparable-place-where-yall-converse-during-the-year","text":"The developer forums are a great place to ask questions! We monitor them throughout the year and reply when we can, and so do other members of the community! I\u2019ve queued up a year\u2019s worth of questions for my 30 minutes in the lab :slightly_smiling_face: but this week is a bit overwhelming with all the new stuff too to make much of a dent. What I've appreciated more about the Apple Developer Forums is that engineers who work on the relevant frameworks will come in with their expertise. There's also full-year lab-style code-level assistance from https://developer.apple.com/support/technical/|Developer Technical Support to facilitate your SwiftUI questions. Don\u2019t be worried that you\u2019ll \u201cwaste\u201d your DTS contacts! When DTS confirms that the issue raised is a bug and a bug is filed by the developer, would it make sense if DTS can mark it as a verified bug so that it gets the attention of the relevant team? DTS commonly elevates that feedback to the engineering team. We have a fantastic partnership with DTS. They\u2019re just the best! Apple Developer Forums have lots of unanswered questions, especially the complex ones / edge cases. Also, there\u2019s sadly little engagement from the community as a whole, so if an Apple engineer misses the question, you\u2019re out of luck. Not sure I've ever seen a good answer on the Apple Developer Forums but I've seen my own questions asked w/o any answers many times. The results there are so low that I've never asked a question there myself. It doesn't seem useful. You\u2019ll never get an answer if you don\u2019t ask. :slightly_smiling_face:","title":"What is the best venue to ask SwiftUI questions during the year? Swift Forums is amazingly useful for Swift language and library questions, with many of the implementers frequenting it, but it is focussed solely on the Swift language itself. Is there any comparable place where y'all converse during the year?"},{"location":"wwdc22/swiftui-lounge.html#with-graphics-and-custom-layouts-being-added-when-should-we-use-native-elements-vs-custom-styling-how-far-should-custom-designs-go","text":"It entirely depends on your use case! We ultimately trust your sense of design on this one since only you know what\u2019s right for your app. To give some general guidance though: Take native elements as far as you can, and if you find you need further customization beyond that to finely polish the experience, then don\u2019t be afraid to use it! One quick disclaimer: It can be easy to drop to custom layouts prematurely. Stacks and Grids are incredibly powerful already, so be sure you can\u2019t get what you need with them before you use the new layout protocol. (edited)","title":"With graphics and custom layouts being added, when should we use native elements vs custom styling? How far should custom designs go?"},{"location":"wwdc22/swiftui-lounge.html#i-saw-that-vstack-and-hstack-conform-to-layout-does-that-mean-using-an-if-else-to-switch-between-the-two-with-the-same-content-will-now-animate-the-change","text":"Yes! To animate between layouts you'll need to switch between the layouts using AnyLayout for example: let layout = isVertical ? AnyLayout(VStack()) : AnyLayout(HStack()) layout { Text(\"Hi\") Text(\"World\") } Check out the Compose custom layouts with SwiftUI talk tomorrow for more details! Looking forward to it. Thank you! This sounds great! Looking forward to getting rid of some UIStackViews These kinds of examples will be in the code snippets soon too! Both that table layout code, and the grid morphing to a scattered layout will be included","title":"I saw that VStack and HStack conform to Layout. Does that mean using an if-else to switch between the two with the same content will now animate the change?"},{"location":"wwdc22/swiftui-lounge.html#hi-great-stuff-so-far-thank-you-i-was-wondering-if-there-are-any-updates-or-opinionated-best-practices-when-it-comes-to-large-scaled-applications-using-swiftui-in-terms-of-state-management-and-dependency-managementinjection-thanks-v","text":"SwiftUI is not opinionated on a specific architecture for state management. We offer the building blocks to build whatever best fit your use case. It might be good to sign up for a lab so that we can discuss the specific of your use case and give you targeted advices I tried signing up for a lab and was denied. It seems the demand is too high for what you guys are able to provide. That is an absolute shame You can also try the DTS Open Hours labs, and I can help you there as well. That's the one I signed up for... and was denied There are DTS labs throughout the week, so you may have missed the first set of appointments. There are still many more openings, though! It would be great to capture your use case in Feedback Assistant before the labs as well, so we can get started quickly or can follow up after the conference, if needed. I was hoping to have some type of lab each day. Having requested that one and then denied, precluded me from requesting any other lab for the day. It's frustrating. I don't know whether I'll get a single lab in this week at this rate. I'll try and use Feedback Assistant like you said, but I don't know about using it in this way... I've only ever used it to send bug reports and feature suggestions. This would be entirely different. I'm new to WWDC this year, and so far this has been a soured experience","title":"Hi great stuff so far. Thank you! I was wondering if there are any updates, or opinionated best practices when it comes to large scaled applications using SwiftUI in terms of state management and dependency management/injection. Thanks :v:"},{"location":"wwdc22/swiftui-lounge.html#thanks-for-the-great-session-talking-about-charts-will-it-be-possible-to-add-a-gradient-color-filling-the-area-underneath-the-line-in-a-line-chart","text":"It is! I made one like this while putting the talk together, I can't remember exactly how I did it. I may have used an AreaMark that fills the area under the line, and they also stack to make some really cool area charts When you're making those kinds of charts, using Color.clear as a part of your gradient makes the Chart look a lot neater in light and dark appearances IMO Thanks a lot! This will enable me to migrate from my custom made charts to SwiftCharts in my existing app! :heart_eyes: I have to say, it's extremely fun to see how quickly you can make custom-rolled charts from various apps in Swift Charts. Like a speed run, only with accessibility, dynamic type, and localization built in There are several sessions about SwiftCharts as well! Definitely check them out.","title":"Thanks for the great session! Talking about Charts, will it be possible to add a gradient color filling the area underneath the line in a line chart?"},{"location":"wwdc22/swiftui-lounge.html#wheres-this-swiftui-party-at-and-dont-tell-me-the-cake-is-a-lie","text":"The cake was baked locally in San Francisco, and we even piped the SwiftUI bird on there ourselves! Using the new .frosting(.blue) modifier That is not real ^ The cake is, in fact, not a lie. :birthday: <@U03HL00QL68> made it! And the SwiftUI party is everywhere that you\u2019re using SwiftUI. Celebrate with us. :partying_face: Can we file feedback for a .frosting() modifier? Maybe it could wrap PencilKit or something. And make sure it tastes delicious, too. File that along with an enhancement request for a replicator. I need iOS to replicate the cake on demand. :smile: Is it an Apple cake? The cake will be sold in exactly 0 Apple Stores","title":"Where's this SwiftUI party at? And don't tell me the cake is a lie.."},{"location":"wwdc22/swiftui-lounge.html#does-grid-replace-vgrid","text":"LazyVGrid and LazyHGrid arrange their children lazily and so are great fits for large amounts of content within a scroll view Grid requires all of its children be loaded up front and because of that has some powerful features that the lazy grids do not Yeah, check out the Composing custom layouts with SwiftUI talk tomorrow for more info","title":"Does Grid replace VGrid?"},{"location":"wwdc22/swiftui-lounge.html#is-there-any-new-control-in-swiftui-like-the-nssegmentedcontrol-used-under-tables-to-add-delete-a-row-this-is-a-common-pattern-in-macos-applications-and-the-current-swiftui-segmented-control-does-not-fit-to-this-use-case","text":"Good news: There\u2019s not a new control, but an old one :smile: Last year we introduced ControlGroup that enables building these kinds of controls (which you might have used NSSegmentedControl for with AppKit). You can create one with Buttons, Toggles, and more! Nice, didn\u2019t know about this one. Will try it out. Thanks! :thumbsup:","title":"Is there any new control in SwiftUI like the NSSegmentedControl used under tables to add / delete a row? This is a common pattern in macOS applications and the current SwiftUI segmented control does not fit to this use case."},{"location":"wwdc22/swiftui-lounge.html#what-is-the-difference-between-onchange-and-onreceive-modifier-onreceive-is-like-a-combination-of-onappear-and-onchange-is-this-the-complete-and-accurate-picture","text":"onReceive is specifically to subscribe to Combine\u2019s Publisher types and produce a side effect. onChange is used to produce a side effect when a property of your view changes. For example you can use that to produce a side effect when the scene phase in the environment changes. Okay, thank you for the explanation! If I use both .onReceive and .onChange on a Published property, will there be a difference in behaviour and is one recommended over the other in this case? It really depends on your use case: Is the value equatable? Is it a lightweight event? Does your view have constraints to adhere to, such as de-duping, debouncing, exponential backoff, etc.","title":"What is the difference between .onChange and .onReceive modifier.? \".onReceive is like a combination of .onAppear and .onChange\", is this the complete and accurate picture?"},{"location":"wwdc22/swiftui-lounge.html#curious-if-theres-updates-related-to-ui-testing-in-swiftui-apps-or-should-i-be-thinking-more-in-terms-of-testing-the-model-layer-that-drives-the-declarative-ui","text":"Our recommendation is still to thoroughly test your model layer. In my experience, the best SwiftUI apps move their logic into model code, using SwiftUI as a declarative mapping from that model state to views. Then your tests of the model layer are effectively tests of the resulting UI as well :slightly_smiling_face:","title":"Curious if there's updates related to UI testing in SwiftUI apps? Or should I be thinking more in terms of testing the model layer that drives the declarative UI?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-enumerate-a-navigationpath-or-replace-certain-elements","text":"It\u2019s not currently possible to enumerate a NavigationPath . Because it\u2019s type-erased, the elements could only be exposed as any Hashable so aren\u2019t directly useful. Generally, if the set of things that can be added to a path form a closed set, which would be the case if you can usefully enumerate it, I\u2019d recommend wrapping your presented values in an enum with associated types. I\u2019m wondering if having a mutable collection of enum values might be generally more useful than using a NavigationPath. Snap! Then use an array of that enum instead of NavigationPath. Sounds like that could work better and might fit nicely with some form of Coordinator-esque style pattern. Thanks!","title":"Is it possible enumerate a NavigationPath or replace certain elements?"},{"location":"wwdc22/swiftui-lounge.html#terrific-updates-all-around-when-animating-the-font-on-a-text-when-can-we-expect-the-font-to-smoothly-interpolate-instead-of-crossfade","text":"Generally changing weights and sizes of the same font will interpolate, but not changing fonts","title":"Terrific updates all around! When animating the Font on a Text, when can we expect the font to smoothly interpolate instead of crossfade?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-set-default-focus-on-textfield-when-it-appeared-first-time-without-workarounds-with-delay","text":"Yeah, checkout the new defaultFocus modifier! :smile: https://developer.apple.com/documentation/swiftui/view/defaultfocus(_:_:priority:)","title":"Is it possible to set default focus on TextField when it appeared first time? Without workarounds with delay?"},{"location":"wwdc22/swiftui-lounge.html#does-the-new-navigationsplitview-preserve-state-when-we-switch-to-a-new-navigation-destination-like-tab-bar-in-uikit-or-do-we-need-to-roll-our-state-restoration-like-what-we-had-to-deal-with-using-the-old-tabview","text":"If you want to switch to a TabView in narrow size classes, you\u2019ll want to design a navigation model type that provides selection information for the NavigationSplitView in regular size classes and a different projection of the same information to a TabView that\u2019s shown in narrow size classes. I\u2019d love Feedback with specific use cases. I\u2019m very interested in making this easier.","title":"Does the new NavigationSplitView preserve state when we switch to a new navigation destination like tab bar in UIKit, or do we need to roll our state restoration, like what we had to deal with using the old TabView?"},{"location":"wwdc22/swiftui-lounge.html#does-transferable-also-support-asynchronous-behavior-like-nsfilepromiseprovider-eg-if-you-do-not-have-an-file-url-yet-eg-cause-a-file-first-needs-to-be-downloaded-from-a-web-server-but-only-when-the-drag-operation-ends","text":"Definitely, the FileRepresentation transfer representation does what you\u2019re describing. <@U03HW7P2WTB>\u2019s amazing talk \u201cMeet Transferable\u201d will be out with more detail tomorrow ( June 8 ) :smile:","title":"Does Transferable also support asynchronous behavior like NSFilePromiseProvider, e.g. if you do not have an file URL yet (e.g. cause a file first needs to be downloaded from a web server, but only when the drag operation ends)"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-correct-way-to-implement-invalidation-when-conforming-some-type-to-dynamicproperty-should-hosting-another-builtin-dynamicproperty-enough-are-these-wrapped-properties-like-state-or-observedobject-guaranteed-to-work-as-expected-in-dynamicproperty","text":"Dynamic properties are fully composable, so any other dynamic properties you use inside of their declarations will be picked up on and invalidated properly. That means supporting invalidation as you would expect is often just a matter of utilizing the support already built into other dynamic properties, like @State","title":"What is the correct way to implement invalidation when conforming some type to DynamicProperty? Should hosting another builtin DynamicProperty enough? Are these wrapped properties like @State or @ObservedObject guaranteed to work as expected in DynamicProperty?"},{"location":"wwdc22/swiftui-lounge.html#how-can-i-hide-the-disclosure-indicator-for-a-navigationlink-within-a-list-for-example-if-i-was-wanting-to-implement-the-built-in-reminders-app-with-the-grid-icons-in-the-list-header","text":"Hi, Christian. The disclosure indicator isn\u2019t currently customizable. With the new NavigationStack and path binding, you can use a regular Button and append your value directly to the path. I\u2019d love a Feedback about your use case for hiding the indicator though! Ooh. Appending directly to the path is gonna be super helpful. Yep, I think appending the path is the best solution. Thanks <@U03HW7P0HQR>. This is an example of the use case, mimicking the Reminders app. Noticing the disclosure indicators at for All and Today . Submitted as FB10082608","title":"How can I hide the disclosure indicator for a NavigationLink within a List? For example, if I was wanting to implement the built-in Reminders app with the grid icons in the List header."},{"location":"wwdc22/swiftui-lounge.html#hello-everyone-im-amazed-by-the-new-text-animations-can-we-animate-text-with-any-font-does-the-font-need-to-respect-some-criteria-thank-you","text":"Yes, you can use any font. Thanks a lot! However, you can't go from one font to another or animate from one font family to another. For example, italic to non-italic will not animate Clear thanks, how about variable fonts that can change weight? I mean if I use static fonts (that have one file for bold, one for medium etc\u2026) I suppose I cannot either change weight becas *because it would be another font family, right? Correct, I wouldn't expect that to ~work~ animate","title":"Hello everyone! I'm amazed by the new text animations, can we animate text with any font? Does the font need to respect some criteria? Thank you!"},{"location":"wwdc22/swiftui-lounge.html#hi-is-there-anything-different-visually-to-the-user-between-a-navigationview-and-a-navigationstack","text":"They\u2019re both structural components, but they do behave a bit differently. Can you elaborate in what way they behave differently? NavigationView renders differently depending on how many views you pass to it and what navigationViewStyle you set on it. NavigationStack takes a single root view and always renders as a stack. Gotcha, thanks for the clarification :)","title":"Hi, is there anything different visually, to the user, between a NavigationView and a NavigationStack?"},{"location":"wwdc22/swiftui-lounge.html#when-opening-the-control-center-via-the-menu-bar-extra-the-button-remains-highlighted-while-the-control-center-is-visible-is-there-a-way-to-replicate-this-using-the-new-menubarextra-api","text":"Hi - thanks for the question! I think this is likely just a bug with the framework at the moment, and that it should behave similar to Control Center by default. If you could please file a feedback for this, I'd appreciate it. Thanks for your response. Honestly, I have not tried the new API yet, I just saw the behavior in a WWDC session video. I will try it once I have installed the beta and will file a feedback if necessary. Excellent, thanks! <@U03HHJH9C66> FB10086096","title":"When opening the Control Center via the Menu Bar Extra, the button remains highlighted while the Control Center is visible. Is there a way to replicate this using the new MenuBarExtra API?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-reorder-items-and-insert-into-an-outline-group","text":"SwiftUI doesn\u2019t have much in the way of automatic support for that, I\u2019m afraid. I\u2019d love a Feedback with details of your specific use case. We did expose public API for disclosureGroupStyle this year, which might be worth investigating. The use case would be a sidebar that the user can organize on their own. I was thinking this could be a normal list, but there a limits with cross-section item moves that I end up tripping over. Makes sense. With the current API, I\u2019d use a view model that vends the items as if they are a flat array, then style the items based on their apparent depth. That would let you use onMove or the awesome new editing support on List to update your view model. Yeah, I think biggest trick there is figuring out that \u201coffset 20\u201d is actually \u201coffset 2 of parent 4\u201d or something like that. Thanks for your input. It's not the first time I've read this solution, and since there's nothing new in this area for 2022, I think I may need to take the plunge and go for it.","title":"Is it possible to reorder items and insert into an outline group?"},{"location":"wwdc22/swiftui-lounge.html#pages-and-numbers-are-both-document-based-apps-which-have-a-beautiful-onboarding-experience-how-do-i-build-such-an-onboarding-experience-for-my-document-based-app-using-swiftui","text":"Hi - this is a great question! While we do not currently have a high-level concept in SwiftUI for this type of flow, it may be possible to combine the new Window scene together with the new OpenDocumentAction and NewDocumentAction to create it. You would need to define the Window as the primary (first) scene in your app, however. It's possible there may be some drawbacks to this approach, so I'd love a feedback for an enhancement in this area. Thanks for the idea. I'll try!","title":"Pages and Numbers are both document-based apps which have a beautiful onboarding experience. How do I build such an onboarding experience for my document-based app using SwiftUI?"},{"location":"wwdc22/swiftui-lounge.html#i-currently-use-navigationview-in-my-app-with-navigationlink-to-go-from-parent-view-to-child-destination-view-it-works-fine-for-what-i-want-do-i-need-to-migrate-to-using-navigationstack-in-my-case-especially-if-im-just-linking-to-destination-views-and-not-needing-to-link-by-value","text":"NavigationView is only soft-deprecated and so will continue to work fine in all its existing use cases with no compiler warnings. If writing new code, however, we recommend you start using NavigationStack . Ok. So NavigationStack with a NavigationLink to a destination view will work the same for me? Yup!","title":"I currently use NavigationView in my app with NavigationLink to go from parent view to child destination view. It works fine for what I want. Do I need to migrate to using NavigationStack in my case, especially if I\u2019m just linking to destination views and not needing to link by value?"},{"location":"wwdc22/swiftui-lounge.html#viewthatfits-has-some-strange-effects-when-the-size-is-animated-is-it-intended-to-work-with-animation-or-should-it-be-avoided","text":"ViewThatFits should support animations, could you file a feedback for this one and share right here? ( hi <@U03HL00QL68> :wave: ) Will do Hi Christian! FB10082380","title":"ViewThatFits has some strange effects when the size is animated.  Is it intended to work with animation or should it be avoided?"},{"location":"wwdc22/swiftui-lounge.html#on-ipados-in-a-navigationview-is-it-possible-to-switch-between-two-column-and-three-column-layout-depending-on-what-is-selected-in-my-sidebar","text":"It isn\u2019t possible to do that directly. Often those style of UIs can be built by swapping the view in the detail area between a single root view and an HStack with two-elements. That is, instead of conceiving of it as a two or three column NavigationSplitView , think of it as a two-column one, where the detail column itself has one or two columns. Great, that's exactly what I'm doing now. It's working fine, just a little extra work to coordinate with my compact view. Thanks! Awesome! I\u2019m glad you found a solution.","title":"On iPadOS in a NavigationView, Is it possible to switch between two column and three column layout depending on what is selected in my sidebar?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-better-way-to-prevent-dynamicproperty-from-invalidating-a-views-body-based-on-different-criteria-i-am-currently-doing-this-with-a-private-observableobject-backing-that-manages-its-objectwillsend-calls-which-seems-to-works-well-but-also-feels-like-i-am-doing-some-backflips-the-context-is-being-able-to-scope-in-on-specific-changes-on-an-observableobject-model-for-performance-reasons","text":"There is no direct way to prevent DynamicProperty from updating. What you are doing is a way to do it. If the purpose is to manage ObservableObject invalidation I would suggest consider refactoring your model into multiple model. Keep in mind that you can also implement your own objectWillChange Publisher. Makes sense. Thanks!","title":"Is there a better way to prevent DynamicProperty from invalidating a view\u2019s body based on different criteria? I am currently doing this with a private ObservableObject backing that manages its objectWillSend calls, which seems to works well but also feels like I am doing some backflips (the context is being able to scope in on specific changes on an ObservableObject model for performance reasons)."},{"location":"wwdc22/swiftui-lounge.html#previously-in-navigationview-on-ipad-the-detail-view-would-be-displayed-with-the-foreach-list-being-collapsed-in-a-sidebar-with-the-new-navigationsplitview-can-i-use-a-modifier-to-not-collapse-the-foreach-list","text":"Thanks for the question. NavigationSplitView takes a columnVisibility binding. You can set it to .all to reveal all the columns programmatically! Great, thank you!","title":"Previously in NavigationView on iPad, the detail view would be displayed with the ForEach list being collapsed in a sidebar. With the new NavigationSplitView, can I use a modifier to not collapse the ForEach list?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-use-swiftui-navigation-for-ios-15-how-can-we-transition-to-it-on-the-existing-project","text":"To learn how to transition your existing projects to the new navigation types, see this handy migration guide: https://developer.apple.com/documentation/swiftui/migrating-to-new-navigation-types Annnnd add to reading list. Thanks!","title":"Is it possible to use SwiftUI Navigation for iOS 15? How can we transition to it on the existing project?"},{"location":"wwdc22/swiftui-lounge.html#hi-curt-in-your-talk-you-used-objectwillchangesequence-but-i-cant-find-it-in-the-new-xcode-beta-where-is-it-thanks","text":"Hi, Malcolm! That\u2019s a method that I implemented on my NavigationModel but didn\u2019t fit into the slides. Ok thanks It\u2019s in the Copy Code for the talk accessible in the Developer app though I think you can use for await _ in store.objectWillChange.values in a similar way? That\u2019s close, Christian! You have to buffer the values too. var objectWillChangeSequence: AsyncPublisher&lt;Publishers.Buffer&lt;ObservableObjectPublisher&gt;&gt; { objectWillChange .buffer(size: 1, prefetch: .byRequest, whenFull: .dropOldest) .values } Great thanks, for some reason the copy codes aren't appearing for me Bummer. There\u2019s also a sample app with similar code: https://developer.apple.com/documentation/swiftui/bringing_robust_navigation_structure_to_your_swiftui_app The sample app is a little more complex than what I shared in the talk. <@U03HW7P0HQR> Instead of objectWillChangeSequence would it be possible to use onChange(of: perform) to track any selection changes and update @SceneStorage data variable? Yep. That approach will work too. I used task in the talk so the restore and save code was in one block, but splitting them into separate onAppear and onChange should also work.","title":"Hi Curt, in your talk you used objectWillChangeSequence but I can't find it in the new Xcode beta, where is it? Thanks"},{"location":"wwdc22/swiftui-lounge.html#how-does-viewthatfits-decide-if-a-view-fits-i-find-it-a-bit-strange-that-text-with-a-line-limit-of-1-still-fits-even-if-its-truncated-for-example","text":"ViewThatFits consults the view's ideal frame size for both dimensions. The latter behavior isn't intended. A feedback # and keeping an eye on later seeds is a good strategy here Adding a fixedSize(horizontal: true, vertical: false) seems to fix the issue but also seems unnecessary. I\u2019ll file a feedback FB10082577 2 for Ryan :slightly_smiling_face:","title":"How does ViewThatFits decide if a view fits? I find it a bit strange that Text with a line limit of 1 still \"fits\" even if it's truncated for example."},{"location":"wwdc22/swiftui-lounge.html#does-swift-charts-have-a-way-to-highlight-values-on-a-line-chart-for-a-specific-x-position","text":"There's a Swift Charts Q&A at 11am PT on Thu. Please ask there! In the meantime, you can always try drawing an additional mark for the highlight. Its possible to modify the .foregroundStyle to have a different color per X position, it depends on what kind of highlight you want to call out. It might be possible to do an annotation as well but the charts team will be the experts! The Build a Productivity App for Apple Watch talk, coming tomorrow, has an example of exactly that! https://developer.apple.com/wwdc22/10133 You can use PointMark (possibly with annotations) to highlight values on a line, such as: Chart { // Create the line. ForEach(data) { LineMark( x: .value(\"X\", $0.x), y: .value(\"Y\", $0.y) ) } // Highlight a point. PointMark( x: .value(\"Highlight X\", 10), y: .value(\"Highlight Y\", 50) ) .foregroundStyle(highlightColor) // You can also add textual annotations to the point: .annotation(position: .top) { Text(\"Highlight\") } } I am sorry for not being clear. What I want is having a vertical line when I tap on a specific X position so that all datapoints on that X position are highlighted. You would decompose the problem into 1) using a chart proxy to get the x position for a tap and 2) highlighting the data points at that x. The lollipop tooltip we have in the Swift Charts sample app may be a starting point to look at.","title":"Does Swift Charts have a way to highlight values on a line chart  for a specific X position?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-access-dragsession-from-dropdelegate-like-uikit","text":"Hi Harry, this API is not available. However, we'd be interested in learning more about your use case here. Could you please explain your scenario and the expected behavior in https://feedbackassistant.apple.com|Feedback Assistant for the SwiftUI team to consider for a future release? Will do One primary use is to determine if the drag started from the same app (either same view or from another scene) The other reason is to use that to pass data (where NSItemProvider works but is async and may be out of order) This could be semi-related, but at the moment if a drag begins and ends without any movement (i.e. you let your finger go immediately after the drag was activated) there\u2019s no communication back to the app. So, for example, if the drag trigger activates a specific UI state, then when the drag is released, it won\u2019t be de-activated because it considers the drag to still be (incorrectly) active. Yes, I think both of these use cases should be captured as feedback. Providing context like this is very helpful in prioritizing new features. Thank you, both","title":"Is there a way to access DragSession from DropDelegate (like UIKit)?"},{"location":"wwdc22/swiftui-lounge.html#how-can-i-get-simething-similar-to-autolayout-priorities-of-being-able-to-have-2-views-in-a-stack-equal-but-only-up-to-a-certain-width","text":"You can pass any kind of information you like to an adopter of Layout . So that \"certain width\" could be passed to the instance of the Layout itself. And the layout priority can be provided via the subviews proxy of Layout . In the swiftinterface file there is discussion of this: /// Views have layout values that you set with view modifiers. /// Layout containers can choose to condition their behavior accordingly. /// For example, a built-in ``HStack`` allocates space to its subviews based /// in part on the priorities that you set with the ``View/layoutPriority(_:)`` /// view modifier. Your layout container accesses this value for a subview by /// reading the proxy's ``LayoutSubview/priority`` property. You can see the rendered version of that text in the docs under the heading \u201cAccess Layout Values\u201d: https://developer.apple.com/documentation/swiftui/layout Thinking about this more.... Reasoning about this in terms of autolayout might make things tricker than they need to be, especially for 2 views. For instance you could instead use a custom LayoutValueKey to say that View A should take 70% of the proposed size, and View B should take 30%. Rough sample code: MyCustomLayout(widthThreshold: 300.0) { viewA.oversizedWidthPercentage(0.7) viewB.oversizedWidthPercentage(0.3) } i was thinking something more like breaking \"constraints\" due to certain conditions based (if possible) Is there something similar for simple Views? E.g. a view which calculates its required height based on the available width. What's the best practice to solve something like this?","title":"how can i get simething similar to autolayout priorities, of being able to have 2 views in a stack equal but only up to a certain width"},{"location":"wwdc22/swiftui-lounge.html#whats-the-intended-use-for-dynamicpropertys-update-method-it-seems-most-of-the-time-the-dynamicpropertys-members-are-stateful-objects-managing-their-own-state-and-using-update-with-them-requires-a-dispatch-async-to-the-next-runloop-cycle","text":"You\u2019re definitely correct that the state management dynamic properties do is usually handled using their own sub-dynamic properties, and custom logic! The update method is called directly before the corresponding view\u2019s body is called, and is more a place for any logic that needs to happen before body runs to occur. So, for things that I assume are not stateful in nature, just derivates of the state? That tends to be the case. That said, there may be some examples of property wrappers that use information about when the view is rendered to drive stateful systems. When debugging, I\u2019ll sometimes use a DynamicProperty with an Int wrappedValue that increments whenever the surrounding view\u2019s body is drawn, for example :slightly_smiling_face: That\u2019s a neat trick! Thanks for sharing and for the answer! Of course! Thanks for the question!","title":"What's the intended use for DynamicProperty's update() method? It seems most of the time the DynamicProperty's members are @Stateful objects managing their own state and using update() with them requires a dispatch async (to the next runloop cycle)."},{"location":"wwdc22/swiftui-lounge.html#can-we-set-the-navigationpath-from-oncontinueuseractivity-and-onopenurl","text":"Hi - great question! This is certainly something you can do, and is a good example of how you can support something like deep linking, for example, by parsing the URL provided to onOpenURL and using it to construct your navigation path.","title":"Can we set the NavigationPath from onContinueUserActivity and onOpenURL?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-show-a-preview-of-a-view-when-the-user-taps-and-holds-on-a-navigationlink-similar-to-how-a-website-is-previewed-when-holding-on-a-link-in-safari-i-tried-using-the-contextmenu-modifier-but-it-only-seems-to-render-button-views-navigationlink-contextmenu-viewiwanttopreview-is-not-previewed-button-other-options-show-correctly","text":"Yes, use the new contextMenu overload that accepts a preview view. NavigationLink( ... ) .contextMenu { Button { ... } } preview: { ViewIWantToPreview() } Wow, handy. Nice work! Thank you, this seems to work quite well! This was one of the things I was stuck trying to figure out for a while while making my Swift Student Challenge submission (and ultimately never figured out :sweat_smile:) Here\u2019s some more detail on that new modifier: https://developer.apple.com/documentation/swiftui/view/contextmenu(menuitems:preview:) Can I have the user tap the preview to open another view (the \u201cfull\u201d view) as well? I'm playing around with it in Xcode previews and it seems like tapping the preview just closes the context menu. No, it's not currently supported\u2014 please file a feedback request. Alright, done! FB10083104 Thanks for the SwiftUI improvements this year by the way, they've been great!","title":"Is there a way to show a preview of a view when the user taps and holds on a NavigationLink (similar to how a website is previewed when holding on a link in Safari)? I tried using the contextMenu modifier, but it only seems to render Button views.  NavigationLink( ... )     .contextMenu {         ViewIWantToPreview() // is not previewed         Button { ... } // other options show correctly     }"},{"location":"wwdc22/swiftui-lounge.html#hello-thank-you-for-amazing-sessions-i-am-curious-is-it-possible-to-navigate-from-swiftui-view-to-uiviewcontroller-is-it-possible-to-pop-out-from-the-swiftui-view-back-to-uiviewcontroller","text":"I\u2019m glad you\u2019re enjoying the sessions! You can push a UIViewRepresentable onto a NavigationStack , and use that to wrap a UIView . That works with the view-destination NavigationLink s inside a NavigationView as well for previous releases.","title":"Hello! Thank you for amazing sessions! I am curious is it possible to navigate from SwiftUI view to UIViewController.   Is it possible to pop out from the SwiftUI view  back to UIViewController?"},{"location":"wwdc22/swiftui-lounge.html#hello-all-i-believe-this-is-the-channel-for-this-question-im-wanting-to-get-your-take-on-how-swiftuis-data-flow-can-change-how-we-architect-our-apps-it-seems-as-if-with-environmentobjects-observableobjects-and-possibly-more-they-change-where-you-get-your-data-from-which-could-cause-variations-in-lets-say-mvvm-what-do-you-guys-think-is-a-good-way-to-make-use-of-these-property-wrappers-and-could-it-change-how-we-architect-things-thanks","text":"The data flow primitives we provide in SwiftUI were designed to be agnostic to the way you design your model. Think of them less as parts of a specific design pattern and more as tools to allow you to hook your model (however you think it would be best designed) into SwiftUI. If you want more information on this, I recommend you check out the fantastic talk by my colleagues, Raj, and Luca: https://developer.apple.com/videos/play/wwdc2020/10040/|Data Essentials in SwiftUI Thanks for the response, that\u2019s an excellent way of putting it. It\u2019s definitely difficult explaining this to an Android developer who relies on older \u201carchitecture\u201d or design patterns. This will probably help a lot. I\u2019m not sure if this would be good to add here, but i created some SwiftUI pattern i think could be beneficial for my team and it leverages wrappers. The thing is, it\u2019s difficult convincing others that it could work since this is how SwiftUI handles data flow. I will definitely check out this video though, this is what i was envisioning too, maybe you can provide some feedback if any: Providing feedback on specific designs is hard to do in this venue, but definitely sign up for a lab! They tend to do better for long-form discussions / questions. Great! I will sign up for the SwiftUI Lab thanks. <@U03HKVDCL7N> Is there any session explaining how to debug SwiftUI views (techniques such as Self._printChanges() )? Sometimes I notice @96 has changed being printed but wasn\u2019t sure which dependency was responsible. I suspect it was @FecthRequest but wasn\u2019t sure There is the SwiftUI Lab which i think would be best fit for any SwiftUI in depth question. You can check it out here: https://developer.apple.com/wwdc22/labs-and-lounges/dashboard/upcoming?q=SwiftUI Fingers crossed it gets accepted, i have a lot of feedback IDs and doubts :slightly_smiling_face:","title":"Hello all, i believe this is the channel for this question. I\u2019m wanting to get your take on how SwiftUI\u2019s data flow can change how we architect our apps? It seems as if, with EnvironmentObjects, ObservableObjects and possibly more, they change where you get your data from, which could cause variations in, let\u2019s say MVVM. What do you guys think is a good way to make use of these property wrappers and could it change how we architect things? Thanks!"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-best-way-to-resign-text-field-focus-from-some-distant-view-in-the-view-hierarchy-scrolldismisseskeyboard-is-a-great-step-in-the-direction-i-need-but-id-like-to-programmatically-trigger-that-same-behavior-for-example-on-some-button-tap","text":"For example, looking at ways to replace this code: UIApplication.shared .sendAction(#selector(UIResponder.resignFirstResponder), to: nil, from: nil, for: nil) // as an action to perform on a view: extension View { func resignFocus() { UIApplication.shared.sendAction(...) } } You can do this with the Focus State API: https://developer.apple.com/documentation/swiftui/focusstate You want to bind a focus state property to your text field using View.focused(_:equals:) , and then set the binding's value to nil / false from your button action as a way to programmatically resign focus and dismiss the keyboard when the bound text field has focus. Making the action available to distant views is a matter of arranging your app's data flow appropriately. There's no single answer, but for example, you could declare your focus state property on your scene's root view and pass a reset action down to any descendant views that need it. Or if the action is created by a leaf view, you can use the preferences system to make the action available to ancestors. Preferences is an interesting idea. Trying to make the focusstate \u201cglobally accessible\u201d was challenging. I\u2019ll give these suggestions a shot! Thanks.","title":"What is the best way to resign (text field) focus from some distant view in the view hierarchy. scrollDismissesKeyboard is a great step in the direction I need, but I'd like to programmatically trigger that same behavior, for example, on some button tap."},{"location":"wwdc22/swiftui-lounge.html#not-sure-if-this-was-already-asked-before-since-its-such-a-common-question-but-whats-the-recommended-way-to-use-a-viewbuilder-for-custom-components-calling-it-right-away-in-the-init-and-storing-the-view-or-calling-it-later-inside-the-body-and-storing-the-view-builder-itself","text":"We\u2019d generally recommend resolving it right away and storing the view Thank you, much appreciated! Storing the view will have better performance than storing a closure (potentially avoiding allocations). Of course, if you need to dynamically resolve a view from a closure (such as if it takes parameters), then storing the closure is also fine! Is this the same for Layout? The example code for AnyLayout shows creating a layout in the body instead of in the View. You can now (last year?) also declare the viewbuilder as just a property. In SwiftUI 1, you had to manually implement the init, but no longer needed. Best (IMHO) to use synthesized init when you can. struct MyView&lt;Content: View&gt;: View { @ViewBuilder var content: Content } (I believe is the correct code, coding from memory) Yes you can attach @ViewBuilder to properties! You can actually support it on both view and closure properties, depending on whichever you need: struct MyView&lt;Content: View&gt;: View { @ViewBuilder var content: Content // or @ViewBuilder var content: (Int) -&gt; Content } > Is this the same for Layout? The example code for AnyLayout shows creating a layout in the body instead of in the View. Apologies, not sure I fully understand the question. Just in case: creating views in body is fine, we\u2019re specifically talking about stored properties , and storing views versus closures for creating views. Sorry not sure if this is not the right place to ask this. When a view uses ScrollViewReader and the parent view\u2019s state changes, the the child view gets reloaded and the scroll position in the child view changes. This happens only when scroll(to:) has been executed. I have filed a feedback FB10037533 Talking about this, can you confirm it\u2019s best for performance to have the @ViewBuilder be a computed (lazy ?) var than a function in my View struct whenever possible? BTW there is a 3rd-party article explains why the view is preferable than the closure https://rensbr.eu/blog/swiftui-escaping-closures/ (If you don\u2019t mind posting 3rd-party article)","title":"Not sure if this was already asked before since it's such a common question. But, what's the recommended way to use a @ViewBuilder for custom components: calling it right away in the init() and storing the view, or calling it later inside the body and storing the view builder itself?"},{"location":"wwdc22/swiftui-lounge.html#when-using-the-layout-protocol-is-it-possible-to-resolve-an-alignment-guide-for-the-layout-container-not-a-subview-to-align-the-subviews-relative-to-the-container-correctly-for-instance-a-layout-that-proposes-size-100-50-would-resolve-horizontalalignmentcenter-to-50-if-unmodified-if-not-what-is-the-correct-way-to-do-this","text":"You may be able to use the alignmentGuide modifier to define how an alignment guide for a view will resolve My goal is to get the CGFloat value from the container view for its own alignment guides in the placeSubviews function. (Specifically, to do a layout similar to VStack where the children align in the left middle or right) Well, you get the bounds of the region that you are laying out inside of, so you can know the midpoint in each dimension, for example You can also set explicit alignment guides for the layout container by implementing either or both of the explicit alignment methods: \u2022 https://developer.apple.com/documentation/swiftui/layout/explicitalignment(of:in:proposal:subviews:cache:)-8ofeu \u2022 https://developer.apple.com/documentation/swiftui/layout/explicitalignment(of:in:proposal:subviews:cache:)-3iqmu Ok, thank you. I also thought maybe using the largest subviews\u2019 guides as the base, then it would work with custom AlignmentIDs","title":"When using the Layout protocol, is it possible to resolve an alignment guide for the layout container, not a subview, to align the subviews relative to the container correctly? For instance, a Layout that proposes size (100, 50) would resolve HorizontalAlignment.center to 50 (if unmodified). If not, what is the correct way to do this?"},{"location":"wwdc22/swiftui-lounge.html#what-happens-to-a-subview-if-you-dont-call-place-on-it-in-placesubviews-in-a-layout-what-happens-if-you-call-it-twice","text":"If you call the method more than once for a subview, the last call takes precedence. If you don\u2019t call this method for a subview, the subview appears at the center of its layout container and uses the layout container\u2019s size proposal. I found this function a bit strange as I was expecting it to return an array of placements instead of running a place function. Is it more efficient the way it\u2019s set currently set up? Actually, one more follow-up. Is there a way to simply not place a view or have it hidden? Imagine a coverflow style view where you only want to show a small set of subviews. You interact with the subviews from within the layout protocol via the subview proxies that you get as input. This lets you do things like propose a size or place the view, but not perform arbitrary operations on the view, or apply arbitrary modifiers. If there\u2019s something specific that you\u2019d like to do but can\u2019t, please do file a feedback with your use case.","title":"What happens to a subview if you don't call .place() on it in placeSubviews in a Layout? What happens if you call it twice?"},{"location":"wwdc22/swiftui-lounge.html#when-is-onappearondisappear-called-how-do-they-compare-to-the-uikit-equivalents-of-didwillappear","text":"hi, the framework makes no guarantees on the specific timeframes on when these methods are called, but you can be sure that onAppear will always be called before the view is visible to the user and onDisappear will never be called while the view is on screen. We have recently updated the documentation for these methods online to clarify these details; see https://developer.apple.com/documentation/SwiftUI/AnyView/onAppear(perform:) and https://developer.apple.com/documentation/SwiftUI/AnyView/onDisappear(perform:) in terms of a comparison to UIKit, there is not a direct parallel. The UIKit methods are invoked specifically around when views become visible/not visible to the user, whereas the SwiftUI calls are tied more to when the views are constructed/torn down, rather than visually presented.","title":"When is onAppear/onDisappear called? How do they compare to the UIKit equivalents of did/willAppear?"},{"location":"wwdc22/swiftui-lounge.html#love-the-new-toolbarbackground_in-and-toolbarcolorscheme_in-modifiers-in-ios-16-is-there-any-way-to-have-them-take-effect-always-they-seem-to-only-make-a-difference-when-content-scrolls-up-beneath-the-navigation-toolbar-and-using-something-like-toolbarvisible-in-navigationbar-doesnt-seem-to-make-a-difference-my-basic-goal-here-is-to-control-the-navigation-bar-background-color-and-the-status-bar-style-so-if-theres-a-better-way-to-do-that-please-let-me-know","text":"Hi - thanks for the question. .toolbarBackground(.visible) should work for this. If you are not seeing that, please file a feedback Okay! Once I file the feedback I'll drop the # here FB10084072","title":"Love the new toolbarBackground(_:in:) and toolbarColorScheme(_:in:) modifiers in iOS 16! Is there any way to have them take effect always? They seem to only make a difference when content scrolls up beneath the navigation toolbar and using something like .toolbar(.visible, in: .navigationBar) doesn't seem to make a difference.  (My basic goal here is to control the navigation bar background color and the status bar style, so if there's a better way to do that please let me know!)."},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-have-a-presented-view-that-switches-between-the-popoversheet-presentation-styles-depending-on-the-size-class-ie-sheet-in-compact-popover-in-regular","text":"Yes! You can do this by creating a custom view modifier that uses looks at the UserInterfaceSizeClass . Custom modifiers are great for conditional combinations of other modifiers, views, etc. and this is a perfect use case :slightly_smiling_face: Brilliant, thank you! We have an example of this use case in the https://developer.apple.com/documentation/swiftui/bringing_multiple_windows_to_your_swiftui_app|BookClub sample app , in ProgressEditor.swift . Doing it this way, the popover/sheet disappears when the size class changes from regular to compact (though not the other way around) due to window resizing on iPad. Does this seem like a framework bug or something I've overlooked?","title":"Is it possible to have a presented view that switches between the popover/sheet presentation styles depending on the size class (i.e., sheet in compact, popover in regular)?"},{"location":"wwdc22/swiftui-lounge.html#fwik-its-a-good-practice-to-have-state-changes-the-deeper-in-the-view-hierarchy-as-you-can-get-in-order-to-avoid-redundant-redraws-are-there-any-best-practices-to-achieve-that-besides-breaking-views-down-to-tiny-elements-and-composing-them-together","text":"The right way to structure your state varies by app. I\u2019m interested in understanding your specific use case. Are you asking this when using @State or ObservableObject ? Both actually. My use case involves a ViewModel which has @Published variables, together with diff element which is kind of a half drawer that involves @State for it's open/closed state. For State, I would generally recommend defining it at the level where you are going to use it. For your ViewModel it could be useful to break your model into multiple ObservableObject that are specific to a specific functionality (like a cell, or a single screen). But I would avoid prematurely breaking you model into very small pieces. SwiftUI does diff your view and will only re-render if it detects that something has changed. \u2022 Everything suggested in the Demystify SwiftUI Session from a year or two back is definitely useful \u2022 Keeping Identifier s separate from values is recommended generally (but not a hard and fast rule), especially in Navigation APIs.","title":"FWIK it's a good practice to have state changes the deeper in the view hierarchy as you can get, in order to avoid redundant redraws. Are there any best practices to achieve that besides breaking views down to tiny elements and composing them together?"},{"location":"wwdc22/swiftui-lounge.html#are-there-any-types-of-macos-apps-or-interfaces-where-you-would-still-recommend-using-appkit-rather-than-swiftui-im-yet-to-invest-the-considerable-amount-of-time-learning-swiftui-and-i-keep-reading-mentions-of-people-saying-its-still-a-mixed-bag-for-macos-development-so-i-dont-want-to-potentially-waste-time-thanks","text":"Good question! Across all platforms, we\u2019d recommend comparing your needs to what SwiftUI provides (so no hard rules/recommendations) \u2014 and keeping in mind that you can adopt SwiftUI incrementally. Within Apple\u2019s own apps on macOS, we\u2019re ourselves using the full spectrum of approaches. From just a specific view/views in an app, e.g. in Mail, iWork, Keychain Access; to an entire portion of the UI or a new feature, e.g. in Notes, Photos, Xcode; and all the way to the majority of an application, e.g. Control Center, Font Book, System Settings. But in the end, I\u2019d recommend starting with a part you\u2019re comfortable with and building up from there! You should look at SwiftUI as another tool in your toolset in enabling you to build the best apps you can. Great answer. Thanks.","title":"Are there any types of macos apps or interfaces where you would still recommend using appkit rather than swiftui?  I'm yet to invest the considerable amount of time learning swiftui and I keep reading mentions of people saying it's still a mixed bag for macos development, so I don't want to potentially waste time.  Thanks!"},{"location":"wwdc22/swiftui-lounge.html#when-using-the-new-menubarextra-in-window-style-is-it-possible-to-control-the-width-and-maximum-height-of-the-content-window","text":"Hi - great question. The window size should be derived from the content provided to it, though we do impose a minimum and maximum on the resulting size. If you are seeing unexpected behavior in this area, please do file a feedback with a sample to reproduce it, and we can take a look. I experience a pretty large minimum width and a pretty small maximum height. I'll file a feedback with a project.","title":"When using the new MenuBarExtra in window style is it possible to control the width and maximum height of the content window?"},{"location":"wwdc22/swiftui-lounge.html#can-text-animate-fluidly-when-using-matchedgeometryeffect-and-one-is-set-to-a-fixedsizehorizontal-true-and-the-other-is-not","text":"Hi Bardia, I'd expect that to work, especially because text transitions work well when the text is changing layouts (multiple lines/alignments). We realize that it can be a little tricky to know how far exactly you can push those beautiful text transitions. Have you tried this out and seen otherwise? Ah I can reply now :slightly_smiling_face: Sorry for the late response (it was very late in London last night). Yep, I\u2019ve tried and have logged FB9156668 including code/clip. Basically it animates perfectly back-and-forth when both text are visible on a single line. However, when the one text goes over two lines \u2013 due to larger text size for example \u2013 and me allowing it to (via fixedSize(vertical: true)), this causes the animation in either direction to not be fluid (best demonstrated in the video). I think that feedback is a different one you've filed. That one looks Text Avoidance related Whoops! FB10101271","title":"Can text animate fluidly when using matchedGeometryEffect and one is set to a fixedSize(horizontal: true) and the other is not?"},{"location":"wwdc22/swiftui-lounge.html#hello-is-it-possible-to-present-a-popover-as-popover-on-iphone-like-its-the-case-with-ios","text":"On iPhone, the .popover(\u2026) modifier always presents as a sheet. There isn\u2019t currently a way to customize that behavior. I\u2019d love a Feedback with details of your use case though! <@U03HW7P0HQR> On a similar note, confirmationDialog for iOS presents an alert, iPadOS presents a popup, macOS presents an alert. Is it possible to make it present an action sheet for iOS? Please file a feedback with your request! (and feel free to paste that feedback in here once it\u2019s filed) If a 3rd party solution is ok, I made a library: https://github.com/aheze/Popovers","title":"Hello, is it possible to present a popover as popover on iPhone, like it's the case with iOS?"},{"location":"wwdc22/swiftui-lounge.html#hello-talking-about-the-date-picker-with-multiple-dates-can-they-allow-to-choose-date-ranges-like-when-you-book-a-flight-so-with-just-two-dates-possible-and-the-highlight-spanning-over-all-of-the-days-in-between","text":"Hi Cristina, currently, MultiDatePicker only supports non-contiguous date selection. So range selection is not supported at the moment. Speaking of booking flights... https://developer.apple.com/documentation/weatherkit/fetching_weather_forecasts_with_weatherkit|FlightPlanner sample app now available. Would be great to have, maybe also with a two months view! If you think you can consider something like this in the future I could file a feedback Yes please do! We'd highly appreciate that > Speaking of booking flights... https://developer.apple.com/documentation/weatherkit/fetching_weather_forecasts_with_weatherkit|FlightPlanner sample app now available. Great! I\u2019m downloading it, thanks!","title":"Hello! Talking about the Date Picker with multiple dates, can they allow to choose date ranges (like when you book a flight), so with just two dates possible and the highlight spanning over all of the days in between?"},{"location":"wwdc22/swiftui-lounge.html#when-creating-a-menubarextra-is-it-possible-to-create-something-that-runs-on-its-own-separate-from-the-parent-app-i-have-an-app-that-users-can-save-data-to-by-means-of-various-app-extensions-if-the-app-is-closed-however-those-changes-dont-get-synced-to-cloudkit-i-was-thinking-of-having-a-menu-bar-app-that-would-essentially-be-my-workaround-always-listening-always-active-can-a-menubarextra-do-that-for-me","text":"Hi - you can certainly define a SwiftUI App with only a MenuBarExtra Scene . If your app should not show a Dock icon, that will require some changes to the Info.plist file, which should be noted in the documentation for MenuBarExtra See https://developer.apple.com/documentation/swiftui/menubarextra Posting that bit of documentation here as well: > For apps that only show in the menu bar, a common behavior is for the app to not display its icon in either the Dock or the application switcher. To enable this behavior, set the LSUIElement flag in your app\u2019s Info.plist file to true . Good to know. So it sounds like I would want to create a separate, MenuBarExtra -only app. Can I bundle that with my Mac app on the App Store? (Wrong lounge, I know\u2026) Hmm, I'm afraid I do not know the answer to that App Store question. That\u2019s alright. I had to ask anyway. Do MenuBarExtra\u2019s support other SwiftUI features, like keyboard shortcuts, so I could invoke it or an action it has via a (global?) shortcut?","title":"When creating a MenuBarExtra, is it possible to create something that runs on its own, separate from the parent app? I have an app that users can save data to by means of various app extensions. If the app is closed, however, those changes don't get synced to CloudKit. I was thinking of having a menu bar app that would essentially be my workaround: always listening, always active. Can a MenuBarExtra do that for me?"},{"location":"wwdc22/swiftui-lounge.html#are-deep-links-possible-in-swiftui-if-so-does-it-work-well-are-there-any-limitations-vs-what-is-possible-in-uikit","text":"Check out the SwiftUI Cookbook for Navigation session and the What's New in SwiftUI session for 2 examples of deep links They are indeed possible, and we think they work pretty well :wink: Of course, there's a lot of routing to consider with any app \u2014 for instance if your deep link is behind some authentication token, you'll need to do the extra work there. The general idea is that a deep link is certain destinations presented \u2014 in order \u2014 on a navigation stack, and with this years' new APIs you can have full control over what is on the navigation stack.","title":"Are Deep Links possible in SwiftUI?  If so, does it work well?  Are there any limitations vs what is possible in UIKit?"},{"location":"wwdc22/swiftui-lounge.html#what-architecture-is-recommended-for-enterprise-apps-when-using-swiftui-for-instance-coordinator-pattern-dependency-injection-containers-etc","text":"Thanks for the question! One of the strengths of SwiftUI as a framework is that it works well with a variety of architectures. We might be opinionated about API design :slightly_smiling_face:, but we want you to have the flexibility to design the data model that works best for your app. The particular data model of any individual app depends on the domain of that app. For example, a car rental app might have a very different architecture from a news reader app. Can you recommend a website that provides various examples of architecture using SwiftUI? One challenge I\u2019ve come across is that the coordinator pattern doesn\u2019t translate well to SwiftUI. You will often have NavigationLink s in your views, whereas the coordinator pattern wants to separate that logic from the view itself. But this years NavigationStack is going to make new coordinator-like patterns much easier to implement. I\u2019m curious to experiment with that this week.","title":"What architecture is recommended for Enterprise apps when using SwiftUI?  For instance: Coordinator Pattern, Dependency Injection Containers, etc."},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-in-general-to-use-timelineview-as-a-global-clock-that-can-be-synced-by-all-the-devices-that-use-the-app-in-order-to-displayshow-something-at-the-same-exact-time-if-so-what-precision-should-be-expect","text":"While TimelineView can be used to update views at a specific point in time, it isn\u2019t designed specifically for cross-device synchronization. You may be able to use it for that kind of use case, but it wouldn\u2019t provide any better precision than using a Timer or other scheduling mechanism","title":"Is it possible in general to use TimelineView as a \"global clock\" that can be synced by all the devices (that use the app) in order to display/show something at the same exact time? (if so, what precision should be expect?)"},{"location":"wwdc22/swiftui-lounge.html#that-presentation-was-simply-beautiful-i-am-awestruck-at-how-swiftui-has-accelerated-this-year-congrats-to-the-team-that-built-it-and-thank-you","text":"Thanks so much! We listen closely to community feedback. We may not always respond directly to the Feedback you file \u2014 there\u2019s a lot of it :slightly_smiling_face: \u2014 but it really helps us prioritize the work. We love seeing all the great things you all are building!","title":"That presentation was simply beautiful. I am awestruck at how SwiftUI has accelerated this year. Congrats to the team that built it, and thank you."},{"location":"wwdc22/swiftui-lounge.html#s0-anylayout-could-be-user-selectable-if-selection-changed-state","text":"Yes! As Paul showed in the video any State or property can drive conditional logic to choose between different Layouts using AnyLayout That is very powerful! Gets rid of all sorts of Case and If views :grinning:","title":"S0 - AnyLayout could be user selectable (if selection changed @State)?"},{"location":"wwdc22/swiftui-lounge.html#can-a-column-span-partial-columns-in-grid-ie-if-i-have-8-items-in-a-three-column-grid-i-want-the-last-row-to-have-two-cells-that-are-112-columns-each","text":"The column spanning uses an integer, but you could try doubling the number of columns you're using Hmm\u2026 that could work, thanks! Using integer multiples works really well. If you want one view to be 3/5 of the width and the other 2/5 you just use 3 and 2 on each (assuming they each take all available space)","title":"Can a column span partial columns in Grid? i.e. If I have 8 items in a three column Grid, I want the last row to have two cells that are 1\u00bd columns each."},{"location":"wwdc22/swiftui-lounge.html#can-i-use-custom-layout-to-create-a-hierarchical-tree-mapview-of-rectangular-boxes-of-data-items-connected-with-lines","text":"That\u2019s a fascinating question, Andrew! Layout could readily handle positioning the boxes. The lines would be the interesting part. Sounds like a good challenge :slightly_smiling_face: I'm just not sure of any other way to do this currently for an app view I want to do. You might experiment with using the layout values to route information. You could also experiment with using Canvas to do the drawing, though that\u2019s a low-level approach that wouldn\u2019t give you accessibility out of the box. Check out the accessibilityRepresentation modifier for that. Any ideas on how to \u201croute\u201d? Ultimately, that\u2019s \u201cjust\u201d math. There are a few open source frameworks for diagram drawing. You might be able to see what algorithms they are using. Maybe alternate row of boxes & row of custom Paths? Use the layout info from the boxes to feed the custom Paths. In fact you can do it using SwiftUI 2021. http://objc.io|objc.io made a series of video to demo how to draw tree image using pure SwiftUI (sorry for 3rd-party resource) https://talk.objc.io/episodes/S01E293-advanced-alignment-part-3","title":"Can I use custom layout to create a hierarchical tree map/view of rectangular boxes of data items connected with lines?"},{"location":"wwdc22/swiftui-lounge.html#do-layoutvaluekey-values-travel-up-through-nested-layouts-also-how-are-they-different-from-preferencekey","text":"LayoutValueKeys only travel up to the nearest Layout container OK, good to know One reason for this is for Layout to facilitate building of encapsulations that can be composed without having unanticipated effects on the other views that surround it","title":"Do LayoutValueKey values travel up through nested Layouts? Also, how are they different from PreferenceKey?"},{"location":"wwdc22/swiftui-lounge.html#do-all-system-stacks-implement-the-layout-protocol-for-use-in-anylayout-is-this-documented-anywhere","text":"The goal is for as many system provided layouts to conform to the Layout protocol as possible, including the stacks. You can find what protocols a type conforms to in the developer documentation Yup, scroll down on this page: https://developer.apple.com/documentation/swiftui/hstack Thanks!","title":"Do all system stacks implement the Layout protocol for use in AnyLayout? Is this documented anywhere?"},{"location":"wwdc22/swiftui-lounge.html#what-examples-of-custom-layouts-do-you-think-a-new-layout-protocol-is-a-best-choice-for","text":"Flow layouts are great for it. There\u2019s a good example in the Food Truck sample app. Whatever you can imagine :upside_down_face: In the State of the Union we mentioned Flow and Radial layouts as good examples A custom layout can be a good fit if there's something super custom in your app I am thinking of a dynamic layout case for seating order around tables. User could create a new Table, specify if the table is rectangular, oval or circular, and specify the number of guests at each side. We could then get a layout for the table as specified, and the user can fill in the guests. In code terms, this could mean that the specific layout should be created dynamically. Would something like this be possible? It's an even better fit for building a generic layout container that you can reuse across your app (like Flow referenced above) Or if you like bee hives\u2026 https://twitter.com/ryanlintott/status/1534367196251574272 Even really subtle, nuanced layouts can bring a little extra level of polish \u2014 plus be composed together! For higher level features like the new form style on macOS we\u2019re using several small custom layouts to get it to perfectly match what the design specs described Jan, that sort of thing is definitely possible! In fact, in the what's new in SwiftUI talk, they show an example of a layout which is very similar to what you\u2019re describing to make a seating chart for SwiftUI\u2019s birthday party! Thanks <@U03HKVDCL7N>, that's great! Yea I saw that, I just hope we can let user specify the dimensions/shape of the table that talk is what got me going on the idea :slightly_smiling_face:","title":"What examples of custom layouts do you think a new layout protocol is a best choice for?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-build-a-layout-that-would-effect-each-child-view-with-viewmodifiers-lets-say-i-always-want-to-style-the-first-view-one-way-and-the-second-another","text":"LayoutValueKey allows you to pass information from your view hierarchy along to layout, but please note that layout can't use this information to make style changes to the view, only to determine its layout. The information is passed through to layout through layout proxies, so the original view isn't accessible from the layout protocol. I guess what I\u2019m looking for is a way to apply modifiers to a Group that would only apply to specific subview indexes. It would only need to apply in one direction. I guess it\u2019s time to write another Feedback.","title":"Is there a way to build a Layout that would effect each child view with ViewModifiers? Let's say I always want to style the first view one way and the second another."},{"location":"wwdc22/swiftui-lounge.html#are-all-the-new-layout-features-available-on-non-ios-platforms-watchos-macos-tvos","text":"Yes, there are :tada: Excellent, thank you! :partying_face: looking forward to the multiplatform targets with this one And just for reference, you can always check the availability of a bit of API right at the top of the page. For example, for Layout : Thank you <@U03HELSV45B>! I didn't know the new stuff was already documented and 'out there' :blush:","title":"Are all the new layout features available on non-iOS platforms? watchOS, macOS, tvOS..."},{"location":"wwdc22/swiftui-lounge.html#im-noticing-a-huge-hit-to-scrolling-performance-when-i-add-a-contextmenu-to-cells-in-my-lazyvstack-when-i-profiled-the-app-it-showed-that-buttons-in-the-contextmenu-was-what-was-taking-so-long-to-render-does-a-context-menu-have-to-render-on-creation-of-a-view-is-there-any-way-to-create-a-lazy-context-menu","text":"Hi - we believe that this issue was fixed in a recent software update. If you are still seeing this, could you please file a feedback? Yes for sure. Thanks! Also <@U03HHJH9C66> what is the best category for SwiftUI feedback? Developer tools, or iOS & iPadOS? <@U03HHJH9C66> i noticed that contextMenu on rows are not updated when the row is updated (for example update the text or symbol based on an action taken). Is this fixed now?","title":"I'm noticing a huge hit to scrolling performance when I add a contextMenu to cells in my lazyvstack. When I profiled the app, it showed that buttons in the contextMenu was what was taking so long to render. Does a context menu have to render on creation of a view? Is there any way to create a lazy context menu?"},{"location":"wwdc22/swiftui-lounge.html#why-would-anyone-use-observable-object-instead-of-state-object","text":"You might want to use @ObservedObject if the lifecycle of your model is not owned by your view. An example is if you are using UIHostingConfiguration and you have your model owned by UIKit in a UIViewController and just passed to a SwiftUI cell. Also @Published properties in ObservableObjects use Combine publishers, which can have their own basket of use cases too Also, in general you want to use @StateObject in a parent view, and if you pass the same ObservableObject instance to child views you can use just @ObservedObject .","title":"Why would anyone use observable object instead of state object?"},{"location":"wwdc22/swiftui-lounge.html#viewmodifier-question-whats-the-difference-between-a-custom-viewmodifier-without-dynamicproperty-that-uses-some-built-in-modifiers-in-bodycontent-and-a-custom-view-extension-func-that-just-use-those-built-in-modifiers-similarly-whats-the-difference-between-a-custom-viewmodifier-with-some-dynamicproperty-and-a-custom-view-with-some-dynamicproperty-also-has-a-viewbuilder-content-property-to-receive-content-to-modify-i-think-two-have-the-same-render-result-and-behavior","text":"Because of the way a ViewModifier is expressed, the engine knows it's not changing the content passed in and can apply performance optimizations (compared to just an extension on View)","title":"ViewModifier question: What\u2019s the difference between a custom ViewModifier (without DynamicProperty) that uses some built-in modifiers in body(content:), and a custom View extension func that just use those built-in modifiers?  Similarly, what\u2019s the difference between a custom ViewModifier with some DynamicProperty and a custom View with some DynamicProperty (also has a @ViewBuilder content property to receive content to modify) ? I think two have the same render result and behavior."},{"location":"wwdc22/swiftui-lounge.html#a-bit-off-topic-to-grids-layouts-but-not-to-customization-is-there-any-way-in-swiftui-to-create-multiple-windows-or-window-like-things-on-mac-and-maybe-ipad-or-pages-on-ios","text":"Hi - thanks for the question! You can certainly compose the different Scene types available in your App to create the functionality you'd like. For instance a WindowGroup and a Window or DocumentGroup and a WindowGroup , etc. hey, I was gonna ask that question later today :joy: Right now we need to use UIKit lifecycle if we want to show HUD that overlays the entire app. For example: https://www.fivestars.blog/articles/swiftui-windows/ it would be really useful to be able to do this without relying on SceneDelegate! I have the same question as well, what would be the recommended way to do what <@U03JMMN8659> says? I thought about using the overlay modifier on the root view of the app but what if there are presented views? Something like what is in that blog post is likely worth a feedback for an enhancement request. Thanks!","title":"A bit off topic to Grids / Layouts but not to Customization - Is there any way in SwiftUI to create multiple Windows (or Window like things) on Mac (and maybe iPad) or Pages on iOS?"},{"location":"wwdc22/swiftui-lounge.html#hi-there-this-is-not-really-related-to-the-layouts-video-we-just-watched-but-rather-a-question-that-i-had-a-while-ago-the-video-was-really-clear-thanks-for-the-amazing-explanations-moving-on-to-the-question-dan-we-use-swiftui-to-create-a-numeric-text-field-i-was-looking-for-something-that-allows-typing-int-numbers-in-my-case-i-had-to-work-to-with-4-digit-numbers-and-a-stepper-wasnt-the-best-choice-instead-i-went-for-a-textfield-that-checks-if-it-can-convert-the-input-string-to-an-int-when-a-button-is-clicked-is-there-a-better-way-to-achieve-this","text":"Check out the TextField initializers that take a formatter. Just noticed a typo, I meant can instead of dan:sweat_smile: Thank you! Really appreciate it. If this is for iOS see also the keyboardType(_:) modifier. This was for the Swift Student Challenge, and I ran my app both in the iPad Simulator and as a Mac app using Mac Catalyst. Thanks!","title":"Hi there! This is not really related to the Layouts video we just watched, but rather a question that I had a while ago. The video was really clear. Thanks for the amazing explanations!  Moving on to the question, dan we use SwiftUI to create a numeric text field? I was looking for something that allows typing Int numbers.   In my case, I had to work to with 4 digit numbers, and a stepper wasn't the best choice. Instead, I went for a TextField that checks if it can convert the input string to an Int when a button is clicked. Is there a better way to achieve this?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-use-a-capacity-gauge-outside-of-the-lock-screen","text":"Yes! Gauge is usable outside of the Lock Screen, and also has a whole host of new styles! I would take a look at the linearCapacity, accessoryLinearCapacity, and accessoryCircularCapacity styles. Please note though that gauge styles prefixed with \u201caccessory\u201d are really intended to appear in contexts like the Lock Screen / widgets, or some similar design context within your app proper, so please give careful consideration to where you use them.","title":"Is there a way to use a capacity Gauge outside of the lock screen?"},{"location":"wwdc22/swiftui-lounge.html#i-implemented-the-new-searchable-apis-yesterday-one-issue-i-have-is-that-when-suggestions-are-presented-it-completely-blocks-out-the-entire-search-view-im-wanting-to-implement-something-similar-to-the-photos-search-feature-where-you-can-see-a-small-box-at-the-top-with-search-suggestions-while-also-display-existing-results-is-there-a-way-to-accomplish-this-with-searchable","text":"If you\u2019d like search suggestions to be rendered inline your results, you should consider them as part of your results and not provide any suggestion views to the searchable modifier. Note though that in iOS 16.0, for a regular width size class, a search bar that is displayed in the trailing navigation bar will display suggestions in a menu rather than over the main content like on macOS. Take a look at the SearchSuggestionsPlacement type and searchSuggestions(_:in:) modifier for customizing your UI around this behavior.","title":"I implemented the new searchable APIS yesterday. One issue I have is that when suggestions are presented, it completely blocks out the entire search view. I'm wanting to implement something similar to the Photos search feature where you can see a small box at the top with search suggestions, while also display existing results. Is there a way to accomplish this with searchable?"},{"location":"wwdc22/swiftui-lounge.html#i-am-using-three-pickers-in-a-row-to-allow-me-to-enter-hoursminutesseconds-however-the-tap-targets-for-the-pickers-are-all-off-by-some-factor-such-that-if-i-try-to-scroll-to-the-right-of-the-middle-of-a-picker-it-actually-scrolls-the-picker-to-the-right-instead-is-this-a-known-issue-is-there-a-workaround","text":"Hey Micheal, could you please file a radar fro this issue? That would be greatly appreciated. Will do - thank you! :grinning: Is this a wheel picker? If so, been there, done that, it's a known bug, but we found a workaround. Lemme know","title":"I am using three pickers in a row to allow me to enter hours/minutes/seconds. However, the tap targets for the pickers are all off by some factor, such that if I try to scroll to the right of the middle of a picker, it actually scrolls the picker to the right instead. Is this a known issue? Is there a workaround?"},{"location":"wwdc22/swiftui-lounge.html#viewthatfits-feels-very-convenient-but-im-concerned-about-making-copies-of-the-same-content-for-each-layout-option-i-assume-a-custom-layout-would-be-required-to-achieve-this","text":"ViewThatFits is a convenience that might not be appropriate if you need something with animated transitions, or in general something with more complex behavior.","title":"ViewThatFits feels very convenient but I'm concerned about making copies of the same content for each layout option. I assume a custom layout would be required to achieve this?"},{"location":"wwdc22/swiftui-lounge.html#can-the-new-charts-api-allow-for-scrolling-similar-to-the-health-apps-charts","text":"Sorry, late answer forgot to hit send: Yes, charts should behave just like any other view in this regard. though you may have to set explicit frames on the chart to specify how big (wide? tall?) you want it to be We just couldn\u2019t wait till the next Q&A, Ok I promise we are done now!!! :nerd_face: Hey <@U03J21CNQ1G> I just realized the health app chart does scrolling that keeps the y-axis stationary, that specifically isn't supported, but we've received a few requests about this, but few feedbacks. So if you file a feedback for the Charts team, we'd appreciate it","title":"Can the new Charts API allow for scrolling? similar to the health app's charts"},{"location":"wwdc22/swiftui-lounge.html#hey-is-it-possible-to-customise-the-font-on-pickers-and-menu-this-release-ive-had-to-create-my-own-menu-quite-a-workload-to-workaround-this-styling-limitation","text":"Could you confirm which platform you\u2019re seeing this issue with? The font should be customizable on macOS but is not currently on iOS. We\u2019d love to hear more about your use case and please do file a feedback with more details so that we can take it into consideration for the future. Yep, it\u2019s on iOS that I\u2019m referring to. I will, okay! The use case is basically wanting to create a bespoke experience that involves using a custom font (Avenir Next). It\u2019s inconsistent/less cohesive if controls such as menus use a different font. i see. yes, please do file a feedback with a sample app if you can, and feel free to paste the number here once you do so that we can send it along to the right place!","title":"Hey! Is it possible to customise the font on pickers and menu this release? I\u2019ve had to create my own menu (quite a workload) to workaround this styling limitation."},{"location":"wwdc22/swiftui-lounge.html#how-does-layout-interact-with-other-view-modifiers-or-constructs-that-can-provide-layout-data-like-offset-geometry-reader-anchor-preferences-etc-for-context-ive-got-a-card-game-type-view-that-currently-lays-out-cards-in-columns-all-powered-by-geometryreader-and-anchor-preferences-because-i-need-specific-frame-data-when-a-user-performs-a-drag-gesture-to-drag-a-card-over-a-region-of-the-screen-to-perform-logic-im-wondering-if-its-possible-to-re-implement-it-in-layout-but-im-unclear-about-if-its-possible-to-get-at-the-anchor-preference-frame-data","text":"A layout is only able to position its direct subviews. It can be superior to GeometryReader because it's able to do that while also computing a sizeThatFits. If you're trying to arrange things that are crossing parent-child boundaries, though, anchor preferences will likely still be necessary. A Layout that contained and positioned all of my card subviews would therefore know their positions - and I imagine I could put those in the cache. I guess I'm wondering if there's a way my layout can provide that information out to my model layer when a drag gesture finishes? More generally - can a Layout relay its layout information externally in some way? Or is that not possible?","title":"How does Layout interact with other view modifiers or constructs that can provide layout data, like offset, geometry reader, anchor preferences, etc? (For context I've got a card game-type view that currently lays out cards in columns all powered by GeometryReader and anchor preferences, because I need specific frame data when a user performs a drag gesture to drag a card over a region of the screen to perform logic. I'm wondering if it's possible to re-implement it in Layout, but I'm unclear about if it's possible to get at the anchor preference frame data.)"},{"location":"wwdc22/swiftui-lounge.html#what-is-recommended-way-to-style-navigation-bar-in-swiftui","text":"Check out the .toolbarBackground modifier, new in iOS 16 and aligned releases. What about iOS 14, 15 is it still UINavigationBar appearance? Sorry. The new API is not able to be back deployed. Does .toolbarBackground apply to only that view or any view pushed into navigation stack as well? I believe it applies to the current view, so you can change the behavior at different levels of the hierarchy. Awesome, is it safe to assume it takes care of animations too? For example can I set an opaque background on one view and a transparent one on the detail view? <@U03HW7P0HQR> by the way, I\u2019d like to use this opportunity to thank you for the awesome cookbook presentation and thank the team for all the goodies this year. :innocent: There\u2019s also toolbarColorScheme if you need to control the text appearance. Finally, you can use a toolbar item in the principal placement to replace the default navigation bar title with an arbitrary SwiftUI view. Does it apply to both large style and inline style?","title":"What is recommended way to style navigation bar in SwiftUI?"},{"location":"wwdc22/swiftui-lounge.html#how-do-i-actually-apply-variable-color-to-an-sf-symbol-along-with-the-percentage-i-want-highlighted-vs-dimmed-is-there-a-swiftui-modifier-for-this","text":"You can make a variable color symbol with: Image(systemName: \"wifi\", variableValue: signalStrength) It\u2019s part of initializing the image, not a separate modifier you apply. Thanks Jacob!!","title":"How do I actually apply Variable Color to an SF Symbol along with the percentage I want highlighted vs dimmed? Is there a SwiftUI modifier for this?"},{"location":"wwdc22/swiftui-lounge.html#hello-i-am-trying-to-create-a-new-custom-layout-i-would-like-to-know-how-to-access-layoutvaluekeys-from-within-a-layout","text":"The documentation for the new layout API is fantastically written and should have all of the information you need to get up and running! It explains it far better than I could in a short question answer. You can check out the docs for LayoutValueKey here. It even has a walkthrough that shows you how to create, set, and retrieve keys! https://developer.apple.com/documentation/swiftui/layoutvaluekey/ Thanks! Yeah, it definitely is fantastically written! Amazing work!","title":"Hello! I am trying to create a new custom Layout. I would like to know how to access LayoutValueKeys from within a Layout."},{"location":"wwdc22/swiftui-lounge.html#fairly-new-to-swift-swiftui-curious-if-there-are-any-changes-to-core-dataswiftui-integration-this-year-using-managedobjects-with-swiftui-seems-to-results-in-view-that-know-a-lot-about-models-or-models-that-have-a-mix-of-business-logic-and-view-logic-which-can-be-confusing-any-suggested-best-practices-for-using-managed-objects-with-swiftui-i-tried-writing-value-type-wrappers-to-do-mvvm-but-that-breaks-down-with-relationships","text":"This is a topic that deserves its own WWDC :joy: I really, really recommend Paul Hudson's videos on this. He helped me a lot with understanding the relationships between SwiftUI and Core Data. Especially important if you plan to use SectionedFetchRequest or generic FetchRequest :zap: There is still a lot of improvement to make Core Data more SwiftUI-esque, here's hoping for WWDC23 :innocent: Hi Tom, Great question. There were no additions to SwiftUI in regards to Core Data. However, we do have our https://developer.apple.com/documentation/coredata/loading_and_displaying_a_large_data_feed|CoreData https://developer.apple.com/documentation/coredata/loading_and_displaying_a_large_data_feed|Earthquakes https://developer.apple.com/documentation/coredata/loading_and_displaying_a_large_data_feed| sample app and https://github.com/orgs/apple/repositories?q=sample-cloudkit&type=all&language=&sort=|CloudKit sample apps all showing best practices with SwiftUI and background contexts. The former performs batch inserts and merges changes on a background thread, then publishes the changes to SwiftUI on the main thread. One place I am struggling with right now is providing the ability to edit an entity with the idea of \"cancel\" and \"save\". <@U03HELS8GHK> Thank you for sharing Earthquakes example, I\u2019ll definitely check it out. Any chance I can get your attention to FB8545866 pretty please? For a very simple Core Data + List implementation, app crashes whenever an item is deleted from the List. It would be awesome if there was a guideline around more complex implementations. > One place I am struggling with right now is providing the ability to edit an entity with the idea of \"cancel\" and \"save\". This notion of edit propagation is described in the https://developer.apple.com/tutorials/swiftui/working-with-ui-controls|Landmarks app in the SwiftUI Tutorials My current plan is to do MVVM by using classes with Value Semantics instead of actual value types but I've just started down that path. Thanks for all the pointers! <@U03HELS8GHK> I'm not sure we can compare the Landmark app based on Struct with an app using Core Data with objects in contexts. How can we use child contexts with SwiftUI? What would be the best approach to edit entities in a child context? Great follow-up question, Axel. We don't have a sample today detailing this exact scenario, so you'll have to apply the same concepts from the Landmarks app to the merge flow shown in Earthquakes. I'd suggest requesting more samples via Feedback Assistant, though, so we can consider more opportunities to improve our learning resources. To clarify, the Earthquakes sample uses private contexts to perform its batch inserts and merges the changes into the main context. So, that sample is a great place to start. Thanks. I explored a lot these sample projects and took some inspiration. But they don't really cover UI edits made by a user. I'll fill some FB.","title":"Fairly new to Swift &amp; SwiftUI.  Curious if there are any changes to Core Data/SwiftUI integration this year. Using ManagedObjects with SwiftUI seems to results in View that know a lot about Models or Models that have a mix of business logic and view logic which can be confusing.  Any suggested best practices for using Managed Objects with SwiftUI?  I tried writing value type wrappers to do MVVM but that breaks down with relationships."},{"location":"wwdc22/swiftui-lounge.html#heya-i-have-a-macos-app-where-i-am-attempting-to-tear-off-certain-swiftui-views-as-floating-free-form-windows-ive-looked-into-some-of-the-windowgroup-api-but-it-doesnt-seem-like-it-supports-what-im-looking-for-essentially-i-have-n-views-of-various-sizes-that-can-either-be-docked-into-a-main-window-or-float-above-it-im-running-in-to-issues-like-sharing-state-between-the-docked-version-and-the-windowed-one-as-there-seems-to-be-a-difference-in-the-relationship-between-how-a-standard-swiftui-update-bindings-works-for-windows-or-views-given-that-what-would-be-some-ways-that-i-could-go-about-this-method-of-docking-undocking-swiftui-views-and-keeping-their-state-in-tact-thank-you","text":"Hi - this is really interesting! I'm not really sure we have any API that entirely fits your concept here, but I'd love to learn more. Can you speak to how you are accomplishing this at the moment? Also a feedback for this with any details you provide would be great! Absolutely! I\u2019m doing this right now in my project at http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat . Posting this now to respond to you, and will follow up real quick with a screenshot and a code snippet. The key classes are all here: https://github.com/tikimcfee/LookAtThat/tree/4dbe4a8f93f5dafd56752433df917352076da664/Interop/Views/FloatableView FloatableView swift basically takes some bindings to an enumerable state type (floating, docked), and then either presents the view directly as a child, or acts as an intermediary \u2018adapter\u2019 between the parent hosting view and a floating window. The window itself ends up as a separate instance of the view, but seems to be more or less functional since it still can participate in shared state updates. The other classes, window delegate and the extensions, help construct and manage the window lifecycle a bit. The two things that are tricky are that the view is still technically bound to an EmptyView that lets the state remain active, and the window is its own \u2018thing\u2019. It seems like there would be a way to sever that connection between the root view that owns the docked version of the view and the window (which is why I was really trying to consider WindowGroups), but I just couldn\u2019t crack this one. And hey, is that feedback from the dev-feedback-app? If so, I\u2019d be happy to share this and our discussion with it. And since I was rude and forgot, thank you for taking your time to talk and take a look! Yep, there should be an app called Feedback Assistant that you can use. Any info you can put in there, like what you have above would certainly be useful. <@U03HHJH9C66> Done! I have a fancy-pants ID too : FB10112377","title":"Heya! I have a macOS app where I am attempting to 'tear off' certain SwiftUI views as floating free form windows. I've looked into some of the WindowGroup API, but it doesn't seem like it supports what I'm looking for. Essentially, I have N views of various sizes that can either be docked into a main window, or float above it. I'm running in to issues like sharing state between the docked version and the windowed one, as there seems to be a difference in the relationship between how a standard SwiftUI update bindings works for windows or views. Given that, what would be some ways that I could go about this method of 'docking / undocking' SwiftUI views, and keeping their state in tact?  Thank you!"},{"location":"wwdc22/swiftui-lounge.html#why-does-navigationlink-pre-render-the-destination-when-its-being-rendered-i-am-loading-heart-rate-data-in-my-destination-view-and-this-slows-down-rendering-significantly-whats-a-better-pattern-to-prevent-accidental-pre-loading-of-data","text":"In general we discourage to do any expensive work in the init of a view. SwiftUI expects the creation of views to be cheap. We recommend doing any side effect or data loading with either .task or .onAppear . Even .onAppear ran last time I tried. Currently I am using this workaround from StackOverflow: public struct NavigationLazyView&lt;Content: View&gt;: View { public let build: () -&gt; Content public init(_ build: @autoclosure @escaping () -&gt; Content) { self.build = build } public var body: Content { build() } } Starting loading of data in onAppear causes some time the user has to wait for the heart rate graph. Also not super desireable. Check out the new NavigationStack . Inside that NavigationLink s can present values instead of views. The resulting view can then do work in onAppear or using task .","title":"Why does NavigationLink pre-render the destination when it's being rendered? I am loading Heart Rate data in my destination View and this slows down rendering significantly. What's a better pattern to prevent accidental pre-loading of data?"},{"location":"wwdc22/swiftui-lounge.html#when-using-uihostingcontroller-is-it-better-to-add-it-to-the-view-controller-hierarchy-with-addchildviewcontroller-or-could-we-use-its-rootview-and-simply-add-it-as-a-subview","text":"You want to always add a UIHostingController as a childViewController . // Add the hosting controller as a child view controller self.addChild(hostingController) self.view.addSubview(hostingController.view) hostingController.didMove(toParent: self) without the view controller relationship things that depend on the UIViewController hierarchy won\u2019t work. Such as UIViewControllerRepresentable There are a few other types as well that depend on this relationship in their underlying representations so its best practice to add the parent/child relationship. I am building a design system implementation where I\u2019m providing reusable components that other teams can simply build their features. I usually start with SwiftUI but this view controller relationship makes things very hard at times, especially if the view in question is complex. I believe there is NSHostingView on the Mac, I wish we had the same on iOS. For the time being, given the requirement, is there any recommendation to fulfill this requirement? For example, can we even use window\u2019s rootViewController as a parent or is there a better way? yes the AppKit underlying infrastructure makes NSHostingView work while UIKit is not able to do so due to the hierarchy requirements. It depends on exactly what you are trying to build, but potentially you could make a reusable view controller that your teams can use instead depending on where the complexity is at. Parenting at the rootViewController would have issues with any navigation calls you might try to make in SwiftUI. Ah got it, thanks for clarifying. It is a little bit overhead to ask the teams to add a child view controller just for a button from the design system. :sweat_smile: And it\u2019s extra challenging if they are adding it to a UIView subclass down in the view hierarchy where they don\u2019t have access to any view controller for example.","title":"When using UIHostingController, is it better to add it to the view controller hierarchy (with addChildViewController), or could we use its rootView and simply add it as a subView ?"},{"location":"wwdc22/swiftui-lounge.html#should-layout-work-fine-with-uikit-representable-views","text":"Yes! To get more control over how UIViewControllerRepresentables size themselves in SwiftUI layout you may want to implement their new sizeThatFits method: https://developer.apple.com/documentation/swiftui/uiviewcontrollerrepresentable/sizethatfits(_:uiviewcontroller:context:)-35p75","title":"Should Layout work fine with UIKit-representable views?"},{"location":"wwdc22/swiftui-lounge.html#with-the-new-menubarextra-is-it-possible-to-get-a-callback-when-the-content-window-appears-or-disappears-ive-tried-onappear-but-its-only-called-on-first-appearance-ondisappear-is-never-called","text":"Hi - thanks for the question. This sounds like it is probably a bug. Could you please file a feedback for this?","title":"With the new MenuBarExtra. Is it possible to get a callback when the content window appears or disappears. I\u2019ve tried onAppear but it\u2019s only called on first appearance. OnDisappear is never called."},{"location":"wwdc22/swiftui-lounge.html#while-i-was-working-with-canvas-and-rendering-images-shapes-and-text-i-was-trying-to-sort-out-if-there-was-a-way-to-get-the-offset-for-the-baseline-value-from-a-resolvedtext-i-wanted-to-align-the-text-baseline-to-a-visual-element-that-i-was-drawing-in-canvas-and-the-only-path-i-found-to-getting-sizes-of-discrete-elements-was-to-resolve-them-and-measure-them-something-akin-to-let-captionedtextsamplesize-cgsize-contextresolvetextlabelfontcaptionmeasurein-size-this-hands-back-cgsize-which-mostly-worked-for-what-i-needed-but-id-really-like-to-align-to-that-baseline-and-for-that-i-need-to-determine-an-offset-is-there-any-public-api-to-help-me-do-that-or-a-technique-i-could-use-to-get-said-offset-that-uses-other-frameworks-or-libraries","text":"The GraphicsContext.ResolvedText type also has firstBaseline (and lastBaseline ) methods. So you can do something like: let baseline = context.resolve(myText).firstBaseline(in: size) :face_palm: I totally missed that. Thank you Jacob!!!!","title":"While I was working with Canvas and rendering images, shapes, and text, I was trying to sort out if there was a way to get the offset for the baseline value from a ResolvedText. I wanted to align the text baseline to a visual element that I was drawing in canvas, and the only path I found to getting sizes of discrete elements was to resolve them and measure them - something akin to: let captionedTextSampleSize: CGSize = context.resolve(Text(label).font(.caption)).measure(in: size)  This hands back CGSize, which mostly worked for what I needed, but I'd really like to align to that baseline - and for that I need to determine an offset. Is there any public API to help me do that, or a technique I could use to get said offset that uses other frameworks or libraries?"},{"location":"wwdc22/swiftui-lounge.html#swiftui-mac-specific-question-when-using-contextmenu-on-a-list-how-can-i-find-out-what-was-actually-selected-when-looking-at-finder-the-selected-item-on-which-the-action-should-be-performed-on-is-either-the-selection-1-or-more-or-it-is-the-item-that-has-been-right-clicked-on-directly-how-can-i-replicate-this-behavior-using-swiftui","text":"Check out the new context menu API that accepts a forSelectionType parameter. This passes the value of the selection into the modifier for you to act on. List(selection: $selection) { ... } .contextMenu(forSelectionType: MySelection.self) { selection in // check selection } Be sure to match the type of your lists selection to the type you provide to the context menu modifier. https://developer.apple.com/documentation/swiftui/view/contextaction(forselectiontype:action:) That is awesome, thank you! Are there any plans to back port \"smaller\" API changes like this into macOS Monterey and earlier?","title":"SwiftUI Mac specific question: When using .contextMenu on a List , how can I find out what was actually selected? When looking at Finder, the selected (=item on which the action should be performed on) is either the selection (1 or more) OR it is the item that has been right clicked on directly. How can I replicate this behavior using SwiftUI?"},{"location":"wwdc22/swiftui-lounge.html#im-a-teacher-but-also-write-iosipados-apps-i-watched-curts-cookingrecipe-section-and-am-trying-to-understand-state-restoration-i-thought-curt-said-adding-scenestorage-and-the-task-to-check-if-navigationdetail-was-nil-would-do-it-but-it-doesnt-seem-to-work-do-you-still-need-to-use-the-oncontinueuseractivity-and-then-have-each-view-in-the-stack-hierarchy-have-a-useractivity-to-createdescribe-a-nsuseractivity-for-that-view","text":"Hi, David! I was a teacher before I went full-time software engineer. What I shared in the talk and in the Copy Code associated with it should work. Or check out the sample project here: https://developer.apple.com/documentation/swiftui/bringing_robust_navigation_structure_to_your_swiftui_app You shouldn\u2019t need to use user activity directly at all for this. Hi Curt, yes, we corresponded when you were a teacher and then spent your sabbatical at OmniGroup. I\u2019ve been tempted by some Apple positions, but can\u2019t relocate right now. Hmm. I tried running it in simulator. I select Apple Pie, select Fruit Pie Filling, press Shift-Cmd-H. Press the stop button in Xcode, and then relaunch the app and it starts at the initial Categories screen. Am I not testing it correctly? Ah! Yeah, Xcode kills background storage unless you hold it \u201cjust right\u201d. The pattern when working with Xcode, is to (1) background the app in the simulator or on device, (2) wait 10 seconds for the system to do its backgrounding work, (3) kill the app using the Stop button in Xcode. Then when you run the app again, it should use the saved value. Ah, I waited a few seconds, but probably not 10. Also, I did the next best thing to joining Apple myself - one of my students is now there (Jeremy who did the AV Foundation \u201cCreating a more responsive media app\u201d talk this year. Hmm, I waited more than 10 seconds and still doesn\u2019t work. After navigating, I press Shift-Cmd-H to return to home screen, wait 15 seconds, press stop in Xcode, and then relaunch from Simulator and still starts over with the Category list. If I use the Run button in Xcode, it starts over at the \u201cChoose your navigation experience\u201d? I\u2019m using an iPhone simulator (iPhone 13) not an iPad simulator if that makes ay difference. Also, I chose Stack as the navigation experience Hmm. And this is with the sample code project? Yes, downloaded from here: https://developer.apple.com/documentation/swiftui/bringing_robust_navigation_structure_to_your_swiftui_app I\u2019m afraid I have to go with \u201cit works for me\u201d for the moment, and then double check with the developer build. Sorry for the frustration here. The 2020 SwiftUI State Restoration example that uses NSUserActivity seems to work. https://developer.apple.com/documentation/swiftui/restoring_your_app_s_state_with_swiftui Ok, thanks. I can check back in a week to see if the code on the website has been updated. Interesting, scene storage uses the same mechanism under the hood. That 2020 example also has SceneStorage at one spot: in the ContentView it has @SceneStorage(\u201cContentView.selectedProduct) as an optional string. but still uses .onContinueUserActivity","title":"I\u2019m a teacher but also write iOS/iPadOS apps. I watched Curt\u2019s Cooking/Recipe section and am trying to understand state restoration. I thought Curt said adding SceneStorage and the Task to check if navigationDetail was nil would do it but it doesn\u2019t seem to work. Do you still need to use the onContinueUserActivity and then have each view in the stack hierarchy have a .userActivity to create/describe a NSUserActivity for that view?"},{"location":"wwdc22/swiftui-lounge.html#just-starting-out-with-swiftui-one-of-the-things-that-is-hard-to-get-my-head-around-is-the-following-in-uikit-i-have-a-label-that-pulses-grows-and-shrinks-based-on-certain-events-so-id-typically-have-an-outlet-to-the-label-and-a-method-pulse-in-swiftui-i-accomplished-the-same-by-having-a-binding-labelstate-which-i-change-in-the-containing-view-so-while-uikit-would-have-an-outlet-for-multiple-labels-in-swiftui-id-have-a-binding-state-for-each-label-is-that-typically-the-correct-pattern","text":"That seems like a totally reasonable approach! Generally, when you have some value that you want to change in SwiftUI, you want to make sure that there\u2019s a clear source of truth for that value. That source of truth can either be @State , or a property within an ObservableObject . When you have a fixed number of controls, having a separate piece of state backing each of those controls is completely reasonable. That said, the sources of truth you provide for your state should mirror your UI, so if you dynamically generate your labels from an array of data, you might then want to have an array of data as the source of truth state for your ui components. If you see that your sources of truth for your data mirror the structure of your UI, you\u2019re probably on the right track :slightly_smiling_face: ok thanks. A bit of looking at the problem from a different axis.","title":"Just starting out with SwiftUI. One of the things that is hard to get my head around is the following. In UIKit, I have a label that pulses (grows and shrinks) based on certain events, so I'd typically have an outlet to the label and a method 'pulse'.  In SwiftUI, I accomplished the same by having a binding labelState which I change in the containing View. So while UIKit would have an outlet for multiple labels, in SwiftUI I'd have a binding state for each label. Is that typically the correct pattern?"},{"location":"wwdc22/swiftui-lounge.html#using-buttonstylemycustonstyle-instead-of-mycustombutton-is-more-swiftui-y-but-why-should-i-prefer-one-over-the-other-considering-mycustombutton-uses-mycustonstyle-under-the-hood","text":"Hi Peter, We strongly recommend separating semantic controls and styles as it provides a lot more flexibility. For example, using the button style modifier will apply to the whole hierarchy, so if you decide you want specific parts of your app to use a different button style, you can just apply it to that specific hierarchy without changing the control itself. With this approach you also get multi-platform behaviors for free. My app\u2019s design uses kerning on most of its buttons, but I can\u2019t put that inside of my ButtonStyle because kerning is only available on Text , which ButtonStyleConfiguration.Label is not. I have a feedback open suggesting passing kerning through the environment (FB10020695). In the meantime, do you have any suggestions on how to do this without having to separately use .kerning() on every button label? Moved out here :smile: https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654722720194909","title":"Using .buttonStyle(MyCustonStyle()) instead of MyCustomButton(...) is more SwiftUI-y. But why should I prefer one over the other (considering MyCustomButton uses MyCustonStyle under the hood)?"},{"location":"wwdc22/swiftui-lounge.html#my-question-is-about-some-code-in-the-swiftui-cookbook-for-navigation-session-around-23-minutes-in-curt-uses-the-task-modifier-to-run-an-async-for-loop-over-the-models-objectwillchange-publisher-in-the-loop-he-then-accesses-an-published-property-of-the-model-but-a-published-property-fires-the-objectwillchange-property-before-it-updates-its-wrappedvalue-doesnt-that-mean-that-when-the-code-accesses-navmodeljsondata-it-will-get-the-out-of-date-model-values-or-is-there-some-async-magic-that-guarantees-the-loop-body-will-not-run-until-later-after-the-objectwillchange-publisher-has-finished-publishing-and-the-published-property-has-updated-its-wrappedvalue","text":"Great question! Task schedules its body on the main actor, which is also where the view update happens. So objectWillChange enqueues the body of the for loop, but it\u2019s enqueued behind the update of the data model.","title":"My question is about some code in the SwiftUI Cookbook for Navigation session. Around 23 minutes in, Curt uses the task modifier to run an async for loop over the model's objectWillChange publisher. In the loop, he then accesses an @Published property of the model. But a @Published property fires the objectWillChange property before it updates its wrappedValue. Doesn't that mean that when the code accesses navModel.jsonData, it will get the out-of-date model values? Or is there some async magic that guarantees the loop body will not run until later after the objectWillChange publisher has finished publishing and the @Published property has updated its wrappedValue?"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-advantage-of-using-custom-symbols-rather-than-svg-with-template-rendering-mode-the-ability-to-programmatically-change-the-colors-of-several-layers-any-other-reason","text":"Inlining within Text is a nice benefit, but doesn't seem to work with custom multicolor symbols. In addition to the great color support, using a custom symbol also helps it better fit with text by growing with fonts, respecting bold text, and matching baselines. Christopher, if you haven\u2019t already, can you file a feedback about that? I have, also submitted a question here about it FB10001506 Thank you!","title":"What is the advantage of using custom Symbols rather than SVG with template rendering mode?  The ability to programmatically change the colors of several layers? Any other reason?"},{"location":"wwdc22/swiftui-lounge.html#would-it-be-possible-to-create-an-infinite-paged-collection-view-in-swiftui-similar-to-the-week-view-in-the-calendar-app-where-you-can-swipe-through-weeks-endlessly-thanks","text":"On way to achieve that would be to have onAppear on a the individual views that the List is scrolling over and use that load in more data. What if you want the @State data to not grow as the user continues to page? Removing items from the array would offset the data and causes a jarring affect. I would be happy to submit a feature request for something that would allow me to build this, I\u2019m just not sure what that would look like :dizzy_face:\u200d:dizzy: One way you can solve the problem is by keeping only identifiers in the @State so that the storage doesn\u2019t grow too much, and load on the demand the actual data as view are coming on screen. A feedback would be great. You can simply describe your use case, what you are trying to achieve, and what kind of road block you are finding. That sounds interesting, ill give it a shot. Currently using the .page TabViewStyle but have also been playing with UICollectionViews as well. Ill submit a request if that doesn't work out. Thanks Any tips to avoid a List to scroll to top whenever @State gets updated? Like in the calendar example, I\u2019d probably want to be able to scroll in both directions and have the view appear somewhere in the middle. I think ideally a way to detect when scrolling was finished for that TabView case would be great, because the onChange fires before it\u2019s done scrolling","title":"Would it be possible to create an \u201cinfinite\u201d paged collection view in SwiftUI? Similar to the week view in the calendar app where you can swipe through weeks endlessly. Thanks!"},{"location":"wwdc22/swiftui-lounge.html#swiftui-on-macos-question-is-there-a-way-to-position-the-windows-when-they-are-opened-in-my-app-the-position-of-the-next-window-always-depend-on-the-last-offset-a-bit-to-the-right-and-down-so-they-are-slowly-crawling-down-the-screen-which-is-not-nice","text":"Hi - thanks for the question! The cascading of windows is the default behavior for WindowGroup . We've also added a new Scene modifier - defaultPosition , which allows you to specify an initial position to use absent any previous state for the window. It takes a UnitPoint , so for example, it would be something like defaultPosition(.topTrailing) . Did you have some other behavior in mind? It'd be great to get more info, if so. It would be nice if I could position a new window just beside the already open window, e.g. to the right or left, depending on the available space. The second window is a \u201ccompagnon\u201d to the document, used for editing. Ah, I see. That makes sense given your use case there. Would you mind filing a feedback for an enhancement? So I have currently a DocumentGroup and a WindowGroup . The WindowGroup instance would have a one-to-one relation to an open document. Ok. Please include those details in the feedback, its definitely helpful when we are designing APIs","title":"SwiftUI on macOS question: Is there a way to position the windows when they are opened? In my app, the position of the next window always depend on the last, offset a bit to the right and down. So they are slowly crawling down the screen, which is not nice."},{"location":"wwdc22/swiftui-lounge.html#hi-hope-non-standard-behavior-out-of-swiftui-is-your-cup-of-tea-because-i-have-a-really-interesting-question-this-happens-using-swiftui-in-xcode-134-on-an-imac-running-macos-124-when-using-datepickerstylecompact-and-i-touch-the-date-to-bring-up-the-calendar-the-final-displayed-date-changes-format-depending-on-the-chosen-date-june-19-2022-looks-ok-but-then-i-get-62022-when-i-move-to-the-next-day-it-gets-better-after-going-left-to-right-in-the-month-and-seeing-62222-when-i-start-moving-right-to-left-from-june-24-2022-the-output-for-the-22nd-comes-out-as-june-22-2022-any-ideas-would-be-welcome-apologies-for-the-weird-problem","text":"You're not alone, I noticed this behavior ever since last summer or so. :slightly_smiling_face: REALLY?! Thank you for letting me know I\u2019m not imagining things! Hi Mayra, Thanks for the feedback. This is a known issue :smiley: and we are fixing it! Hooray! Thank you. Will this be in Xcode 14? Oops\u2026 sorry, can\u2019t comment on future work\u2026","title":"Hi! Hope non-standard behavior out of SwiftUI is your cup of tea, because I have a really \u201cinteresting\u201d question. This happens using SwiftUI in Xcode 13.4 on an iMac running macOS 12.4. When using .datePickerStyle(.compact), and I touch the date to bring up the calendar, the final displayed date changes format depending on the chosen date. \u201cJune 19, 2022\u201d looks ok, but then I get \u201c6/20/22\u201d when I move to the next day. It gets better: after going left to right in the month, and seeing \u201c6/22/22\u201d, when I start moving right to left from June 24, 2022, the output for the 22nd comes out as \u201cJune 22, 2022\u201d. Any ideas would be welcome! Apologies for the weird problem!"},{"location":"wwdc22/swiftui-lounge.html#what-are-good-tells-that-i-should-use-implicit-vs-explicit-animations-in-swiftui","text":"I usually default to withAnimation to set up animations based on when a change occurs. That way, whatever UI changes as a result will implicitly be animated. In some cases, View.animation is useful when you have a certain view or view hierarchy you always want to change with an animation, regardless of what triggered it. A good example is the knob of a Switch: it should just always animate when it changes. (And make sure to pass an appropriate value, like .animation(myAnimation, value: mySwitchValue) , so that you only animate when the property you care about changes, and don\u2019t accidentally animate too often.) Thank you for your reply Jacob I\u2019ve been having some difficulties deciding on best approaches especially when creating reusable components with baked in behaviours. This might be stretching it, but this is a gist that lays out my problem: https://gist.github.com/kanevP/a9d54f5c60eddabca000b1a45a59e70b Would love to hear your thoughts on the different animation usages.","title":"What are good tells that I should use implicit vs explicit animations in SwiftUI?"},{"location":"wwdc22/swiftui-lounge.html#i-have-some-custom-multicolor-symbols-that-i-want-to-inline-with-text-eg-textlook-at-my-pretty-symbol-imagemysymbolsymbolrenderingmodemulticolor-but-they-dont-render-as-multi-color-is-this-an-expected-limitation-can-you-think-of-any-work-arounds-that-still-provide-all-of-the-benefits-of-inlining-it-works-great-for-system-provided-multicolor-symbols-fb10001506-has-a-sample-project","text":"it could be a bug, or it could be some problem with your symbol template. For instance the template must be at least version 2 to support multicolor. Does the SF Symbols app work with it and is it able to display the symbol in multicolor? Yeah, they render properly outside of Text And in the symbols app oh, the problem only occurs when the symbol is used inline with text? Correct Even duplicating a system/apple provided multicolor symbol has the same result, so nothing to do with my template if the symbol is working correctly in other contexts, that sounds like it could just be a bug on our side let me look into it further","title":"I have some custom multicolor symbols that I want to inline with Text, e.g.  Text(\u201cLook at my pretty symbol \\(Image(\u201cmySymbol).symbolRenderingMode(.multicolor))\u201d  But they don\u2019t render as multi color, is this an expected limitation? Can you think of any work arounds that still provide all of the benefits of inlining? It works great for system provided multicolor symbols\u2026    FB10001506 has a sample project."},{"location":"wwdc22/swiftui-lounge.html#in-a-wwdc20-session-data-essentials-in-swiftui-theres-a-point-in-the-video-where-the-presenter-touches-on-using-a-single-data-model-that-can-have-multiple-observableobject-projections-to-target-swiftui-updates-to-specific-views-how-can-i-achieve-this","text":"The exact solution really depend on your data model but its can simply be as having a model that contains a bunch of properties where each of these type is an ObservableObject ; or you could have a function that given an identifier return you the ObservableObject that corresponds to that id. What we wanted to get across was that you want and you can scope your ObservableObject to specific screen of functionality of your app. I think I understand, thank you <@U03J7BQQNPJ>!","title":"In a WWDC20 session \"Data Essentials in SwiftUI\" there's a point in the video where the presenter touches on using a single data model that can have multiple ObservableObject projections to target SwiftUI updates to specific views. How can I achieve this?"},{"location":"wwdc22/swiftui-lounge.html#in-our-app-we-can-push-a-view-containing-a-custom-view-above-the-navigation-bar-we-achieved-it-by-using-a-navigationview-inside-a-navigationview-displaying-a-custom-view-with-the-back-button-hidden-is-it-possible-to-have-a-custom-view-above-the-navigation-bar-with-navigationstack-is-it-possible-to-use-a-similar-workaround","text":"Nested navigation stacks don\u2019t compose the same way. An inner navigation stack is effectively a no-op; it doesn\u2019t add anything and any path bound to it should be ignored. Are you trying to achieve something like a very tall navigation bar? I\u2019d love a Feedback with details of your use case! Thanks for the follow-up! You can find the use case, a video and a sample project demonstrating our current approach with a NavigationView inside a NavigationView in this radar: https://feedbackassistant.apple.com/feedback/10112679 I would love to achieve the same effect with NavigationStack","title":"In our app, we can push a view containing a custom view above the navigation bar. We achieved it by using a NavigationView inside a NavigationView (displaying a custom view with the back button hidden).  Is it possible to have a custom view above the navigation bar with NavigationStack? Is it possible to use a similar workaround?"},{"location":"wwdc22/swiftui-lounge.html#forgive-me-if-this-question-has-the-same-answer-as-the-scrolling-chart-question-but-could-we-utilize-a-scrollview-and-a-chart-to-display-a-live-chart-so-new-data-added-is-always-visible-to-the-user-instead-of-having-to-manually-scroll-it-into-view","text":"Yes, this should work, but will extra work. You may have to observe the data and manually keep the scroll view positioned at the very end It sounds like you want to use to be able to scroll backwards through an otherwise-live feed, but if that isn't the case, the data plotted in your chart could always be just the last 7 elements of an array or something conceptually similar Yeah, basically keep a horizontal scrollview fixed to the end and allow the user to scroll back to the left (start) of the view. Is there a modifier or additional context to manually position the location of the scrollbar in a scrollview? Have you seen https://developer.apple.com/documentation/swiftui/scrollviewreader|ScrollViewReader s? Ah I have not. This is perfect thanks! <@U03HL00QL68> When I use a ScrollViewReader and the parent state changes the scroll position changes. I have filed a feedback FB10037533 Thank you for the sample project. Definitely a bug! <@U03HL00QL68> thanks a ton!! for confirming that, hope it gets fixed","title":"Forgive me if this question has the same answer as the scrolling chart question, but could we utilize a scrollview and a chart to display a live chart? So new data added is always visible to the user, instead of having to manually scroll it into view."},{"location":"wwdc22/swiftui-lounge.html#question-regarding-the-swiftui-instrument-is-there-a-value-of-avg-duration-that-no-views-body-should-go-above-when-rendering-for-avoiding-performance-issueshitches","text":"View\u2019s body properties should be cheap and avoid any potentially expensive work. If there are things you need in your views body, its best to pre-compute that into a piece of state that you can pass your views. Concretely, each frame has a hard deadline of generally 1/60s or 1/120s (depending on the refresh rate of the display) to render its content. Thats 8ms in the fastest case. Thats for the entire screen to render its content though, not just your single view . So if any one view is taking more than a few ms to finish calling body, you\u2019ll run into issues. If you\u2019re using Swift concurrency to move work off the main thread, consider watching this session: https://developer.apple.com/videos/play/wwdc2022/110350/ This is all mostly related to scrolling performance. Other kinds of things like app launch or navigation pushes will typically take longer than multiple frames to render, but those situations have different expectations around frame deadlines.","title":"Question regarding the SwiftUI Instrument: Is there a value of avg. duration that no View's body should go above when rendering for avoiding performance issues/hitches ?"},{"location":"wwdc22/swiftui-lounge.html#we-can-now-use-foregroundcolorwhiteshadow-on-sfsymbols-will-this-work-with-custom-pngssvgs-as-well","text":"foregroundStyle s apply only to shapes, image masks, and text. If the Image is configured as a template, then foreground styles should be applied. If the image is non-template, they won't Yes if you set render as to template for your image asset Excellent, thank you! :pray: Thanks Mirko! and thanks for the great question Jan","title":"We can now use .foregroundColor(.white.shadow(\u2026)) on SFSymbols. Will this work with custom PNGs/SVGs as well?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-have-a-customizable-toolbar-with-swiftui-on-ipad","text":"Hey Johan, Yes it is possible :tada: Checkout the \"What's new in SwiftUI\" talk on the Advanced Controls section to have a peak. And make sure to tune in for the \"SwiftUI on iPad: Add toolbars, and documents, and more\" talk tomorrow to dive deeper. :smiley: Note that the same APIs will work on macOS too.","title":"Is it possible to have a customizable toolbar with SwiftUI on iPad?"},{"location":"wwdc22/swiftui-lounge.html#swiftui-mac-specific-question-when-using-navigationview-is-there-a-way-to-start-the-app-with-a-the-sidebar-already-collapsed-or-do-it-programmatically-i-saw-this-in-on-of-the-videos-bonus-question-can-this-also-be-done-on-macos-monterey","text":"Hi - thanks for your question. The new NavigationSplitView API provides an initializer which takes a binding indicating the visibility of the columns. This can be used on macOS to indicate that the sidebar should be initially collapsed. There is an example in the documentation here: https://developer.apple.com/documentation/swiftui/navigationsplitview/ Thank you!","title":"SwiftUI Mac specific question: When using NavigationView, is there a way to start the app with a the sidebar already collapsed (or do it programmatically)? I saw this in on of the videos. Bonus question: Can this also be done on macOS Monterey?"},{"location":"wwdc22/swiftui-lounge.html#hi-there-is-there-any-way-to-get-my-offscreen-nested-stateobjects-to-not-be-deallocated-when-scrolled-offscreen-within-a-lazystack-the-only-solution-ive-found-is-to-hoist-them-to-the-top-level-of-the-lazystack-eg-within-the-content-of-a-foreach-but-that-feels-kinda-clunky-to-have-to-do-in-every-scenariowondering-if-theres-another-option","text":"I think \"hoisting\" your state into an ancestor is your best bet here As an aside, StateObject is more commonly used with model objects and you generally don't want to tie your model object's lifecycle to the view In the case of a model object it might make sense for an ancestor model object to be maintaining references to these children model objects <@U03HELTEP9T> What guarantees (if any) do we have about the longevity of a StateObject property on a view? Should I try to move view-owned objects to the root App struct and inject them via EnvironmentObject? I've already run into issues when changing device orientation, with my view hierarchy being torn down and recreated, losing any ephemeral state Thanks! Yes in this scenario it's effectively a State , just incidentally modeled as a StateObject for other reasons \u2014 however we were finding that a State was getting reset as well back to the initial value as well in that same scenario. I assume that's expected as well? <@U03JX6SFGMS> ya State would be expected to behave the same as StateObject here since the motivation is to limit memory growth by avoiding preserving state for an unbounded number of (offscreen) views","title":"Hi there, is there any way to get my offscreen nested StateObjects to not be deallocated when scrolled offscreen within a LazyStack? The only solution I've found is to \"hoist\" them to the top level of the LazyStack, e.g. within the Content of a ForEach, but that feels kinda clunky to have to do in every scenario\u2014wondering if there's another option."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-on-watchos-for-swiftui-to-present-a-modal-where-the-stays-bar-is-hidden-for-example-how-the-keyboard-menu-in-the-phone-app-hides-the-time-the-entered-phone-number-is-displayed-in-the-status-bar-area-instead-thank-you","text":"<@U03JJQ3BMB7> if you present a modal that has a confirmation action, the time will be hidden for you automatically, but otherwise, the time will always be there. Ok, thank you. Is it possible to do this with watch kit or is the time always shown? I'm just wondering if it's possible for me to recreate the keyboard menu in the phone app. Thank you! I don't think so. Can you provide a screenshot of the menu on the phone that you are trying to mimic ? I am trying to make a calculator like app, and liked this layout from the phone app. I liked that the input pad keys were large and the entered numbers were in the status bar area if you wanted to do that, you can do it in a modal sheet on watchOS, and you can provide a custom confirmation action as a button in the top right (like the delete button is), and you could provide a navigationTitle for content in the navigation bar, and then provide your own cancellationAction. but the time will only hide if you actually put content in the navigation bar in that spot (via confirmation action placement toolbar) Great thank you! Would you mind pointing me towards the documentation on custom confirmation actions, please? Thank you! https://developer.apple.com/documentation/swiftui/toolbaritem/init(id:placement:showsbydefault:content:) and then tap on the placement link Should describe those That\u2019s awesome! Thank you for your help! :grinning: no problem! feel free to check in back here and @ me to say whether it works or not <@U03HHHXDL03> - Thank you for your help the other day! I got it working! :grinning: That is so cool! I'm happy to hear!","title":"Is there a way on watchOS for SwiftUI to present a modal where the stays bar is hidden? For example, how the keyboard menu in the phone app hides the time. The entered phone number is displayed in the status bar area instead. Thank you!"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-have-an-onchangeofperform-modifier-but-only-call-the-closure-when-the-state-was-changed-by-the-user-and-not-programmatically","text":"onChange executes its closure whenever the value change (that\u2019s why we require the value to be Equatable ). So onChange only know about comparing the value that your are providing. If you want to make that distinction you probably should model that difference in your state. We have the same use case and we\u2019re hooking into the action closure of a button separately for example. I\u2019d love to have a way similar to .valueChanged events from UIKit though. :sweat_smile:","title":"Is there a way to have an onChange(of:perform:) Modifier but only call the closure, when the state was changed by the user and not programmatically?"},{"location":"wwdc22/swiftui-lounge.html#are-there-any-xcode-build-settings-that-might-cause-swiftui-previews-in-a-largeexisting-project-to-render-text-incorrectly-ive-noticed-that-textdoublevalue-floatvalue-will-incorrectly-render-doubles-as-nan-in-previewssimulators-doubles-renders-correctly-on-device-and-floats-dont-have-the-issue","text":"Thank you for reporting this issue. Did you file a feedback with this information? If not we would really appreciate if you could do that. I haven't been able to reproduce this in a small, isolated project. The same code works without issue in a Swift Playground or a new Xcode project. What's the best way to file a feedback in this case without uploading my full repository and all of its dependencies? You can start with just the information you posted here. <@U03J7BQQNPJ> thanks, please see FB10112975 It's an issue on both Xcode 13 and Xcode 14 Thank you :man-bowing::skin-tone-3: <@U03J7BQQNPJ> LOL right after I submitted the FB, I found the source of the issue. It was the Xcode scheme's \"Localization debugging: Show non-localized strings\" setting that was apparently clobbering the SwiftUI strings. Unexpected nevertheless\u2014I'll update the FB now.","title":"Are there any Xcode build settings that might cause SwiftUI previews in a large/existing project to render text incorrectly?   I've noticed that Text(\"\\(doubleValue) \\(floatValue)\") will incorrectly render doubles as -NaN in previews/simulators. Doubles renders correctly on device, and floats don't have the issue."},{"location":"wwdc22/swiftui-lounge.html#there-has-been-a-lot-of-controversy-regarding-the-observedobjectwrappedvalue-initializer-is-it-safe-and-encouraged-to-use-it-or-do-we-have-an-alternative-for-it-this-year","text":"This initializer is fine to use! In fact, the ObservedObject(wrappedValue:) initializer is invoked every time you construct an ObservedObject , even if you don\u2019t explicitly write it yourself. When you write: @ObservedObject var myObservedObject = myModel , The Swift compiler converts that standard property wrapper syntax to a call that looks something like: var _myObservedObject = ObservedObject(wrappedValue: myModel) . The controversy I think you\u2019re referring to is using that initializer explicitly in the initializer for one of your views. Typically, we see people doing this to allow observed objects to be instantiated with specific information. That is also something which we think is fine to do. We recognize that the ergonomics of that case is not ideal (since you have to access the de-sugared property wrapped (in the example I gave, _myObservedObject ), but it\u2019s not at all harmful. What about the State initializer? I think the confusion comes from this comment: /// Don't call this initializer directly. Instead, declare a property /// with the ``State`` attribute, and provide an initial value: I think ObservedObject also has a the initialiser init(initialValue:) is that preferred? The documentation doesn't mention any warning on this initialiser The state initializer worries me a bit more. Not because it\u2019s dangerous \u2014 it\u2019s totally fine to use it yourself (as I mentioned, the normal syntax is just sugar for the fully spelled out case) \u2014 but because I can\u2019t think of as many cases where you need that syntax for @State that aren\u2019t dangerous. Remember that @State is initialized once per lifetime of the whole view, not once per time a view\u2019s initializer is called The views representation will be recreated on demand. That means that if you\u2019re re-initializing the state every time the views init is called, you\u2019re going to be clobbering your own state. So that\u2019s fine to do, but make sure that you\u2019re only using it to set the initial value of a state, and that you\u2019re not resetting your state depending on some initializer value. <@U03HKVDCL7N> I believe same applies for @Binding and co. too? Yup! But for @Binding is it still harmful? Because source of truth is already on a parent view or somewhere up there? Yeah, binding is also one of the cases that would sound alarm bells for me :warning: Yeah fair, I\u2019d still be cautious instead of making assumptions. Very nice tip though, thank you! :heart: As a final note: regarding the \u201cdon\u2019t call this initializer directly,\u201d that\u2019s mostly because as I mentioned, the cases where you \u201cneed\u201d the underlying initializer (for @State and @Binding especially) are few and far between. Most of the time, you\u2019d want to be using the standard property wrapper syntax, so we want to make sure people reading the docs look there first. Is the concern because State variable's lifetime is outside the view's lifetime and initialising the State variable in the view's initialiser would keep creating a new State every time the view get's created? A recent use case where i would have used it was to pass in a value to a structs initializer, where that struct is also @State Interesting. The place I was using it was that I had a @FetchRequest that was based on a relationship to another object, so I was passing the object into init and then creating the fetch request based on it but was keeping that object as State. Maybe I don't really need it to be State. But maybe I shouldn't be creating the FetchRequest wrapper that way either? <@U03J21ZD15Y> I noticed in this case that the FetchRequest is init everytime the View is init. It does not behave like a State or StateObject where the property is tied to the lifetime of the View. So if I use a predicate or sortDescriptors updated by my subview, any update in the super view will reset the predicate/sortDescriptor. So these should be higher in the hierarchy, or based on a State. <@U03J20KFJG3> <@U03J21ZD15Y> same thing here FB9956812. The FetchRequest struct mistakenly contains a var object that it inits every time which breaks SwiftUI's View identity matching so body is called every time. Really hope it is fixed in this beta cycle. @Tom you can init a FetchRequest with params like this: private var fetchRequest: FetchRequest&lt;Appointment&gt; private var appointments: FetchedResults&lt;Appointment&gt; { fetchRequest.wrappedValue } `init(date: Date, userID: String) {` `fetchRequest = FetchRequest(fetchRequest: Appointment.sortedFetchRequest(userId: userId, date: date))` `}` That wrappedValue trick is taken from the SectionedFetchRequest header comments.","title":"There has been a lot of controversy regarding the ObservedObject(wrappedValue: ) initializer. Is it safe (and encouraged) to use it, or do we have an alternative for it this year?"},{"location":"wwdc22/swiftui-lounge.html#is-using-environmentobject-for-dependency-injection-of-entities-not-directly-related-to-the-view-state-like-a-service-to-fetch-values-from-network-or-a-telemetry-logger-considered-bad-practice-thinking-on-a-mvvm-architecture-context","text":"I wouldn't consider it a bad practice Be mindful when using plain @Environment , that if you're passing a struct, any change in value will invalidate any views reading that value from the environment But if you're using @EnvironmentObject with a class that's effectively immutable that shouldn't be a problem In an MVVM context, as @Environment and @EnvironmentObject values are passed into the view, how can we get them out of the view to view model or any other helper type? I wish I could use @Environment and @EnvironmentObject directly in the view model that is associated with the view for example <@U03HELTEP9T> Is there any documentation on Self._printChanges() There was one instance where it said @96 changed I was not sure what it referred to. Does it refer to @FetchRequest ? <@U03J1V9U7U3> it's best to learn the View struct, it is actually a view model so you don't need another view model object in SwiftUI. If you do use them you'll just get the bugs typical of objects that SwiftUI has been designed to eliminate by its use of value type structs. For simple use cases, yes, I agree. I do exactly this in my own app, but it doesn\u2019t scale in a larger team working on a complex project. Especially as we\u2019re adopting SwiftUI incrementally.","title":"Is using @EnvironmentObject for dependency injection of entities not directly related to the view state, like a Service (to fetch values from network) or a telemetry logger, considered bad practice? Thinking on a MVVM architecture context."},{"location":"wwdc22/swiftui-lounge.html#i-have-a-uikit-app-on-ios-i-am-at-the-point-of-needing-to-decidedo-i-ship-it-for-macos-with-catalyst-or-given-im-doing-a-major-redesign-anyway-do-i-migrate-it-to-be-a-swiftui-app-i-am-wonderingare-any-of-the-featurescapabilities-that-i-would-have-in-a-catalyst-app-to-make-the-app-more-mac-like-that-are-not-available-on-macos-using-a-native-swiftui-app-eg-the-bring-your-ios-app-to-the-mac-session-showed-the-ability-in-catalyst-to-enabledisable-the-traffic-light-buttons-when-they-were-not-appropriate-for-that-window-is-this-kind-of-control-also-available-in-swiftui-on-macos","text":"Hi - thanks for the question! With regards to your last part around the window controls (traffic lights) - SwiftUI will enable/disable these depending on the content of the scene. So, for example, a content view with a fixed size, will have the zoom and fullscreen button disabled. I'd also like to point out that on macOS Ventura, SwiftUI App lifecycle windows will be fully flexible by default (they can be resized to the maximum allowed by the screen). This can be opted out with a new Scene modifier - windowResizability . ie: WindowGroup { } .windowResizability(.contentSize) Enabling/disabling of full screen behavior is also related to the maximum size of the window's contents and how that relates to the screen size of the device If there are other aspects that you would like to customize here, a feedback would be appreciated as well.","title":"I have a UIKit app on iOS. I am at the point of needing to decide\u2014do I ship it for macOS with Catalyst, or given I\u2019m doing a major redesign anyway, do I migrate it to be a SwiftUI app.  I am wondering\u2014are any of the features/capabilities that I would have in a Catalyst app to make the app more \"Mac like\" that are not available on macOS using a native SwiftUI app? e.g., the \"Bring your iOS app to the Mac\" session showed the ability in Catalyst to enable/disable the traffic light buttons when they were not appropriate for that window. Is this kind of control also available in SwiftUI on macOS?"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-best-way-to-debug-issues-with-the-live-preview-when-an-error-isnt-thrown-the-build-doesnt-actually-fail-and-the-diagnostic-button-isnt-available-in-some-instances-when-i-resume-the-preview-a-build-for-previews-is-launched-and-then-stops-shortly-after-pausing-the-preview-again","text":"In the new Xcode the diagnostic button is now always available in Xcode's menu Editor > Canvas > Diagnostics, so you can access them even when the canvas doesn't show an error banner. Hard to say without the diagnostics (and potentially your project) what the root cause is, but from the symptoms it sounds like maybe your project has a script phase that modifies some of the source as part of the build? If you file a feedback with the diagnostic we might be able to help narrow down where things are going wrong.","title":"What is the best way to debug issues with the Live Preview when an error isn't thrown / the build doesn't actually fail and the \"Diagnostic\" button isn't available? In some instances when I resume the Preview, a \"Build for Previews\" is launched and then stops shortly after, pausing the preview again"},{"location":"wwdc22/swiftui-lounge.html#i-noticed-a-month-ago-on-one-of-my-views-that-any-textfield-i-add-to-my-form-within-a-vstack-extends-the-space-below-the-form-i-only-noticed-as-the-view-contains-the-form-in-question-and-then-an-hstack-with-buttons-the-space-between-the-form-and-hstack-increases-with-each-textfield-i-add-to-the-form-i-created-a-stackoverflow-question-linked-at-end-but-couldnt-figure-it-out-i-got-around-this-by-surrounding-it-in-a-scrollview-realized-my-other-forms-were-fine-and-thats-why-so-i-continued-on-figured-id-ask-here-though-httpsstackoverflowcomquestions72293879the-textfields-on-my-swiftui-macos-project-are-making-my-window-height-too-tallhttpsstackoverflowcomquestions72293879the-textfields-on-my-swiftui-macos-project-are-making-my-window-height-too-tall","text":"hi, this does look unexpected. could you file a feedback with a sample project that reproduces it attached? please do paste the number here so that we can make sure it gets routed to the right place Sure. Where do I go to file the feedback again? you can use the feedback assistant on your Mac or iOS device if they are enrolled in the apple developer or public betas, as well as http://feedbackassistant.apple.com|feedbackassistant.apple.com a Mac enrolled in the beta programs should have Feedback Assistant visible in Launchpad; an iOS device should have an icon for it on the Home Screen. On a Mac, you can also open the assistant directly from /System/Library/CoreServices/Applications . Just created it (through XCode). FB10112924 It's uploading the attachments right now","title":"I noticed a month ago on one of my Views that any TextField I add to my Form (within a VStack) extends the space below the Form. I only noticed as the view contains the Form in question and then an HStack with buttons. The space between the Form and HStack increases with each TextField I add to the Form. I created a stackoverflow question (linked at end) but couldn't figure it out. I got around this by surrounding it in a ScrollView (realized my other forms were fine and that's why) so I continued on. Figured I'd ask here though. :) https://stackoverflow.com/questions/72293879/the-textfields-on-my-swiftui-macos-project-are-making-my-window-height-too-tall|https://stackoverflow.com/questions/72293879/the-textfields-on-my-swiftui-macos-project-are-making-my-window-height-too-tall"},{"location":"wwdc22/swiftui-lounge.html#on-watchos-is-it-possible-to-call-the-keyboard-from-swiftui-straight-from-a-complication-i-think-we-still-have-to-do-a-wkinterfacecontrol-or-go-through-a-textfield","text":"<@U03J7JKA23F> no, unfortunately not with TextField or TextFieldLink. Could you please file a feedback requeset?","title":"on watchOS is it possible to call the keyboard from swiftUI straight from a complication? i think we still have to do a wkinterfacecontrol or go through a textfield."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-recommended-best-practice-for-managing-stateobjectsobservableobjects-in-a-primary-detail-type-setup-ive-been-commonly-using-stateobjectwrappedvalue-to-inject-a-stateobject-into-a-detail-view-but-of-course-because-wrappedvalue-is-an-autoclosure-it-doesnt-get-updates-to-the-data-value-that-may-occur-from-the-primary-view-passing-the-entire-primary-stateobject-to-the-detail-view-can-expose-more-data-to-the-detail-view-than-necessary-and-creating-and-managing-an-observableobject-owned-by-the-primary-stateobject-can-be-boilerplatey-and-messy-to-manage-any-approaches-ive-overlooked-or-tips-on-how-to-approach-those-tradeoffs","text":"My intuition is that for a primary-detail setup, you would want the detail using @ObservedObject (or even @Binding ) instead of @StateObject . @State and @StateObject are defining a source of truth and lifetime for the state/model. You want the lifetime of the state/model to live on beyond changing the selection in the primary. @ObservedObject will listen to changed in the model object and update the view without establishing a new source of truth. > I agree with <@U03HELTEP9T> and would like to add that I read somewhere that using (wrappedValue:) of State/StateObject in this way is discouraged. I will try to find the source. > For your use case, I would use @StateObject in root, then @ObservedObject in detail, or @Binding in detail if you only want to expose something specific Is the recommendation to inject the entire root StateObject into the detail as an ObservableObject, or for the root StateObject to create and maintain an ObservableObject which has the subset of relevant data, and inject that into the detail view? the latter Content view <- State Object, holds the truth Detail view <- ObservedObject/Binding, refers to the truth Thanks. That was my instinct too... I just don't want to manage it. :stuck_out_tongue: But it's probably a good choice. If you wanna take the pro route :sunglasses: you will rely on protocols, and inject instances as needed using a custom property wrapper. The instances will be stored using a custom class (or save yourself trouble and use something like Resolver)","title":"Is there a recommended best practice for managing StateObjects/ObservableObjects in a primary-detail type setup? I've been commonly using StateObject(wrappedValue:) to inject a stateobject into a detail view, but of course because wrappedValue is an autoclosure it doesn't get updates to the data value that may occur from the primary view. Passing the entire primary StateObject to the detail view can expose more data to the detail view than necessary. And creating and managing an ObservableObject owned by the primary StateObject can be boilerplatey and messy to manage. Any approaches I've overlooked, or tips on how to approach those tradeoffs?"},{"location":"wwdc22/swiftui-lounge.html#with-navigationpath-ive-already-felt-the-need-to-pass-it-down-to-child-views-as-a-binding-or-environmentobject-so-that-they-can-influence-the-stack-programatically-is-that-a-reasonable-approach-or-am-i-overlooking-something","text":"Passing the binding or passing a navigation model as an environment object are both reasonable approaches with the API we have available today. I hope we can make this more ergonomic. I\u2019d love a Feedback with your use case! I personally like the navigation model approach, since I can easily do other work when the navigation state changes without putting that code in my Views. But either approach works fine. So, a ViewModel (probably observableobject) that dictates navigation? Yeah. That works for the way I think about navigation.","title":"With NavigationPath, I've already felt the need to pass it down to child views as a @Binding or @EnvironmentObject so that they can influence the stack programatically. Is that a reasonable approach or am I overlooking something?"},{"location":"wwdc22/swiftui-lounge.html#could-a-windowgroup-value-that-has-no-stored-dependencies-be-recreated-along-the-lifecycle-of-an-app","text":"Hi - would you be able to provide some more detail here? I'd like to make sure I understand your question fully. If some value is stored not in a @State in the WindowGroup, for example, let uuid = UUID(). Will it stay the same in this specific case for the lifetime of the app. This is of course a bad practice, but I've seen it quite some time on the internet, and I would like to know if in this specific case, the value could be recreated or not as the app runs. This is more a theoretical question to try to get a better grasp around this specific value lifecycle. In other words, are WindowGroup susceptible of being recreated even if they have no stored dependencies, no @State, etc. I see. I think for this, I would likely recommend storing this in an app-level model, and providing that value to the WindowGroup But are there (private) system events that can recreate the WindowGroup? Mostly a theoretical question. I would always store something safely elsewhere, but I don't know if in this specific case, this is overzealous. I don't think it's being overzealous, if that is what you are concerned about. Thanks!","title":"Could a WindowGroup value that has no stored dependencies be recreated along the lifecycle of an app?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-use-state-et-al-from-within-a-custom-layout-or-is-there-a-different-way-we-should-parameterize-our-layouts-i-noticed-that-the-protocol-doesnt-inherit-from-view-so-i-wasnt-sure-if-using-the-property-wrappers-would-work","text":"You can add input parameters to your layout, and you can attach values to particular views using the LayoutValueKey protocol. To store data between calls into your layout, you can use the cache, but that\u2019s only to avoid repeated calculations, and you should be prepared for the cache to go away at any time. Okay, so anything stateful should live in the parent view (and be passed as init args) or in a child view (and be read through a LayoutValueKey ) ? Yes Makes sense, thanks!","title":"Is it possible to use @State, et. al from within a custom Layout? Or is there a different way we should parameterize our layouts? I noticed that the protocol doesn't inherit from View, so I wasn't sure if using the property wrappers would work."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-newer-way-to-debug-a-view-while-its-in-preview","text":"Best way to debug a preview now would be to use Xcode's menu item Debug > Attach to Process (or the Process by Pid or Name) and select/use the name of your app. Then update a string literal in that view and it should cause the View's body to get called and hit any breakpoints you've got in that code. Hope that helps! OMG! That is amazing. Awesome thanks!","title":"Is there a newer way to debug a view while it's in preview?"},{"location":"wwdc22/swiftui-lounge.html#i-would-like-to-migrate-my-very-complex-imessage-app-extension-to-swiftui-how-do-i-deal-with-the-msmessagesappviewcontroller-do-i-need-to-use-some-mix-of-uihostingcontroller-and-uiviewrepresentable","text":"You should use the UIHostingController as a child of the MSMessagesAppViewController","title":"I would like to migrate my (very complex) iMessage app extension to SwiftUI. How do I deal with the MSMessagesAppViewController? Do I need to use some mix of UIHostingController and UIViewRepresentable?"},{"location":"wwdc22/swiftui-lounge.html#when-implementing-pathin-cgrect-of-a-shape-are-dynamic-property-wrappers-assumed-offer-the-correct-values-or-is-this-only-valid-for-bodies","text":"Shapes don\u2019t have the same data flow primitives as views do, so things like state, environment, etc. won\u2019t work, but don\u2019t despair! You can still get the results that I think you might want. Since you\u2019ll eventually be showing your shapes somewhere in a SwiftUI View , you can hoist your data flow up to the SwiftUI View level, then pass down that information about state, environment, etc. down to the shapes.","title":"When implementing path(in: CGRect) of a Shape, are dynamic Property Wrappers  assumed offer the correct values, or is this only valid for bodies?"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-best-way-to-make-a-textfield-wrap-i-noticed-some-new-api-regarding-linelimits-and-axes-and-wondering-if-those-provide-some-functionality-in-this-area","text":"TextFields can be initialized with a new axis parameter. If you provide a .vertical axis, then platforms like iOS and macOS will wrap the text of the text field in a scrollable container instead of letting the text expand horizontally. See one of its inits here: https://developer.apple.com/documentation/swiftui/textfield/init(_:text:axis:)-7n1bm Awesome, thanks! These text fields will also respect the line limit modifier. So specifying a lineLimit(3) will let the text field grow vertically up to 3 lines and then it will stop growing and scroll instead.","title":"What is the best way to make a TextField wrap? I noticed some new API regarding lineLimits and axes, and wondering if those provide some functionality in this area."},{"location":"wwdc22/swiftui-lounge.html#as-we-get-more-and-more-familiar-with-swiftui-we-use-geometryreader-less-and-less-but-what-are-some-tells-that-theres-no-way-around-it","text":"If you want a visual effect like corner radius, opacity, or color to depend on your geometry, GeometryReader is the way to bridge between these two worlds I particularly use GeometryReader a ton to contextualize Gestures (e.g. to see whether the user has dragged all the way to the left or right of a control). Is there a different way? Layout gives you a superior tool for encapsulating layout logic for re-use. And while using GeometryReader you can position children, the GeometryReader can't reflect back any custom sizing behavior. This is one of the reasons it can become unwieldy when used in complex layouts\u2014because the GeometryReader becomes fully flexible. Ben, would the new SpatialTapGesture help? https://developer.apple.com/documentation/swiftui/spatialtapgesture What is the difference between SpatialTapGesture and TapGesture ? :thinking_face: <@U03HELSV45B> Is it possible to understand the extent of the view from SpatialTapGesture ? My main use case is determining where the touch is within a view (ie knowing that a touch is 10% left and 30% from the top) \u2026I'm also squinting at SpatialTapGesture and seeing whether I could hack \"true\" pinch to zoom (with location) using this new API :thinking_face: > What is the difference between SpatialTapGesture and TapGesture ? For SpatialTapGesture the event in onEnded now provides the tap location. Ah, SpatialTapGesture only seems to have onEnded ... FB9489089 and FB9488452 remain!","title":"As we get more and more familiar with SwiftUI we use GeometryReader less and less, but what are some tells that there's no way around it?"},{"location":"wwdc22/swiftui-lounge.html#would-you-recommend-to-use-stateobservedobject-for-any-cases-where-where-we-dont-expect-the-object-to-mutate","text":"If the values you\u2019re represented are never going to change, there\u2019s no need to use @State or @ObservedObject . I highly recommend just making and using a standard struct . <@U03HKVDCL7N> what about reference types in views? Would let suffice? Should I ever be concerned that as the view gets thrown away and recreated my UISelectionFeedbackGenerator instance (for example) will be continuously re-created as well? Let would not suffice for reference types! A let binding for a reference type just says: \u201cThis will always point to the same instance of the object\u201d. It doesn\u2019t guarantee that that instance is immutable. If you can guarantee that your reference type is completely immutable, then you should be fine, but that can be a hard thing to ensure, so be cautious (and I would highly recommend using structs where you can) Petar, I wouldn\u2019t ever be concerned that your view gets thrown away and recreated. That\u2019s by design. SwiftUI views aren\u2019t linked to their actual UI representation, but rather are lightweight descriptions of views that can be recomputed quickly and easily. You should expect your views to be thrown away and recreated very frequently. Thank you <@U03HKVDCL7N>, and just to hammer to point: Don\u2019t worry not just that the views are getting recreated, but also what instances get recreated with them.","title":"Would you recommend to use @State/@ObservedObject for any cases where where we don't expect the object to mutate?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-enable-interaction-with-the-new-device-variants-in-previews","text":"No, unfortunately not. The preview is only interactive when viewing it in the Live mode, and all the other modes are static previews Is there a way to make the Live mode preview landscape? yes, use the Device Settings button to change the orientation to be landscape","title":"Is there a way to enable interaction with the new device variants in previews?"},{"location":"wwdc22/swiftui-lounge.html#with-the-new-any-keyword-will-we-still-need-anyview-to-type-erase-views-or-can-we-declare-a-collection-with-any-view-now-will-this-also-have-the-same-performance-implications-of-using-type-erased-views","text":"Ideally you should not use AnyView and over the year ViewBuilder has improved enough that you should be able to eliminate most, if not all, its usage. That said, yes you will still need to use AnyView because you need to actually instantiate that type, and not just use View as a type. any View is just defining an (existential container) box. is it possible to use view builders as function params or property types without Generics? we are using AnyView to present content. there are different types of content each represented by an enum with associated value. This forced us to use AnyView to have concrete types. What would be your suggestion to move away from AnyView in this use case? Thank you :relaxed: If you have finite number of views you are displaying described by an enum could you switch over that enum in body of a view? ViewBuilder does support switch statements I have similar use case. Sometimes I would like to not couple the view which other views it present, but instead use other Flow/Factory logic that shows whats needed depending on the business logic. It forces me to use AnyView . E.g. struct MainView: View { @ObservedObject var viewModel: MainViewModel var channelsViewFactory: (Int?) -&gt; AnyView var body: some View { Text(\"ABC\") channelsViewFactory(viewModel.selectedId) } } ... extension AppDependencies: MainViewFactory { func makeMainView() -&gt; MainView { return MainView( channelsViewFactory: { id in if loggedIn { self.makeChannelsView(for: id) } else { self.makeSignInView() } } } } It really depends on your use case. But most of the time, if you're in the situation where you think you need [AnyView] or [any View] , what you should likely do is invert the view dependency flow and have [AnyDataModel or [any DataModel] instead, then create your views based on the type of data provided at runtime. In \u0141ukasz case, you could introduce generics into MainView (e.g., MainView&lt;ChannelsView: View&gt;: View ) and then create a container view above MainView to encapsulate the generic constraint(s).","title":"with the new any keyword, will we still need AnyView to type erase views, or can we declare a collection with any View now? will this also have the same performance implications of using type erased views?"},{"location":"wwdc22/swiftui-lounge.html#we-can-now-use-preview-to-instantly-see-different-dynamic-type-sizes-and-schemes-all-at-once-when-using-swiftui-in-xcode-can-we-also-see-different-devices-at-the-same-time-eg-all-iphones-that-support-ios16","text":"We do not currently have variations on devices/device types available. We are actively monitoring for what set of variations the community would find the most useful, so please do file enhancement feedback reports with examples of where and how you think a new set of variations would be useful. Thank you <@U03H32HKZ0F>! Previewing the app live on all iPhones (or Apple Watches, or whatever) at once would be absolutely smashing! :zap:","title":"We can now use preview to instantly see different dynamic type sizes and schemes all at once when using SwiftUI in XCode. Can we also see different devices at the same time? e.g. all iPhones that support iOS16"},{"location":"wwdc22/swiftui-lounge.html#string-input-on-watchos-is-quite-limiting-because-it-happens-in-a-non-customizable-modal-are-there-any-tips-or-guides-on-how-to-achieve-autofill-of-a-textfield-on-watchos","text":"<@U03JMMN8659> can you specify the text content type on your text fields? If you specify .username, .email, or .password, on the appropriate textfields, the system will provide the little keychain icons for autofill appropriately. please let me know if this works. In my case it is custom strings. In my app, the user can see a List of mushroom species and search for a specific species using TextField. On iOS, I implemented another List that overlays the screen and shows results satisfying the user-typed value. The user can tap one of the results to complete the string. Not sure how to proceed in watchOS though that sounds like a great use case. could you file a feedback with this info so we can pass it along? yes, that^ And until then .. . Can you create a modal, which has a list, and at the top of the list, you include a TextField, and below the textfield you include list items that are your autocompletions, and if they tap one of those, they get the short cut values, and if they tap the textfield they can enter the raw text? <@U03HW7PE3SM> will do! <@U03HHHXDL03> not quite, since each mushroom species is unique, so the selection depends on the first several characters of a string. For example, typing \"ama\" will bring up mushrooms from the \"Amanita\" genus. Also, on watchOS, once the user taps on the textfield, the entire screen is covered by the modal, so there is no space for dynamic suggestions to come up while the user is typing. Demo of how this works on iOS I wonder if the new quicktype bar in the keyboard in watchOS 9 would be able to autocomplete some of these? oh! you could totally implement this with search have you tried .searchable on watchOS ? I mean, it might not be exactly the same experience, but it would be very close. one minute, lemme look up documentation for it, it was an API we did last year, and I'm trying to page it back in I\u2019d start here: https://developer.apple.com/documentation/swiftui/adding-search-to-your-app lol, you found it before me... distractions are difficult to avoid :slightly_smiling_face: ya give that a shot, using suggestions based on the text, and using results, etc. It would not help in this case, but getting a number keyboard for watchOS could help in other input fields! thank you both <@U03HHHXDL03> and <@U03HELSV45B>! I will definitely take a look. But I'm not sure if searchable is the right solution, because I need to pass the user-typed value anyway, even if no results are found. For example, imagine the user just found a mushroom species that's not in my List! In this case, I allow the user to create a finding using whatever he/she typed (e.g. \"Unknown mushroom\", or \"Some goop\"). If the string is \"\", I also allow the finding to be saved as an \"Unknown species\". Is this something achievable using .searchable? ya, you could have one of the search results always be the literal string that the user provided So the first result would be the literal, and the rest of the results would be actual search results. you might want to talk with the design lab folks about differentiating the literal from the actual results, but you should be able to achieve something like that aha, interesting. And the matching results will somehow pop onto the screen as the user is typing? the user will have to type the first few characters and click the done button in the top right, so not exactly. if searchable is helpful, great, but if you still feel like there's room for something to be better, please file a feedback request. ok, thank you. I will definitely take a look and explore. <@U03HHHXDL03> thank you for your patience and kind support! :heart: may you find only choice mushrooms on your hikes :mushroom: ha! no problem at all. maybe I'll find a new species!","title":"String input on WatchOS is quite limiting, because it happens in a non-customizable modal. Are there any tips or guides on how to achieve autofill of a textField on watchOS?"},{"location":"wwdc22/swiftui-lounge.html#my-apps-design-uses-kerning-on-most-of-its-buttons-but-i-cant-put-that-inside-of-my-buttonstyle-because-kerning-is-only-available-on-text-not-buttonstyleconfigurationlabel-i-have-a-feedback-open-suggesting-passing-kerning-through-the-environment-fb10020695-in-the-meantime-do-you-have-any-suggestions-on-how-to-do-this-without-using-kerning-separately-on-every-button-label-nor-making-a-custom-button-view","text":"The only other thing I can think of is to create a Button wrapper view that requires a Text be passed in as a parameter. Then attach kerning to the Text yourself and pass it to the underlying Button Haha, was hoping to avoid that so we didn\u2019t need to handle other permutations like buttons with text and an icon, etc :slightly_smiling_face: One other avenue I\u2019ve been exploring is a custom Text view that accepts kerning through an Environment value applied by the ButtonStyle, though that has led to needing to reimplement multiple init s from Text. Doesn\u2019t feel too great, either :sweat: Thanks! <@U03JELC631P> the approach I normally take here Is to create a reusable view specifically for the button label rather than an entire button view. This results in code that composes nicely. I still use button styles for styling buttons more generally. Thanks <@U03HT7Z5NAK>, that\u2019s sort of the direction I was headed with the \u201ccustom Text view\u201d idea\u2014think like: Button(action: {}) { MyAppText(\"Hello World\") } and then the ButtonStyle would set kerning in the environment for MyAppText to look at. Still means our team needs to remember to use MyAppText instead of Text , but maybe that\u2019ll be easier to remember than peppering .kerning everywhere :smile:","title":"My app\u2019s design uses kerning on most of its buttons, but I can\u2019t put that inside of my ButtonStyle because kerning is only available on Text, not ButtonStyleConfiguration.Label. I have a feedback open suggesting passing kerning through the environment (FB10020695).  In the meantime, do you have any suggestions on how to do this without using .kerning() separately on every button label nor making a custom button View?"},{"location":"wwdc22/swiftui-lounge.html#how-would-one-go-about-creating-a-bubble-chart-with-swift-charts","text":"You can use the symbolSize(by:) modifier ( https://developer.apple.com/documentation/charts/chartcontent/symbolsize(by:) ) to size the points by data. To make the points look like bubbles, you can use .symbol(Circle().strokeBorder(lineWidth: 1)) so the symbols are drawn as stroked circles. Here is an example: Chart(data) { PointMark( x: .value(\"Wing Length\", $0.wingLength), y: .value(\"Wing Width\", $0.wingWidth) ) .symbolSize(by: .value(\"Weight\", $0.weight)) .symbol(Circle().strokeBorder(lineWidth: 1)) } great, thanks","title":"How would one go about creating a Bubble Chart with Swift Charts?"},{"location":"wwdc22/swiftui-lounge.html#swiftui-macos-how-can-i-control-how-the-focus-moves-around-the-controls-of-my-view-when-i-press-tab-currently-it-jumps-from-top-left-to-right-then-down-but-i-have-two-columns-in-my-window-and-would-prefer-it-go-down-first-on-the-left-side-and-then-again-up-to-the-right-is-there-some-functionality-to-pre-define-the-order-of-the-controls","text":"You can use View.focusSection() for this, which is newly available on macOS 13.0. Marking a view as a focus section causes the Tab loop to cycle through all of the section's focusable content as a group before moving on to the next thing in layout order. So, something like this should get you the sort of column-wise navigation you're after: struct ContentView: View { var body: some View { HStack { VStack { FocusableView() FocusableView() } .focusSection() VStack { FocusableView() FocusableView() } .focusSection() } } } Thanks, that's nice! Also, a plug for my colleague Tanu's WWDC21 talk: Direct and reflect focus in SwiftUI https://developer.apple.com/videos/play/wwdc2021/10023/ It goes over some tvOS use cases for View.focusSection() , and also describes how to work with focus programmatically using focus state bindings.","title":"SwiftUI macOS: How can I control how the focus moves around the controls of my view when I press \"tab\"? Currently it jumps from top left to right, then down. But I have two columns in my window and would prefer it go down first on the left side and then again up to the right.  Is there some functionality to pre-define the order of the controls?"},{"location":"wwdc22/swiftui-lounge.html#with-the-new-menubarextra-api-on-the-mac-if-i-want-the-menu-item-to-be-persistent-after-my-app-quits-do-i-need-to-do-any-extra-work-eg-adding-a-login-item-or-does-the-system-automagically-make-sure-it-appears-after-rebootlogout","text":"Hi - this is a great question. You can certainly use MenuBarExtra to create a standalone menu bar app, though you may want to look into the Info.plist changes noted in the documentation: > For apps that only show in the menu bar, a common behavior is for the app to not display its icon in either the Dock or the application switcher. To enable this behavior, set the LSUIElement flag in your app\u2019s Info.plist file to true . If the menu bar app is used in conjunction with a main app, then you'll need to package it together (ie, the \"helper app\" model). Adding a login item sounds like the way to go if you want it to show at login regardless of the user's preference for restoring state. We'd also certainly welcome any feedbacks for enhancement requests in this area as well. Got it, that combined with the new login item APIs this year should make this sort of thing a lot easier. Thanks! <@U03HHJH9C66> Please make MenuBarExtra available in Catalyst so we can use it with HomeKit which is Catalyst only on Mac at present.","title":"With the new MenuBarExtra API on the Mac, if I want the menu item to be persistent after my app quits, do I need to do any extra work (e.g. adding a login item) or does the system automagically make sure it appears after reboot/logout?"},{"location":"wwdc22/swiftui-lounge.html#is-there-any-specific-style-guide-for-swiftui-there-are-some-cases-in-swiftui-that-im-not-sure-whats-the-best-fe-var-body-some-view-myamazingview-more-readable-or-var-body-some-view-my-amazingview-a-one-liner","text":"We don\u2019t have any specific style guide! Use whatever you and your team think feels best. Personally, I\u2019m never opposed to writing one liners on one line in Swift though :slightly_smiling_face: The language is so beautifully concise, so I like to embrace that.","title":"Is there any specific style guide for SwiftUI ?  There are some cases in SwiftUI that I'm not sure what's the best. f.e. var body: some View {      myAmazingView }  (More readable)  or   var body: some View { my AmazingView }  (A one-liner!)"},{"location":"wwdc22/swiftui-lounge.html#if-your-app-is-always-behind-an-authentication-session-what-is-a-good-approach-for-blocking-the-apps-content-when-authentication-is-required-in-uikit-apps-it-was-common-to-display-a-separate-uiwindow-atop-your-apps-main-window-is-this-still-a-good-way-of-handling-it-in-a-swiftui-app","text":"Interesting question! Without knowing the details of your app, my inclination would be to try the RedactionReasons API. You can use the .redacted modifier to set a redaction reason. SwiftUI controls will automatically react to that, hiding sensitive data. You can also read the reason from the environment to redact in custom controls. Not the most elegant, but I\u2019ve done something like this if let _ = auth.sessionToken { ContentView() } else { AuthView() } You could also use an overlay modifier at the root of your hierarchy to present a view over the whole app, then adjust that view\u2019s opacity based on the log in state. Yeah, it's not a cookie cutter kind of problem but curious about other ideas Personally, I\u2019d prefer swapping rootViewController of the app\u2019s window in a UIKit environment. I think the closest approach in SwiftUI would be as Andrew shared above. Not sure if this would cover the need in question though. This is my approach currently, but it\u2019s got some drawbacks... I\u2019m interested to know how others are doing. WindowGroup { switch userSessionManager.isLoggedIn { case true: TabViewScreen() // .viewModifiers //(environmentObjects, etc.) case false: LoginScreen() } } Yeah, the main drawback of conditionally showing different content is you kind of destroy the user's current context as you show the LoginScreen() which is why I like the window approach I tried using .fullScreenCover instead to fix that. But that does not guarantee that your sensitive data will always be covered up when the user is logged out mid-session. e.g. when another sheet or fullScreenCover from a subview is already shown, and even when it is dismissed later. .objectWillChange isn\u2019t called again and the login screen does not pop back up. This is actually still a problem with the above approach. So sadly I tend to avoid using sheets and fullScreenCovers for that reason.","title":"If your app is always behind an authentication session what is a good approach for blocking the app's content when authentication is required? In UIKit apps it was common to display a separate UIWindow atop your app's main window. Is this still a good way of handling it in a SwiftUI app?"},{"location":"wwdc22/swiftui-lounge.html#is-it-recommended-to-use-switch-case-views-for-eg-showing-different-view-types-in-a-list-or-grid-instead-of-using-specific-types-via-generics-etc-are-there-any-better-ways-like-listitems-item-switch-itemtype-case-someviewitem-case","text":"When you use a switch within a @ViewBuilder closure, you are in fact using the power of generics :wink:, but in most common cases you hopefully don\u2019t have to think about that overtly. In general, using switch in view builders is a great tool! The best approach really case-by-case though, just as when deciding to use a switch versus other control flow in regular imperative code. Are there any specific alternatives to a switch you were considering?","title":"is it recommended to use switch-case views for e.g. showing different view types in a list or grid instead of using specific types via generics etc.?. are there any better ways? like: List(items) { item      switch item.type {      case ...:             SomeView(item)      case : ...      } }"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-preferred-method-to-allow-user-interaction-with-data-points-in-a-chart-imbedded-in-a-scrollview","text":"Can you provide a little more detail here? How are you trying to interact with the data points in the Chart ? And the scroll view scroll gesture is interfering maybe? My app uses data visualization to allow the user to edit errors. So I would like the user to tap on the portion of the chart they are interested in and link to a List below where that data row is then selected for editing There can be hundreds of data points in the list so finding the correct one to edit can be cumbersome without the touch feature I currently don\u2019t use a scroll view but would like to as this would allow more space for tapping on the data element. This talk https://developer.apple.com/wwdc22/10137 has some examples of adding interaction I think that would be orthogonal to the question of embedding the chart in a scroll view, but it may complicate hit detection depending on how big the chart is It is an iPhone app so I don\u2019t have lots of space.","title":"Is there a preferred method to allow user interaction with data points in a chart imbedded in a scrollview?"},{"location":"wwdc22/swiftui-lounge.html#does-branching-in-a-views-body-via-if-else-or-switch-cause-the-creation-of-each-branchs-views","text":"It only creates the views for the branch of the if / switch that\u2019s actually taken. This is why conditional / inert modifiers are so important, too. Highly recommend \u201cDemystify SwiftUI\u201d from WWDC 2021 for more info. That was an awesome session, I definitely need a refresher :sweat_smile: I\u2019ve watched it \u2026 way too many times. :sweat_smile: This topic actually deserves a small section of a sample app. We should be shown how to use ViewState enums, rather than if-else statements within View! :zap:","title":"Does branching in a views body (via if-else or switch) cause the creation of each branch's views?"},{"location":"wwdc22/swiftui-lounge.html#what-is-the-best-way-to-pass-down-data-managers-specifically-a-media-player-to-subviews","text":"You can pass them just as argument to your subview initializer. If you have multiple subviews or the subview that needs to use that data manager is further down the hierarchy you could use the Environment to pass is down Great. Is there an example of this somewhere? The documentation for EnvironmentKey has code example for it https://developer.apple.com/documentation/swiftui/environmentkey Wonderful, thank you!","title":"What is the best way to pass down data managers, specifically a media player to subviews?"},{"location":"wwdc22/swiftui-lounge.html#is-it-a-known-problem-that-using-a-custom-layout-with-multiple-subviews-doesnt-compile-for-example-this-minor-change-to-the-customlayoutsample-project-adding-a-second-subview-to-the-use-of-myequalwidthhstack-wont-compile-struct-buttonstack-view-var-body-some-view-viewthatfits-choose-the-first-view-that-fits-myequalwidthhstack-arrange-horizontally-if-it-fits-buttons-texthello-myequalwidthvstack-or-vertically-otherwise-buttons-or-am-i-just-doing-it-wrong-fb10113527","text":"That\u2019s a known issue. See the iOS and iPadOS release notes here: https://developer.apple.com/documentation/ios-ipados-release-notes/ios-ipados-16-release-notes You can search for \u201ccustom Layout\u201d","title":"Is it a known problem that using a custom Layout with multiple subviews doesn't compile? For example, this minor change to the CustomLayoutSample project, adding a second subview to the use of MyEqualWidthHStack, won't compile:  struct ButtonStack: View {     var body: some View {         ViewThatFits { // Choose the first view that fits.             MyEqualWidthHStack { // Arrange horizontally if it fits...                 Buttons()                 Text(\"hello\")             }             MyEqualWidthVStack { // ...or vertically, otherwise.                 Buttons()             }         }     } }  Or am I just doing it wrong?  FB10113527"},{"location":"wwdc22/swiftui-lounge.html#how-can-i-change-the-text-color-of-the-scales-at-the-sides-in-a-swift-charts","text":"You can change the color of the axis labels with the .chart{X/Y}Axis modifier, where you can configure the foregroundStyle of AxisValueLabel . Here is an example changing the label color for the Y axis: .chartYAxis { AxisMarks { _ in AxisGridLine() AxisTick() AxisValueLabel().foregroundStyle(Color.red) } } You can change the color of the grid line and tick in a similar way too. Could it be that this currently doesn't work in the developer beta? Because I just tried the following code and it applied the colors on the grid line and tick, but not on the label next to the tick. .chartYAxis { AxisMarks { _ in AxisGridLine() .foregroundStyle(.yellow) AxisTick() .foregroundStyle(.blue) AxisValueLabel() .foregroundStyle(.red) } } Yes, looks like this is a bug in the beta. Feel free to file a bug via Feedback Assistant.","title":"How can I change the (text) color of the scales at the sides in a Swift Charts?"},{"location":"wwdc22/swiftui-lounge.html#as-we-are-incrementally-adopting-swiftui-in-a-uikit-app-are-there-guidelines-for-replacing-the-patterns-that-we-are-used-to-in-a-uikit-app-like-delegate-pattern-notifications-target-actions-etc-some-of-these-are-obvious-but-i-kind-of-feel-im-doing-something-wrong-whenever-i-pass-a-closure-into-a-view-to-get-a-feedback-to-replicate-delegate-pattern-for-example-sweat_smile","text":"Delegates do a lot of different things in UIKit, and so the way that they should be translated into SwiftUI can vary heavily depending on what they\u2019re being used for in UIKit. In some situations they\u2019ll become closures that you pass around, in others, a Binding or State , sometimes a onChange(of:) , etc. UIKit and SwiftUI have two very different programming models, so often times, concepts don\u2019t map over in a one-to-one way. I\u2019d highly recommend you book a lab appointment so we can chat more about what specific case you\u2019re trying to translate, and where your pain-points are there :slightly_smiling_face: Awesome, will definitely do that. Can you please touch on using Combine for such feedback mechanisms? I also explored passing a subject into a view to observe a feedback but that also felt a bit wrong.","title":"As we are incrementally adopting SwiftUI in a UIKit app, are there guidelines for replacing the patterns that we are used to in a UIKit app, like delegate pattern, notifications, target-actions etc? Some of these are obvious but I kind of feel I'm doing something wrong whenever I pass a closure into a view to get a feedback to replicate delegate pattern for example. :sweat_smile:"},{"location":"wwdc22/swiftui-lounge.html#im-working-with-coreimage-filters-and-currently-using-the-cicontext-to-create-a-cgimage-which-is-turned-into-a-uiimage-which-initializes-a-swiftui-image-is-there-a-better-or-more-efficient-way","text":"You should be able to avoid the intermediate UIImage and initialize a SwiftUI Image directly with a CGImage omg, how did I not see this? Thank you!!","title":"I'm working with CoreImage filters, and currently using the CIContext to create a CGImage, which is turned into a UIImage, which initializes a SwiftUI Image. Is there a better or more efficient way?"},{"location":"wwdc22/swiftui-lounge.html#how-can-i-add-use-an-svg-in-my-swiftui-app","text":"You can add an SVG in the asset catalog and it can be rendered via a named Image . Checking \u201cpreserve vector content\u201d (or similar) in the asset item preferences will render it from vectors instead of raster. Drag SVG to Assets. In Inspector, check \"Preserve vector data\", and set scale to \"single scale\" Is there any way to load SVG image from file system? Also, an important tip that should be more known: SVG foreground color can be changed dynamically! So you don't need different color versions of your SVG image if you want to change color while your app is running","title":"How can I add use an SVG in my SwiftUI app?"},{"location":"wwdc22/swiftui-lounge.html#i-noticed-that-when-a-subview-uses-a-binding-to-published-variable-from-an-observableobject-the-subviews-body-will-re-execute-any-time-any-of-the-other-published-variables-in-the-observableobject-are-modified-even-if-this-modified-variable-isnt-used-by-the-subview-at-all-is-this-a-bug-or-a-performance-concern","text":"The subview updates based on the objectWillChange publisher, which is a composite of all the published properties. This is by design, but can require fine tuning of models. Different apps can handle this different ways, so this might be an excellent question for a lab. Or even a Developer Technical Support ticket! Thank you! And views are incredibly cheap in SwiftUI! Don\u2019t hesitate to break them up into smaller pieces. And generally avoid side effects in your view\u2019s initializer. Generally, IMO, I wouldn\u2019t expect performance concerns from the body being repeatedly invoked. Yeh pass properties of the object into subviews and then only the views that were given different values should have their body called. However if a view has a FetchRequest its body is called every time because of a long term bug!","title":"I noticed that when a subview uses a binding to published variable from an ObservableObject, the subview's body will re-execute any time any of the other published variables in the ObservableObject are modified. Even if this modified variable isn't used by the subview at all. Is this a bug or a performance concern?"},{"location":"wwdc22/swiftui-lounge.html#i-currently-have-a-large-data-set-over-10k-datapoints-that-im-working-with-are-there-any-tips-on-how-to-improve-the-graph-performance-when-working-with-creating-line-graphs-from-data-sets-of-that-size-or-larger-context-is-cycling-related-data-in-a-macos-app-that-is-presenting-6-or-more-in-a-lazyvgrid","text":"We recommend simplifying the data before rendering it. For your example, you could simplify the line or summarize the data points in small intervals (let\u2019s say you have data for a year, you summarize the data for a day). The added advantage is that you can summarize using mean, min and max and show the full range within the small interval. Sorry if non-Apple people aren't supposed to comment on this, but an Apple resource I found helpful with this kind of thing is using antialiasing filters to resample the data. https://developer.apple.com/documentation/accelerate/resampling_a_signal_with_decimation Thank you for the pointer <@U03JKLW88TZ>. Another useful resource are the number and date bins we released this year: https://developer.apple.com/documentation/charts/datebins and https://developer.apple.com/documentation/charts/numberbins .","title":"I currently have a large data set (over 10k datapoints) that I'm working with. Are there any tips on how to improve the graph performance when working with creating line graphs from data sets of that size or larger?  Context is cycling related data in a macOS app that is presenting 6 or more in a LazyVGrid."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-get-areamark-to-work-with-chartyscaledomain-automaticincludeszero-false-as-soon-as-i-add-an-areamark-it-seems-to-always-include-0-in-the-scale","text":"You could try setting yStart and yEnd for the area, here is an example: Chart(data) { AreaMark( x: .value(\"Date\", $0.date), yStart: .value(\"Start Price\", 100), yEnd: .value(\"Price\", $0.price) ) } .chartYScale(domain: .automatic(includesZero: false)) The automatic option will use nicely rounded numbers. If that's not working for you, you can also set the domain directly like domain: 100 ... 1000 :thinking_face: the first suggestion doesn\u2019t quit work because there is some padding added to the y range. I\u2019ll give a specific range a try next. Just setting a range doesn\u2019t quit work either\u2026 I guess I would need to do both? Umm, could you try turning off roundLowerBound in axis marks? Full code would be like this: Chart(data) { AreaMark( x: .value(\"Date\", $0.date), yStart: .value(\"Start Price\", 100), yEnd: .value(\"Price\", $0.price) ) } .chartYScale(domain: .automatic(includesZero: false)) .chartXAxis { AxisMarks(values: .automatic(roundLowerBound: false)) } roundLowerBound tries to add a round number below the data range, that might be why we are seeing a 40,000 value below the area like in your first screenshot. That works","title":"Is there a way to get AreaMark to work with .chartYScale(domain: .automatic(includesZero: false))? As soon as I add an AreaMark it seems to always include 0 in the scale."},{"location":"wwdc22/swiftui-lounge.html#can-swift-charts-be-adapted-for-non-discrete-data-like-curves-etc-today","text":"You can render curves by sampling data points along the function and then rendering that. You can add interpolation to make the line look smooth. Swift Charts cannot directly render functions. Thanks Dominik M for clarifying the point of functions and Swift Charts. <@U03HW7NSMFT> I was wondering if there were some kind of closure-based API to encapsulate the function, with parameters like range and increment Examples There is not but you could sample a few points and then render the line.","title":"Can Swift Charts be adapted for non-discrete data, like curves, etc today?"},{"location":"wwdc22/swiftui-lounge.html#is-there-any-sample-code-or-documentation-on-how-to-display-vector-fields-with-swift-chartsupside_down_face","text":"Like this? Yep This screenshot is from the Hello Swift Charts talk. Let me find the code for it. You can use a custom symbol ( Arrow ). Chart(data, id: \\.x) { PointMark(x: .value(\"x\", $0.x), y: .value(\"y\", $0.y)) .symbol(Arrow(angle: CGFloat(angle)) .foregroundStyle(by: .value(\"angle\", angle)) .opacity(0.7) } ... struct Arrow: ChartSymbolShape { let angle: CGFloat let size: CGFloat func path(in rect: CGRect) -&gt; Path { let w = rect.width * size * 0.05 + 0.6 var path = Path() path.move(to: CGPoint(x: 0, y: 1)) path.addLine(to: CGPoint(x: -0.2, y: -0.5)) path.addLine(to: CGPoint(x: 0.2, y: -0.5)) path.closeSubpath() return path.applying(.init(rotationAngle: angle)) .applying(.init(scaleX: w, y: w)) .applying(.init(translationX: rect.midX, y: rect.midY)) } var perceptualUnitRect: CGRect { return CGRect(x: 0, y: 0, width: 1, height: 1) } } This is not the complete code but should give you enough to make the example. Nice! That\u2019s the thing I was looking for, thank you! Alright, I\u2019m getting closer :smile: What\u2019s left is to figure out what the format was used for data in the sample and where angle came from :thinking_face: Very cool. And here we go It is quite delightful to use this framework and the performance is great. Many thanks to the team :pray: Though, documentation seems to be out of sync with reality a bit. For example, this snippet from PointMark does not compile. I\u2019m assuming that the functionality with KeyPaths is not in the first beta yet since I recall similar syntax from the sessions","title":"Is there any sample code or documentation on how to display vector fields with Swift Charts?:upside_down_face:"},{"location":"wwdc22/swiftui-lounge.html#how-are-large-datasets-handled-is-there-automatic-sampling-or-some-kind-of-recommended-limit","text":"There is no automatic sampling in Swift Charts. I would expect the \"preferred\" sampling method to vary on a case-by-case basis. I think the best approach is to see what works for you and then sample in a manner that is honest to your data as you run into performance issues. We're always looking to improve the performance of the framework! File feedback if you run into issues. As a general design principle, I would recommend simplifying / aggregating data as it becomes large. It's rarely useful to look at large amounts of raw data on the screen, as you run into the limits of 2d rendering wrt to occlusion, fidelity, and noise resolution. Does that help <@U03HZ4PT2ER>? <@U03H3193G3H> A little, so are you recommending at most one mark per screen point? and would we use GeometryReader to get that info? One point per screen point is probably excessive too (from a legibility perspective, if not a performance one)... it would help to get an idea of what you are trying to achieve & what your data looks like We draw a lot of scientific data, some is discrete, other is calculated/continuous Can you give a rough estimate to what number of marks the Charts will scale? Like a 1000? I\u2019d also like to plot data that comes in every other second in a scroll view, and would probably have to cut off at some point? Our data is available at once, and can have 5000 points. It will be static with no points just the lines cc <@U03HHJH7RJN>! It depends on the machine performance and complexity of the chart. It's usually a good idea to test with your actual data and chart to figure out how many points can be drawn with acceptable user experience. No rough scale? Like 100, 1000, 10k, 100k no problem? On say an iPhone 12 Mini? Based on my experience it's about 1000 to 2000 for smooth animations. Thanks! You may be interested in the date and number bin apis we added this year: https://developer.apple.com/documentation/charts/numberbins https://developer.apple.com/documentation/charts/datebins . You ca use them to group your data and then summarizing the data in each group/bin. <@U03HW7NSMFT> Interesting, we already have histograms, which we handle differently from this, but I will take a look For large data sets scrolling is inevitable. I guess we have to embed some Chart views into something's like a lazyhgrid - assuming that we can get rid of any padding/scaling numbers to get a seamless/infinite chart. Is that doable? <@U03JRNE4KJL> you might be able to do that with the chartOverlay and manual drawing with the axis objects, then handle gestures for scrolling","title":"How are large datasets handled, is there automatic sampling or some kind of recommended limit?"},{"location":"wwdc22/swiftui-lounge.html#how-easy-is-it-to-support-interactions-like-pan-to-move-and-pinch-to-zoom","text":"For pan-to-move, you can use a SwiftUI gesture in conjunction with ChartProxy . From the gesture, you get the the pan distance, and then you can use ChartProxy to figure out the pan distance in the data domain. Then, you can set the domain for the X scale with .chartXScale(domain: start + offset...end + offset) , where you can adjust the offset to pan the chart. Can a similar strategy be used for pinch-to-zoom? Yes, you can use a pinch to zoom gesture (or any other gesture) and hook up the events in a similar way. Without location information (like the new SpatialTapGesture ) it would be a bit unnatural as we wouldn't know where to center the zoom, right? MagnificationGesture doesn't provide location information, unless I've missed something Feel free to file a feedback to SwiftUI for providing location information to gestures. For now I think you can try implement a UIView with UIPinchGestureRecognizer and then wrap the view with UIViewRepresentable so it can be used in SwiftUI.","title":"How easy is it to support interactions like pan-to-move and pinch-to-zoom?"},{"location":"wwdc22/swiftui-lounge.html#can-we-highlight-specific-data-points-and-dim-the-rest-similar-to-what-httphealthapphealthapp-does-when-selecting-minmax-latest-etc-in-heart-rate-and-other-charts","text":"absolutely! An easy way to do this is to store the desired highlighted data's ids in a @State and check against that in your ForEach of Marks . Something along the lines of ForEach(data) { value in BarMark(x: ..., y: ...) .foregroundStyle(value.id == state.highlighted ? Color.red : Color.gray) }","title":"Can we highlight specific data points and dim the rest; similar to what http://Health.app|Health.app does when selecting \u201cmin/max\u201d, \u201clatest\u201d, etc in Heart Rate and other charts?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-use-a-chart-to-display-a-large-amount-of-data-for-example-display-a-waveform","text":"Yes! https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654798399767779?thread_ts=1654798325.944549&cid=C03HX19UNCQ We generally recommend aggregating very large datasets since there are only so many marks that can be rendered and reasonably read by a user. Swift Charts can process and render a reasonable number of data points. Thank you!","title":"Is it possible to use a Chart to display a large amount of data? For example, display a waveform."},{"location":"wwdc22/swiftui-lounge.html#is-there-multiple-axis-support-logarithmic-axis-support","text":"A Swift Charts chart can only have one axis per x and y. Swift Charts supports logarithmic scales. For example, you can specify a log scale on x with .chartXScale(type: .log) Thanks You can however, have multiple sets of labels / ticks / grid lines by adding multiple AxisMarks they will share the same scale, but might be useful for things like displaying both C and F on the same chart Yeah, we often have requirements to show multiple data sets with varying scales for comparisons Can you put those different sets of marks on opposite sides of the chart? In https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654799167892029 I talked about ways to show multiple metrics as separate charts.","title":"Is there multiple axis support? Logarithmic axis support?"},{"location":"wwdc22/swiftui-lounge.html#swift-charts-is-really-fun-thanks-for-adding-this-im-having-a-blast-i-have-one-question-about-the-axis-labels-on-charts-when-i-use-the-chartxaxis-modifier-to-show-custom-axismarks-with-an-array-of-values-dates-in-my-case-the-chart-doesnt-show-the-last-value-on-that-axis-the-last-date-is-it-possible-to-turn-that-on-simple-chart-chartxaxis-data-contains-two-data-points-with-a-date-the-entire-chart-only-has-these-two-data-points-axismarkspreset-extended-values-datamapdate","text":"Love to hear it <@U03J21TPWPM>! I think I can guess what is going on here. If a label is placed beyond the edge of the axis, (as can be the case with the last value of a list of dates, as it would define the end of your date range) it won't be rendered, as it would run off the edge of the chart. Off the top of my head, solutions may include: adding a bit of \"padding\" in your date range or switching the axis style. does that help? Yes, thanks :slightly_smiling_face:","title":"Swift Charts is really fun! Thanks for adding this, I'm having a blast! :)  I have one question about the axis labels on charts. When I use the chartXAxis modifier to show custom AxisMarks with an array of values (dates in my case), the chart doesn't show the last value on that axis (the last date). Is it possible to turn that on?  //  Simple Chart .chartXAxis {   // data contains two data points with a date. The entire chart only has these two data points   AxisMarks(preset: .extended, values: data.map(\\.date)) }"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-have-multiple-scales-per-chart-lets-say-im-doing-weather-and-i-want-one-line-with-cloud-cover-a-0-100-scale-and-rainfall-per-hour-in-say-mm-the-x-axis-on-both-of-those-would-be-time-but-the-y-axes-are-different","text":"An axis always has one scale. So x and y have one scale. You can have multiple axes to display e.g. temperature in C and F. If you want to show multiple measures, I would recommend multiple charts with aligned x-axes. You can even hide the axis of the upper chart to create a more compact view.","title":"Is there a way to have multiple scales per chart? Let's say I'm doing weather and I want one line with cloud cover (a 0-100% scale) and rainfall per hour (in, say, mm). The x axis on both of those would be time, but the y axes are different."},{"location":"wwdc22/swiftui-lounge.html#hey-richard-im-in-love-with-the-swift-charts-framework-i-like-how-we-can-customise-the-charts-to-fit-our-needs-i-noticed-some-layout-issues-with-the-legend-when-there-are-a-lot-of-items-to-display-or-when-the-names-are-long-do-you-plan-to-make-the-legend-layout-automatically-wrapuse-multiple-rows-when-there-is-not-enough-space-i-noticed-the-icloud-design-on-ios-switches-from-a-hstack-to-a-vstack-when-the-device-accessibility-is-set-to-true-my-feedback-with-more-infos-fb10125848","text":"Thank you for your feedback!","title":"Hey Richard, I'm in love with the Swift Charts framework. I like how we can customise the charts to fit our needs. I noticed some layout issues with the Legend, when there are a lot of items to display, or when the names are long. Do you plan to make the legend Layout automatically wrap/use multiple rows when there is not enough space? I noticed the iCloud design on iOS switches from a HStack to a VStack when the device accessibility is set to true. My feedback with more infos: FB10125848"},{"location":"wwdc22/swiftui-lounge.html#adding-data-to-charts-and-the-modifiers-always-requires-this-value_-function-that-requires-a-label-key-what-exactly-is-the-purpose-of-that-label-key-is-it-some-kind-of-identifier-does-the-label-key-in-a-foregroundstyle-have-to-match-one-in-a-linemark-for-example-if-referencing-the-same-data-thanks","text":"The label key is used for generating a good default description for the chart for VoiceOver users and the audio graph. For example, if you have x: .value(\"time\", ...), y: .value(\"sales\", ...) , VoiceOver users will hear something like \"x axis shows time, y axis shows sales\". The label key in foregroundStyle doesn't necessarily need to match LineMark , but it's a good practice to use the same label if the data is the same. Awesome, thanks for the explanation! :slightly_smiling_face:","title":"Adding data to charts and the modifiers always requires this .value(_:) function that requires a label key. What exactly is the purpose of that label key? Is it some kind of identifier? Does the label key in a foregroundStyle have to match one in a LineMark, for example (if referencing the same data)?  Thanks! :)"},{"location":"wwdc22/swiftui-lounge.html#are-there-any-limitations-to-the-result-builder-syntax-used-to-add-marks","text":"<@U03J21ZK802> can you clarify what you're looking for here? How many entries can we put in there? View builders are typically limited to 10. Would grouping help if there is an arbitrary limit based on the builder? Got it! Right now it's 10. Grouping into ForEach's etc would help! If you have a use case that requires more, please file feedback~","title":"Are there any limitations to the result builder syntax used to add marks?"},{"location":"wwdc22/swiftui-lounge.html#swift-charts-looks-very-cool-and-well-designed-what-were-the-inspirations-for-its-api-design-it-feels-very-similar-in-many-aspects-to-ggplot2-which-itself-is-apparently-based-on-the-grammar-of-graphics-book-was-that-an-inspiration","text":"Thank you. Its\u2019 great to hear that you enjoy the API. Swift Charts is a grammar based visualization language, which means charts are composed of building blocks (marks and mark properties) instead of picked from a list of chart types (vertical bar chart, bubble chart, waterfall chart etc). The Grammar of Graphics was hugely influential for many visualization APIs like ggplot, D3, Vega and also Swift Charts.","title":"Swift Charts looks very cool and well designed. What were the inspirations for its API design? It feels very similar in many aspects to ggplot2 which itself is apparently based on \"The Grammar of Graphics\" book. Was that an inspiration?"},{"location":"wwdc22/swiftui-lounge.html#hi-how-can-we-remove-or-configure-the-space-between-the-bars-barmark-is-it-possible-to-define-the-bar-width-thanks","text":"BarMark has a width parameter that takes a MarkDimension ! It can be a fixed value or a ratio (of the space the bar is allocated). Thanks. When trying, even with ratio = 1 or inset = 0 it seems to always have some space between the bars. <@U03JSSE35GQ> mind sharing the code? and a screenshot (if you're comfortable) No problem. `*import* SwiftUI` `*import* Charts` `*struct* Point {` `*let* month: Date` `*let* sales: Int` `*let* color: Color` `*let* eat: Bool` `*init*(month: Date, sales: Int, color: Color, eat: Bool = *false*) {` `*self*.month = month` `*self*.sales = sales` `*self*.color = color` `*self*.eat = eat` } } `*func* date(year: Int, month: Int, day: Int = 1) -&gt; Date {` Calendar.current.date(from: DateComponents(year: year, month: month, day: day)) ?? Date() } `*struct* ContentView: View {` `*var* body: *some* View {` Chart(data, id: \\.month) { p *`in`* `BarMark(` `x: .value(\"Month\", p.month, unit: .month),` `y: .value(\"Sales\", p.sales),` `width: MarkDimension.ratio(1)` `)` `.foregroundStyle(p.color)` `.annotation(position: .top) {` `*if* p.eat {` Image(systemName: \"seal\") } } .annotation(position: .bottom) { `*if* p.eat {` VStack { Text(\"\") Text(\"23\") } } } } // .chartXAxis { // AxisMarks(values: .stride(by: .month)) { value in // if <http://value.as|value.as>(Date.self)!.isFirstMonthOfQuarter { // AxisGridLine().foregroundStyle(.black) // AxisTick().foregroundStyle(.black) // AxisValueLabel( // format: .dateTime.month(.narrow) // ) // } else { // AxisGridLine() // } // } // } } `*let* data: [Point] = [` Point(month: date(year: 2021, month: 6), sales: 300, color: Color.yellow), Point(month: date(year: 2021, month: 7), sales: 350, color: Color.yellow), Point(month: date(year: 2021, month: 8), sales: 400, color: Color.orange), Point(month: date(year: 2021, month: 9), sales: 450, color: Color.red), Point(month: date(year: 2021, month: 10), sales: 420, color: Color.green), Point(month: date(year: 2021, month: 11), sales: 410, color: Color.green), Point(month: date(year: 2021, month: 12), sales: 390, color: Color.green), Point(month: date(year: 2022, month: 1), sales: 430, color: Color.green), Point(month: date(year: 2022, month: 2), sales: 450, color: Color.green), Point(month: date(year: 2022, month: 3), sales: 500, color: Color.green), Point(month: date(year: 2022, month: 4), sales: 550, color: Color.green), Point(month: date(year: 2022, month: 5), sales: 700, color: Color.green), Point(month: date(year: 2022, month: 6), sales: 800, color: Color.green), Point(month: date(year: 2022, month: 7), sales: 900, color: Color.yellow), Point(month: date(year: 2022, month: 8), sales: 1000, color: Color.orange), Point(month: date(year: 2022, month: 9), sales: 1200, color: Color.red), Point(month: date(year: 2022, month: 10), sales: 1300, color: Color.green), Point(month: date(year: 2022, month: 11), sales: 1310, color: Color.green), Point(month: date(year: 2022, month: 12), sales: 1200, color: Color.green), Point(month: date(year: 2023, month: 1), sales: 1400, color: Color.green), Point(month: date(year: 2023, month: 2), sales: 1600, color: Color.green), Point(month: date(year: 2023, month: 3), sales: 1800, color: Color.green), Point(month: date(year: 2023, month: 4), sales: 2000, color: Color.green), Point(month: date(year: 2023, month: 5), sales: 2200, color: Color.green), Point(month: date(year: 2023, month: 6), sales: 2400, color: Color.green), Point(month: date(year: 2023, month: 7), sales: 2500, color: Color.yellow, eat: `*true*),` Point(month: date(year: 2023, month: 8), sales: 2450, color: Color.orange), Point(month: date(year: 2023, month: 9), sales: 2430, color: Color.red), Point(month: date(year: 2023, month: 10), sales: 2100, color: Color.green), Point(month: date(year: 2023, month: 11), sales: 2000, color: Color.green), Point(month: date(year: 2023, month: 12), sales: 1800, color: Color.green), Point(month: date(year: 2024, month: 1), sales: 1500, color: Color.green), Point(month: date(year: 2024, month: 2), sales: 1300, color: Color.green), Point(month: date(year: 2024, month: 3), sales: 1400, color: Color.green), Point(month: date(year: 2024, month: 4), sales: 1410, color: Color.green), Point(month: date(year: 2024, month: 5), sales: 1450, color: Color.green), Point(month: date(year: 2024, month: 6), sales: 1500, color: Color.green), Point(month: date(year: 2024, month: 7), sales: 1700, color: Color.yellow), Point(month: date(year: 2024, month: 8), sales: 1800, color: Color.orange), Point(month: date(year: 2024, month: 9), sales: 1830, color: Color.red), Point(month: date(year: 2024, month: 10), sales: 1900, color: Color.green), Point(month: date(year: 2024, month: 11), sales: 1950, color: Color.green), Point(month: date(year: 2024, month: 12), sales: 2000, color: Color.green), Point(month: date(year: 2025, month: 1), sales: 2050, color: Color.green), Point(month: date(year: 2025, month: 2), sales: 2100, color: Color.green), Point(month: date(year: 2025, month: 3), sales: 2150, color: Color.green), Point(month: date(year: 2025, month: 4), sales: 1900, color: Color.green), Point(month: date(year: 2025, month: 5), sales: 1850, color: Color.green), Point(month: date(year: 2025, month: 6), sales: 1800, color: Color.yellow), Point(month: date(year: 2025, month: 7), sales: 1700, color: Color.yellow), Point(month: date(year: 2025, month: 8), sales: 1600, color: Color.orange), Point(month: date(year: 2025, month: 9), sales: 1500, color: Color.red), Point(month: date(year: 2025, month: 10), sales: 1400, color: Color.green), Point(month: date(year: 2025, month: 11), sales: 1200, color: Color.green), Point(month: date(year: 2025, month: 12), sales: 1000, color: Color.green), Point(month: date(year: 2026, month: 1), sales: 800, color: Color.green), Point(month: date(year: 2026, month: 2), sales: 700, color: Color.green), Point(month: date(year: 2026, month: 3), sales: 650, color: Color.green), Point(month: date(year: 2026, month: 4), sales: 600, color: Color.green), Point(month: date(year: 2026, month: 5), sales: 500, color: Color.green), Point(month: date(year: 2026, month: 6), sales: 300, color: Color.green), Point(month: date(year: 2026, month: 7), sales: 250, color: Color.yellow), Point(month: date(year: 2026, month: 8), sales: 200, color: Color.orange), Point(month: date(year: 2026, month: 9), sales: 210, color: Color.red), Point(month: date(year: 2026, month: 10), sales: 200, color: Color.green), Point(month: date(year: 2026, month: 11), sales: 220, color: Color.green), Point(month: date(year: 2026, month: 12), sales: 250, color: Color.green), Point(month: date(year: 2027, month: 1), sales: 300, color: Color.green), Point(month: date(year: 2027, month: 2), sales: 400, color: Color.green), Point(month: date(year: 2027, month: 3), sales: 500, color: Color.green), Point(month: date(year: 2027, month: 4), sales: 600, color: Color.green), Point(month: date(year: 2027, month: 5), sales: 800, color: Color.green), Point(month: date(year: 2027, month: 6), sales: 1000, color: Color.green), Point(month: date(year: 2027, month: 7), sales: 1200, color: Color.yellow), Point(month: date(year: 2027, month: 8), sales: 1400, color: Color.orange), Point(month: date(year: 2027, month: 9), sales: 1600, color: Color.red), Point(month: date(year: 2027, month: 10), sales: 1800, color: Color.green), Point(month: date(year: 2027, month: 11), sales: 2000, color: Color.green), Point(month: date(year: 2027, month: 12), sales: 2200, color: Color.green), Point(month: date(year: 2028, month: 1), sales: 2400, color: Color.green), Point(month: date(year: 2028, month: 2), sales: 2450, color: Color.green), Point(month: date(year: 2028, month: 3), sales: 2300, color: Color.green), Point(month: date(year: 2028, month: 4), sales: 2310, color: Color.green), Point(month: date(year: 2028, month: 5), sales: 2320, color: Color.green), Point(month: date(year: 2028, month: 6), sales: 2400, color: Color.green), Point(month: date(year: 2028, month: 7), sales: 2500, color: Color.yellow), Point(month: date(year: 2028, month: 8), sales: 2600, color: Color.orange), Point(month: date(year: 2028, month: 9), sales: 2670, color: Color.red), Point(month: date(year: 2028, month: 10), sales: 2500, color: Color.green), Point(month: date(year: 2028, month: 11), sales: 2300, color: Color.green), Point(month: date(year: 2028, month: 12), sales: 2000, color: Color.green), Point(month: date(year: 2029, month: 1), sales: 1500, color: Color.green), Point(month: date(year: 2029, month: 2), sales: 1000, color: Color.green), Point(month: date(year: 2029, month: 3), sales: 750, color: Color.green), Point(month: date(year: 2029, month: 4), sales: 500, color: Color.green), Point(month: date(year: 2029, month: 5), sales: 2, color: Color.green), ] `*let* averageValue = 137` } `*extension* Date {` `*var* isFirstMonthOfQuarter: Bool {` Calendar.current.component(.month, from: `*self*) % 3 == 1` } } `*struct* ContentView_Previews: PreviewProvider {` *`static`* `*var* previews: *some* View {` ContentView().frame(width: 400, height: 100.0) } } The screenshot is from the SwiftUI preview. cc <@U03HHJH7RJN>! this is a pixel boundary rendering artifact. One solution is to make the width just a tadddd over 1 so they overlap just a bit It's a rendering artifact that happens when the bars doesn't align with pixel boundaries. There are a couple of ways to address it: \u2022 As Halden mentioned, you can make the bars slightly wider (with something like .inset(-1) or .ratio(1.01) ) \u2022 You can also tweak the width of the plot area (with .chartPlotStyle { $0.frame(width: plotAreaWidth) } ) based on the number of bars, so that the bars are aligned exactly with pixel boundaries. This is more complex to do, but will give the best result. Thanks ! :+1:","title":"Hi, How can we remove or configure the space between the bars (BarMark) ? Is it possible to define the bar width ? Thanks."},{"location":"wwdc22/swiftui-lounge.html#do-you-have-any-suggestions-for-displaying-goal-progress-data-saw-in-earlier-qa-that-radialrings-charts-are-not-supported-but-other-types-could-be-used-looking-for-visualization-similar-to-health-activity-rings-thinking-about-3-bars-normalized-to-of-progress-toward-goal","text":"You could make a chart with three bars one for each category. If you set the scale to have a domain from e.g. 0 to 100, then the length of a bar indicates the progress towards a goal. A horizontal bar chart could work well. Thanks <@U03HW7NSMFT> that's exactly what I was thinking, too. But unsure on upper bound of chart when goal is exceeded. For rings, if my steps goal is 5000 and I actually take 12000, then it would show 240% for that bar. That\u2019s an interesting consideration. You could have a chart that extends beyond 5000 but adds a vertical RuleMark to annotate the goal. Since you may show different metrics (e.g. steps and distance), you may want to normalize them to % of goal so they have the same scale/axis. Thanks again, I'll give this a whirl!","title":"Do you have any suggestions for displaying \"goal progress\" data? Saw in earlier Q&amp;A that radial/rings charts are not supported, but other types could be used.  Looking for visualization similar to Health activity rings.  Thinking about 3 bars normalized to % of progress toward goal."},{"location":"wwdc22/swiftui-lounge.html#hi-everyone-is-there-a-suggested-update-rate-that-youd-recommend-for-real-time-charts-when-trying-this-out-on-a-simulated-iphone-i-quickly-approach-90-cpu-usage-if-i-try-to-update-a-line-graph-more-than-40-timessecond-for-example-when-trying-to-visualize-a-simulated-accelerometer","text":"Humans have a hard time with anything more than 10Hz anyway... We recommend benchmarking your app to find the right balance between the update rate and the amount of data on your line chart. We'd also appreciate feedback through Feedback Assistant for your use case! If I may follow up: if we're to alter the drawing rate so it adds new data points in batches, does Charts have built-in functionality for that, or would we need to set up the Combine publishers manually? I didn't see anything in the documentation, but I understand the docs could also be in beta Honestly at 40Hz I think you're looking at game semantics, i.e. Metal or at least SceneKit Yeah when I previously worked on the project that would use my own pre-\"Swift Charts\" charts. I experimented with rendering the charts using Metal, and that was not a very productive week :sweat_smile: You would need to write a small framework, but SceneKit (or even SpriteKit?) could do a lot of the work But I think you need to isolate your fast-updating surfaces and not drag all of UIKit/SwiftUI into it otherwise the performance will crash","title":"Hi everyone! Is there a suggested \"update rate\" that you'd recommend for real-time charts? When trying this out on a simulated iPhone, I quickly approach 90%+ CPU usage if I try to update a line graph more than 40 times/second (For example, when trying to visualize a simulated accelerometer)."},{"location":"wwdc22/swiftui-lounge.html#we-have-data-that-doesnt-necessarily-progress-linearly-from-one-point-to-the-next-for-example-the-x-and-y-values-could-either-increase-or-decrease-from-one-point-to-the-next-would-those-points-be-linked-as-if-they-were-points-on-a-path-or-would-it-upset-the-system","text":"I\u2019m not sure I fully got the question but you can either draw points as separate with PointMark or linked with a LineMark . Adding linked points is well supported by Swift Charts. Sounds like \"can I do XY scatter with joined lines\" Ahh. The answer is yes. Use a LineMark . Thanks","title":"We have data that doesn't necessarily progress linearly from one point to the next, for example the x and y values could either increase or decrease from one point to the next. Would those points be linked as if they were points on a path or would it upset the system?"},{"location":"wwdc22/swiftui-lounge.html#i-am-wondering-why-pie-charts-are-visible-in-promotional-material-or-in-the-sessions-but-dont-seem-to-be-supported-with-swift-charts-i-know-that-pie-charts-can-be-misleading-but-they-are-still-ideal-for-some-cases","text":"We'd appreciate feedback through Feedback Assistant! Will do! FB10138361 FB10138491 thanks <@U03HUM5PDHV>!","title":"I am wondering why Pie Charts are visible in promotional material or in the sessions but don't seem to be supported with Swift Charts. I know that Pie Charts can be misleading, but they are still ideal for some cases."},{"location":"wwdc22/swiftui-lounge.html#i-have-a-horizontal-bar-chart-with-two-text-overlay-annotations-on-each-bar-one-aligned-leading-and-one-aligned-trailing-when-the-bars-are-short-the-annotations-overlap-and-eventually-truncate-is-there-an-easy-way-to-hide-overlayed-annotations-when-the-bars-are-too-short-to-accommodate-their-intrinsic-widths-or-should-i-explore-other-layout-configurations-eg-one-annotation-with-an-hstack-containing-two-text-views-spanning-the-context-target-region-width","text":"There isn't a way to hide overlayed annotations right now, but feel free to file a feedback :slightly_smiling_face: In this situation, you can explore other layout options as you mentioned. For example, a single annotation with HStack { Text(...); Spacer(); Text(...) } Thanks.","title":"I have a horizontal bar chart with two Text overlay annotations on each bar, one aligned leading and one aligned trailing. When the bars are short the annotations overlap and eventually truncate. Is there an easy way to hide overlayed annotations when the bars are too short to accommodate their intrinsic widths or should I explore other layout configurations (e.g. one annotation with an HStack containing two Text views spanning the context target region width)?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-for-external-developers-to-extend-swift-charts-and-add-new-chart-types-that-arent-supported-out-of-the-box-for-example-pie-charts-or-charts-with-3d-styling","text":"You can create custom marks but they need to be cartesian. So you can make e.g. a candlestick or boxplot. Swift Charts does not support pie charts or 3D charts. See https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654618060787259 for a discussion of alternatives. Thank you. So I would assume a 2D bar graph with a fake 3D effect for the bar marks should be easily doable. But no circular graphs (for now). Yes, I would agree with that.","title":"Is there a way for external developers to extend Swift Charts and add new chart types that aren't supported out of the box, for example pie charts, or charts with 3D styling?"},{"location":"wwdc22/swiftui-lounge.html#is-there-a-method-to-modify-or-specify-the-label-style-of-a-chart-for-instance-my-chart-includes-price-averages-for-home-heating-oil-i-want-to-clearly-display-on-the-label-that-the-double-is-in-dollar-format-on-the-y-axis-at-this-time-when-i-attempted-a-brief-demo-app-it-simply-made-the-y-axis-label-as-an-int","text":"<@U03JF5S79RQ> sorry for the late reply! Yes, there are a few options here. You can use AxisMarks(format: FormatStyle) or AxisValueLabel(format: FormatStyle) or AxisValueLabel { Text(\\(..., format: ...)) } :heart: Perfect, thanks! If your units are more verbose, a design alternative is to clarify them in the title or headline of your chart, to save horizontal space on the y axis. At this time I'm tracking price averages per month over the last 12 months, the user will have the ability to toggle between the last 12 and 6 months. The X axis is the Month, the Y axis is the Average Price. the former is likely enough then, makes sense!","title":"Is there a method to modify or specify the label style of a chart? For instance, my chart includes price averages for home heating oil. I want to clearly display on the label that the Double is in dollar format on the Y axis.   At this time, when I attempted a brief demo app it simply made the Y axis label as an Int."},{"location":"wwdc22/swiftui-lounge.html#can-charts-be-used-in-the-new-preview-option-to-see-a-live-chart-in-the-preview-without-needing-to-go-to-that-screen-such-as-current-screen-allows-new-data-entry-and-the-preview-shows-the-updated-chart-also-can-previews-be-used-from-widgets-to-preview-the-chart-without-loading-the-app","text":"Can you clarify what you mean by the preview option? Charts can be in any SwiftUI View. contextMenu(menuItems:preview:) Using the contextMenu in a touch and hold scenario for a button that would lead to the view with a graph - can the live graph be shown? or is it only for fixed assets like an image? You can use Swift Charts to draw live data. But does it update for the preview, or when you've loaded the view with the chart? Swift Charts behaves like other Views and updates the same way. So is the contextMenu preview a View in its own right? I\u2019m not familiar with the contextMenu preview but it looks like it might work. Give it a try and if it doesn\u2019t work file a feedback. Okay, thanks Dominik","title":"Can charts be used in the new Preview option to see a live chart in the preview without needing to go to that screen? Such as current screen allows new data entry and the preview shows the updated chart? Also, can previews be used from widgets to preview the chart without loading the app?"},{"location":"wwdc22/swiftui-lounge.html#does-charts-support-polar-coordinates","text":"It does not, but feedback is appreciated!! Will do. This is a great start!","title":"Does Charts support polar coordinates?"},{"location":"wwdc22/swiftui-lounge.html#hi-sorry-if-this-was-already-asked-but-how-can-i-add-a-text-or-sf-symbol-at-the-top-of-a-vertical-barmark-i-did-not-found-the-information-at-httpsdeveloperapplecomdocumentationchartshttpsdeveloperapplecomdocumentationcharts-thanks","text":"You can use the annotation modifier to add an annotation on top of the bar, where the content of the annotation is a SF Symbol image. Here's an example: BarMark(...) .annotation(position: .top) { Image(systemName: \"sfsymbol_name\") } Note that you can also use Text or other views as the content of the annotation. Parfect ! This is amazing how configurable the Charts are. It will save me a lot of time in my application! Thanks.","title":"Hi, Sorry if this was already asked, but how can I add a text or SF Symbol at the top of a vertical BarMark ?  (I did not found the information at https://developer.apple.com/documentation/charts/|https://developer.apple.com/documentation/charts/ ) ? Thanks."},{"location":"wwdc22/swiftui-lounge.html#the-charts-are-amazing-i-saw-the-example-for-the-interval-bar-chart-in-the-docs-and-it-got-me-thinking-i-could-represent-a-simple-weekly-view-of-a-students-course-schedule-if-i-flipped-the-axis-i-made-a-proof-of-concept-and-it-worked-really-well-i-calculate-the-duration-of-a-meeting-in-minutes-to-create-the-size-of-the-barmark-using-ystart-and-yend-is-this-a-good-way-to-do-this-or-can-it-be-achieved-with-just-the-start-date-time-and-end-date-time-i-also-had-to-reverse-the-chartyscale-to-get-the-am-meetings-to-apear-first-not-sure-if-that-is-expected-or-not-struct-series-identifiable-let-coursename-string-let-meetings-meeting-var-id-string-coursename-struct-meeting-let-startdate-date-let-enddate-date-let-startminute-double-minutes-since-7am-let-endminute-double-chartdata-series-in-foreachseriesmeetings-id-startdate-element-in-barmark-x-valuecourse-elementstartdate-unit-day-ystart-valuestart-time-elementstartminute-yend-valueend-time-elementendminute-annotationposition-overlay-alignment-top-vstack-textseriescoursenamefontcaption-textelementstartdatestringvaluedateformat-timefontcaption-cornerradius80-foregroundstyleby-valuecourse-seriescoursename-chartyaxis-axismarksposition-leading-values-strideby-60-axis-in-axistick-axisgridline-axisvaluelabelcentered-false-if-axisindex7-13-textaxisindex7-am-else-textaxisindex7-12-pm-chartxaxis-axismarksposition-top-values-daysoftheweek-_-in-axisgridline-axistick-axisvaluelabelcentered-true-chartyscaledomain-automaticreversed-true","text":"<@U03JDV4PQR0> beautiful!! You can use Date here instead of converting to integer minutes and it should go top to bottom by default. cc <@U03HHJH7RJN> Looks great! Maybe try setting the width of the BarMarks to 100%? amazing Use Date as the unit of x? sorry, I was trying to answer your question about the y axis Int and Double will go bottom up (low to high) while dates will go top down (early to late) lmk if that helps / if you have follow-up Qs I tried: yStart: .value(\"Start Time\", element.startDate), yEnd: .value(\"End Time\", element.endDate) But it did not go well :rolling_on_the_floor_laughing: Ok I understand now. I think you're approach is good! (turning it into a number and then applying a reverse and customizing the axis labels). This is a clever solution, I dig it. The problem with the startDate and endDate approach is that its interpreting the dates including the date, so your Y axis became a mirror the X axis. Very cool alternative use of Charts > <@U03H3193G3H> There are times when a student would not have class on Monday or Friday but I want the X axis to include all the weekdays. I passed an array of the dates to AxisMarks(position: .top, values: daysOfTheWeek) but I noticed Friday did not appear. I'm just noticing this too, would you mind filing feedback for this issue? this looks like a bug cc: <@U03J21TPWPM> in case this is the same issue you're running in to and my response to you is incorrect Is it possible to get a reference to the colors automatically assigned to the series by the foregroundStyle? I may want to create my own legend in a vertical format. Unless legends can be formatted vertically? You can try positioning the legend on the trailing edge of the chart using the chartLegend modifier The default color scale is currently as follows: all system colors .systemBlue , .green , .orange , .purple , .red , .teal , .yellow <@U03H3193G3H> Thanks for letting me know :slightly_smiling_face: This looks very similar, yes. In my use case, I could fix it by adding additional data points, since it was a very simple graph and I don't mind the extra points. Just in case it is a bug, I'm gonna send a report with the project, maybe it helps :slightly_smiling_face: thanks <@U03J21TPWPM>! (btw <@U03J21TPWPM> if you don't want to add extra points, you can adjust the chartXScale(domain:) or range manually and achieve the same result.) Ah, that sounds even better :smile:. Thanks! Filed as FB10139046. Thank you <@U03H3193G3H>! Congrats to you and the team on Swift Charts. Thank you <@U03JDV4PQR0>! Awesome chart, hope to see more! Filed mine as FB10139144 <@U03H3193G3H> :slightly_smiling_face: Thanks for everything, this framework is awesome! btw <@U03JDV4PQR0> I have to agree, this is a really cool use of the new charts! I really like it :smile:","title":"The charts are amazing! I saw the example for the Interval Bar Chart in the docs and it got me thinking I could represent a simple weekly view of a student's course schedule if I flipped the axis. I made a proof of concept and it worked really well! I calculate the duration of a meeting in minutes to create the size of the BarMark using yStart and yEnd. Is this a good way to do this or can it be achieved with just the start date time and end date time? I also had to reverse the chartYScale to get the AM meetings to apear first, not sure if that is expected or not.  struct Series: Identifiable {     let courseName: String     let meetings: [Meeting]     var id: String { courseName } } struct Meeting {     let startDate: Date     let endDate: Date     let startMinute: Double //Minutes since 7am     let endMinute: Double }   Chart(data) { series in   ForEach(series.meetings, id: .startDate) { element in      BarMark(            x: .value(\"Course\", element.startDate, unit: .day),             yStart: .value(\"Start Time\", element.startMinute),          yEnd: .value(\"End Time\", element.endMinute)         ).annotation(position: .overlay, alignment: .top) {             VStack {                Text(series.courseName).font(.caption)              Text(element.startDate.stringValue(dateFormat: .time)).font(.caption)           }       }   }   .cornerRadius(8.0)  .foregroundStyle(by: .value(\"Course\", series.courseName)) } .chartYAxis {   AxisMarks(position: .leading, values: .stride(by: 60)) { axis in        AxisTick()      AxisGridLine()      AxisValueLabel(centered: false) {           if axis.index+7 &lt; 13 {               Text(\"(axis.index+7) AM\")          } else {                Text(\"((axis.index+7)-12) PM\")             }       }   } }.chartXAxis {    AxisMarks(position: .top, values: daysOfTheWeek) { _ in         AxisGridLine()      AxisTick()      AxisValueLabel(centered: true)  } }.chartYScale(domain: .automatic(reversed: true))"},{"location":"wwdc22/swiftui-lounge.html#can-we-transform-a-mark-when-the-user-presses-and-releases-it-like-we-could-do-with-a-buttonstyle-if-not-we-can-probably-use-an-overlay-but-can-we-change-the-size-of-marks-without-changing-their-values-for-example-with-a-scale-effect","text":"Currently we don't have a scaleEffect modifier. You can use something like width: .fixed(hovering ? 100 : 80) in BarMark constructor. This will make the bar wider when hovering is true. Thanks, I think I can make it work then!","title":"Can we transform a mark when the user presses and releases it, like we could do with a ButtonStyle. If not, we can probably use an overlay, but can we change the size of marks without changing their values (for example with a scale effect)?"},{"location":"wwdc22/swiftui-lounge.html#hi-im-curious-on-how-charts-handles-time-based-data-eg-temperature-over-a-day-when-data-can-be-received-at-any-point-of-the-day-not-a-set-interval-apart-some-chart-libraries-that-have-previously-existed-did-not-handle-this-and-just-spaced-out-data-incorrectly","text":"Hi <@U03HMESB695>! Charts handles Date as a continuous variable, so this should be fine. If you do want to treat the data as spaced out evenly (e.g., exactly one measurement per day) you can tell the framework to do so by using the unit parameter in a value Thanks! I definitely don\u2019t want it spaced evenly. Great to hear it just handles it approriately.","title":"Hi, I'm curious on how Charts handles time based data e.g. temperature over a day when data can be received at any point of the day, not a set interval apart? Some chart libraries that have previously existed did not handle this and just spaced out data incorrectly."},{"location":"wwdc22/swiftui-lounge.html#hi-ive-got-a-set-of-data-i-wish-to-chart-that-is-discrete-states-over-time-for-example-onoff-states-of-a-light-or-an-enumeration-a-door-locks-state-example-httpstwittercomaaron_pearcestatus1527452719027728384s21tux-sjhyosmo89k0u4lf_jahttpstwittercomaaron_pearcestatus1527452719027728384s21tux-sjhyosmo89k0u4lf_ja-how-would-you-recommend-tackling-this-chart-rectanglemarks","text":"Yes. A Rectangle or Bar Mark. Thanks, I\u2019d need to precalculate the xStart and xEnd for this correct? I suppose you have some kind of start/end time already, no? You can pass those to xStart and xEnd. Yeah, I can work those out so its doable! Thanks","title":"Hi,  I've got a set of data I wish to chart that is discrete states over time, for example on/off states of a light, or an enumeration a door lock's state. (example: https://twitter.com/aaron_pearce/status/1527452719027728384?s=21&amp;t=ux-SjhyOSmO89k0u4Lf_jA).|https://twitter.com/aaron_pearce/status/1527452719027728384?s=21&amp;t=ux-SjhyOSmO89k0u4Lf_jA).  How would you recommend tackling this chart? RectangleMarks?"},{"location":"wwdc22/swiftui-lounge.html#whats-the-best-way-to-use-chartrenderer-to-draw-to-a-cgcontext-with-slightly-different-styles-eg-draw-shapes-as-hollow-or-dashed-lines-etc","text":"Currently there's no API to draw marks with styles other than what's supported in ShapeStyle . Feel free to file a feature request via Feedback Assistant. <@U03HHJH7RJN> Basically we'd like to replicate our current draw-to-PDF behaviors which favor line art, simple coloring, and optional line dashes for monochrome As distinct from the on-screen styling That sounds very cool! I'll file an internal feature request for this. <@U03HHJH7RJN> So does that mean we can't pass in a ChartView and apply styles just for the rendering? No, currently there isn't a way to configure styling of the renderer itself. You could try an approach like exporting the rendered content as PDF or SVG and apply the styling by transforming the resulting vector graphics. ok, thank you","title":"What's the best way to use ChartRenderer to draw to a CGContext with slightly different styles (e.g. draw shapes as hollow, or dashed lines, etc)?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-assign-a-gradient-between-specific-linemarks-eg-to-show-a-change-in-value-in-temperature-from-cold-to-hot","text":"You can draw an AreaMark with .foregroundStyle(gradient) below the two lines. Thanks! Would this change the line or apply an area under them? It will add an area under them, the lines will be intact. Is it possible to apply a gradient direct to the line? More as a function of the temp over time (left to right) on the line itself than just decorative You can use .foregroundStyle(gradient) on the LineMark as well. with a LinearGradient that goes from left to right. That\u2019s perfect! thanks","title":"Is it possible to assign a gradient between specific LineMarks? E.g. to show a change in value in temperature from cold to hot?"},{"location":"wwdc22/swiftui-lounge.html#hi-again-are-charts-planned-to-be-available-also-on-ios-15-thanks","text":"Swift Charts is iOS 16 only since Swift Charts relies on features that are not available in earlier versions.","title":"Hi again, Are Charts planned to be available also on iOS 15 ? Thanks."},{"location":"wwdc22/swiftui-lounge.html#whats-the-best-way-to-that-we-can-use-swift-charts-in-a-predominately-uikit-app","text":"<@U03K1BXG8RG>, Sara has a session on this where she adds a Swift Chart to a UIKit app :slightly_smiling_face: https://developer.apple.com/videos/play/wwdc2022/10072/ oh hi <@U03HL05BUJG> Yes the new UIHostingConfiguration lets you use SwiftUI in cells and UIHostingController has some new ways to track size changes that make it easier to use it in stack views or in embedded view controller! Definitely check out the talk, anywhere I talk about using a SwiftUI view you can use a Chart as well! <@U03HL05BUJG> <@U03H3193G3H> thank you all super excited about this :100:","title":"What's the best way to that we can use Swift Charts in a predominately UIKit app?"},{"location":"wwdc22/swiftui-lounge.html#except-the-wwdc22-videos-and-httpsdeveloperapplecomdocumentationchartshttpsdeveloperapplecomdocumentationcharts-which-documentation-do-you-recommend","text":"In the documentation you will also find a sample app that could be quite useful https://developer.apple.com/documentation/charts/visualizing_your_app_s_data Thanks.","title":"Except the WWDC22 Videos and https://developer.apple.com/documentation/charts|https://developer.apple.com/documentation/charts , which documentation do you recommend ?"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-share-one-of-the-axis-between-2-charts-or-have-2-subcharts-on-the-same-axis-eg-share-x-axis-have-have-2-different-y-axies-on-either-side-with-their-own-independent-domain-range-or-have-a-plot-when-another-plot-to-its-right-that-shares-the-y-axis-but-has-its-own-x-axis-commonly-used-when-drawing-a-heatmap-to-show-a-histogram-attached-on-the-side","text":"<@U03HMDSQ9JB> Hope I'm understanding your question correctly. You can set specify two charts to have the same chartScale and chartAxis using the appropriate modifiers, but we don't support dual scales on the same chart","title":"Is it possible to share one of the axis between 2 charts or have 2 subcharts on the same axis, eg share x-axis have have 2 different y-axies on either side with their own independent domain range.   Or have a plot when another plot to its right that shares the y-axis but has its own x-axis (commonly used when drawing a heatmap to show a histogram attached on the side)"},{"location":"wwdc22/swiftui-lounge.html#when-using-swift-package-manager-modules-xcode-previews-will-crash-when-you-try-to-preview-resources-that-are-contained-within-another-module-eg-featuremodule-is-trying-to-preview-a-swiftui-logoview-contained-in-a-designsystem-module-that-loads-an-image-from-that-module-it-works-fine-when-the-app-is-built-and-run-is-this-a-known-issue-that-will-be-fixed","text":"Thank you for the feedback! This is a known issue, and we are actively working on resolving it. Excellent! I\u2019m so happy to hear that. I filed FB10070029 if you want an extra dupe number Are you the same Jeremy that wrote this? https://dev.jeremygale.com/swiftui-how-to-use-custom-fonts-and-images-in-a-swift-package-cl0k9bv52013h6bnvhw76alid :slightly_smiling_face: That\u2019s me :slightly_smiling_face: I was just going to post that, thanks <@U03J4D7EZNY> Very interesting post, I read it this morning :slightly_smiling_face: For anyone else running into this limitation, my blog post above has a workaround for how to keep your previews working until it gets properly fixed in Xcode :slightly_smiling_face:","title":"When using Swift Package Manager modules, Xcode Previews will crash when you try to Preview  resources that are contained within another module. e.g. FeatureModule is trying to preview a SwiftUI LogoView() contained in a DesignSystem module that loads an image from that module. (It works fine when the app is built and run). Is this a known issue that will be fixed?"},{"location":"wwdc22/swiftui-lounge.html#with-swiftui-previews-i-get-this-error-specifically-for-my-macos-target-the-same-view-works-fine-for-ios-launchexternaltoolerror-failed-to-build-importcsvcontactspreviewpageswift-failed-to-launch-swiftc-humanreadablenserror-nsinvalidargumentexception-comappledtpreviewsfoundationexceptionerror-0-nslocalizedfailurereason-too-many-arguments-4170-limit-is-4096-what-does-this-mean-btw-i-filed-fb10004742-already","text":"Thanks for filing the feedback! This is a known issue we're investigating. Thanks!","title":"With SwiftUI previews, I get this error specifically for my macOS target (the same view works fine for iOS):  LaunchExternalToolError: Failed to build ImportCSVContactsPreviewPage.swift Failed to launch swiftc. ================================== |  HumanReadableNSError: NSInvalidArgumentException |  |  com.apple.dt.PreviewsFoundation.ExceptionError (0): |  ==NSLocalizedFailureReason: too many arguments (4170) -- limit is 4096   What does this mean? BTW, I filed FB10004742 already."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-exportsave-a-screenshot-directly-from-previews-i-use-screenshots-cmdshift4-now-and-share-them-with-product-ownersdesigners-but-itd-be-super-handy-to-have-a-button-that-saves-the-preview-with-the-bezel","text":"There's no way to do this now, unfortunately -- but that's a good idea! Filing a feedback request would help us keep track of interest in that feature. I've also been doing this a ton lately for sending to coworkers.. and taking screen recording videos Done! FB10134111 Thanks! I created a separate project that I can copy-paste the preview implementation to run on the simulator just to be able to take a screenshot :sweat_smile: Not directly related to previews, but you might be able to automate some of your snapshots using the new ImageRenderer view announced this year: https://developer.apple.com/documentation/swiftui/imagerenderer (Doesn't include the device chrome, of course.) That's neat, I missed that one <@U03H32HJQEB> would it include status bar though? :thinking_face: no, it would just be the contents of the View Is there a way to show the status bar in previews? Actually, you know what might be cool? What if we had a System Share button on the preview that shared an image? Then we could tap it and share the view all in a couple clicks! We could even utilize a such feature for App Store screenshots with the help of multiple variations now. These sounds great. Please file feedback! I just added the System Share to my Feedback that I filed.","title":"Is there a way to export/save a screenshot directly from Previews?  I use screenshots (CMD+SHIFT+4) now and share them with product owners/designers but it'd be super handy to have a button that saves the preview with the bezel."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-add-a-custom-variants-to-xcode-previews-fe-i-think-itd-be-a-good-idea-to-add-rtlltr-variants","text":"There is not currently a way to add custom variants, but that's a great idea! Opening a Feedback request will help us track interest in that feature. Please also file individual feedbacks for any variants group (like our existing color scheme, type size and orientation) you think should be included by the system. Same applies for the widget variant groups too! Done! :rocket: FB10139550","title":"Is there a way to add a \"Custom variants\" to Xcode Previews?  f.e. I think it'd be a good idea to add RTL/LTR variants"},{"location":"wwdc22/swiftui-lounge.html#my-previews-take-generally-a-long-time-to-render-often-failing-timeout-and-sometimes-its-quicker-to-just-launch-a-run-on-the-simulator-also-it-takes-some-huge-place-on-disk-40gb-any-hint-to-optimise-all-this-and-make-the-previews-quick-light-and-reactive","text":"If you're experiencing long times to render there might be something specific about your setup. Filing a feedback with a sysdiagnose taken while the preview is taking the long time to render would be very helpful for us to diagnose this. As far as general tips to keep things fast, it helps to break up you views into smaller components where possible. And if your app project itself takes a long time to build, then it could be very helpful to split your Swift UI views out into a separate framework target so they could be built without the burden of the rest of the project. <@U03H32HJQEB> Could you elaborate on separating out the SwiftUI views? I am currently working on an objc app and adding new SwiftUI screens, but the builds can sometimes take a while because the rest of the app is somewhat large. Anything I can do to speed that up without having to copy paste the SwiftUI code to a blank SwiftUI project would be awesome <@U03H32HJQEB> Yes I was just thinking about this last advice \u2014 although perhaps with a Package rather than a framework. (I don't even know objc really so I try to stay out of it) <@U03JEMNCV18> I think the general theme is trying to focus on building as little as possible. If you can build out those new SwiftUI views in a framework / package, you can opt to build just that package rather than all of the app. I've considered doing that, but have never dove into creating a framework or package. But i'm tempted now.. Yup, packages would be fine, too. It's even easier to use them in Xcode now. The larger point is to break out the parts that need Swift UI into their own targets that can be built independently. It might even help to have a separate \"previewing\" app target that you link your Swift UI views into instead of your main app. It all depends on your setup, of course. The tl;dr is that Previews needs to build some target to get its work done. If your whole app target is large and slow to build then it will be the bottleneck for previews that you can investigate. Of course you might find other bugs on our end that slow things down. Please file feedback reports with sysdiagnosen! :)","title":"My Previews take generally a long time to render, often failing (timeout) and sometimes it's quicker to just launch a run on the simulator. Also, it takes some huge place on disk (40GB+). Any hint to optimise all this and make the Previews quick, light and reactive?"},{"location":"wwdc22/swiftui-lounge.html#as-of-xcode-13-there-are-some-gotchas-with-swiftui-previews-and-custom-build-configurations-not-named-debug-or-release-swiftui-previews-are-not-compiled-out-unless-the-active-build-configuration-is-exactly-named-release-source-files-under-the-preview-content-directory-are-compiled-out-for-non-debug-builds-whether-this-is-looking-for-the-active-build-configuration-to-not-be-named-debug-or-whether-its-based-on-code-optimization-levels-i-do-not-know-so-what-this-means-is-if-you-have-a-custom-build-configuration-say-a-staging-or-beta-or-whatever-which-is-essentially-a-variant-of-release-and-if-you-have-swiftui-previews-in-your-app-and-if-those-previews-are-not-wrapped-in-if-debug-directives-and-if-you-have-source-code-to-support-swiftui-previews-stashed-under-the-preview-content-directory-then-your-project-will-likely-fail-to-build-as-the-previews-will-not-be-compiled-out-but-they-will-depend-upon-development-asset-code-in-preview-content-that-is-compiled-out-do-you-know-if-this-is-resolved-in-xcode-14","text":"Thanks for the feedback! Right now the recommendation is to limit the Preview Content folder (which is really just a convenience for the DEVELOPMENT_ASSET_PATHS build setting) to assets like asset catalogs, preview data, etc. For code, we recommend you continue to keep it in your normal source paths, but to wrap with #if . Note: the compiler will normally remove any preview providers from the final binary without needing the #if , but the #if is a guarantee it won\u2019t be included. It\u2019s specifically preview data that we are using in the previews. The problem is our Staging target, and the mismatch between what is being compiled out for Previews vs Preview Content. It seems like those two should agree on what is being compiled out. I\u2019ve also had this problem, I made this SO post for it. https://stackoverflow.com/questions/63078880/why-does-archive-fail-if-there-is-code-for-swiftui-previews-in-the-preview-conte I filed FB8969539 for this issue. Yep, that\u2019s exactly it Jeremy. So annoying because we sometime forget to wrap our new views in the DEBUG check. I need to invest in some script / linter to check this, but I was hoping this problem would go away in Xcode 14. If you don\u2019t put any code in Preview Content, then the compiler will usually be able to correctly remove any branches of code used only for previews. But it is code, and that seems very useful. Do you have other recommendations? I think you\u2019re recommending to move that code out of Preview Content and :crossed_fingers: that it is removed from release builds? I\u2019ll give that a shot. Thanks! How would it get removed from the release builds? I don\u2019t know of any other mechanism other than #if DEBUG I think that\u2019s what is being recommended \u2013 move the code out of Preview Content and manually wrap the entire code in an if DEBUG check. But it seems less than ideal since Preview Content is built for exactly this use case. What is compiled out should match between PreviewProvider and Preview Content. Fully agreed. The compiler optimizes by removing non-public symbols that aren\u2019t referenced from any top level code. This will remove most preview code that is not referenced by other public or used types. Just to clarify my understanding, you are saying that it\u2019s not supported use case to have code inside the Preview Contents folder that will be stripped out in Release builds? Note that this contradicts what was implied in WWDC 2020's session \u201cStructure your app for SwiftUI previews\u201d. > \u201cWhat\u2019s great about Development Assets is they apply not only to files like Asset catalogs, but also to code. Let\u2019s look at what\u2019s inside that preview content folder that we just added.\u201d > > By using the navigator, we can look inside and see two Swift files. These Swift files contain code that I only need for development and debugging and testing my app, and I don\u2019t want to include these when my app is actually deployed. https://developer.apple.com/videos/play/wwdc2020/10149/?time=637 \u2022 SwiftUI previews - compiled out of exactly \u201cRelease\u201d \u2022 Preview Content \u2013 NOT compiled out of \u201cDebug\u201d Ultimately it seems odd to me that these two things have different strategies. Everything discussed in this thread is fine but we have to acknowledge that everything we\u2019re currently doing (#if DEBUG) or recommended in this thread is a workaround for the disconnect between the above bullet points. Great thoughts and responses everyone! > Just to clarify my understanding, you are saying that it\u2019s not supported use case to have code inside the Preview Contents folder that will be stripped out in Release builds? It\u2019s not unsupported ( :slightly_smiling_face: ) to put code in Preview Content, but we\u2019ve found it to work best when Preview Content is limited to assets. > Note that this contradicts what was implied in WWDC 2020's session \u201cStructure your app for SwiftUI previews\u201d :upside_down_face: Thanks for the reply <@U03HW8Y0RFB>","title":"As of Xcode 13, there are some gotchas with SwiftUI Previews and custom build configurations not named Debug or Release.  \u2022 SwiftUI Previews are not compiled-out unless the active build configuration is exactly named Release. \u2022 Source files under the Preview Content directory are compiled-out for non-debug builds (whether this is looking for the active build configuration to not-be-named \u201cDebug\u201d or whether it\u2019s based on code optimization levels, I do not know)  So, what this means is\u2026  \u2022 If you have a custom build configuration, say a \u201cStaging\u201d or \u201cBeta\u201d or whatever, which is essentially a variant of \u201cRelease\u201d\u2026. \u2022 And if you have SwiftUI Previews in your app\u2026 \u2022 And if those previews are not wrapped in #if DEBUG directives\u2026 \u2022 And if you have source code to support SwiftUI Previews stashed under the \u201cPreview Content\u201d directory\u2026  Then your project will likely fail to build, as the previews will not be compiled out, but they will depend upon development asset code in \u201cPreview Content\u201d that is compiled out.  Do you know if this is resolved in Xcode 14?"},{"location":"wwdc22/swiftui-lounge.html#are-there-any-new-swiftui-preview-tools-for-previewing-views-on-devices-of-varying-os-versions-for-example-viewing-how-a-view-would-look-on-an-ios-16-device-vs-ios-15-that-uses-the-compiler-checks","text":"Thanks for the question! There\u2019s not currently a variant mode in the canvas for different iOS versions. However, there is a neat solution! If you have a physical iPhone that is running iOS 15, you can use on-device previews to see your simulator preview in the canvas (running iOS 16) and the preview running on device (running iOS 15). And any changes you make will automatically be updated in both places at the same time! If you have other use cases for seeing multiple OS versions, please file a Feedback Request so we can take a look, thanks! Should feedback be under SwiftUI, Xcode, simulator, or something else? Sent - FB10139747 Any of those is fine, it will make it to us, but Xcode is the fast track :slightly_smiling_face: Cool, I picked the right one","title":"Are there any new SwiftUI Preview tools for previewing views on devices of varying os versions? For example viewing how a view would look on an iOS 16 device vs iOS 15 that uses the compiler checks"},{"location":"wwdc22/swiftui-lounge.html#whats-the-best-practice-for-working-with-core-data-and-swiftui-previews-such-that-we-dont-end-up-interacting-with-the-actual-store-data","text":"Core Data supports pointing to in-memory storage which won't need the file system and would work great in any situation where you don't want to persist the changes permanently, even in Previews. https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/CoreData/PersistentStoreFeatures.html","title":"What's the best practice for working with Core Data and SwiftUI previews, such that we don't end up interacting with the actual store data?"},{"location":"wwdc22/swiftui-lounge.html#as-preview-utilizes-some-generated-code-for-dynamically-updating-values-etc-i-have-faced-naming-clashes-a-few-times-in-a-rather-crowded-file-and-it-wasnt-obvious-at-first-i-had-to-dig-through-logs-to-figure-out-some-helper-types-i-had-were-clashing-what-would-be-suggestions-to-avoid-such-cases-is-there-a-guideline-that-we-can-refer-to","text":"That's interesting. In order to be able to help we'd have to take a look at a concrete example. If you're able to repro the issue in a sample project, could you file a feedback request for us? <@U03HL18CEEQ> thanks for answering, I don\u2019t have any such project at the moment but I\u2019ll sure keep in mind to file a feedback next time.","title":"As preview utilizes some generated code (for dynamically updating values etc), I have faced naming clashes a few times in a rather crowded file. And it wasn't obvious at first, I had to dig through logs to figure out some helper types I had were clashing. What would be suggestions to avoid such cases? Is there a guideline that we can refer to?"},{"location":"wwdc22/swiftui-lounge.html#sometimes-the-xcode-previews-stop-working-for-no-apparent-reason-the-last-example-of-this-was-it-said-connection-interrupted-or-something-like-that-so-i-question-is-what-are-some-tips-for-fixing-broken-previews-btw-the-new-previews-are-great-love-not-having-to-create-multiple-previews-for-landscape-and-portrait","text":"Thanks for the question. We know there are cases where previews don\u2019t work and we\u2019re actively working on addressing them. The best way to help us is to file a Feedback Request and attach the previews diagnostics. Look for an upcoming message about great tips for filing previews bugs to make it easy for us to investigate! Thanks! <@U03HW8Y0RFB> One unrelated issue I encountered today was that the compiler took too much time to compile the code because it was trying to guess the type of a rather simple expression for an human but not for a computer. Perhaps you could handle this specific case by displaying a dedicated message so that the developer precisely knows what the preview is waiting for. Thanks for the feedback <@U03JSSE35GQ>, that\u2019s a great idea. We\u2019ve been tracking a number of issues related to type checking, so we\u2019re on it! :slightly_smiling_face:","title":"Sometimes the Xcode previews stop working for no apparent reason. The last example of this was it said \"connection interrupted\" or something like that. So, I question is what are some tips for fixing broken previews? BTW: The new previews are great! Love not having to create multiple previews for landscape and portrait."},{"location":"wwdc22/swiftui-lounge.html#views-within-an-extension-arent-always-exposed-to-previews-this-will-create-a-compile-issue-enum-mynamespace-extension-mynamespace-struct-mycontainerview-view-var-body-some-view-mychildview-struct-mychildview-view-var-body-some-view-textmyview-struct-contentview_previews-previewprovider-static-var-previews-some-view-mynamespacemycontainerview-is-this-a-known-issue-im-hoping-its-not-expected-upside_down_face","text":"Thanks for the question! Ha, yes, definitely not expected :slightly_smiling_face: This is a known issue that we are actively looking at. In the meantime, a workaround is to add a typealias inside of the struct that references the other view. For example in your example I added typealias MyChildView = MyNameSpace.MyChildView inside of MyContainerView and the preview renders.","title":"Views within an extension aren't always exposed to Previews. This will create a compile issue:  enum MyNameSpace { } extension MyNameSpace {     struct MyContainerView: View {         var body: some View { MyChildView() }     }     struct MyChildView: View {         var body: some View {Text(\"MyView\") }     } }   struct ContentView_Previews: PreviewProvider {     static var previews: some View {         MyNameSpace.MyContainerView()     } }  Is this a known issue? I'm hoping it's not expected. :upside_down_face:"},{"location":"wwdc22/swiftui-lounge.html#question-re-the-layout-inspector-inspecting-swiftui-curious-if-there-are-publicsanctioned-ways-to-introspect-a-swiftui-view-for-details-similar-to-what-is-presented-in-the-details-panel-on-the-right-when-selecting-a-view","text":"Hey Tyler, great question! The current recommendation is to attach to the process and use the view debugger. The runtime view debugger does not provide the same level of detail that you get when using the inspector in Previews \u2013 are there specific details that you are looking for in the view debugger? <@U03HHK591AP> Can you suggest how to use background(color.red.ignoreSafeArea()) together with .cornerRadius? It just dont work :( Sorry, I realized that this question wasn\u2019t actually preview-specific until after I posted. Previews are great for eyeballing layout and validating behavior, but I use the inspector to validate specific fonts, frames, etc. Historically we\u2019ve used internal tooling for inspection against UIViews, etc. from tests and interactive inspectors. We\u2019re rethinking this approach as we migrate to SwiftUI, but still curious if there\u2019s a public way to ask an arbitrary view for this info. The Xcode inspector seems to be able to focus on an arbitrary view and dump useful info. Re previews and eyeballing - I take this back. There\u2019s a lot I can check in non-interactive mode. However, there are still other cases in which I\u2019d like to have access to this type of general info outside of the context of previews. In particular, what level of detail are you looking for with the font? i.e. Is it a custom font that you\u2019re trying to validate or are you trying to make sure something is actually using a system font like .heading We use custom fonts, yes. Consider the scenario - we have a large app; currently, I (or say, a designer) can nav to an arbitrary screen, open up our in-process inspector; focus on a view and inspect its attributes. All without connecting via Xcode and attaching. My assumption here is that we\u2019re entering a new world with new tools and introspection like this isn\u2019t straight forward. But was still curious if there was something I\u2019m missing. Gotcha. To answer your question, I don\u2019t believe that level of introspection is available but we\u2019re interested in hearing more about what you\u2019re looking for. Could you please open a feedback request to help us track interest in a feature like that? Absolutely! Thanks.","title":"Question re the layout inspector inspecting swiftui - curious if there are public/sanctioned ways to introspect a swiftui view for details, similar to what is presented in the details panel (on the right when selecting a view)."},{"location":"wwdc22/swiftui-lounge.html#adding-border-to-views-when-developing-helps-debuggingdeveloping-your-code-adding-borders-to-every-view-in-the-screen-can-be-tiring-is-there-a-way-currently-to-automatically-add-borders-to-all-the-views-in-the-screen-with-the-press-of-a-buttonsetting","text":"If you'd like to see the borders of each view in the preview you can use the non-interactive mode then in the menu bar go to Editor > Canvas > Show View Bounds. Additionally, you can toggle \"Show Clipped Views\" in the same menu to show or hide views that extend beyond the edges of the preview. Thank you! :heart:","title":"Adding border to views when developing helps debugging/developing your code. Adding borders to every view in the screen can be tiring.  Is there a way currently to automatically add borders to all the views in the screen with the press of a button/setting?"},{"location":"wwdc22/swiftui-lounge.html#currently-xcode-previews-fail-to-find-the-module-bundle-and-associated-resources-for-views-in-spm-modules-this-has-a-workaround-manually-locating-the-bundle-but-adds-additional-boilerplate-code-id-like-to-know-its-a-known-issue-ive-seen-that-previews-got-a-substantial-update-in-xcode-14-and-i-was-surprised-to-see-this-was-not-addressed","text":"Thank you for the feedback. Yes, we know this is a common issue and are still finding the best path forward to solve it. Thanks for the patience! as far as i rememeber, .module is only generated when a Swift package target has resources. so it might be a better workaround to add an empty file to a package :slightly_smiling_face: https://developer.apple.com/documentation/packagedescription/target/resources :crossed_fingers::skin-tone-3:Thank you for the answer!","title":"Currently Xcode Previews fail to find the .module bundle (and associated resources) for views in SPM modules. This has a workaround (manually locating the bundle), but adds additional boilerplate code.  I'd like to know it's a known issue. I've seen that Previews got a substantial update in Xcode 14 and I was surprised to see this was not addressed."},{"location":"wwdc22/swiftui-lounge.html#is-there-a-way-to-have-the-preview-window-size-to-be-automatically-the-same-as-the-device-used-for-the-preview-it-usually-opens-wider-than-necessary-on-my-small-screen-","text":"There is not currently a way to have the preview canvas match the size of the device being previewed, though it is a known request. Feel free to file a Feedback request to help us track interest in that feature, thanks! OK, Thanks.","title":"Is there a way to have the preview window size to be automatically  the same as the device used for the preview ? It usually opens wider than necessary on my small screen ;-)"},{"location":"wwdc22/swiftui-lounge.html#not-a-question-thank-you-for-the-feature-to-provide-multiple-variations-itll-simplify-my-code-dramatically-as-i-was-constantly-boiler-plate-extending-views-for-lightdark-multiple-accessibility-sizes-to-see-how-it-worked-out-huge-benefit-mucho-gracias","text":"Thank you, Joseph. Glad to hear you are enjoying the new variations support. Please file feedbacks with any additional variations you think would be useful! Sorry if this was already asked / answered, but is it possible to view different devices on the same tab in XCode 14? (e.g. iPhone 13, iPhone 13 mini, iPhone 13 Pro Max) Haven't had a chance to test it out yet Just in general, I wanted to say that Xcode Previews are life-changing. I honestly feel that I\u2019m about 10x more productive with the combination of SwiftUI + Xcode Previews. Being able to rapidly iterate on a view to get it just how I want completely changes my approach to things. Now I just dive in, and start coding, and continually refine until it\u2019s perfect. With .xibs, I would give up far before the point of perfection.","title":"Not a Question: Thank you for the feature to provide multiple variations, it'll simplify my code dramatically, as I was constantly boiler-plate extending views for light/dark + multiple accessibility sizes to see how it worked out. Huge benefit, mucho gracias!!!"},{"location":"wwdc22/swiftui-lounge.html#is-it-possible-to-print-to-the-console-while-in-swiftui-previews","text":"This is a known issue we're actively investigating. :slightly_smiling_face: <@U03HW8XNUGH> Thank you! I've loved using Preview, and it's the only thing that's preventing me from using it for everything :smile: (Also not necessarily the best solution, but sometimes I end up just adding a debug label attached to state and use that as a print\u2026) :upside_down_face: FWIW, os_log and Logger do work fine in previews and then you can look for those with the Console app like any other syslog output.","title":"Is it possible to print to the console while in SwiftUI previews?"},{"location":"wwdc22/swiftui-lounge.html#first-i-love-that-the-interactive-mode-is-default-i-think-it-makes-the-entire-preview-much-more-useful-it-seems-that-the-selectable-mode-no-longer-works-the-way-i-would-expect-is-there-an-explanation-somewhere-of-what-i-can-do-in-that-mode","text":"We have not made any changes to the behavior of the selectable mode. Can you clarify what is behaving differently? Here's a screencast. It's possible that it's due to something in the project. It's behaving differently that I would expect. Ah. Hmm, that does look like a bug. It should be selecting the text / button. If you file a Feedback Request we\u2019d be happy to take a look to see if there\u2019s something specific to your configuration. You can use the Editor > Canvas > Diagnostics option that will give us all the info we need to double check. Thank you! phew! good to know \u2014 will file a radar. Thank you!","title":"First, I love that the interactive mode is default \u2014\u00a0I think it makes the entire preview much more useful. It seems that the selectable mode no longer works the way I would expect. Is there an explanation somewhere of what I can do in that mode?"},{"location":"wwdc22/uiframeworks-lounge.html","text":"uiframeworks-lounge QAs by FeeTiki Was something changed on UIDragInteraction? When I tried doing local drag and drop (move) in my app, the drop gets cancelled if my finger is not perfectly still when I lift it. Bug? That may be bug. Feel free to file a feedback and join a UIFrameworks lab where we can look into this together! It would also be interesting to know if you have any conflicting gesture recognizers. If you do have any other gesture recognizers attached, I would suggest disabling all of them and enabling them one at a time to see which one is causing the conflict. That\u2019s a very good suggestion <@U03HELXCTGV>! I have a soup of gesture recognizers. Might be a UISwipeGR! Any chance we can lower the duration of the lift gesture (without doing some ugly hacks)? I know the duration is reduced if it\u2019s dragged with a pointer. Does the browser-style navigation bar/controller manage the forward navigation stack automatically, and if so, is it accessible for, say, a custom swipe forward gesture a la Safari? The browser mode doesn\u2019t manage a forward navigation stack, so you\u2019ll have to bring your own implementation (and a gesture if you like). It may also be more flexible to bring your own back/forward stack for this case, as if your app adapts to compact modes you may want to fallback to a more traditional navigation style (the navigation bar will do so automatically) Looking at the UINavigationItem docs, I see there's a new backAction property but there's not a corresponding one for going forwards. So is the forwards button itself something I'd also need to add myself? typically for forward/backward navigation we would recommend using UIBarButtonItem s for that (in the leading position) the backAction gets translated slightly differently, and would (in browser mode) produce a typical back button rather than what you likely want for \u201cgo back\u201d in the browsing stack its certainly a subtlety here, but this is what Files does and what we would probably say is best I see. So putting my own bar button item in the leading position would place it after the builtin back button but before the title And to match the appearance, it seems like the back button is just using the chevron.left SF symbol in the default size/config the default is chevron.backward (which is left in LTR). If you specify this as the image for a UIBarButtonItem UIKit will supply the proper symbol config to match so to sum up, to get File\u2019s style, set 2 items with chevron.backward & chevron.forward and manage your own stack (replacing content as necessary). Then you can reorganize when in compact to a more traditional UI Awesome, thank you! Thanks for the great session! Love UIKit so much. In a UIKit app if you enable Mac Catalyst with the Mac idiom, is it possible to opt into native macOS SwiftUI? By default it uses the iOS style for Form for example which isn't ideal when you're building UI that'll only be used in your Mac app. Thanks! No, you can either use SwiftUI in native macOS (AppKit) mode, or Catalyst (UIKit) mode, but not both. Thank you for such a great improvments! As of iOS 15, when you re using .estimated() size of NSCollectionLayoutSection s boundarySupplementaryItems you can`t simultaneously use its visibleItemsInvalidationHandler properly. Does that fixed in iOS 16? Hi, we still don\u2019t support mutating items via visibleItemsInvalidationHandler in Compositional Layouts that are self-sizing. If you\u2019d like to talk about your specific use case, I\u2019m happy to talk about alternatives, in either our Q&A later today (Pacific Time), or in a 1:1 Lab session. Probably a 1-1 lab might be better for this question. Thank you! 1-1 lab sounds good Will multiple windows be supported in iOS? Right now we need to use UIKit lifecycle if we want to show HUD that overlays the entire app. For example: https://www.fivestars.blog/articles/swiftui-windows/|https://www.fivestars.blog/articles/swiftui-windows/ Hi there, do you mean supported in SwiftUI ? I think this might be a better question for the <#C03HX19UNCQ|>. Hi <@U03HELXCTGV>, thank you, yes you're right We'd like to improve our app & framework to have a better desktop-class experience. For that, we'd like to utilize UIContextMenuInteraction and the new UIEditMenuInteraction . How should we decide where to use one or the other? Can context menus on iPadOS be presented \"in place\", like on macOS, without a targeted preview? UIEditMenuInteraction powers the new light-weight edit menu. With edit menu interaction, on right-click on iPadOS (and macOS), it\u2019ll also automatically present the more compact context menu without a targeted preview. Use Edit menu interaction in contexts where you don\u2019t need a preview, or where the menu shouldn\u2019t block the content (i.e. text, or some canvas type view like Keynote). Thank you! If I understand correctly, would you recommend using Edit menu to present actions for inserting objects onto a canvas like in Keynote (after a long press or a right click), because a context menu would block the content? UIEditMenuInteraction also has an arrow you can point at something, unlike UIContextMenuInteraction . Thank you, Glen! And from what I\u2019ve seen so far, Edit menu can be triggered programmatically as a result of a user action or custom gesture. To make sure: can context menus be triggered in that way as well? Anyway, thank you both for taking the time to answer my question! We do not allow for programmatic activation of context menus, and only for the lightweight edit menu. Got it. Thank you for clarifying this! In iOS 15, compositional layout with orthogonal scrolling behaviour with a single section in non-full screen size also scrolls vertically, what is the correct way to prevent vertical scrolling for single horizontal section? If it is the outer (collection) view that is scrolling, you can set alwaysBounceVertical = false on it to prevent the scrolling. However if it is just a single section you may be better of not using an orthogonal section but just a horizontally scrolling collection view. This might be a great question to discuss in the labs later this week to find a solution that works best for you. Thanks, other solution we tried to go back with the flow layout but we then loose the nice things about the declarative relation as in compositional layout. In new UICalendarView, is it possible to change the layout? Like in the iOS Calender app? That isn\u2019t possible at the moment. Please file a feedback for us if this is something you\u2019d like to see us add in the future. Sure, thanks. Since setValue:forKey: on UIDevice is not longer supported to force orientation. Is there are step by step explanation of how to force landscape orientation for 1 UIViewController only? You can call +[UIViewController attemptRotationToDeviceOrientation] and return an updated UIInterfaceOrientationMask from -[UIViewController supportedInterfaceOrientations] . It\u2019s also important to note that setValue:forKey: to access and modify internal state is never a supported way to do things. You might get lucky but you should really consider this as not being supported and should file feedback to request API for the feature you are trying to access. Thanks, I somehow missed attemptRotationToDeviceOrientation so I\u2019m going to give it a try in a sample project. If that doesn\u2019t work i\u2019m going to see if I can join a lab. I\u2019ve always avoided using setValue:forKey: but a client was stubborn and demanded a fullscreen button that turned the orientation so I had too. But thanks I\u2019m going to give it a try :slightly_smiling_face: Hey <@U03JRR4R3CY> we have also such a button and the same problem now. After the button press we support that the user can rotate back with a physical orientation change. It's not a nice solution in my opinion, but it seems to work to temporarily override supportedInterfaceOrientation, call attemptRotationToDeviceOrientation() and then reset supportedInterfaceOrientation to all normally allowed orientations again. I started making a prototype here. Maybe it's useful for you too. Is there a way to customize the colors of the chevron that\u2019s shown in the UINavigationBar when returning a UIMenu in the new titleMenuProvider function on UINavigationItem? The chevron will adapt automatically for light/dark mode, but there's no way to specify custom colors. If you feel that this is necessary for your app, please file a feedback report! Ouch, that\u2019s unfortunate! But thanks for answering my question :pray: The session mentioned NSWindow.collectionBehavior, but how will Stage Manager interact with NSWindow.level? More specifically, if I have a window in my app that I globally float above all other windows by setting the level to NSWindow.Level.screenSaver, will that window still appear in Stage Manager? Window levels still work the same way in Stage Manager, in that they\u2019ll impact the z-ordering of your window with respect to other windows on screen. Otherwise, its relationship to other windows in center stage (like whether it\u2019s considered \u201cfloating\u201d or \u201cauxiliary\u201d) is dictated by its collection behavior. Thank you Jeff! I'd really love to learn some more about ExtensionKit/ExtensionFramework and extension UIs. Are there any sessions that cover these, and if not, what would be a good lounge/lab to ask? There aren\u2019t any videos or labs specifically about ExtensionKit, but there\u2019s some new documentation that explains how it (plus ExtensionFoundation) work together to host UI and non-UI extensions https://developer.apple.com/documentation/extensionkit I\u2019ve been checking out that documentation! It looks awesome. It appears that there may need to be some additional host configuration (perhaps in the Info.plist) to register extension points. Is that right, and is that documented anywhere? The documentation specifically says \u201cA Mac app can also declare its own extension points so that other apps can extend the Mac app\u2019s functionality.\u201c, but I cannot find any details on how. It looks like the details on that might be under-documented in the current seed. Could you write up a docs feedback using the Feedback Assistant app? Filed FB10061654. Thank you so much! Are there any good labs for asking about these frameworks? There aren\u2019t any labs about ExtensionKit this year. One thing I did notice, though, which might be catching you \u2014 if your documentation viewer is set to Objective-C, you won\u2019t see the Swift-only API in ExtensionKit and ExtensionFoundation. Ah yes, look at that! I was using the Swift view though. It looks like I\u2019ll have to wait for updated docs - as far as I can tell, it is impossible to configure a hosting app without some additional information. Hi <@U03JKSLQ7J6>, There is some additional extension point configuration required. The \"Host\" app must declare an extension point by installing am .appextensionpoint file in the app bundle's Extensions directory. This is a minimal extension point declaration file: <@U03JNQ1ABFC> oh wow this is incredibly helpful, I\u2019m really excited to check this out! If I have an app that's primarily written in AppKit, is there any way to use SwiftUI to define toolbar items from within an AppKit window? Or does the window itself need to be created by SwiftUI too for that to work? You can attach your own NSView hierarchies to NSToolbarItems. Within these, you can use the NSHostingController or NSHostView classes offered by SwiftUI to embed a SwiftUI view hierarchy in an AppKit view hierarchy. It is correct that the SwiftUI .toolbar() modifier only works with windows that are created and managed by SwiftUI. But, as Chris said, you can bring SwiftUI content into your NSToolbar using a hosting view. Got it, thanks! Assuming I am not doing anything custom or low-level involving Metal, are there any AppKit-specific APIs to be aware of to fully support ProMotion displays? macOS 12 added these new APIs to NSScreen. // Variable Rate Refresh @interface NSScreen () /** The maximum frames per second this screen supports. */ @property (readonly) NSInteger maximumFramesPerSecond API_AVAILABLE(macos(12.0)); /** The minimum refresh interval this screen supports, in seconds. This is the shortest amount of time a frame will be present on screen. minimumRefreshInterval and maximumRefreshInterval will be the same for displays that do not support variable refresh rates. */ @property (readonly) NSTimeInterval minimumRefreshInterval API_AVAILABLE(macos(12.0)); /** The maximum refresh interval this screen supports, in seconds. minimumRefreshInterval and maximumRefreshInterval will be the same for displays that do not support variable refresh rates. */ @property (readonly) NSTimeInterval maximumRefreshInterval API_AVAILABLE(macos(12.0)); /** The update granularity of the screen's current mode, in seconds. The display will update at the next boundary defined by the granularity, after the minimum refresh interval has been reached. When 0, the display can update at any time between the minimum and maximum refresh rate intervals of the screen. Fixed refresh rate screen modes will return the refresh interval as the update granularity (e.g. 16.66ms for 60Hz refresh rates), meaning updates only occur at refresh rate boundaries. */ @property (readonly) NSTimeInterval displayUpdateGranularity API_AVAILABLE(macos(12.0)); /** The time at which the last framebuffer update occurred on the display, in seconds since startup that the system has been awake. */ @property (readonly) NSTimeInterval lastDisplayUpdateTimestamp API_AVAILABLE(macos(12.0)); @end If you are managing your own refresh timer, these can be helpful. But if you\u2019re just doing normal AppKit stuff, you don\u2019t need to take any action. NSAnimation functionality, for example, will \u201cdo the right thing.\u201d In Jeff\u2019s AppKit session, he mentioned heightOfRow timing will be different in Ventura for dynamic row heights. Does it still work with auto-layout auto-resized row heights? I don\u2019t implement heightOfRow. Row height estimation as discussed in the session only apply to NSTableViews whose delegate implements heightOfRow. Auto-layout based rows will continue work as before. Excellent. Thanks. Sorry for the double post. Hi again! Another question that Myke said would be best for this venue. Still working with SwiftUI, Xcode 13.4 under macOS Monterey: tried to restrict orientation for deployed app to iPhone/portrait orientation by using the check box in GeneralDeployment orientation, but it still tries to work when going to landscape left/right. In the process, it refuses to scroll in those orientations, and when I return to portrait the headers/UI elements are \u201cscrunched.\u201d This is not solved until I send the app to background, and open the app again. Any ideas on how to tackle this would be appreciated, thanks! You have a portrait-only iPhone app deployed to the Mac App Store, and on the Mac, the app is rotatable, in spite of being portrait-only? That should not happen. Please use Feedback Assistant to file a bug report! Good catch! Apologies for the confusion, and incorrect question submission: this is an iPhone app still in development, and has not been deployed to the Mac App Store. Ah, thank you for clarifying! Is there a lounge where SwiftUI/iOS questions could be submitted? I obviously lost my way\u2026 We do have swiftui-lounge for SwiftUI questions, but this might be a more generic UIKit question, so you're not in the wrong place, here. For context, is the entire app SwiftUI? > Is there a lounge where SwiftUI/iOS questions could be submitted? There's a SwiftUI Q&A later today: https://developer.apple.com/wwdc22/110393 Apologies for the delayed response: it is entirely in SwiftUI. The original version of the app was in UIKit, and the problem is not present there. Thank you for the info on the SwiftUI Q&A lounge! np! We're following up with some SwiftUI folks on the expectation here <@U03J21EKNSE>, I just created a super simple SwiftUI test project and scoped it to portrait only on the iPhone in \"Deployment Info -> iPhone Orientation\" and the app doesn't rotate when I rotate the device physically If you have more information on this we'd love to hear it Otherwise, if you can reduce this to a sample project and file a feedback we'd like to look into it! Hello again Steve M. Thanks for doing this. Were you using Xcode 14 for this test project? I am seeing this in Xcode 13.4. I am having trouble downloading 14 (not surprising with everyone trying to download!). I will search for info on how to submit feedback (never done it before); hope to get it fixed! Thank you for the follow up; have a good afternoon! I was using Xcode 14. And for Feedback, there's a handy page here: https://developer.apple.com/bug-reporting/ Thank you: incentive to dare use a beta version of Xcode (another first time!) Does NSTableView support dynamic row heights for multi-column tables? When I tried that, if I had multiple columns, it didn't resize the row to the tallest cell's content. Automatic row height rows should size to fit the tallest cell's content. If that's not working for you, we'd love you to write a feedback, ideally with a sample project. Ok will do. Thanks. How can NSWindow.collectionBehavior be set in a SwiftUI app? And if it can be, does that apply to the entire app or only a specific WindowGroup/DocumentGroup? There isn\u2019t currently a way to request different collection behaviors from SwiftUI \u2014 so that would be an excellent enhancement request in Feedback Assistant! Is there a way to present the menu of a UIEditMenuInteraction that\u2019s associated with an instance of UITextInput? I\u2019m working on a text editor where I\u2019m programmatically showing the menu. Previously I could use UIMenuController.shared.showMenu(from:rect:) . What\u2019s the equivalent with UIEditMenuInteraction to show a menu with all the standard editing actions? Yep, UIEditMenuInteraction has a method called presentEditMenu(with:configuration) , which can be used in the same way. The default menu will contain the standard edit actions. You can override the menu if you'd like via menuForConfiguration on UIEditMenuInteractionDelegate. <@U03HELXCTGV> Thanks for your replies. I tried grabbing the UIEditMenuInteraction from UITextInput\u2019s interactions and present it with presentEditMenu(with:) . However, I\u2019ll need to pass a configuration with an ID. When supplying nil as ID, the menu isn\u2019t shown because it doesn\u2019t have any command and/or actions. If I pass UITextSelectionView.SelectionCommands as ID it works but that feels a bit wrong. I\u2019ve only gotten this ID by happenstance because my console showed some errors earlier. Is this the recommended way of doing it? You should install your own UIEditMenuInteraction; if nothing is showing when you call presentEditMenu(with:) , then there\u2019s a good chance that nothing is responding YES to canPerformAction:withSender: so I\u2019d verify that your actions can be performed. Also, if you just want to add/remove/change some menu items for the edit menu that comes up on a UITextInput like text view etc., you can override buildMenuWithBuilder: on any suitable view controller in your hierarchy and use UIMenuBuilder to change the menu. Xcode 13.4.1 / M1 Max MacBook Pro / macOS 12.4 a pure SwiftUI app run on \"My Mac (Mac Catalyst)\" choosing a SignInWithAppleButton() added in a view crashes the app with: _AuthenticationServices_SwiftUI/SignInWithAppleButton.swift:303: Fatal error: Attempting to present ASAuthorizationController from a SwiftUI view not in a hierarchy. This should not be possible, please file feedback. Anyone ware of this and what might be done to fix this? This sounds like a bug in SwiftUI. Please, file a Feedback request and attach this sample project Already did (FB10033240). Was hoping for some additional clues :slightly_smiling_face: This currently prevents me from updating an active app in the store with SignInWithApple functionality :confused: Thanks for the feedback number, I\u2019m taking a look now On the new SDKs, I\u2019m not reproducing a crash with the attached sample app. Have you tried with the new Xcode beta yet? not yet. Lacking a second M1 config to install beta on without compromising my working production enviroment. I\u2019ve added some detail to the bug, and also sent it to the AuthenticationServices team to double check. I don\u2019t have an immediate workaround, but from what I\u2019m seeing this should be resolved in the newer SDKs, so please update us once you have the chance to test if you\u2019re still encountering issues <@U03JH6HKC66> You can run the Xcode 14 beta alongside your existing Xcode install without issue. Thank you. I will try to get the beta SDKs running on my system asap so i can double check. Would be a shame if it could only be resolved with the new SDKs as it will be a while until i can upload to AppStore with them. <@U03J22A0C4S> i was under the impression i would need macOS 13 for Xcode 14. But seems i misread that. It's the other way around. Yeah, runs fine under 12.4. 13 is incompatible with Xcode 13. (Hopefully not permanently.) <@U03HHJP6N3C> i tried with Xcode 14.0 / macOS 12.4 I can still reproduce this behavior with Xcode 14.0 / macOS 12.4. Make sure to add and run Supported Destination Mac Catalyst . Designed for iPad is working fine. iOS Deployment Target 15.5 / macOS 12.3 (any higher settings give me a \"The app is incompatible with the current version of macOS\" on macOS 12.4). Thank you for the additional info, after changing some project settings I am reproducing the crash. Looking\u2026 Based on the crash backtrace, this does seem to be a bug with the SignInWithAppleButton in Mac Catalyst. I\u2019ve gotten it to the right folks, we don\u2019t have a workaround for you right away but we\u2019ll update the bug as we look into it. Thank you for bringing this to our attention! Thank you. I am happy you were able to reproduce the bug so i am sure it\u2019s not something i am doing wrong. Looking forward for a fix / updates. Does \"UIFrameworks\" mean not Swift UI? Is this code for non-SwiftUI UIFrameworks? If this is a pure SwiftUI question you should use the SwiftUI lounges. If this is a question about how to use UIKit/Appkit with SwiftUI this might be a good lounge to ask your question. K. Point I am making is general, that maybe using \"UIFrameworks\" as a way of saying \"Not Swift UI\" is not clear. I think UIKit could get its own room (IDK what AppKit people would want). Apple is probably going to be making this distinction for a while so maybe think about what you call \"UIFrameworks but not SwiftUI\" Starting from iOS 16, buttons on the screen don\u2019t receive touches when a UIMenuController (nay, a menu of a UIEditMenuInteraction) is presented. When attempting to select a button on the screen that\u2019s outside of the menu, the menu will be dismissed but the selected button will not receive the touches. This is a change from iOS 15. Is this intentional or a bug in beta 1? The behavior in iOS 16 poses a problem in my app that lets users select some text, in which case the UIMenuController may be shown, and act on that selected text by selecting a button on the screen. Hey hey! This is definitely a bug and we are tracking this internally already. Awesome! Glad to hear that. Thanks for your reply <@U03H31CT6S3> Yeah np! And if you find any issues, please feel free to file a feedback for us to address as soon as we can! Filing feedback on issues like this is very helpful! It\u2019s filed as FB10080311 Is the new UIHostingConfiguration bound only to collection and table view cells? Or is this API designed to be used in other places where I might \"inject\" SwiftUI views in the middle of custom UIKit view hierarchies? UIHostingConfiguration is designed to be used with UICollectionView and UITableView cells. Watch the video \u201cUse SwiftUI with UIKit\u201d coming on Thursday later this week to learn more about it, and other ways to integrate SwiftUI into your UIKit app! https://developer.apple.com/wwdc22/10072 Thank you! So if I\u2019m vending a mostly UIKit-based framework and I want my users to be able to customize/replace a part of the hierarchy with SwiftUI, embedding a child UIHostingController continues to be the most recommended way to support this? UIHostingController looked to be a much better fit\u2026 :smile: UIHostingController is the most general-purpose API to embed SwiftUI inside your UIKit app. Because it is a view controller, you can use it anywhere you can present or embed a view controller in UIKit, and all SwiftUI features are supported inside of it (including ones that require a connection to the view controller hierarchy in UIKit). UIHostingController also has some new features in iOS 16, so be sure to watch the \u201cUse SwiftUI with UIKit\u201d video to learn more about all of this later this week. Noted, I\u2019ll be sure to do so. Thanks again for answering my questions! Most SF Symbols appear to have transparent background colours. In AppKit, is there a way to get a white background colour so that coloured symbols look good on NSPopupButtons? Right now when the selection is hovering over an NSMenuItem with a coloured SF Symbol for the image, the symbol colour doesn't change to make it viewable. For example, if your accent colour is yellow and the NSMenuItem's image is yellow, it basically disappears from view (yellow on yellow). In general, we don\u2019t alter color images for states like menu highlight, because your image might be something like an icon which doesn\u2019t make sense to draw with a white foreground. However, I think your SF Symbol example is a good case where we could be smarter, since we know more about the underlying symbol image. That would be an excellent enhancement request for us. Excellent. I\u2019ll submit feedback for it. When setting to multi-coloured SF Symbols, NSPopupButton just completely removes all the colours. But explicitly setting colours for them using pallet configuration does show the colours, but doesn\u2019t invert or change the highlight state when traversing the popup button. Here\u2019s my popup button example with SF Symbols. NSMenu does have a delegation mechanism that can tell you when an item has been highlighted. Try implementing -menu:willHighlightItem: , and altering the highlighted item. Since these are symbols, you could use -[NSImage imageWithSymbolConfiguration:] to get a new image with a different configuration, and set that to the highlighted item\u2019s image property. Just be sure to restore the original image back to the previously-highlighted menu item and when the menu closes ( -menuDidClose: ). Is it possible to access swiftui environment/modifiers inside UIViewRepresentables? Yes. The context passed to make/updateUIView(context:) has an environment property you can read from. https://developer.apple.com/documentation/swiftui/uiviewrepresentable/makeuiview(context:) Or you can use @Environment in your representable struct. Thanks :blush: Is there a known issue with keyboard shortcuts in UITextView where if you type command-B then press \u2018B\u2019 straight after, the \u2018B\u2019 key press is not registered? (It\u2019s when the keystroke is the same letter as the previous command key combination.) I thought it was our bug originally, but haven\u2019t made any headway into what\u2019s causing it. Is this a new issue in iOS 16? Are you able to reproduce this in a sample app? This is not a known issue at the moment Are you still holding the command key on the second 'B' press? This is an issue in iOS 15 (not tried in 16). Definitely not still holding command. Sample app is our next step, although we hit it in two apps so far (but with some shared code so the bug could be in there). Unfortunately I\u2019m not able to reproduce this in a UITextView with allowsEditingTextAttributes enabled. Hitting \u2318B toggles boldface on the selected text, then hitting B replaces the now-bold text into a \u201cb\u201d. If you\u2019re able to reproduce this in a sample app, and file a bug report with the report attached (and post the report here), that would help us investigate the issue further! What should I do when I encounter performance issue when I have hundreds of CAShapeLayers (ink strokes) inside a huge UIScrollView content view? Should I consider rasterization solutions such as CATiledLayer? To start, it might be better to try and see if you can flatten it and use a single shape layer instead of several separate ones. You should be able to combine the shapes together by using things like CGPathCreateCopyByUnioningPath . Depending on what you're trying to do, it might even be better to try and use a Metal shader instead. I encourage you to schedule a lab appointment so we can look at it together! <@U03HELXCTGV> Actually having one giant shape layer with huge amount of points is performing worse. I had to split a long stroke into multiple CAShapeLayers to prevent frame drops. I\u2019ve never used Metal before, it seems daunting. Where should I start? I\u2019m simply trying to add a lot of bezier paths into the canvas. You can try shouldRasterize = YES , which can be set on the shape layer or some super layer. Sometimes that will avoid the perf issues. You might want to try using SwiftUI Canvas, which you can embed in your UIKit app if you need to. SwiftUI Canvas uses Metal to render the draw commands you issue it, so it\u2019s very performant. https://developer.apple.com/documentation/SwiftUI/Canvas Caveat: shouldRasterize does slow down animation though, so keep it on only when your UI is at rest. It won\u2019t give you as much control as using Metal directly, but if you just need simple bezier paths, it might be a good higher level fit. Actually it\u2019s for inking, so I\u2019ll need to be able to dynamically add points, as well as moving them around. You may also want to try making a custom UIView subclass, override drawRect() , and use CoreGraphics to draw the ink strokes instead. This will allow you to only draw the strokes that are visible (in the rect parameter passed in to drawRect ). Yeah, so you would bracket your addition/animation/moving code with shouldRasterize = NO and shouldRasterize = YES . Does scrolling in the scroll view count as animation? Also, if I turn on shouldRasterize , things get blurry after I zoom in. How do I trigger a rerender? It shouldn't, but YMMV. The idea is that if nothing changes within the shape layer, it should be safe to rasterize w/o negative impact. I think you should be able to call setNeedsDisplay Scrolling does not count as animation. Animation would be if you were animating the individual CAShapeLayer between different bezier paths. Animating the position of CAShapeLayers with shouldRasterize enabled should also be okay. So, I have a giant container UIView (inside a scrollview), containing a ton of small but overlapping CAShapeLayers. What\u2019s the difference between turning on shouldRasterize on the container view vs. the individual shapelayers? I would guess that rasterizing the container view would be most performant, because scrolling would only be moving the one rasterized bitmap, rather than having to recomposite multiple bitmaps. But you'll really have to try this out to see if it works for you. Another thing you can try is setting drawsAsynchronously = true on the shape layers. This will allow the layers to perform the actual drawing on background threads, which may be faster, but since it won't be synchronized with the main loop, you might \"see\" the strokes being drawn independent of the current CA transaction. I think setting shouldRasterize on only the container view will rasterize all of the overlapping CAShapeLayers into one bitmap. Setting shouldRasterize on the only the individual CAShapeLayers will rasterize each bezier path separately, and CA will not cache the composition of them all together. I\u2019m not sure which of these is more or less performant, and the answer might depend on your implementation. Say I have a 10000pt height container view with two tiny shape layers on top-left and bottom-right, does setting shouldRasterize on the container view create a giant bitmap that will kill the app due to memory pressure? When I worked practically on these sorts of issues, I would have to tweak shouldRasterize depending on what was being displayed. E.g. turn it off for large shapes, turn it on for small shapes. You may even have to figure out which shapes could be combined in a rasterization e.g. by putting them in a common layer and setting the flag there. The main advantage of CAShapeLayer here (vs some manual rasterization like drawing into a context) is good memory use for large, unrasterized shapes and inbuilt animation between states. I noticed that with a lot of CAShapeLayer , backboardd gets saturated causing the frame drops, while none of my app\u2019s CPU and GPU are high. CALayers (including CAShapeLayer) are rendered with Metal in another process (as you\u2019ve observed), so that\u2019s why you\u2019re seeing that process get saturated, not your own. > Say I have a 10000pt height container view with two tiny shape layers on top-left and bottom-right, does setting shouldRasterize on the container view create a giant bitmap that will kill the app due to memory pressure? I think the memory pressure would be in another process, not your app (since CAShapeLayer is rendered out of process), but in general I think this would cause memory pressure on the system. So I don\u2019t think you want to use shouldRasterize on large layers. Is there a way to debug memory use by this rendering process? That\u2019s a good question, and I\u2019m not sure. Can you put these questions together in a developer forum post? If you paste the link to a post here, I can forward it on to the Core Animation team. Hmm, I\u2019m afraid I don\u2019t have time right now to write a post. Is there a lab I can go to to talk to the CA team? Is it Core Graphics Lab or UIKit lab? I\u2019m afraid not. A forum post is the best way to communicate with the Core Animation team. Yes, we have labs on Thursday and Friday. Make sure to sign up! What's the simplest way to edit the AirDropped file's thumbnail and filename for the iOS sharesheet? Is there an updated or modern best practice version of the sample code: https://developer.apple.com/library/archive/samplecode/sc2273/Introduction/Intro.html#//apple_ref/doc/uid/DTS40013842|https://developer.apple.com/library/archive/samplecode/sc2273/Introduction/Intro.html#//apple_ref/doc/uid/DTS40013842 Provide a LPLinkMetadata object to the activity view controller. The easiest way to do this is using a UIActivityItemsConfiguration object. let sharedItem = NSItemProvider(contentsOf: url) let aic = UIActivityItemsConfiguration(itemProviders: [sharedItem]) let previewImage = UIImage(...) let previewImageProvider = NSItemProvider(object: previewImage) let lm = LPLinkMetadata() lm.URL = url lm.imageProvider = previewImageProvider aic.linkMetadata = lm let avc = UIActivityViewController(activityItemsConfiguration: aic) // present avc Hi, how can I change some properties of NSWindow (such as the titlebar color) when using Catalyst? Use the UITitlebarTitleVisibilityHidden option on UITitlebar.titleVisibility to disable the titlebar material, window title, and its toolbar. You can then provide your own view underneath. The window\u2019s safe area insets still apply and the area beyond those insets remain draggable. Note that the UITitlebar cannot have an NSToolbar in order for this to work. Also feel free to file feedback requesting enhancements if you wish to do this while maintaining a window toolbar! Therefore there is no way I can keep the window toolbar so that the user can still see (for example) the window title (and modify the background color at the same time)? That is correct, there is no way to do that by default. But if you set the titlebar visibility hidden, since you then manage your own content in that region you can add a UILabel with the window title Design-wise, on macOS we do generally prefer to use the standard toolbar/titlebar backing for consistency across the system. It plays an important role in communicating which window is currently frontmost. There are exceptions, like the new Weather app, but they\u2019re fairly exceptional. It\u2019s totally OK to go for something custom (we love creative designs!) but if you do, you\u2019re also taking responsibility for those affordances. <@U03HB0LK3FY> Thanks for the suggestion! In fact, I was trying to keep the titlebar in order to give the Catalyst app a more \u201cmacOS\u201d feel, but without simulating the default appearance with custom labels, since it almost never looks quite the same as the system I saw an app that shows the multitasking dots at the top of the screen, but still hides the reset of the status area. I could not figure out how to do that. Do you have a suggestion on how best to do this? I think returning true from an override of prefersStatusBarHidden on a view controller in your app will achieve this effect. Ah, yes, that does it. Thank you! UIHostingConfiguration question - I was expecting onAppear calls to be made on the hosted Views as cells move on screen, but it seems that they line up with (I assume?) the reused cell population. If I adjust your UseSwiftUIWithUIKit demo to increase the cell counts and add a print on onAppear I see fewer logs than I would expect to see. And no logging once I scroll through all of the content. Curious if this expected behavior before, or a function of this being beta? Great question, the intent is for onAppear used inside UIHostingConfiguration to behave like it does in a SwiftUI List , however it\u2019s a known issue in the current iOS 16 beta that it isn\u2019t called with the expected timing :thumbsup: Fantastic, thank you! I would like to allow users to configure command keys for menus in my app. However, I have not found a way to change the command keys (or add / remove menus) after the initial builder phase occurs (via the AppDelegate). As a test, I built my menus, and then used a Dispatch after to remove the View menu after 4 seconds. I'm reusing the same builder that was passed in. However, nothing happens. Is the expected? Is there a way to update the command keys after launch? Updating the main menu needs to happen synchronously in the -buildMenuWithBuilder: method. If you want to update the main menu later, you need to call UIMenuSystem.main.setNeedsRebuild(), and that\u2019ll call buildMenuWithBuilder again (lazily). awesome! Also note that on macOS / Catalyst, it is best to avoid a dynamically changing menu bar structure. All items should be in the same location at all times, so they're easy to discover and their location is easy to remember. Instead of adding/removing items, use validation to disable items when they are not applicable. Catalyst question: I would like create a custom cursor that is dependent on settings in the UI that only works inside a specific view (e.g., a tool). Normally on Mac, that would be done with an NSTrackingArea. Since UIPointerInteraction is not supported on Mac, how should I go about doing this? UIPointerInteraction should work with Mac Catalyst, and allow you to customize the cursor using NSCursor API when hovering inside a view using its UIPointerInteractionDelegate methods. Can you share some more detail about where you are seeing this not work? If this isn\u2019t working, please file a feedback with a sample project so we can take a closer look! OK - I'm a little confused by what you wrote. I have code which uses UIPointerInteraction and implements the delegate's func pointerInteraction(_ interaction: UIPointerInteraction,styleFor region: UIPointerRegion) On iOS, I get the desired cursor: let cursor = UIBezierPath(ovalIn: rect) return UIPointerStyle(shape: UIPointerShape.path(cursor)) That does not do anything on Mac. It sounds like I should conditionalize inside the delegate method and call NSCursor? Ah, I should use willEnter /willExit and call NSCursor. Ah, yes. UIPointerStyle is available on Mac Catalyst to reduce conditionalization, but won\u2019t affect the NSCursor appearance the same way it does on iPad. And yep, that was just the answer I was typing out! Thank you - the thing I noticed is that if the cursor is already in the view, then the cursor doesn't get set (e.g., if you have the mouse over something and you click, which then displays a new view which now encompasses the cursor. In that case, the methods for willEnter gets called but the cursor doesn't update for some reason. I have to move the cursor out of the view and back in (I have logging to show that). If that's not expected, I can make a project + feedback. This is definitely at least something we\u2019d like to take a look at, so please file a feedback! FB10082348 submitted. Is it now possible to create custom app-specific extensions that are not pre-defined by Apple or am I misreading the ExtensionKit docs? If this is possible, is there a good session to watch this week? Here is a previous question on ExtensionKit. Seems like there is no session on that this year, though. https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654627443299689 > Is it now possible to create custom app-specific extensions that are not pre-defined by Apple or am I misreading the ExtensionKit docs? It is possible for macOS apps to define an extension point. Other 3rd party apps can then embed AppExtensions that target the extension point. > If this is possible, is there a good session to watch this week? There are no sessions planned for ExtensionKit this week. (edited) Great news! This is something I'm excited for even if it only works with macOS. Are these extensions App Store compatible? Yes. Probably it is more core services but I can't find anything like that, so: We have a case where iPad is the heart of a system. Or rather iPads. In the aircraft cockpit it is one of the most powerful and flexible units. Now - we need two or more devices to communicate (pilot, co-pilot, cabin crew...). Yet regulators have very specific need that one device is a master and manages synchronization with others. We found that iPad is not providing things like services (e.g. http/soap/rest/socket server) that would be running no matter if app is open or closed (pilots are using many apps for different purposes) - is that achievable in any way? Clients would love to see iPad-only solution but we are limited now to use windows machine acting as server and iPad devices are just clients (with quite similar functionality). This is a very great question. Right now it is not generally possible to have code running indefinitely while an app is in the background. We have special endpoints for things like voip but do not support generic background processes. I\u2019d suggest filing feedback and also reaching out to evangelism about potential use cases that you might have for this. Also a disclaimer: this question has legal implications since you are mentioning aviation which is explicitly mentioned in the iPadOS license agreement under 7.5. So the answer above is purely based on the technical feasibility as we can not give you legal advice here. <@U03JFDDG9QD> Wondering if Multipeer Connectivity framework would work for you. Multipeer is odd cause there's no real \u201chost\u201d and \u201cclient\u201d\u2026. You could manage a host using communicated meta data though. I also think you disconnect from the network when the app gets backgrounded too\u2026 You might want to take a look at CoreBluetooth, as it explicitly allows an app to continue scanning for peripherals while backgrounded. See this doc This is not a recommendation for your application. Please contact developer relations to explore legal and other implications. In case you're still checking your thread - talk to the folks at ditto.live who are ex-Realm engineers doing a lot of work with in-plane communications. Looks great! I assume apps that already have a More (overflow) menu should migrate the items to additionalOverflowItems. Is that correct? for iOS 16 I would say definitely! In part it will help you a lot in Stage Manager, as you won\u2019t have to do as much to respond to size changes there are a few different ways to interact with it. centerItemGroups will flow into it first, but so will trailing items (as a group via the older rightBarButtonItems API, and by-group in the new trailingItemGroups API). there is also the additionalOverflowItems that can be used to add content directly to the overflow button\u2019s menu \u2013 we think this will be rare, but can be useful for specific use cases. Thank you. And from the sample code, it looks like the optional group does NOT flow into the overflow button's menu. optional groups should flow if there isn\u2019t enough room but items that are not in the customization currently go no where (not directly accessible). We\u2019re considering some control over that behavior however. That was what I was getting at. For apps with lots of commands, that would be helpful. Would you like a radar :slightly_smiling_face: always helpful! Will do. Excellent presentation, David. Thank you. This may not be a question with a straightforward answer, but... A degree of consensus in the developer community prior to the current beta releases has been that using SwiftUI inside Catalyst is the most difficult path, and it would be better to target macOS natively directly with SwiftUI. In trying to give the best desktop-class experience on iPad, would it be your recommendation to using a native SwiftUI app, if that is an option, or a Catalyst app with SwiftUI components? Poorly worded as of course what I mean is desktop-class experience on iPad while also targeting macOS as well... Catalyst support is continuously improving. I think that it really depends on on which works best for your App. If you have the luxury to try both you might want to compare the outcomes of each. Basically use whatever works best for your concrete situation. There is no one size fits all solution. The range here can be from an app 100% written in UIKit to 100% written in SwiftUI. And it could be a macOS version written in AppKit, using Catalyst or SwiftUI. In what state is your current code base? Is this an iPad app you're transitioning to the Mac? > Poorly worded as of course what I mean is desktop-class experience on iPad while also targeting macOS as well... Oops, just saw this What stood out for me in the What's New in SwiftUI session was the advances in Forms. Previously, Forms on macOS seemed to require some, platform specific additions to get nice looking layouts without using Catalyst. With iOS 16 and macOS 13, it looks like you can get great looking Form layouts with nearly the same SwiftUI views. Really nice! Late to the party to see these additional replies. :slightly_smiling_face: My app code base is currently all UIKit, pretty tidy, ready to be modular and move to the next ideal UI architecture. Are there any particular reasons (that you can reveal) for why there\u2019s no general UIHostingView, only UIHostingController, whereas AppKit has NSHostingView available? This is intentional, because our architecture relies on a UIView being a part of a view controller hierarchy (and thus, being \"owned\" by a view controller). Think of things like modal presentations, responder chain, etc. This is partially touched on in Use SwiftUI with UIKit as well! https://developer.apple.com/wwdc22/10072 Thank you! Will SwiftUI slowly replace UIKit? I'm surprised that there is still a lot of content focused on UIKit this year. This video is mostly about UIKit code and I was wondering how I can implement them in SwiftUI, like the editor style toolbar? SwiftUI and UIKit are two different layers of abstraction, and one is not supplanting the other. When deciding what to use for your app, you should pick whatever is the best tool for the job. Taylor K also had a great answer to this: https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654645179571169?thread_ts=1654645084.628639&cid=C03HX19UNCQ To add to this, for a lot of developers this means a mix of UIKit and SwiftUI, but it can also mean to start a new app that is written 100% in SwiftUI or UIKit. For the last bit of your question, there will be a couple SwiftUI talks later in the week that go into the new iPad features: https://developer.apple.com/videos/play/wwdc2022/110343/ https://developer.apple.com/videos/play/wwdc2022/10058 Is there any reason why Live Text feature is not supported in iOS Keyboard extensions? Using the API either crashes the extension or just prints error logs, depending on the iOS version. Have you filed a feedback for this yet? It would be helpful to have the crash logs so we can take a look at what happened - if you have a feedback ID I can pass it on to the appropriate people Keep in mind also that the operating system maintains a much stricter memory allowance for keyboard extensions, so it's pretty likely that your extension is being killed by the OS for using too much memory. <@U03HELXCTGV> 100% it didn't. I was testing it on the template project + I have nightmares where I see our keyboard crashing in 2022 with the same OOM crash, using more than 66 Mb :rolling_on_the_floor_laughing::rolling_on_the_floor_laughing::rolling_on_the_floor_laughing: ... so I won't confuse it :innocent: In case you're interested about details of why it affects me and my team so much, the feedback ID is FB10034138 . Would really appreciate for any response on that. <@U03H31CU1C7> sure. FB10022377 There's no system diagnose, but you won't need it, cz there's a test project attached to the ticket. Thanks in advance. For UIEditMenuInteraction, all the examples in the sessions showed APPPENDING items to the menu. Is it possible to have our custom items at the beginning or even mixed in with the 'suggested' ones if we appropriately shuffle them into the returned menu? You can! suggestedActions is just an array, so you can prepend to it, append to it, or just return a UIMenu with an entirely different set of children if you want. What sessions can we watch to start learning more about Live Activities for the iOS home screen? Please schedule a 1-1 lab and we will probably be able to better answer this question. Or come to our QA again tomorrow. When an iPad is connected to a low dpi external monitor, will apps run at a 1x scale factor with 1x assets used? Or will they stay at 2x with some sort of downscaling? 1x external displays will always be reflected as 2x today. So you do not need to worry about providing 1x assets Thanks! Np! For UIMenuElements, if I change the UIMenuElementAttributesHidden attribute while it's visible, will it disappear? Similarly, if I change the title/image, do they update if already visible? No, it won't. The only way to update a visible menu is using UIContextMenuInteraction.updateVisibleMenu() if you're using UIContextMenuInteraction directly, or by setting the button's menu property to a new value if you're using a UIButton . Menus are copied when consumed by the system, so any instance you may be holding onto is no longer directly connected to what's visible. (off to Feedback Assistant) Not sure if this is the right time for this, but... In iOS 16 UIDevice.name has changed to only return the model name (E.g \"iPhone\" instead of \"Matt's internet communicator\"). I think in one of the sessions I heard that there is an entitlement to get the old behaviour, but I can't find any reference to it anywhere. The entitlement is com.apple.developer.device-information.user-assigned-device-name . You can find out more about it in the \u201cWhat\u2019s new in privacy\u201d video. There are some restrictions around what you can do with it (more in that video). In my case it would be on supervised devices, so hopefully not too restrictive What is the behavior of UIDevice.name ? Would it return iPhone 13 Pro or just iPhone if you don't have the entitlement to get the device's name? Thanks! Just \"iPhone\" or \"iPad\". Yeah, it returns the same thing as UIDevice.model. I\u2019m trying to remember. Is there a method that returns the internal model number i.e \u201ciPhone10,6\u201d Off the top of my head, I think the answer for UIDevice and this is no, but I'm checking No, I don't believe that is something UIDevice will return It would be worth filing a feedback report if you need more fine-grained model delineation, with a description of what you intend to use it for. It\u2019s useful for us to hear things like that first-hand and have documented requests to consider. I believe there are lower level unix functions that will return this (perhaps sysctl or uname ) It would be useful for setting up a support request message prefilled with the user's device type (along with app version, etc) Yes my use case matches that of Lowell :slightly_smiling_face: FB5524321 UIDevice API desired to get model name as it appears in Settings 48331816 yeah - I think in the past I used sysctl - can\u2019t remember the key though Searching seems to indicate both uname and sysctl hw.model key might return this, but I haven't verified that at all Might be worth a try at least Seems its via utsname and uname : https://stackoverflow.com/questions/26028918/how-to-determine-the-current-iphone-device-model (Assuming that code still works in iOS 16) Navigation item center groups: does it make sense to swap them completely based on some factor, like object selection state? If so, should one change UINavigationItem.customizationIdentifier then? Any other considerations? It can, and thats a valid use for multiple customization identifiers on a single UINavigationItem. We should continue to properly save & restore state when you update. I\u2019m pretty sure we\u2019ll update things properly regardless of the relative ordering of you setting .centerItemGroups vs .customizationIdentifier, but if you find that not to be the case file a report if you just want to conditionally make functionality available though, you might look at using the hidden property we just added to UIBarButtonItem and UIBarButtonItemGroup Ohh, I missed hidden , seems useful. But I think for completely different modes, using different customization identifiers might make more sense. I will try to implement it and maybe on tomorrows labs (assuming I will get my ticket :wink:) I will share my experience. As a side note: would be great if one could present \u201cvertical\u201d edit menu from UIBarButtonItems , even if initialised via touch. you mean the EditMenuInteraction ? Yes. I\u2019m sure there is a way to initialise it from UIBarButtonItem , but my note was specifically about always using vertical style in such case. I don\u2019t think we\u2019ve really thought that much about it actually, usually you want the edit interaction to be directly on whatever thing is being edited. I think there are ways you might be able to get something useful like that, but I\u2019d want to talk to the engineer that worked on edit menu interaction So, another thing to chat about on labs. Thank you David :slightly_smiling_face: Do you think it's a good idea trying to convert modals to use structured concurrency on iOS? So instead of usual callback the caller would await on result property. Yes! For modal flows that return a result via a delegate, I think using structured concurrency to await the result instead is an interesting approach to try. Do UIMenuElementAttributesKeepsMenuPresented and UIMenuElementSize translate to same/similar behaviour in Mac Catalyst? Since macOS menus are modal, UIMenuElementAttributesKeepsMenuPresented has no effect on the Mac. Small or medium UIMenuElementSize s will produce standard (full width) menu elements since that design paradigm doesn't exist on the Mac. I thought so, but was worth asking :slightly_smiling_face: Not having UIMenuElementAttributesKeepsMenuPresented -like behaviour seems particularly problematic, since it enables great UX on iPad. With no direct Mac behaviour, one must look for completely different solution there. Yeah, that's totally understandable. It's just incompatible with how Mac menus work currently. I am wondering how many of these API are available in a native SwiftUI app? I am taking my iOS app to both iPadOS and macOS in a current redesign, which will require iOS 16/can require macOS 13, but I need to decide between using Catalyst vs. going SwiftUI native. If your writing a pure SwiftUI app, then you should compile it native for each platform. e.g. disabling the traffic light buttons when not applicable. does your SwiftUI app rely on UIKit for something? The question is the opposite, really. It\u2019s currently a UIKit app. I am at the point of needing to decide\u2014do I ship it for macOS with Catalyst, or given I\u2019m doing a major redesign anyway, do I migrate it to be a SwiftUI app. I am wondering\u2014are any of the features/capabilities shown here in deploying as a Catalyst app that are not available on macOS using a native SwiftUI app? This would be a good question for the SwiftUI lounge. OK If you already have a UIKit app that\u2019s working well, proceeding with a Catalyst app would definitely be the quickest path to Mac, and you can always start to mix and match SwiftUI alongside at the same time if you aren\u2019t ready to commit fully. As far as how these properties are exposed in SwiftUI, I\u2019m not sure off the top of my head, but there are additional SwiftUI Q&As that should be able to provide a concrete answer for you We have a CallKit VOIP app that supports iPad (along with iPhone) and we want to use it as a Mac app for Apple silicon Macs. We have two issues: 1) On Mac there is no request for microphone access. The microphone request as it is on iOS/iPadOS is not compatible on macOS? We should add different permission requests according the available platform (iOS vs macOS)? 2) On Mac there is no CallKit UI for the incoming calls? The expectation here is to just run our app and make the app's windows the front window without any CallKit UI view or status bar indicator? Thank you!! PushKit will deliver the VoIP push, and it's delivered to the app, which is launched if needed. But on macOS, CallKit will not provide any UI. It is up to your app to provide its own UI when it receives the payload. I'm not certain about the microphone access question. That might be something best asked in one of the upcoming labs for AVAudioSession, AVKit, or AVFoundation. Which API are you using to attempt microphone access? <@U03J217TL5R>? Oh, I see! So, the launch of the app is what expected on VoIP push reception without UI. Then, we are OK with that! We should only see the microphone access issue\u2026 Hmm, I don't have access in our code right now (out of office time zone here). But, as far as remember we use AVAudioSession and we check if category == PLAY_AND_RECORD in order to check microphone availability\u2026 There are two upcoming AVAudioSession labs: June 9th from 11-2 PDT and June 10th from 1-3 PDT. That would be the best place to get your microphone access question answered. Thank you guys, <@U03HB0DARPG> and <@U03HB0DC1PY> I'll check the labs and I'll be prepared with the exact microphone request code that we use! :wave: I'm trying to hide my UINavigationBar bar with .slide. However, the hiding animation is broken, it disappears in one go. The show animation slides in correctly though. Is this a known issue? I would suspect that your doing something that UIKit isn\u2019t really prepared to manage properly \u2013 can you paste in the code your using? even without an example, my rough guess here would be that if your setting UINavigationController.isNavigationBarHidden we are probably just pulling the navigation bar out of the hierarchy before your animation begins. If you are calling it with animated:true then we may just be doing things that preclude your animation entirely. And if your setting UINavigationBar.isHidden then that won\u2019t be animatable I'm hiding the status bar simultaneously to get a full screen experience. When I remove the status bar animation, the nav bar animates fine. FB8980917 has a sample app. I pulled up the sample from that issue, it seems to be working in iOS 16, have you tried it there? (I haven\u2019t checked on anything older at the moment, so not sure if this is a new change or not) Are there any ways to support bulleted lists (unordered and ordered) via the UITextView? Or what is the preferred approached here. Thanks This wasn\u2019t available in UIKit until this year, but in the betas you\u2019ll see NSTextList (a long-standing feature available only in AppKit until now). Check out this video tomorrow for more: https://developer.apple.com/wwdc22/10090 This is great! Thank you! I'm currently trying to use a UICellAccessory with a custom view in a UICollectionViewListCell, but I get a degenerate layout feedback loop when expanding a list section. Is there something I need to be doing differently? It's possible. Every layout feedback loop is different. I strongly recommend signing up for a lab where an engineer can do some joint debugging with you. If you can't get a lab, please try the developer forums. Meanwhile, I'll see if I can help you a little here. Do you have logs from the layout feedback loop debugger? I have a feedback with sample project for this issue as I believe it is an unexpected behavior. If someone would be willing to take a look it would be much appreciated! FB9207332 Awesome! I love a sample project! Me too! Thanks for your response! <@U03JU8F54C8> I'm not reproducing the LFL on an iPad with your sample app. Of course, I'm working on an unreleased build, and it's possible we have a fix for the issue. On the other hand, maybe this is device specific, and I should be trying this on an iPhone instead? Oh! I'll check the screen recording\u2026 I'll try it on a phone. If we have a fix, I'll be duping your FB report Just opened my sample. It does happen for me on the iPhone sim in Xcode 14 beta 1. We experienced this issue intermittently on the iPad as well but my specific example may not exactly produce it. Were you able to reproduce on the iPhone simulator? Just also reproduced on a test device. Since I'd bet it's not encouraged to post files here, I'll post my debugger output to the feedback. Apologies that it's very long, I think I set the debugging threshold high. Ok. Just attached that file to the feedback. Thanks so much again for taking a look. macOS ExtensionKit question. Does an extension have to be delivered within an app bundle? Is it possible to build a standalone .appex and have it be discoverable by an hosting application? It is not possible to deliver an extension outside of an app bundle. Extensions must be contained in an app bundle in the Extensions directory. Ah hello! Thanks for your help yesterday! Would be awesome to not need to create wrapper apps to deliver extension-only payloads. I\u2019ll file a bug report. Will all of ExtensionKit\u2019s capabilities be available to apps delivered outside the App Store using Developer ID? Happy to help. Yes ExtensionKit\u2019s capabilities are supported apps delivered outside of the App Store. The host app and the extension must be code signed and the extension must be sandboxed. Ah this is great info - particularly about the sandboxing requirements for extensions! I\u2019m just getting started, so perhaps this is a silly question. But, I\u2019ve been playing with the extension Xcode template, and it is helpful it getting an idea of how to structure everything. I find myself doing awkward things to get at the NSXPCConnection and its properties, since it is only available from the AppExtensionConfiguration. I\u2019m just passing it right though back up to the main extension class. Does this sound like it makes sense? Is there any way for my main app to pass security-scoped bookmarks, or other privileged file-access mechanism back down to the extension, or will each extension that needs this kind of thing need to request access from the user itself? ExtensionKit doesn't provide specific API to grant file access to an extension. I'm not knowledgeable enough about security scoped bookmarks to comment at the moment. Ok that\u2019s totally understandable. But, could an extension create an open dialog for gaining file access? Yes, extensions can use NSOpenPannel Ok wonderful! And from my frantic internet searching just now, it does seem possible to share file accesses between apps and XPC services. So, I feel like that covers what I would need. I realize that I\u2019m perhaps monopolizing your time/attention. Please feel free to ignore as needed :smiley: Do you have any thoughts on the best way to communicate the NSXPCConnection from the configuration back up to the extension class? I\u2019m envisioning needing to use the remoteObject and exportedObject from extensions for bidirectional communication with the host app. WRT configuring XPC connections: We recommend that the app that defines the extension point provided a framework to extension developers. This framework should specialize the ExtensionKit protocols. This is a quick example of something such a framework might implement struct ExampleConfiguration&lt;E:ExampleExtension&gt;: AppExtensionConfiguration { let exportedObject: ExportedObject init(_ appExtension: E) { self.exportedObject = ExportedObject(appExtension) } /// Determine whether to accept the XPC connection from the host. func accept(connection: NSXPCConnection) -&gt; Bool { connection.exportedInterface = NSXPCInterface(with: TextTransformerXPCProtocol.self) connection.exportedObject = exportedObject connection.resume() return true } class ExportedObject: NSObject, TextTransformerXPCProtocol { let appExtension: E init(_ appExtension: E) { self.appExtension = appExtension } func transform(text: String, reply: (String) -&gt; Void) { let transformedText = appExtension.transform(text: text) reply(transformedText) } } } /// The AppExtension protocol to which this extension will conform. /// This is typically defined by the extension host in a framework. protocol ExampleExtension : AppExtension { func transform(text: String) -&gt; String } extension ExampleExtension { var configuration: ExampleConfiguration&lt;some ExampleExtension&gt; { // Return your extension's configuration upon request. return ExampleConfiguration(self) } } An extension implementation would look like this: @main final class ExampleAppExtension: ExampleExtension { func transform(text: String) -&gt; String { return text.uppercased() } } ok this makes total sense, and is really helpful. Thank you very much! Is there any way that I could accept-list locally-bundled extensions within the host app? Or, would those always need explicit user approval as well? Oh, and I assume EXAppExtensionBrowserViewController is the only way to approve extensions? Correct so even host-bundled extensions must be given explicit approval? Yes, you can file feedback if this will cause problems. My cursory testing with extensions seems to have a lower first-request latency than a plain XPC service, which would be really cool! Am I right about that? Extensions are launched when the host creates the AppExtensionProcess instance so it is already running by the time you send the first XPC message. If you include the time waiting for the AppExtensionProcess instance to be created I suspect the overall latency will be equivalent. I have not measured this. Ok this makes sense. I was also curious about the .appextensionpoint file. Aside from documentation, which I assume is forthcoming, it was interesting this wasn\u2019t incorporated into the app\u2019s Info.plist. Is there something here than made separate files a better fit? I have an answer for this: > Is there any way for my main app to pass security-scoped bookmarks, or other privileged file-access mechanism back down to the extension, or will each extension that needs this kind of thing need to request access from the user itself? You can expect Security scoped bookmarks to grant the extension access to files that the host app has access to. That is awesome news. Sandboxing was, so far, the thing giving me the most concern. Is the app-bundling requirement for extensions something that might change before Ventura ships? Extensions must be bundled in a container app's bundle. Beyond that I cant comment on the future direction of the platform. I understand. Thank you so much for your time, you\u2019ve been immensely helpful. I\u2019m incredibly excited about these APIs, and cannot wait to do some real work with them. Glad to help. I'm excited to see what people do with these APIs. The new self-sizing UITableView and UICollectionView cells look fantastic. Is there anything similar for table view headers and footers? Is there a recommended technique to have headers and footers resize to accomodate dynamic type for example? The new selfSizingInvalidation functionality, including manually calling invalidateIntrinsicContentSize yourself, as well as the automatic .enabledIncludingConstraints mode for Auto Layout, work in UITableViewHeaderFooterView too! The feature is not just limited to cells :smiley: (And for UICollectionView, this includes self-sizing supplementary views too.) Thank you, that\u2019s great news! I saw that the San Francisco font has new compressed, condensed and expanded style widths. However I do I access these styles with UIFont or UIFontDescriptor? You can use UIFontDescriptor.TraitKey.width with a float value. We know that some suggested constants for the various \u2018standard\u2019 widths would be helpful - like the constants for \u2018standard\u2019 widths for the use with UIFontDescriptor.TraitKey.weight , so keep an eye out for them in future betas. Great, thanks. Are the float values for the width described anywhere? https://developer.apple.com/documentation/uikit/uifontwidthtrait?language=objc \u2026or probably more useful, https://developer.apple.com/documentation/uikit/uifontdescriptor/traitkey/1616684-width > The valid value range is from -1.0 to 1.0 . The value of 0.0 corresponds to the regular glyph spacing. Would you be able to share some insight on how the various Collection Views are implemented within the Calendar app? The paging that goes on endlessly is something I have been trying to recreate for a while but it's tricky. Any hints would be greatly appreciated :) You can achieve endless scrolling by essentially adding and removing sections while scrolling and making sure to adjust the content offset when doing so. UIScrollView (and thus UICollectionView ) will keep its inertia and make it look seamless. If you have specific use cases this might be a great thing to discuss in a 1on1 lab. Thanks, i\u2019ll try applying for a lab again :crossed_fingers: iOS 16 offers a great new API addition in the way of UIHostingConfiguration for rendering SwiftUI views in List CollectionView cells, however we\u2019d love to support this type of interop on iOS 15 too. \u2028When using UIHostingControllers in UICollectionView list cells, is there any specific considerations that should be taken into account in addition to managing cell reuse issues? Unfortunately there isn\u2019t an officially supported way to use SwiftUI inside of UIKit cells prior to iOS 16. Embedding a UIHostingController inside of cells is not recommended. Just to be clear, it's not recommended due to performance issues, right? <@U03HW7U6QF3> This is great to know, thank you! Is there any further light you can shed on why? As Lior comments, I am guessing due to performance overhead or reuse pitfalls? i love this new feature!!!! While performance is certainly a concern, the main issue is really around the nature of needing to embed a view controller (the UIHostingController) in each cell. Generally speaking, putting view controllers inside cells is discouraged in UIKit. This is because of how cell reuse works, as well as the way information propagates through the view controller and view hierarchy in UIKit. So using UIHostingController in cells creates problems and can result in some surprising bugs. (A common example we\u2019ve seen are issues with safe area information propagating into SwiftUI.) These are all things that the new UIHostingConfiguration API addresses! TIL Makes complete sense. So glad to see the new UIHostingConfiguration API and we'll be eager to adopt it once our minimum version is 16! Thanks to you and team for your hard work. Despite that advice not to use UIHostingController in cells, provided that you are using the supported UIHostingConfiguration API starting with iOS 16, if you are able to get something working with UIHostingController on previous iOS versions and you have tested it well enough yourself that you know it works and meets your needs, you can probably get away with that because UIKit & SwiftUI aren\u2019t changing anymore on those older iOS versions. And since you\u2019re using UIHostingConfiguration going forward, you won\u2019t run into any new issues with the unsupported UIHostingController implementation in the future. But again, this is not officially supported, so you are on your own, and you need to thoroughly test to make sure you can find an implementation that works well enough. <@U03HW7U6QF3> would these concerns also apply to general usage of UIHostingControllers for leaf node views, like buttons? VC pattern would generally not be used there in pure UIKit world, and UIHostingControllers force us into it. Are there perf concerns? Is leaf node SwiftUI in UIKit discouraged, or is it okay if we properly add the VCs in the VC hierarchy? Not really, the specific issues mentioned above are really about having view controllers inside cells (inside of a UIScrollView), due to cell reuse and scrolling performance. Don't tempt me too much Tyler! I seem to have a knack for finding myself in uncharted territory, and will occasionally go to great lengths to avoid making a xib or writing constraints! :joy: Thanks <@U03HW7U6QF3>! I will misuse this thread to once again request a UIHostingView for leaf node SwiftUI views in UIKit :smile: FB10019256 Feedback requests are greatly appreciated :pray: Be sure to check out the video \u201cUse SwiftUI with UIKit\u201d when it goes live tomorrow as well! https://developer.apple.com/videos/play/wwdc2022/10072/ What happens to the display scale when connecting to external displays of different resolutions? Thanks! 1x displays will be reflected as 2x when connected to the iPad Cool. What about 6k displays, are those still 2x? I believe so, but will check I don't know if it's okay to jump in here, and I may not be understanding the question. But, for non-M1 iPads, external displays are treated as 1x displays in my experience (I get a UIScreen that is 3840 x 2160 with a scale of 1x). I brought this up in my lab meeting today with Owen and I've filed a feedback report on it. I believe that is expected And there should be no change there between iOS 15 and iOS 16 OK - then I didn't understand the answer you had where 1x displays are reflected as 2x. You were talking with Owen - was this a question about Catalyst? Because that's very different. It was in the catalyst lab yes, but I'm running my catalyst app on iPad. My answer assumed the question was related to Stage Manager, I should have clarified Sorry, let me rephrase then. On iPadOS, when using stage mode on an external display, what does the system return to my app as the displayScale in UITraitCollection ? E.g. if I read it from my UIWindowScene \u2019s rootViewController ? 6k will come across as 2x, 1x displays will come across as 2x, and we don't support anything greater than 6k at the moment, so you should see 2x in this case thanks! np! Is there a way to make a palette window in a Catalyst app (like a Photoshop palette - it gets events, but it's not a true document window), and it has a smaller title bar / HUD appearance? This is not currently possible. We would appreciate a feedback request detailing your use case though! Here it is:FB10114581 In the past I\u2019ve tried incorporating UITableView / UICollectionView into a SwiftUI app using UIViewRepresentable (for the advanced drag and drop features). After experiencing some bugginess with the table\u2019s cell reuse, I was advised by an Apple engineer to build a UIKit or AppKit app. A year later, with this latest iteration of SwiftUI, do Apple engineers stand by that guidance or has SwiftUI become more hospitable to complex UIKit elements like UITableView and UICollectionView? For Apple\u2019s apps that are comprised mostly of SwiftUI (like Shortcuts), I\u2019m curious if you could share which framework is used for the life cycle (foundation). Thanks for all the great work! Hi, we have a couple of sessions coming out tomorrow that will talk about this in some more detail: https://developer.apple.com/videos/play/wwdc2022/10072/|Use SwiftUI with UIKit and https://developer.apple.com/videos/play/wwdc2022/10075/|Use SwiftUI with AppKit . In addition, we also have a Q&A for SwiftUI & UIFrameworks tomorrow morning at 8 AM PDT. I\u2019d encourage you to ask again there, as well as sign up for a SwiftUI Lab to discuss specific use cases! Will do, thank you!! Documentation mentions UITextView in iOS16 uses TextKit 2. I\u2019ve managed to disable word wrap via textContainer.widthTracksTextView = false and setting the textContainer.size to a value wider than the view\u2019s bounds, but looks like UITextView still doesn\u2019t support horizontal scrolling, nor it tracks the cursor. Is that something that\u2019s expected to work, or not yet? If I\u2019m understanding you correctly, I don\u2019t believe that this is intended to be supported. Did it used to work for you with TextKit 1? No, but I was under the impression that the new TextKit engine is going to support it. I suspect you have good reasons why you\u2019re not doing this, but if you need a one-line text input (no wrapping, scrolls) UITextField is your class. You could likely set up a UITextView to not scroll and be at your desired width, then place it inside a UIScrollView - but you would not get any sort of autoscrolling. \u2026but you could scroll yourself by setting content offset when the text view\u2019s reports to its delegate that the selection has changed. (Obviously your question is still rolling around in my head :slightly_smiling_face:) If this is something you really need and are unable to accomplish reasonably, feedback is always appreciated ( https://developer.apple.com/bug-reporting/ ). I\u2019d like the content to overflow beyond the view\u2019s bounds, without wrapping text, which is preferred in plain text (fixed-width font) editors (code editor, markdown, etc). Think of it as a soft-wrap feature, where the UITextView on an iPhone screen or a windowed iPad app is actually narrower and doesn\u2019t fit the 80-column text content. I noticed that the NSTextContainer allows for a wider content than the viewport\u2019s size, but the UITextView doesn\u2019t seem to implement auto-scrolling, as the caret moves beyond the text view\u2019s visible area. Makes sense. You could try implementing UITextViewDelegate.textViewDidChangeSelection(_:) and calling setContentOffset with the result of something calculated from caretRectForPosition: or the like (note that UITextView implements all of UITextInput , which has a bunch of methods about geometry). But you\u2019re definitely doing something that is not in the designed-for use cases. Thanks for the suggestion, I just tried it, but it seems that the widthTracksTextView is not supported on iOS/iPadOS. As soon as the UITextView \u2019s frame changes its size, the textCointainer \u2019s size is set and the lines are wrapped. I also filed the suggestion: FB10115560 Ah - yeah, you\u2019re right. We override those based on the \u2019scrollEnabled ' state. I will return to my \u2018non-scrollable text view inside a scroll view\u2019 suggestion then. But I realize it\u2019s clunky. Thanks for filing the feedback! Like 'func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator)` there is no callback methods to track info-pannel visibility. Whats the best way to identify when info-pannel appears and disappears? This is a better question for AVKit. It looks like there's an AVKit lab at 3 PM on Thursday, June 9th: https://developer.apple.com/wwdc22/110548 Noted thanks! np! I have a view controller that has multiple UITextFields. When I\u2019m in one of those text fields and press tab on an iPad, it focuses on subsequent text fields. However, when I run via Catalyst on macOS, tabbing while in a text field inserts a tab character instead of focusing on the next text field. Do you know how to enable tabbing to focus on the next text field? This was a bug that should be fixed in macOS Ventura Ok awesome, thanks! Is there a known workaround for this issue in macOS Big Sur? <@U03HELWUJN9> unfortunately there is not but you should check on latest betas. Thanks! Does Continuity Camera support a file promise? Previously only the data itself was returned with kPasteboardTypeFilePromiseContent . NSFilePromiseReceiver should handle this type of pasteboard data as a file promise. how are you trying to call in the file promise? <@U03HL05FX6Y> Sorry for the wait, lab. We were doing it manually with the older APIs We had not yet adopted NSFilePromiseProvider/Receiver Our code dates to 2019 NSFilePromiseReceiver is pretty nice because it's compatible with any code using the other techniques At the time we implemented Continuity Camera the file promise was never \"kept\", only extracting the data in-process worked And we use file promises in other places, we're familiar with the technique Try it out with NSFilePromiseReceiver. If it's still failing, file a feedback report with a small sample project. ok Hi, I\u2019m currently trying to update my Apple Watch app to support watchOS 9 however the VideoPlayer in SwiftUI is crashing when playing a video and Xcode prints something to do with volume HUD class name changing and just wondering if this is something that\u2019s changed with watchOS 9 or it\u2019s something that I need to update in my code somewhere. Can you ask in the <#C03GSLANZJT|> or the <#C03HX19UNCQ|>? Also, if you\u2019re able to provide them the crash log, that\u2019ll help investigate the issue! Yep I can do both, I'll make sure to be on my Mac too so I can get the log for them. Thanks both :) Hi, In my CarPlay app, In templateApplicationScene(_:didConnect:) I get the provided interfaceController and store a reference to it, but most of the times when I try to access interfaceController.carTraitCollection The app crashes because carTraitCollection is Uninitialized , so it makes creating image for different display scale (2x, 3x) or different display styles (dark, light) impossible. I tried to make sure that I always access it from main thread but still have the same problem. This is a better question for someone on the CarPlay team. There's a Q&A Digital Lounge on Friday at 1 PM Pacific time. Also, probably worth filing a feedback on it in case it isn't being tracked anywhere! Looks like we do have feedback tracking this and the CarPlay team is looking into it. With UIActivityViewController, is it possible to provide multiple representations of the same object (a URL and text, e.g., through an NSItemProvider) as well as LPLinkMetadata? UIActivityItemsConfigurationReading is the only way I've gotten the NSItemProvider to work, but that doesn't support link metadata. And UIActivityItemSource supports the metadata, but not the item provider. You can provide link metadata using UIActivityItemsConfigurationReading by implementing the activityItemsConfigurationMetadata(key:) function, and watching for the linkPresentationMetadata key. Remember also that you have the option of using the prebuilt UIActivityItemsConfiguration class, which has properties that you can set for these metadata. When I return LPLinkMetadata from the UIActivityItemsConfiguration.metadataProvider closure, it doesn't use the image, title, or subtitle in the metadata Strike that, the metadataProvider closure is never even called with the linkPresentationMetadata key <@U03H31CQZ0F> I've filed FB10116030 about this Thank you. I\u2019ll take a look. In Catalyst, is there a way to only show the preview for a link when dragging a link in a UITextView instead of the whole view? The associated UIDragInteractionDelegate methods don't seem to work. Can you elaborate a bit more on your setup and what you're seeing? Are you installing your own UIDragInteraction on the UITextView ? Are you seeing different behavior on iOS vs macOS? If it's difficult to describe in Slack, I encourage you to sign up for one of the UIKit labs tomorrow or Friday. Yes, it's a custom UIDragInteraction and it works correctly on iOS If you drag on iOS, only the area around the link is highlighted, but if you drag on Catalyst, the entire view is highlighted Is there a reason you couldn't rely on the text view's built in drag interaction and customize the behavior via UITextDragDelegate ? I think it's because my text view has isEditable and isSelectable set to false. But also using the UITextDragDelegate methods doesn't work either. With the new self-sizing cells, is there a way to run an animation alongside a cell resize animation (or at least, adding a custom completion handler to the system-provided resize animation)? With the new selfSizingInvalidation feature (aka \u201cself-resizing cells\u201d), when a cell is resized with animation, the cell will also receive a full layout pass as part of the same animation. This means that any layout changes to subviews inside the cell\u2019s contentView will also animate by default alongside the resizing of the cell itself. There isn\u2019t an explicit way to manually attach animations to that resize animation when it\u2019s initiated directly by a cell via invalidateIntrinsicContentSize \u2014 do you have something specific in mind you\u2019re trying to do? You can of course continue to use the apply method on diffable data source (re-applying the current snapshot), or performBatchUpdates on the collection/table view directly if you are not using a diffable data source, to resize visible cells as you would prior to iOS 16. And both of those APIs do provide a completion handler you can use. I've got a text view that I'm hiding/showing that I want to appear to progressively expand as the cell grows. I've had some trouble doing this previously with the old beginUpdates/endUpdates, but it sounds like it should be doable with the self-resizing cells stuff Relatedly, the text view is in a stack view and the cell isn't automatically detecting the change when isHidden is changed on the text view (which should, AIUI, cause the stack view to change its internal constraints which should count as an auto layout change). I have to call invalidateIntrinsicContentSize myself to get the size to update. Could that be something on my end, or is it more likely a framework bug? > I\u2019ve got a text view that I\u2019m hiding/showing that I want to appear to progressively expand as the cell grows. I\u2019ve had some trouble doing this previously with the old beginUpdates/endUpdates, but it sounds like it should be doable with the self-resizing cells stuff The new self-resizing cells functionality does work great with multiline UITextView ( scrollEnabled = false ) inside of the cell contentView , when you want the cell to size-to-fit the text view as more text is added! I\u2019m still not sure what you had in mind around adding alongside animations or completions though. > Relatedly, the text view is in a stack view and the cell isn\u2019t automatically detecting the change when isHidden is changed on the text view (which should, AIUI, cause the stack view to change its internal constraints which should count as an auto layout change). I have to call invalidateIntrinsicContentSize myself to get the size to update. Could that be something on my end, or is it more likely a framework bug? If you have an easy place to call invalidateIntrinsicContentSize yourself, that\u2019s actually the best option in general combined with the default selfSizingInvalidation = .enabled . The .enabledIncludingConstraints mode can be more expensive, as it requires checking for size changes on any Auto Layout constraint changes. But based on your description this does sound like something that is supposed to work automatically when you have set .enabledIncludingConstraints , so if you don\u2019t see any obvious issues on your end, it would be great if you could submit a Feedback report with a small sample project attached for us to take a look at! https://feedbackassistant.apple.com It's not adding/removing text from the view that I'm doing, but hiding it wholesale. And since it's in a stack view, it was getting removed from the view hierarchy altogether\u2014without being animated, so I thought I'd need a custom animation to cover that up. Now I'm thinking it would work better to add a constraint setting the height to 0 to \"hide\" it without removing it, and thus letting it animate I can stick to calling that method myself, it's pretty easy to do, I was just hoping for a little magic, lol. I'll try to drum up a sample project for a feedback that repros the issue, though Gotcha \u2014 and yes a sample project would be great! \u201c\u2026when a cell is resized with animation, the cell will also receive a full layout pass as part of the same animation.\u201d Is this true for updates via performBatchUpdates as well? My recollection is that there were issues around the cell view instance getting swapped out which would end up killing the animation within the cell. Are cell views always preserved across empty batch updates, or am I misremembering? > My recollection is that there were issues around the cell view instance getting swapped out which would end up killing the animation within the cell. Are cell views always preserved across empty batch updates, or am I misremembering? For empty batch updates, as well as the new iOS 16 self-sizing invalidation feature, the existing cells won\u2019t be replaced or reused. Only when using the reload API to reload an item/section will the cell get replaced with a new one. Thanks Tyler! (Don\u2019t forget about the reconfigure API introduced in iOS 15 to update cells without replacing them, too!) Is there a way to get a notification when a new item (e.g. image) is added to UIPasteboard remotely (e.g. through Universal Clipboard). Currently I can poll changeCount to detect changes, but I receive no changedNotification unless the copy originated from the app itself. I'm trying to implement a feature where the app conveniently offers a paste button when there's something on the UIPasteboard. That would've been handy. We do the same but with users' texts, suggesting them a button that Creates document right from the pasteboard. But it probably doesn't work with universal clipboard. I'm not sure if it works for us in the iPad split view, but I assume we check pasteboard everytime our app becomes active. However, AFAIK, it is not possible with universal clipboard We generally discourage polling the pasteboard in any way, it can be resource-intensive. However, feel free to request a public notification for when the pasteboard changes, you can check for item count + metadata w/o triggering the paste alert. The new paste button does enable/disable itself when the something appears on the pasteboard though. Hmm, is it customizable? Would be grateful for a link on the session about that ) \u201cHowever, feel free to request a public notification for when the pasteboard change\u201d. Is there an API to do this? Did you mean I should file a feedback? Also, there seems to be an issue where the UIPasteboard.general.changeCount gets incremented by two every time I start editing a text view (without copying anything) https://developer.apple.com/wwdc22/10068?time=683 > Did you mean I should file a feedback? Yes. \u201cyou can check for item count + metadata w/o triggering the paste alert.\u201d When you said metadata, are there any existing ones I can look at (to detect duplicates) without showing the alert? Re: changeCount , we don't guarantee that it will increment +1 each time. Multiple processes etc. may end up bumping it up more. I see metadata = types of items. So e.g. -[UIPasteboard hasStrings] is a metadata query, but -[UIPasteboard strings] is a data query. So, assuming I want to detect when a new image is pasted elsewhere\u2026 hasStrings will not change (already true), and changeCount is an unreliable way of detecting new ones. So I\u2019m basically out of luck\u2026 You can also run pattern detection w/o triggering the paste alert, so e.g. looking for URL's and addresses via detectPatternsForPatterns:completionHandler: . This is also considered metadata and won't trigger the alert. changeCount will get incremented with a new image \u201c changeCount will get incremented with a new image\u201d. Yes, I\u2019m observing that. However since changeCount gets incremented no matter what the type of the content is, it seems like I cannot get a image-specific changeCount , is that correct? You can check if the data is an image and the change count is incremented. You\u2019re right! I have a bug in my code :man-facepalming:. Thanks! Does SwiftUI support the new toolbar modes for iPad or do you need to use UIKit? It does! Check out the toolbar role API: https://developer.apple.com/documentation/swiftui/view/toolbarrole(_:) And the secondary action toolbar item placement: https://developer.apple.com/documentation/swiftui/toolbaritemplacement/secondaryaction For more information, I go into these APIs in the 2 part of SwiftUI on iPad series: https://developer.apple.com/videos/play/wwdc2022/10058 https://developer.apple.com/videos/play/wwdc2022/10058 Thank you so much! Hello, should we approach SwiftUI views as data models for real views? Where should we place data formatting logic? If SwiftUI view is a data model we can data formatting logic there? How can we test our data formatting logic in this case? The most convenient way to unit test your data formatting logic (like a Foundation FormatStyle) will be to factor the formatter out and assert against the String it produces I generally wouldn't consider a SwiftUI view to be a data model Factoring out the formatter might take the form of lifting into a model or view model Or it might just take the form of specifying it in a computed property on the View and running the assertion against that So, we can use property testing to assert data formatting directly inside SwiftUI Views. Is it good practice? How do you deal with this problem at Apple in big projects? Unit testing is a bit of an art and so I wouldn't say it's a necessarily a good or bad practice to assert directly against the SwiftUI view. I think the concerns are more pragmatic: how easy is it to assert against the SwiftUI view vs against a model. Does one way or the other require extra boilerplate? Does one way or the other lead to fewer false negatives? Thanks Kyle for your answers :raised_hands: Does this new APIs are backward comaptible with UIKit project or just only iOS 16? In general all APIs that are introduced in new versions of our operating systems always require those versions to be used. However you can conditionalize your code to only use those new features when running on that OS and have your app back deploy to old versions without those features. Ok got it, but it might double the implementation and maintenance That, unfortunately, is a technical limitation. These features are implemented in frameworks that are shipping with the OS and that are heavily integrated with the OS, so they require the new OS to work. In a lot of cases you can just add the new features when running on the new OS and just do nothing on older versions, in which case it is enough to just check the current version you are running at run time before calling out to the new APIs. Ok got it thanks, I think we might need to wait until next two years lol :smile: What\u2019s the naming convention you\u2019d recommend using for a SwiftUI view - with or without View suffix? I think I\u2019ve seen both in Apple examples. A good rule of thumb is that if the name we are picking has a clear visual representation, say Text , Image , Toggle , we omit the view suffix. In cases like ProgressView where Progress really would feel more like the data itself than its visual representation we add the View suffix. Specifically in the case of Progress we would clash with Foundation.Progress which will require developer to always fully qualify the type name which is not ideal. How does one ask a new question in this channel? I don't see a way to do it. That makes a lot of sense, thank you Luca! Clint \u2013 click the + in the bottom left and select \u201cAsk a question\u201d: Thanks for clarifying--the message next to that plus button says \"only certain people can post\" so that threw me off Once again, here goes the question about SwiftUI views in Table/Collection cell. :grin: If we'd like to tinker with our own UIHostingConfiguration to support older OS versions, what would be the recommended way to update cell's height when used with DiffableDataSource? Currently, the most common scenario I see is a pair of tableView.beginUpdates() tableView.endUpdates() It would be nice to recap on our available options. On iOS 15 and earlier, you request an update to the size of self-sizing cells in UICollectionView and UITableView by: \u2022 If using diffable data source, re-applying the diffable data source\u2019s current snapshot with animatingDifferences: true \u2022 If not using diffable data source, performing empty batch updates on the collection or table view directly (that\u2019s the same as the begin/end updates you mentioned) Got it, thanks! Is there a proper way to implement dynamic menu items on macOS with CommandMenu? (Menu items that change with keyboard modifier keys.) I'm currently polling on a timer, but that is... unsatisfying. In AppKit, this was done using the isAlternate property. (FB9406583) There is not a way to do this at the moment. Thanks for filing (and referencing) the feedback, we will route it to the appropriate component! We've experiencing a watchdog termination in the background due to a function inside UIKit sleeping. Please see the following backtrace: Thread 0 Crashed: 0 libsystem_kernel.dylib 0x00000002065bba2c __semwait_signal + 8 1 libsystem_c.dylib 0x00000001d9c700e4 nanosleep + 220 (nanosleep.c:104) 2 libsystem_c.dylib 0x00000001d9c70e14 usleep + 68 (usleep.c:52) 3 QuartzCore 0x00000001d292bc84 CABackingStoreCollectBlocking + 264 (x-misc.cpp:186) 4 UIKitCore 0x00000001d1184778 __35-[UIWindowScene _prepareForSuspend]_block_invoke + 60 (UIWindowScene.m:1273) 5 UIKitCore 0x00000001d1243b90 -[_UIContextBinder purgeContextsWithPurgeAction:afterPurgeAction:] + 472 (_UIContextBinder.m:221) 6 UIKitCore 0x00000001d12874ec -[UIWindowScene _prepareForSuspend] + 88 (UIWindowScene.m:1271) 7 UIKitCore 0x00000001d1ec935c __130-[UIApplication _updateStateRestorationArchiveForBackgroundEvent:saveState:exitIfCouldNotRestoreState:updateSnapshot:windowScene:]_block_invoke_2 + 228 (UIApplication.m:11237) 8 UIKitCore 0x00000001d128e4dc -[_UIAfterCACommitBlock run] + 72 (_UIAfterCACommitQueue.m:137) 9 UIKitCore 0x00000001d11a1864 -[_UIAfterCACommitQueue flush] + 192 (_UIAfterCACommitQueue.m:228) 10 libdispatch.dylib 0x00000001ce7eae6c _dispatch_call_block_and_release + 32 (init.c:1517) 11 libdispatch.dylib 0x00000001ce7eca30 _dispatch_client_callout + 20 (object.m:560) 12 libdispatch.dylib 0x00000001ce7faf48 _dispatch_main_queue_drain + 928 (inline_internal.h:2622) 13 libdispatch.dylib 0x00000001ce7fab98 _dispatch_main_queue_callback_4CF + 44 (queue.c:7770) 14 CoreFoundation 0x00000001ceb3d800 __CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__ + 16 (CFRunLoop.c:1795) 15 CoreFoundation 0x00000001ceaf7704 __CFRunLoopRun + 2532 (CFRunLoop.c:3144) 16 CoreFoundation 0x00000001ceb0abc8 CFRunLoopRunSpecific + 600 (CFRunLoop.c:3268) 17 GraphicsServices 0x00000001eac3e374 GSEventRunModal + 164 (GSEvent.c:2200) 18 UIKitCore 0x00000001d147a648 -[UIApplication _run] + 1100 (UIApplication.m:3511) 19 UIKitCore 0x00000001d11fbd90 UIApplicationMain + 364 (UIApplication.m:5064) 20 [REDACTED] 0x0000000104e87460 main + 112 (main.m:27) 21 dyld 0x00000001055e5ce4 start + 520 (dyldMain.cpp:879) Hi there! This looks like a potentially issue in CoreAnimation. If you haven't already, can you file a feedback with the entire crash log? Sure thing. Thanks! :slightly_smiling_face: Thank you! Apps generally have around 5 seconds to stop processing when they\u2019re backgrounded. If an app is still \u2018alive\u2019 and doing things at that point, it will be terminated to preserve system responsiveness. In this 5 seconds, we usually reconfigure your apps interface multiple times (rotating, switching to light/dark mode etc.) to take screenshots to use in the application switcher UI. What might be happening here is that your app is taking a long time to lay out for these screenshots. It may be finishing in just under five seconds - then this system code (which frees up memory used by CA that\u2019s not necessary while your app has nothing on screen) runs - and during that we hit the timeout. If your crash trace has a \u201ctermination reason\u201d that mentions \u201cwatchdog\u201d that\u2019s an indication that this might be happening. Thanks for reading my novel. <@U03HELXN8CV> That's super interesting! I never considered how much work goes into creating screenshots for the app switcher My doubt is regarding drag and drop on SwiftUI I think there are 2 approaches but I am stuck with both the approaches WWDC22 - When using the new draggable , dropDestination , Transferable API, I am only able to drag single items. I am unable to drag multiple items in a list. I have filed a feedback FB10128110 WWDC21 - I have faced a couple of issues for drag and drop introduced in WWDC21 (onDrag, onDrop, itemIdentifier), the Feedback ids are FB9854301, FB9854569, FB9855245, FB9855532, FB9855567, FB9855575. Note: All feedbacks include a sample project with detail steps and some even have screenshots and videos Please let me know me if my approach is wrong or if I am missing something. Unfortunately I didn't manage to get a SwiftUI lab session (got declined), so any assistance on the above would be much appreciated Hi <@U03JCHKCDB4>! First, thank you for filing the radars! Getting feedback on the new APIs and knowing which enhancements are anticipated is important. For now, I have one pointer that could be useful: for Lists and ForEach, we expect you to use the onInsert modifier. onDrag is designed for other types of views. FYI, onInsert on a Table with 0 rows did not work last year (FB9265795) on macOS, it seems the problem persists this year with macOS. Haven\u2019t test it in iOS, which now supports Tables too. > Thanks for the response, I am a bit confused here, so when should I use draggable ? Did you mean draggable instead of onDrag ? Yeah, I was talking about last year APIs. draggable is new this year, analogous to onDrag and making use of Transferable 1. Ok that clarifies one part, please bear with me as I slowly understand this. So draggable is the new approach however does it only support simple cases now (like single item drag for non-list and non-ForEach views)? 2. Also using draggable I noticed there was no way to generate preview , or specify the item is to be moved (no green + circle) instead of copied 3. So would slowly draggable grow powerful that would one day support all cases instead of ondrag ? <@U03HW7P2WTB> 4. Also I would be really over the moon, if someone could have a look at the above posted Feedback IDs, as it would help me ship my app which is a bit stuck. 1. Yes. 2. This is correct as well 3. Can\u2019t comment on that. 4. Sure! Can\u2019t promise anything, but thanks for bringing them up <@U03HW7P2WTB> \u2022 Thanks a lot!!! Fingers crossed \u2022 So in the meantime I think I will continue to use the older APIs ( onDrag / onInsert / onDrop / itemProvider ) \u2022 I however I am excited about the new APIs with Transferable looks pretty cool, fingers crossed again one day I get to use them. \u2022 Thanks for patiently answering all the questions, really appreciate it <@U03HVFXKB5L> I am a huge fan of your blog posts and app, has helped me tremendously including for drag and drop, most of my above questions were on the mac. Thanks a lot!!! Thanks <@U03JCHKCDB4>, it seems I have a lot of work ahead to update both the app and the blog :wink: So excited with some of the additions this year, specially Layout and NavigationPath. <@U03HW7P2WTB> Thank you so much for helping me get the lab appointment, really appreciate it, made my day!!! You guys have no clue how excited i was \u2026 I am really grateful to you guys for answering all my questions. I am so sorry i forgot note down his name \u2026 I have been watching so many videos to get his name to tag him on this post. Please let him know as well Sure! Thank you as well for filing the feedback and asking great questions! It was nice meeting you today. Are there any major limitations to be aware of with the new UIHostingConfiguration for collection view cells using SwiftUI, that may not be addressed in the session video? We do try to address the majority of the limitations in the video! Theres a spot at the end where we talk about how UIViewControllerRepresentable will not work there. The video also covers considerations around data flow. https://developer.apple.com/wwdc22/10072 Ok, still waiting for the session to become available. Thanks :grin: I was wondering if we can use the SwiftUI View protocol with the new 'any' keyword? And if it'll be helpful or a good solution in some example use-case? any View does not itself conform to View , so is not a replacement for AnyView today. Also any View is only defining an existential container that you can use in a type signature but you can\u2019t instantiate that type. There might be use cases for using any View to store that into collection, for example: [any View] . But most of the time, if you\u2019re in the situation where you think you need [AnyView] or [any View] , what you should likely do is invert the view dependency flow and have [AnyDataModel] or [any DataModel] instead, then create your views based on the type of data provided at runtime. Great, thank you for the detailed explanation! You are welcome! Was your question based on a concrete problem you are trying to solve? No, I just watched the Embrace Generics session today and this question immediately popped to mind :slightly_smiling_face: We have compositional layout with CollectionView and content is coming from remote including image so based on the image height we adjust the height of the cells, so until we determine the height of the image we have ideal height, but the problem is user can keep scrolling down while images are getting downloaded cells are drawn with ideal height but when image get downloaded we fix the height for the cell during which we see collection view jitters/jump up down due to content size changes Any ways to improve this? Here is the video Hi Pavan, there are a few different strategies you can take here, depending on how you\u2019re updating your cells. What you\u2019ll want to do is to apply a content offset to the scroll view that is the inverse of the increase in the size from cells above the currently visible area. This would need to be performed synchronously, however. The details here depend on the specifics of your layout definition, though. Another alternative is to use .estimated for your item definitions, Then, if their size changes, UICollectionView will automatically adjust the content offset to preserve the semantic scroll position. Hi Aditya, yes we are using estimated height and the above results with that, so what we are doing once we get actual size we are reloading the section to determine the actual height but during which content size changes hence collection view jumps I didn\u2019t understand the inversion logic you said, sorry Ah, reloading the section. That is unfortunately a known issue with UICollectionView currently, where we don\u2019t preserve the scroll position when items/sections are reloaded. I see, do you think this could be solved in SwiftUI using ScrollView + LazyVStack? Any other recommendation is to improve this UI will be great help since this just works fine in android where our customers complaints for iOS :disappointed: Have you tried reconfiguring the individual items instead? Use either the reconfigure method on diffable data source, https://developer.apple.com/documentation/uikit/nsdiffabledatasourcesnapshot/3804468-reconfigureitems or UICollectionView.reconfigureItems(at:) : https://developer.apple.com/documentation/uikit/uicollectionview/3801889-reconfigureitems reloading performs a delete + insert hmm i see, but our app supports from iOS 13 but this API looks like from iOS 15.0 unfortunately Prior to iOS 15, you can update the contents of the cells directly (by fetching them from UICollectionView.cellForItem(at:) , and call performBatchUpdates: with an empty block to update their visible sizes. Cells that are off screen, or not currently prepared for display (like prefetched cells) will return nil from cellForItem(at:) , and you can just specify the full-size image when they\u2019re dequeued the next time. > Cells that are off screen, or not currently prepared for display (like prefetched cells) will return nil from cellForItem(at:) , and you can just specify the full-size image when they\u2019re dequeued the next time. Do i need to return nil for this specific case? > Prior to iOS 15, you can update the contents of the cells directly (by fetching them from UICollectionView.cellForItem(at:) , and call performBatchUpdates: with an empty block to update So with this approach will collection view preserves the scroll offset? No, you\u2019d be calling cellForItem(at:) on the UICollectionView, so it will return nil if a prepared cell does not currently exist. > So with this approach will collection view preserves the scroll offset? Yes! That should preserve the scroll offset. aha it\u2019s just like below? ```psuedo code // loop over cells let cell = UICollectionView.cellForItem(at:...) cell.image.height = xxx // Call performBatchUpdates collectionView.performBatchUpdates { <empty> } yep! That should work! Definitely encourage you to try it out. Ok thanks a lot for the great tips let me give a try :slightly_smiling_face: Just curious to know, does this issue can be solved if we adapt SwiftUI from iOS 14.0? Also, an alternative approach to all of this would be\u2013if you had control over the server\u2013for the server to tell you what the size of the image is before-hand. If that\u2019s not possible, you could also try reading the first few bytes of the image data to get the size. unfortunately that would be hard to get the sizes from the server since we don\u2019t control the data instead merchants does in our platform, but anyways we could give a try with suggested approach first and see Fair. Feel free to file a feedback if you run into issues, perhaps even try to get a lab appointment. Yes i got one for tomorrow but will give a shot on this, thanks --- > #### Hello! How can I get started with UIKit? I currently know SwiftUI only (which, in my opinion, is amazing btw!), but I'd like to learn a little bit of UIKit too, because I might use it in some cases too in the future. Thanks for asking this question. It\u2019s always good to know both frameworks so you can leverage the power of both and choose whatever is best for the problem you are solving. There are great, brand new documentations this year for both SwiftUI and UIKit. You can find a guide to get started here: <https://developer.apple.com/tutorials/app-dev-training#uikit-essentials> Thank you very much! Really appreciate it. We also have a new sample app this year that we are using for the Desktop Class App talks, which is a very feature rich app that I would recommend checking out here: <https://developer.apple.com/documentation/uikit/app_and_environment/building_a_desktop-class_ipad_app> Thank you! Aside from that there are also tons of other resources on the internet that have a very high quality, so I\u2019d also suggest looking around a bit there and see what people recommend. I must say that the tutorials are amazing. I followed some of them to learn Swift and SwiftUI. Thank you for this great tutorials and documentation! --- > #### Using ShareLink in SwiftUI and Transferable, we will now be able to share custom types between users. How does this compare to sharing Core Data entities using a .shared CloudKit database? As I understand, the main difference is that sharing using Transferable copies the item from User A and pastes it for User B, so in the end there will be 2 instances of this item, with no synchronization. Any subsequent changes to any of the items will remain local for that user. In contrast, sharing using .shared CloudKit database means a single entity will be synchronized for both users. Am I on the right track? Are there some other key differences to be aware of? Hi Jan! &gt; How does this compare to sharing Core Data entities using a .shared CloudKit database? There is a difference: `ShareLink` is used for sharing here and now, not for persistent storage. Core Data is a database\u2013and its intended use is for storing items on the disk. &gt; As I understand, the main difference is that sharing using Transferable copies the item from User A and pastes it for User B It depends on which transport APIs we are using. For example, Drag and Drop allows sharing items within a single process or between different processes. Copy/paste works similarly. If we are using `FileRepresentation`, we can specify the `allowAccessingOriginalFile` parameter for `SentTransferredFiles` and tell the system if should make the copy, or not. `Transferable` doesn\u2019t provide any synchronization, just passes around data. Thank you <@U03HW7P2WTB>! So in a use case where we are sharing persistent data between users of the same app (for example, a recipe created by the user in a recipe app), is using ShareLink discouraged in favour of sharing using Core Data + CloudKit? I wouldn\u2019t say *discouraged*, it just has different purpose. `ShareLink` allows to present the share sheet or another system sharing interface, so users could send over the recipes to their friends in Messages, or pasted into Notes, etc. Does this make sense? Yes I think so - in this case, the recipe would have to be converted to something Messages/Notes can understand, like String or Image. Right? --- > #### Good morning! Thank you for a wonderful WWDC! I was trying to play around with the new PhotoPicker. I\u2019m able to initiate the picker but I\u2019m not quite sure how to assign the selected photo to an image in my SwiftUI view. Would you please provide a code sample? I was trying to look into the documentation but I couldn\u2019t find anything that would resolve my problem. Good morning Euguene. We're glad you enjoyed WWDC! Did you try out the code snippets from the What's new talk? That has an example covering that. Yup. Also didn't work on my machine The PhotoPicker won't dismiss itself. And also I'm not sure where/how to grab that image It's a bug. Check my thread here for a fix. <https://twitter.com/xmcgraw/status/1534557535495147520?s=21&amp;t=wihRQOSc4PXqFcu1fWt-dQ|https://twitter.com/xmcgraw/status/1534557535495147520?s=21&amp;t=wihRQOSc4PXqFcu1fWt-dQ> You can access the Data of the image (Transferable). --- > #### Having implemented several SwiftUI apps, I still consider its constraints concerning custom designs a challenge to communicate to designers. Apart from HIG or trying out, do you know any resources/tooling which helps to implement good design? Like are their Sketch export tools to generate code for components/symbols? I know there are Figma (and potentially Sketch) plugins for generating SwiftUI code from designs, however I haven't personally used them so I can't vouch for how well they work Apple also provides design tool plugins, which make it easier for designers to create mock ups that use stock system components and styles: <https://developer.apple.com/design/resources/> Of course, nothing is going to be as high fidelity as the designer jumping into SwiftUI directly. Many designers have found SwiftUI to be surprisingly approachable and there are a number of SwiftUI resources available on the internet targeted at teaching designers to code SwiftUI views. Yeah, stock components are well supported and I know it\u2019s in Apples interest to push in that direction (which I like), but reality bites more often with custom tweaks and they are sometimes really hard to solve\u2026 I wish PaintCode would be updated to export to SwiftUI\u2026 --- > #### Is there a way to coordinate between SwiftUI DragGesture with their UIKit counterparts in the UIGestureRecognizerDelegate like `shouldBegin` or `shouldRequireFailure`? Sorry, no, there's no interop between SwiftUI gestures and UIKit gestures. How about multiple SwiftUI DragGestures? <@U03HHJNTFM0> You can ensure exclusivity with this: <https://developer.apple.com/documentation/swiftui/gesture/exclusively(before:)> Does it work if there are two DragGestures applying to different views? No, the exclusivity is only for a single gesture application. Then my question would be how to coordinate them They seem to conflict each other, and optionally I want to disable one over another It's not supported. Please file a feedback request for this. Will do, thanks <@U03HHJNTFM0> <@U03HELT4EG5> --- > #### About SwiftUI\u2019s Layout: When creating a custom Layout, is it OK to use another Layout within the func like sizeThatFits or placeSubviews, including the built-in Layout (like HStack)? More specifically, is it possible to compose some HStack and VStack within my custom Layout? In that case I have to create a LayoutSubview by myself, is it possible? Yes this is an explicit use case the Layout protocol was designed to support Yes, that should be fine, although you'll need to manage the cache(s) of any intermediate layouts you use. (E.g. store them in your own layout's cache storage, and invalidate them as needed.) You can also create subsets of the subviews collection, to pass to the inner layouts. \u2026 use the LayoutSubviews subscript(bounds:) operator for that. Any examples of layouts inside layouts we could look at? I don't think there are any available examples of that yet. I\u2019ve made one that works but doesn\u2019t yet deal with cache completely. <https://gist.github.com/ryanlintott/d03140dd155d0493a758dcd284e68eaa> For example, I\u2019m implementing the (Vertical) Flow Layout. After doing some calculation and chunked the `subviews`, firstly I use those chunked subviews to make some `HStack`s, then I should embed these `HStack` into a `VStack` but I cannot convert these `HStack`s to `LayoutSubviews` that I can pass to `VStack`\u2019s `subviews`parameter, right? --- > #### Hi there! What's the recommended way to repeatedly fetch data in a SwiftUI app, (so that we don't push updates from a different thread)? In general, I would suggest to factor out the logic that fetches the data into its own type. You always want to execute this kind of side effect not on the main thread and the hop back onto the main thread to set the data on the model. Swift\u2019s actors are a great tool to encapsulate this kind of logic. So for example you could the model that is offered to the view be an `ObservableObject` that is bound to the main actor, and have a separate actor (hence running on a separate thread) that takes care of the fetching and report back new data to your observable object. do you have an example of that? Thanks! In my case, I had a model that conformed to ObservableObject, but somehow I still got the warning. Will look into it though! I would like to see sample code for this. Words do not describe the nuts and bolts of coding..... I think conformance to ObservableObject is not the issue here. What you want is for your function to be wrapped in the Main actor, so updates to UI are pushed on the main thread: @MainActor func getData() {} Yeah, I understood that the conformance to ObservableObject is not the problem, but rather the way I used to asynchronously fetch data to the model. Thanks! Here\u2019s a link about Swift Actors that i just found: <https://developer.apple.com/videos/play/wwdc2021/10133/> --- > #### Would you use SwiftUI or UIKit to implement a list that can change layout between a table and grid layouts? I currently have a UICollectionView with different layouts but I can't animate between the layout change. ~\u2026what\u2019s your iOS target version~ :smiling_imp: As always we recommend using the tool that is best for the job. UICollectionView does support animating layout changes via `setLayout:animated:` so there is no need to rewrite this in SwiftUI. If there are specific things you want to achieve that can only be achieved in SwiftUI, then of course converting it to that is a very valid option. <@U03JRNE4KJL> 15 and 16 This also might be a good question for a 1on1 lab if you are facing specific issues with collection view animating between layouts. If there is time available I'll try to grab one :sweat_smile: I forget if I was using `setLayout` before and what the crash was. I'm using compositional layout and returning a different `NSCollectionLayoutSection` based on if the layout should be a grid or list, and calling `self.collectionView.collectionViewLayout.invalidateLayout()` , but I can try that and see if it works, or maybe there is a better way to construct multiple layouts? Right, UICollectionView can\u2019t currently animate changes to the layout stemming from an invalidation. If you require an animation, I would call `setCollectionViewLayout(_:animated:completion:)` --- > #### Assume that I keep the whole app state in the state object in my app struct? This way I can make sure that all the views are in a consistent state and have a single source of truth. How can I tune performance because in this case, SwiftUI starts diffing the whole app view hierarchy on every single change of the app state? You\u2019re completely right that with a single observable object, you\u2019ll end up invalidating large parts of your view hierarchy whenever a change occurs. Invalidating even large amounts of the SwiftUI view hierarchy should be an inexpensive operation, as view descriptions are rather lightweight, but if you are running into performance issues here, there are a few things you can do to help. The first recommendation I have is to split out some of the values which are only relevant to a certain subset of views into their own observable object. This is likely going to get you the most performance win of any of these suggestions, but if you don\u2019t want to make that architectural change, there are still some things you can do: \u2022 Avoid marking non-published values of your `ObservableObject` published \u2022 (Assuming you\u2019re using `EnvironmentObject` to make sure your single `StateObject` can be accessed throughout your view hierarchy) ensuring you only declare dependencies on the `EnvironmentObject` in places it\u2019s needed \u2022 And if you still need to optimize further, writing a custom `objectWillChange` implementation for your `StateObject`s which only does invalidation when changes that should actually affect the UI occur (in cases, for example, where published values have multiple different representations that should display in the same manner). Thanks Sam :+1: --- > #### What is correct way to specify file type that allowed for Drag&amp;Drop from Finder. I mean if I use ```UTType.fileURL``` it allows to Drop any type of files, but if I specify ```UTType.image``` that not allow Drop at all. What is correct approach if I want allow Drop only images from Finder? Use the generic UTType.fileURL. And also override draggingEntered to inspect the exact file types on the pasteboard. Return true/false as appropriate. Also when inspecting the pasteboard, use the options to auto filter the list for you. ```swift func readObjects( forClasses classArray: [`AnyClass`], options: [`NSPasteboard`.`ReadingOptionKey` : Any]? = nil ) -&gt; [Any]? If I use SwiftUI DropDelegate.dropEntered(info:) how I can get access to the pasteboard from DropInfo object? Or there is another approach in this case? Let me chime in. DropInfo has a member called hasItemsConforming(to:) member, https://developer.apple.com/documentation/swiftui/dropinfo/hasitemsconforming(to:)-47irh , which allows to check for the content types you are interested in. Does this help? I try to use it, but unfortunately, it does not help in my case. Or maybe I just do something wrong. This method only checks the dropped content type. In my case it\u2019s UTType.fileURL but it does not check is this an image or an mp4 file. Is it iOS or Mac? macOS Then the first idea on top of my head is to try to initialize NSImage with the URL/Data from the NSItemProvider and see if it succeeds. If the app isn\u2019t restricted to images from Finder only, you could register for receiving NSImage.imageTypes Oh, great idea. I\u2019ll try to initialize NSImage from NSImageProvider Thank you Julia! I'm trying to mimic the 3-view layout in Xcode, Pages, Numbers, etc. using SwiftUI. Sidebar | Document | Inspector I can use the NavigationSplitView for the Sidebar and Document, but I can't find a way to animate the Inspector in a right sidebar. The SwiftUI animation usually animates all the child views in the Inspector's view. Is there a way to do this in SwiftUI on the Mac? Hi James - thanks for the question. One thought we had here is potentially using an HSplitView as the view for your detail, and having one of its children be the inspector. The animation not working is likely a bug, and we'd definitely appreciate a feedback about it. How are you trying to do the animation currently, if I may ask? I\u2019ve tried something like with previous code (NavigationView is apparently deprecated) _NavigationView {_ _SideBarView(survey: $document.survey)_ _HSplitView {_ _DetailView(document: $document)_ _.padding()_ _InspectorPane(survey: $document.survey)_ _.animation(.linear, value: uiState.showInspectorPane) // looks terrible_ _}_ _}_ I\u2019ve tried moving the .animation inside the InspectorPane() struct, but the effect is still bad. Ok, sorry you're hitting that. It would be helpful if you could include those details in the feedback. But this is the expected approach, but using NavigationSplitView? That feels like a good expression of this structure, yes Thanks. Is there a way to allow duplicate CommandMenu item .keyboardShortcuts? In AppKit macOS menus, this works, but not in SwiftUI. The duplicate shortcuts are never active at the same time - they depend on the type of the selection focus. (FB9931615) Assigning the same shortcut to different menu items is not recommended, even for AppKit apps. The user should be able to predict the outcome of each shortcut. If the selection focus changes which menu item is invoked, that would be confusing. The context (math vs text selection) is different enough to avoid confusion, but more importantly the change from AppKit to SwiftUI forces a UI change in the new app version which breaks current users muscle memory. I see you've already filed feedback, thanks. I don't suppose that the two different actions are similar enough that you could combine them into the same menu item, with the same shortcut? The Copy menu item is a good example of something that applies in a variety of different circumstances, and is strongly responder chain / focus driven... In the one case where I had overloaded a shortcut, no. (Command-I for italics in text, as people are very used to. In math, formatting is determined by content, so there is no Italic action. Command-I was overloaded for Isolate, and had been for 30 years, even before the app supported text blocks at all. It is a very, very old application.) I have rewritten 35 years of code in Swift and SwiftUI with the goal of maximum bug-for-bug compatibility. While there are inevitably going to be differences, I would like to make the transition as seamless as possible for users, many of whom have been using the software for decades. Ron, in your AppKit version, did you use the same selector for each item that had a matching command key? I\u2019m asking just to understand the approach that worked for you in the old codebase. SwiftUI Layout isn't possible for dynamic content, right? Since each subview need to be there at calculation time. It could depend what you mean by dynamic content The SwiftUI layouts you write yourself, like the system provided HStack and VStack, require all children up front However, often we use \"dynamic\" within the team to refer to content driven by a ForEach And that is absolutely supported with custom SwiftUI Layouts If the subviews change size the layout should respond to those size changes. The main thing that SwiftUI Layout doesn't support today is the lazy/incremental layouts that don't need to load all views up front There is not a way to build your own LazyHStack and LazyVStack on top of SwiftUI Layout... or at least not one that is as efficient in lazy loading content But as <@U03JLPYRCFK> alluded to, the SwiftUI framework automatically manages dependencies between views and will recall your layout's sizeThatFits or placeSubviews whenever anything needs to be recomputed (including child size changes) Thanks for the answer, <@U03HELTEP9T>. I mean something like Lazy in my question, which you already answered :grin: So, if the Layouts could\u2019ve been lazy, that means that the views in the Layout would\u2019ve been loaded only when needed (when they appeared on screen)? I\u2019m asking this to make sure I understood the concept of lazy grids. <@U03JRP87THN> That's correct. A lazy Layout theoretically wouldn't have access to all of its children up front from sizeThatFits or placeSubviews, and so that limits some of the things it can calculate up front. Paul covers this well at the beginning of the \"Compose custom layouts with SwiftUI\" talk from yesterday: https://developer.apple.com/videos/play/wwdc2022/10056/ Thank you very much for the clarification! When using SwiftUI with Core Data, should we setup the predicate and sort descriptors for a FetchRequest in the View init or in the onAppear of the View, when they\u2019re based on object passed from the containing View? I noticed some issues when using the onAppear because items are displayed then removed (with animation). If you want to be sure that the FetchRequest is setup before the view does any work you might want to set that up in the initializer. I think it would be worth to file a feedback regarding the issue that you are seeing with onAppear . How to update FetchRequest for searchbar changes? On this point in particular, I think we deserve a more SwiftUI-esque layer for communicating with Core Data. Turning the process of fetching data into a View is not very intuitive and breaks MVVM Is there a best practice for Core Data and Swift UI app project on http://developer.apple.com|developer.apple.com ? <@U03JRQ81NEL> here I can help. You pass the searchValue to your view with FetchRequest. Then in the init, you add an NSPredicate specifying that only results that contain searchValue should be shown, and then create a FetchRequest with that predicate Thanks <@U03J7BQQNPJ>. I'll fill a feedback. Do you confirm that the FetchRequest is reset when the containing View is redrawn? For example, if the SubView customises the predicate or sortDescriptor (in a context where the SubView apply some filters). The FetchRequest is not tied to the View lifecycle contrary to State or StateObject. Is there a specific session this week that discusses App Extensions, and the new AppExtension API ( https://developer.apple.com/documentation/extensionfoundation/appextension)?|https://developer.apple.com/documentation/extensionfoundation/appextension)? I tried searching and found nothing. Hi! There\u2019s no session related to AppExtensions, but if you have specific questions about it, feel free to submit those and we\u2019ll respond as best we can :slightly_smiling_face: Just an introduction and overview. Is there some sample code. The docs are pretty sparse. For example, will this API allow me as Developer A to open up a part of my app and UI to Developer B to extend, or is it limited to extending Apple apps? Currently we have no documentation beyond the published API reference. In SwiftUI, is there a way to listen for navigation events or would that have to be custom from UIKit delegates? Thanks for the question, Martin. With the new iOS 16 NavigationStack, you can bind a path to the stack. Then you can use the .onChange() modifier to watch for changes to the path. What about non navigation events like dismissal of sheets or full screen modals? Is there a stateful way to be informed of when these events are done so we can say present a follow up confirmation or pop the navigation stack back? Hey. David. Those dismiss events will reset the binding that caused the modal presentation to appear. So the same recommendation works there. You can use onChange() to observe that binding too. What if you\u2019re programmatically changing those bindings? If I set an isPresented var to false to dismiss a modal, how would I be able to know when the modal is actually gone? If I change the navigation backing state right away to pop a view or try and set a sheet as not to display , I\u2019m usually presented with inconsistent UI behavior or a message saying that I\u2019m trying to present / dismiss in an invalid state. Ah, I see. I\u2019m usually stuck doing something like asyncAwait after and then playing with the timeframe and that\u2019s, well, gross. The delay hack is one approach. Yah, and I does work for sure Another approach people seem to have some success with is using a common sheet modifier for a variety of presentations. Then you can drive the sheet with an optional enum state. Set it to nil to dismiss, or switch between different non-nil values to present different sheets. We\u2019re also made some improvements in iOS 16 to make the invalid state issue much less common. For example, we try to delay the presentation of the next sheet until the previous one is gone. So you might find that things just work now. :slightly_smiling_face: Yah, and that does work, but when you mixing sheets, full screen modals (leveraging into uirepresentable) and navigation stack changes, the interaction doesn\u2019t line up nicely. OK, I\u2019ll have to test around and file some feedbacks if I still find cases. Thanks I\u2019d love a Feedback with details of your use case. Sure; I\u2019ll see if I can extract out the sample cases. Thank you! Do ids in a ForEach just need to be unique within that ForEach or globally? For instance using the new Charts, all my data is keyed by date and there are several places where I have adjacent ForEaches that include mostly the same dates. The IDs must be unique within the container, so for example if you have a List with two ForEach's inside it, they must generate unique IDs within that List. I am new to SwiftUI, what is the best way to Switch between Views? For example, depending on the condition of a @State variable how can I switch between a HomeView, SignInView and SignUpView? I\u2019ve personally done this before with a parent \u201ccontainer\u201d view, an enum to represent the different possible views, and an @State var that I use in a switch statement to swap out views. I have @Binding vars in all my child views so they can programatically change what\u2019s shown in the parent. However, with programatic navigation in NavigationStack that\u2019d probably be the preferred way to do this, check out the https://developer.apple.com/videos/play/wwdc2022/10054/|SwiftUI cookbook for navigation session by <@U03HW7P0HQR> for more You can actually use switch statements directly in SwiftUI! e.g. enum Screen { case home case signIn case signUp } ... @State private var selectedScreen: Screen ... var body: some View { switch selectedScreen { case .home: HomeView() case .signIn: SignInView() case .signUp: SignUpView() } } Yeah that\u2019s how I\u2019ve done it But I mostly did that because I couldn\u2019t use a NavigationStack on macOS before NavigationView is what I used before the new navigations were introduced. Depending on your needs, NavigationStack is also a great tool to use for this, directly manipulating the current path :+1: But if you just need to swap views in and out, a switch works just fine. For example, it\u2019s common to use a switch to choose the detail view of a NavigationSplitView based on the current selection. Thank you so much! TextKit 2 question: just watching the video about how glyph APIs are no longer available. I have code that capitalises characters at glyph rendering time, by finding the appropriate character range for the glyph range, converting to upper case, and re-generating glyphs for the new string. It\u2019s important to me to not change the contents of my text storage to have the upper case characters \u2014 this should behave like a display time attribute. How would you go about doing this in TextKit 2? We don\u2019t have any TextKit 2 experts around this morning - but there\u2019s a lab tomorrow morning at 10AM PDT - that would be a great place to ask this. The TextKit folks are very friendly :-) Look for \u201cTextKit 2, Fonts, and SFSymbols Adoption lab\u201d. I will ask there! I'd love to know the answer to this too SwiftUI: When using Self._printChanges() in the body of Views, what should we pay close attention to? Is @Self printed often harmful? What are the numbers @44 we see when using FetchRequest in the View. @Self self is not inherently harmful. The purpose of Self._printChanges() is not to indicate that something is wrong but rather as a tool to match your expectation with what is happening at runtime. What you want to be on the look out for is that you don\u2019t do unnecessary invalidation which might cause performance issue. If you see @ followed by any number it means that we couldn\u2019t find the field (property) name, so printed the byte offset of the swift view struct field instead. Thanks <@U03J7BQQNPJ> for the precisions! Helpful. I'm seeing lots of @Self printed when a View has an EnvironmentObject dependency (a Core Data object), and the object publishes a change. For example, a List of Core Data objects (owned) related to the EnvironmentObject (owner). If I update an owned object from the List, the relationship between the owner and the owned is updated, the owner EnvironmentObject publishes an update. This break the animation that should be displayed when updating the owned item in the List. Hi ! I\u2019m still having an issue with generic view models in SwiftUI. I\u2019m having trouble to understand what should I choose between @ObservableObject or @StateObject when injecting a generic view model into a SwiftUI view. I\u2019m commonly doing this: swift protocol ViewModelProtocol { } struct MyView&lt;ViewModel: ViewModelProtocol: View { @ObservedObject var viewModel: ViewModel } let myViewModel = MyViewModel() let myView = MyView(viewModel: myViewModel) In this case, the viewModel property should be @StateObject or @ObservableObject ? I may add some precision to my code example. The ViewModelProtocol conforms to ObservableObject . And MyViewModel should be a class conforming to ViewModelProtocol This particular example should be @ObservedObject https://developer.apple.com/wwdc22/10072 . There are a few nuances that are explained in this year's Use SwiftUI with UIKit and last year's Data Essentials sessions. With @StateObject , MyView would own the model, and is at liberty to make a copy of it when initialized. That means that the myViewModel variable would become \"disconnected\" from the @StateObject . https://developer.apple.com/videos/play/wwdc2020/10040|Data Essentials in SwiftUI Thanks for the quick reply ! What is a common case where @StateObject should be useful ? I feel like when I\u2019m using it, I can\u2019t mock any data because the model can\u2019t be injected so it makes unit test or screenshots automation harder If your view wants to own an ObservableObject view model, and the lifetime of that object should match the lifetime of the identity of the view, you should use StateObject. You can initialize a StateObject with an external value using this code in your initializer: _viewModel = StateObject(wrappedValue: existingModel) Note that the value passed to that initializer is accessed once and can't change over time. Thank you. It makes sense. With your solution, @StateObject allows me to mock the data passed to the view (example: if the view displays a list of books, I can pass the mocked book list to the view initializer and then instantiate the StateObject ). But it does not allow me to mock the state of the view (example : if the view model controls wether or not the view is in editing mode) Edit : typo Using SwiftUI cells in UIKit is a fantastic feature in iOS 16, however that will not work if my app is still supporting iOS prior to iOS 13. Right ? The use of new APIs requires iOS 16, so unfortunately you can not use these in iOS 15 or before. However you can use them in an app that is available on iOS 15 and before conditionally when the user is running iOS 16. Thanks <@U03HELWUJN9> So I will always need to wrap my code in the if @available block Correct. That is the recommended way to do this. Thanks for confirming <@U03HELWUJN9> :pray::skin-tone-2: Hi there! This might not be a UI-specific question (sorry for that), but I'd like to know the answer to use the new SwiftUI APIs. How can I make sure I support older iOS versions, for example iOS 15 in my SwiftUI app, while still using the new features for iOS 16 users? Are there any macros for that? You can conditionalize your code based on whether iOS 16 APIs are available like this: if #available(iOS 16, *) { // iOS 16 code } else { // iOS 15 and earlier code } Thanks! This is exactly what I was searching for. Is there any difference between if #available and @available ? Also note that Xcode will see that you are targeting an older version and will offer to put that check for you automatically. Either as if #available , or prefix your entire function or struct with @available(iOS 16.0, *) I saw someone talk about @available in another thread, the question above this one. Thank you! @available is used to annotate your declarations to mark them as requiring a newer iOS version. For example, if you have a method that takes in an object/struct only available on iOS 16, then you need to annotate the method as @available(iOS 16, *) #available is a runtime check to be used in a condition. @available is an annotation for a method that makes that method unavailable. If your iOS 16 only code is run in a method that is only available in iOS 16 then the compiler won\u2019t complain. So you can use a mixture of both. Thank you! Right, the compiler will then complain if you try to call that method on an older iOS version. Frameworks like UIKit and SwiftUI use this to prevent new API introduced in iOS 16 from being called on iOS 15, for example. Interesting! Thank you very much, I really appreciate it! Usually one way suffice, but sometimes, you can use both together: struct ContentView: View { var body: some View { VStack { if #available(macOS 13.0, *) { MyNewView() } else { // Fallback on earlier versions MyOldView() } } } } @available(macOS 13.0, *) struct MyNewView: View { var body: some View { Grid { GridRow { Text(\"Hello\") } GridRow { Image(systemName: \"hand.wave\") } } } } struct MyOldView: View { var body: some View { VStack { Text(\"Hello\") Image(systemName: \"hand.wave\") } } } Is there a way of disabling scrolling on a List, or would the best option be to use UIHostingConfiguration inside of a UICollectionView that's wrapped in a UIViewRepresentable so that I can disable scrolling on the collection view itself? Use the new scrollDisabled modifier. https://developer.apple.com/documentation/swiftui/menu/scrolldisabled(_:) Ah, I completely missed that, nice! Is there an analog to AppKit's flangsChanged notification, to change the appearance of SwiftUI controls based on keyboard modifiers? I'm currently polling on a timer to do this, but that is unsatisfying. (FB9601140) Hi Ron, You could try using something like this: import AppKit import SwiftUI class KeyboardModifierMonitor: ObservableObject { @Published var eventModifiers = EventModifiers() var localEventMonitor: Any? var globalEventMonitor: Any? init() { localEventMonitor = NSEvent.addLocalMonitorForEvents( matching: [.flagsChanged], handler: { [weak self] event in self?.eventModifiers = EventModifiers(event.modifierFlags) return event } ) globalEventMonitor = NSEvent.addGlobalMonitorForEvents( matching: [.flagsChanged], handler: { [weak self] event in self?.eventModifiers = EventModifiers(event.modifierFlags) } ) } deinit { localEventMonitor.map { NSEvent.removeMonitor($0) } globalEventMonitor.map { NSEvent.removeMonitor($0) } } } extension EventModifiers { init(_ flags: NSEvent.ModifierFlags) { self.init() if flags.contains(.capsLock) { insert(.capsLock) } if flags.contains(.shift) { insert(.shift) } if flags.contains(.control) { insert(.control) } if flags.contains(.option) { insert(.option) } if flags.contains(.command) { insert(.command) } if flags.contains(.numericPad) { insert(.numericPad) } if flags.contains(.function) { insert(.function) } } } which you would then use in your controls like so: public struct MyAmazingButton: View { @StateObject fileprivate var eventMonitor = KeyboardModifierMonitor() public var body: some View { let modifiers = eventMonitor.eventModifiers let isOptionHeld = modifiers.contains(.option) let isShiftHeld = modifiers.contains(.shift) &lt;...&gt; Nice! Will do. Oh darn! I did try exactly that a year ago. The problem is that while the macOS menu is down, the message is not delivered. The behavior in AppKit of opening a menu, then trying the modifier keys to see what changes while the menu is down is lost. EventMonitors don't fire during tracking loops, and while a menu is open, AppKit is tracking that via an event tracking loop. You can submit a feedback request to get modifier changes during event tracking. But what it really sounds like is that you want a way to provide alternate menu items via SwiftUI. Should I file a separate fb to get modifer events during event tracking, or is FB9601140 sufficient since all I really want is alternate menu items via SwiftUI. <@U03HZ42MBV3> FB9601140 does't discuss menus at all. Observing modifier changes to update what's in the menu isn't the right way to go about it. Please file a FB requesting what you really need, which is a way to specify alternate menu items in SwiftUI. Sorry, wrong number, juggling too many open issues. that should have been FB9406583 Ah, yes. That radar covers it. Thanks! UINavigationBarAppearances have different behavior between iOS 13 and iOS 15. How can we get the behavior of iOS 13 in the current version of iOS (where the navigation bar doesn't disappear upon scrolling and can take a background color easily)? And wouldn't it make sense if a breaking change is introduced to document the code necessary to restore the previous behavior? The primary change from iOS 13 to iOS 15 is that the scrollEdgeAppearance always applies regardless of if you have a collapsible section or not (large title or collapsing search bar) so if you want the pre iOS 15 behavior, just configure the scrollEdgeAppearance the same as the standardAppearance , which should just be 1 extra line of code to assign your configured standardAppearance to the scrollEdgeAppearance . I tried that but it didn't work -- it wouldn't take the named color. can you post the code your using? *let* nav = UINavigationController(rootViewController: spotListVC!) *self*.window!.rootViewController = nav `*let* spotBlue = UIColor(named: String(\"PrimaryBlue\"))` *`if`* `*#available*(iOS 15.0, *) {` `// iOS 15 and above` `*let* navigationBarAppearace = UINavigationBarAppearance()` `navigationBarAppearace.configureWithOpaqueBackground()` `navigationBarAppearace.backgroundColor = spotBlue` `navigationBarAppearace.titleTextAttributes = [.foregroundColor: UIColor.lightText]` `nav.navigationItem.standardAppearance = navigationBarAppearace` `nav.navigationItem.scrollEdgeAppearance = navigationBarAppearace` `*let* buttonAppearance = UIBarButtonItemAppearance()` `buttonAppearance.normal.titleTextAttributes = [.foregroundColor: UIColor.white]` `nav.navigationItem.standardAppearance?.buttonAppearance = buttonAppearance` `nav.navigationItem.compactAppearance?.buttonAppearance = buttonAppearance` `} *else* { // iOS 14 and below` `// Fallback on earlier versions` `*let* navigationBarAppearace = UINavigationBar.appearance()` `navigationBarAppearace.tintColor = spotBlue` `navigationBarAppearace.barTintColor = spotBlue` `navigationBarAppearace.titleTextAttributes = [NSAttributedString.Key.foregroundColor:UIColor.white]` `nav.navigationBar.tintColor = UIColor.white` `}` Try adding let _ = nav.navigationBar.standardAppearance and seeing if that resolves it. I think you might be hitting a bug when exclusively using per-item appearance only Ok, let me check No joy. Was there a particular place to add that line? no where in particular no. do you have a feedback request on this? or can you attach a sample (assuming we can do that here!) When I add these lines: nav.navigationItem.compactAppearance = navigationBarAppearace nav.navigationBar.standardAppearance = navigationBarAppearace it will engage the blue background upon scrolling... But it will disappear as soon as the tableView reaches the top edge again. And no, I don't have any feedback request -- this is my first interaction regarding this topic. you would want to set the scrollEdgeAppearance as well \u2013 the compactAppearance is used in landscape on smaller phones only (and realistically unless you need to customize compactAppearance separately from standardAppearance it is rarely necessary to customize it at all) Oops, I think I added standard twice. I think I just found it. I wasn't distinguishing between navigationBar and navigationItem Sometimes I think a handy little diagram to go along with the explanations would go a long way toward making some of this documentation easier to follow because it can be easy to miss a subtle word change like that. Thanks for your help! no problem! One last minor issue in the button on the navBar. The backButton word is the proper color, but the \"<\" chevron is not. What controls that? Right now I have: *let* navigationBarAppearace = UINavigationBarAppearance() navigationBarAppearace.configureWithOpaqueBackground() navigationBarAppearace.backgroundColor = spotBlue navigationBarAppearace.titleTextAttributes = [.foregroundColor: UIColor.white] `nav.navigationItem.standardAppearance = navigationBarAppearace` `nav.navigationItem.scrollEdgeAppearance = navigationBarAppearace` `nav.navigationItem.compactAppearance = navigationBarAppearace` `nav.navigationBar.standardAppearance = navigationBarAppearace` `nav.navigationBar.scrollEdgeAppearance = navigationBarAppearace` `nav.navigationBar.compactAppearance = navigationBarAppearace` *let* buttonAppearance = UIBarButtonItemAppearance() buttonAppearance.normal.titleTextAttributes = [.foregroundColor: UIColor.white] `nav.navigationItem.standardAppearance?.buttonAppearance = buttonAppearance` `nav.navigationItem.scrollEdgeAppearance?.buttonAppearance = buttonAppearance` `nav.navigationItem.compactAppearance?.buttonAppearance = buttonAppearance` `nav.navigationBar.standardAppearance.buttonAppearance = buttonAppearance` `nav.navigationBar.scrollEdgeAppearance?.buttonAppearance = buttonAppearance` `nav.navigationBar.compactAppearance?.buttonAppearance = buttonAppearance` I think thats still controlled by the navigation bar\u2019s tintColor only. its unfortunate, but feedback is always welcome I don't see that as a possible setting of the UINavigationBarAppearance correct, you\u2019d have to set it on the navigation bar directly. thats part of the unfortunate-ness :cry: Ah, I just did that... Ok, so now I get it Whew... Glad you guys were here this week! :smile: its definitely come up a few times that having that available in the appearance objects would be super helpful, just hasn\u2019t been high enough priority to get it done yet. Feedback is definitely appreciated in this area though! With MenuBarExtra, is it possible to have a primary action that's triggered when the menu bar item is clicked and then a separate menu that's shown when the item is option clicked? Hi - thanks for the question. This isn't something we have support for at the moment, but a feedback with any details you can provide would certainly be helpful. FB10134356! SwiftUI and EnvironmentObject: do we have to pass the object every time we use a NavigationLink? My app crashes when ViewA adds an EnvironmentObject Post when displaying ViewB, and ViewB navigates to ViewC where ViewC accesses the Post object. Cf: https://developer.apple.com/forums/thread/707659|https://developer.apple.com/forums/thread/707659 I think the answer on the Dev Forums is the correct one, does it solve this problem? > Your TagView requires BOTH a post and a tag but you are only passing a tag environment object... I posted an answer but it seems the post failed. So writing it again here: the TagView can only be accessed from a PostView where the post is in the environment (because it's the purpose of the PostView to display the Post and details about it). So why isn't the Post object available from the Environment in the context of the TagView if I push a View from this PostView? <@U03HL00QL68> Navigation destinations inherit their environment from the stack in which they appear, not from the context in which they are presented. So the navigation environment of a previous view on the stack doesn\u2019t affect the subsequent view. I think you could move your .navigationDestination(for: Tag.self) into PostView , then propagate both the tag and the post into the TagView from there. Thanks <@U03HW7P0HQR> this works (I was doing that). But I\u2019m still not sure I understand exactly: only EnvironmentObject set on the NavigationStack { }.environmentObject(store) are available for all views presented by NavigationLinks in the Stack? If I pass an object from within the Stack (like in my example), then it's not really in the Environment for views presented from within the Stack? Right. The parent environment is the one for the whole stack. By which I mean the surrounding NavigationStack , not the views on the stack. (Tough to describe this without pictures.) Thanks <@U03HW7P0HQR> for the explanations. But in my example, I pass the post as an EnvironmentObject when a Post is selected. And it's directly available in the PostView. So this post object is passed in the Environment but not for pushed Views from the PostView. I was expected the object to be available in all the \u201cflow\u201d (cf screenshot). Or this flow. I thought that if a Cart object created in the UserLogin View (probably in a NavigationStack) and passed in the Environment from this UserLogin View, it was accessible in the PaymentDetails in the same Stack. But this is only true if the Cart is passed before the NavigationStack? A view can\u2019t have two parent environments. Consider the two ways we could propagate the environment: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 NavigationStack \u2502 \u2502 NavigationStack \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u251c\u2500\u25b6\u2502 Root View \u2502 \u2514\u2500\u25b6\u2502 Root View \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u25bc \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u251c\u2500\u25b6\u2502 Pushed View \u2502 \u2502 Pushed View \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u25bc \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500\u25b6\u2502 Pushed View \u2502 \u2502 Pushed View \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 We chose the one on the left and leave it to developers to propagate any additional information as needed. This reduces the number of dependencies. As of Monterey, List supports drag-and-drop reordering of items on macOS and iOS/iPadOS, but it seems that other types (e.g., HStack , VStack , etc.) support drag-and-drop reordering of items on macOS only in Catalyst apps. In any other kind of macOS app, the same SwiftUI code that works as expected in iOS/iPadOS and Catalyst doesn't work (i.e., a drag cannot be started at all). The code compiles just fine, but it doesn't actually work. How can I support drag-and-drop reordering of items (or sub- View s) in SwiftUI code in HStack s, VStack s, etc. in non-Catalyst macOS apps? Hi John! Could you please file a feedback via Feedback Assistant? It would also be great if you could include a code snippet and describe the desired behavior vs what you see. Here's some code that allows a drag to be started on macOS in Catalyst but not on macOS outside Catalyst: https://pastebin.com/VvJgDq0x Hm, have you tried to provide the contents to the NSItemProvider instead of nil ? It is possible that empty NSItemProvider s are treated slightly differently on different platforms. Changing that line to this worked. Thanks! *return* NSItemProvider(item: item *as* NSSecureCoding, typeIdentifier: \"public.text\") Hi, is there any recommended way of making controls inside a SwiftUI\u2019s Form look more native on macOS in an app made with Catalyst? Would I have to have an AppKit bundle/plugin that renders those SwiftUI views? To enable macOS-native UI in your Catalyst app, go to your target's settings: General \u2192 Deployment Info \u2192 Mac Catalyst Interface, and switch from \"Scaled to Match iPad\" to \"Optimize for Mac\". Controls in SwiftUI will automatically adapt to be more Mac-like in that mode. Also check out the new Form styles, to have the controls arranged in a traditional Mac columnar layout. From what I understand, this will put Mac controls into the iOS style Form - it will not enable the macOS style Form. Unless this is changing this year, there\u2019s no way to get native macOS SwiftUI in Catalyst. FB9994506: Ability to use macOS SwiftUI in iOS app using Catalyst optimized for Mac idiom This year, there is the new FormStyle support, with FormStyle.columns available on all platforms, including Catalyst. That creates a Form layout like the default on native macOS with trailing aligned labels next to the leading aligned controls Form { Picker(\"Notify Me About:\", selection: $notifyMeAbout) { Text(\"Direct Messages\").tag(NotifyMeAboutType.directMessages) Text(\"Mentions\").tag(NotifyMeAboutType.mentions) Text(\"Anything\").tag(NotifyMeAboutType.anything) } Toggle(\"Play notification sounds\", isOn: $playNotificationSounds) Toggle(\"Send read receipts\", isOn: $sendReadReceipts) Picker(\"Profile Image Size:\", selection: $profileImageSize) { Text(\"Large\").tag(ProfileImageSize.large) Text(\"Medium\").tag(ProfileImageSize.medium) Text(\"Small\").tag(ProfileImageSize.small) } .pickerStyle(.inline) } .formStyle(.columns) https://developer.apple.com/documentation/swiftui/formstyle/columns Woooo I love that for us thanks!! <@U03HW7NMP6D> Is there any possibility of creating my own form style? For example if I wanted to backport the new .columns style back to macOS Monterey. FormStyle is only public this year, so not able to be backported. You would have to create your own Form replacement as a whole to do something like that Awesome, thanks for your help! Perhaps a bit of a simple question for such a space, but I am really wondering the best way to get started with SwiftUI. I have done the Apple App Dev course where I build scrumdinger, I've perused the documentation, and I'm very familiar with c, bash, and 6502 assembly (I build games for the NES). However, I'm having a bit of a hard time remembering the syntax. Mostly though, I guess I'm looking for a place to start my own development, for ideas of things to implement to get better at Swift. I have always wanted to at one point in my life work for Apple, like since I saw the iPhone come out in 2007, I'm 19 now. Sorry if this has been a bit all over the place, I'm just so excited to be here. Hi Jakob! > However, I'm having a bit of a hard time remembering the syntax. Mostly though, I guess I'm looking for a place to start my own development, for ideas of things to implement to get better at Swift. I'd recommend not worrying about learning and getting better at Swift or SwiftUI. Instead I'd recommend finding something you're excited to build and... just building it. I've found when you do that, you end up learning the language syntax and framework APIs without even trying :slightly_smiling_face: When you say building it, what parts of it? That is a great suggestion though, thanks! Just building an app or a game for yourself using Swift/SwiftUI, like it sounds like you're already doing for NES! Don't worry about \"writing the code the right way\" to start. Just worry about getting something working Ahhh gotcha, I read your response wrong, thank you. I have a big problem with putting more effort into looking like I'm doing work rather than actually doing it, so it's refreshing to hear that I'm not really correct for that Can onTapGesture be used more than once on a single view? \u2028Like single-click to select, double-click to open in a window? ItemView() .onTapGesture(perform: selectItem) \u2028 .onTapGesture(count: 2, perform: openItemInWindow) onTapGesture can indeed be used more than once but you need to get them in the right order, so that the double-tap fails before the single-tap succeeds. Something like this: ItemView() .onTapGesture(count: 2) { ... } .onTapGesture { ... } Thanks. I'll give it a try. What is the recommended way to conditionalize code for view modifiers that are only available on newer versions of iOS? For example, if I have a View, and previously was using the regular onTapGesture modifier on iOS 15, but when running on iOS 16, I want to instead use the new version that provides location ? We recommend factoring out the common parts and then using if #available checks to use the relevant modifier. Something like this: let common = commonViewParts if #available(iOS 16.0, macOS 13.0, *) { return common.newModifier() } else { return common.oldModifier() } Thanks. It would be great to have a cleaner way to do this in the future, especially as new modifiers get added in future versions of the operating systems (imagine an app targeting iOS 18, that had conditional branches for 18, 17, 16, and 15) Please do file some feedback with this request! Just filed FB10135113 thank you! Hi, I asked this yesterday and was told to ask the SwiftUI team and include a log so here I am\u2026 I\u2019m trying to update my watchOS app to support watchOS 9 however when trying to play a video using VideoPlayer the app crashes immediately and the console shows \u201cUnable to find class NACVolumeController\u201d. Is there any around this or should I file a feedback? HI <@U03HZ3L98TF> that sounds like a potential issue in the Seed would you mind filling a feedback on it and posting the feedback number here so we can take a look for you! Thank you! Any additional details on how you\u2019re calling the videoPlayer or even a small sample project if it repros will be super helpful! Hi, Thank you I'm just making a small project and filing a feedback\u2026. I should have something soon Error reproduced, filing feedback Small additional request from the watch team if its not too much of a hassle is to get a sysdiagnose from a watch when it crashes. Instructions here: https://download.developer.apple.com/iOS/watchOS_Logs/sysdiagnose_Logging_Instructions.pdf Sorry for late reply busy multitasking, the link says I don't have permission to view it\u2026 I am logged in and have a paid developer membership. Not sure why hmm https://developer.apple.com/bug-reporting/profiles-and-logs/?platform=watchos its the sysdiagnose instructions from this page Yep that appears to have worked Thank you right I'll get the log and finish the feedback When it comes to concurrency and SwiftUI, how are you managing creation/injections of your actor objects/business layer so that your views have access to them or do you have any specific recommendations based on the newer Swift concurrency model. I've been reviewing through some of the 2021 videos in conjunction with the new discussions this year and I tend to see .shared in the sample code, which would mean a singleton which I'd frankly rather avoid. We been injecting interactors through the @Environment or @Environment object but this has issues because using @StateObjects we don't necessarily have access to the @Environment object during init when we've been trying to binding subscriptions to our @Published vars. If we use .task {} would we have access to @Enviroment at the time the task closure is run? In this way we could probably set access to our common actors through the @Environment or @EnvironmentObject and then set up flows that just bind to @State via an async Sequence. Is there a way to know when @Environment is actual set for a view so we can respond to in within a @StateObject to set up subscription bindings either through publishers or async sequences? For pretty much any property wrapper in SwiftUI its value become available just before the body of the view is called. If you have ever implemented a custom DynamicProperty that is exactly when update() is called. Yes, by the time the .task closure run your environment is available and you can capture its value. I just want to remind you that you should not conform an actor to ObservableObject because the expectation is that all the ObservableObject instance are isolated to the main thread. Yah, of course the ObservableObject would not be an actor. The actor is that business layer object that we need to call in to perform operations and that\u2019s what I\u2019m trying to figure out how the best way for a view to reach out to them would be. I don\u2019t like making actors singletons as I\u2019d rather have them injected or passed to the View so we can have a test version, etc. Thank you! I\u2019ve found that because there is some unknown amount of time between init() of a view or especially the init() of the @StateObject of view and when the environment is set that it\u2019s hard to know when to run configuration code related to this injection. It\u2019s only safe to access any of the property wrapper from within body The offerCodeRedemption(isPresented:onCompletion:) modifier seems very specific and maybe should have been something exported from StoreKit. Going forward will specialized UI like this always be added to core SwiftUI or will we start seeing other frameworks implement SwiftUI modifiers. offerCodeRedemption(isPresented:onCompletion:) is actually exported from StoreKit like you're asking. We're curious to learn what gave you the impression that it was exported from SwiftUI directly (and if that's a bad thing)? Interesting! The documentation here: https://developer.apple.com/documentation/swiftui/view/offercoderedemption(ispresented:oncompletion:)?changes=latest_minor makes it look like it is part of core SwiftUI. I don't see anything that indicates it is part of StoreKit. Ya that is very subtle. Thanks for pointing that out Is it possible with SwiftUIs new NavigationStack to hide the tabbar of a TabView when the destionation view appears. With the existing NavigationView it was possible but not so easy to handle the navigation title and buttons. Take a look at the new toolbar visibility accepting modifier. This is new in SwiftUI and allows configuring the hiding or showing of different bars like the navigation bar, or the tab bar. ContentView() .toolbar(.hidden, in: .tabBar) See https://developer.apple.com/documentation/swiftui/presentedwindowcontent/toolbar(_:in:) When using Charts to draw a line graph, is there a way to get the last point to be at the trailing edge of the chart view? I\u2019m finding that it always has the trailing edge as a multiple of values used on the x axis. You can use the .chartXAxis(content:) modifier passing an AxisContentBuilder that either completely customizes the x-axis, or you could first try out this initializer of AxisMarks passing true for those roundLowerBound and roundUpperBound /// Automatically determines the values for the markers, /// approximating the target number of values. public static func automatic( desiredCount: Int? = nil, roundLowerBound: Bool? = nil, roundUpperBound: Bool? = nil ) -&gt; Values Thank you. I'll have a look at using that. SwiftUI in UIKit: What is the recommended way of animating changes to SwiftUI view size when inside a UIHostingController? The problem I\u2019m facing is that while the SwiftUI view itself will animate, and with the new sizing options the UIHostingController will do intrinsic content size invalidation automatically, the hosting view bounds change isn\u2019t animated, it just jumps. This is a problem especially when other UIViews are constrained to the hosting view with Auto Layout. Is there any way to solve this other than manually tracking SwiftUI size changes and triggering a separate layout pass + animation of the hosting view superview? Could you please file a feedback for this? Sure! Does that mean there's no solution to this at this time, or you just need an example? Both! I wrote small testing NavigationStack struct ContentView: View { var body: some View { NavigationStack { VStack { NavigationLink(value: 123) { Text(\"Click Me\") } } .toolbar { ToolbarItem { Button { } label: { Text(\"Done\") } } } .navigationTitle(\"Title\") .navigationDestination(for: Int.self) { value in DetailView() } } // .toolbar(.hidden, in: .windowToolbar) } } struct DetailView: View { var body: some View { Text(\"Detail View\") .navigationTitle(\"Title\") .toolbar { ToolbarItem { Button { } label: { Text(\"New Button\") } } } } } But when I click on 'Click Me' button the back button automatically appear in the toolbar. How I can hide or customize this back button? PS: It's on macOS It isn't possible to hide the back button, but a feedback asking for this could help us gauge additional interest for it :pray: Especially whether you'd like to hide or customize Thank you! I will post a feedback report. Can I somehow show an alert or make some operation when a user clicks the back button in the toolbar? Or now it isn\u2019t possible too? For example to stop a task or to warning a user that there is some unsave changes and wait for user response. I'm trying to have .onDelete for delete and .swipeActions for other actions. It is not working this way. Any idea? The thing is I try to have delete as part of .swipeActions. However, I can't find the way to set up the delete animation like the one .onDelete has. The moment you add .swipeActions it up to you to define the delete action (SwiftUI will stop synthesize that for you in the swipe action drawer). You want to create a button with a destructive role to achieve the same result: Button(role: .destructive) { delete() } label: { Label(\"Delete\", systemImage: \"trash\") } <@U03J7BQQNPJ> On the iPad, In a list when a swipe action is used which shows a confirmation dialogue, the popup is not shown on the correct cell. For Example cell 5 is swiped, confirmation dialogue points to a different cell cell. Feedback FB10026540 <@U03JCHKCDB4> thank you for taking the time to file a feedback. <@U03J7BQQNPJ>, thanks for the reply. I do have destructive button like you mentioned. However, it doesn't have the animation like the one onDelete has. Any idea how to set it up? <@U03J7BQQNPJ> Regarding feedback FB10026540 I have updated the project and added more details on my observation. Thanks a ton for the immediate response!!! In SwiftUI when you had a List view with items that were continuously synced with a server and you displayed the details view of a selected item then the app jumps back to the main list. Are NavigationStacks solving this problem? Hi, Arnfried. Thanks for the question! Generally the popping back problem is because there is a state change that\u2019s invalidating the view that contains the NavigationView . This causes SwiftUI to discard the NavigationView and create a new one, popping you back to the root. Check out Demystify SwiftUI from #wwdc21 https://developer.apple.com/wwdc21/10022 for more details on view invalidation and identity. With NavigationStack , you can bind a path, so the popping back shouldn\u2019t happen. But you should still investigate why the identity is changing. That\u2019s likely a source of performance problems, since lots of extra work will happen. In the new navigation API, that new work will be to replace the entire navigation hierarchy. Isn\u2019t it better to stop syncing List with a server in time if you are in the detail view? That\u2019s really up to the specific app design. Many times that would be right, but some apps might want the root view to be always correct immediately when the user pops the stack. Or maybe the backend doesn\u2019t vend the details separately. Hi Alexey, stopping the sync is in our case not possible. As Curt C mentioned, it can als be a state change that causes the back popping. It is really the NavigationLink as he said. I tried to set identifiers for the link itself or the destination view but the effect is still there. But I will test it with the new NavigationStack. Maybe this solves the problem. Explicitly setting an identifier isn\u2019t a solution if the structural identity of the view containing the NavigationStack is also changing. Explicit identifiers can be used to force an identity change, but not to prevent one. In practice the identity of a view is a combination of its structural and explicit identity. We're using an attributed string with the labelColor on a NSStatusItem s button. However, when displaying this on an inactive display the color isn't dimmed as expected. Is there a way to have the text appear dim when on an inactive display using this approach? Hey <@U03JKFBJG69>! Status items should automatically look great on secondary displays, with no extra work needed for apps to support this appearance. :sunglasses: :thinking_face: However, if you\u2019re not seeing that is the case for a status item that you\u2019re using or creating, please provide this feedback through http://feedbackassistant.apple.com|feedbackassistant.apple.com . A sample project and the version of macOS would be greatly helpful in reproducing this issue. Feel free to send me a link to that feedback you\u2019ve filed, in this thread. :+1: Thank you :slightly_smiling_face: Is there other content in the status item? For example, does the status item\u2019s button have a (templated) image? Yes, the status item\u2019s button has a image which is a template image Ah ha. Thanks for that info! :slightly_smiling_face: <@U03HEM646TX> Filed as FB10144807 I can reproduce this on macOS 12.4. You should be able to reproduce it via the following code snippet: import Cocoa @main class AppDelegate: NSObject, NSApplicationDelegate { let statusItem = NSStatusBar.system.statusItem(withLength: NSStatusItem.variableLength) func applicationDidFinishLaunching(_ aNotification: Notification) { let useTemplateImage = true // set this to false and notice how the text appears for the status item on an inactive display statusItem.button?.imagePosition = .imageRight statusItem.button?.attributedTitle = NSAttributedString(string: \"Hello World\", attributes: [NSAttributedString.Key.foregroundColor : NSColor.labelColor]) statusItem.button?.image = NSImage(named: useTemplateImage ? NSImage.quickLookTemplateName : NSImage.statusAvailableName) } } Is there a way to recreate the appearance of the widgets on the Lock Screen using UIVisualEffectView in an app using either SwiftUI or UIKit? I'm specifically interested in recreating the appearance of the circular widget. A variety of the gauge styles used in circular complications on the Lock Screen are available in app as well! You can access them using any of the SwiftUI gauge styles with an \u201caccessory\u201d prefix. Please be aware though that those styles are really only intended for Lock Screen / widgets and contexts in your app that should have a similar look and feel, so please be thoughtful about where you use them. <@U03HKVDCL7N> Thanks for your reply. I realise that my question was poorly phrased. I\u2019m looking for a way to recreate the vibrancy that the Lock Screen widgets use. Would that be done using a UIVisualEffectView? with UIVisualEffectView you can try the UIVibrancyEffect s created with a UIBlurEffectStyleSystem*Material , but I\u2019m not sure that will necessarily match what the lock screen is doing The documentation for makeCoordinator states: \"Creates the custom instance that you use to communicate changes from your view to other parts of your SwiftUI interface.\" could you explain what that means? It doesn't make sense to me because normally the coordinator is an NSObject that is the delegate to a UIView. I have seen some examples of Coordinator classes that take in an @Binding, that was passed in to the UIViewRepresentable and then into the Coordinator. Is that what this means? The Coordinator you return can be any type, including an NSObject subclass. That coordinator will then be passed into makeUIView and you can assign it as the delegate of your view. I explained this in my session \"Use SwiftUI with AppKit\", starting at around 12:38 into the video. https://developer.apple.com/videos/play/wwdc2022/10075/ Thanks <@U03HB1A8VRU> I just watched it. I've never seen that technique of initing the coordinator with self and also setting self in updateNSView. Seems overly complex to me but I'll try it and see how it compares with just forwarding the @Binding. I think it would be useful if the documentation for makeCoordinator was updated to say - instance that communicates changes back to the UIViewRepresentable struct. Passing a binding works great as well, but doesn't scale as well if you have multiple pieces of state to pass through. You're welcome to do it either way! There's no way of posting images on the question. But I ll add it once the question gets posted. Using SwiftUI, I m trying to use SwiftChart to add color underneath my line graph. I was wondering if there's a good resource on how I can do that? Yes, let's see what you're trying to do! Check out AreaMark ! The design is on the right for my Stair Climber app (join beta here https://stairmasterclimber.com ) and what i got is on the left On it right now I forget the exact modifier to specify the color underneath the area mark, it may be foregroundStyle passing a gradient in this case probably from orange to clear :OOOOO YOU ARE A GENIUS :bow: And is there a way to add the border color? Which border color are we talking? Oops i mean the line color On the top of the area Ah this is one of my favorite parts of Charts Swift Charts are amazing! So you can have multiple marks in one chart You see the line on top of the area? Would I basically just need to add a line chart on top? :open_mouth: There may be a modifier specifically for the line color of an AreaMark so, I'll check with some Swift Charts engineers, but in the meantime, it's still instructive to add a LineMark plotting the same data after the AreaMark The line mark will be superimposed on top of the area mark So literally you have two charts on top of each other, but conceptually, it looks like a single styled chart I hope you're adding HealthKit integration ;) YES I AM Health Kit & Game Kit Well if you've successfully gamified Stair Stepping, that sounds awesome StairMasterClimber, because elevators are so old-school We have 30 people so far, I would be honored 100 privacy safe and no ads Sounds really interesting! :partying_face: alright HealthKit and Swift Charts :raised_hands::skin-tone-2: going to be awesome You\u2019re a genius sir. Almost there, just need to add the gradient!! <@U03HL00QL68> One last question, how can I add the little circle dots for the data points? Do i need to create a new question for that? This is my code currently, Unbelievably short and easy What's the recommended way of using a property wrapper such as @FocusState such that it will compile targeting iOS 14 for example? AFAIK you can't add an availability annotation You are correct that you can\u2019t apply availability to stored properties in Swift. There are a few techniques you can use to work around this. Most commonly, you can factor out the portion of your UI that uses this property into it\u2019s own View , and apply availability to that entire view. However, the best way to structure the code really depends on your specific use case, and we agree that this can be tricky to handle in some situations. If you have a specific use case you\u2019re okay with sharing, please file feedback. Real-world code examples are incredibly helpful when designing future improvements to the language and the framework. Thanks for the reply, would be helpful if it was possible to directly init a FocusState SWIFT-UI question: How to identify the visibility (in index or id) inside a TableColumn View for paginate remote content. Hi Ratnesh! Similar to techniques of paginating content in Lists, you can also use onAppear for TableColumn views in Table as well using the same technique. You\u2019d want to compare the id of the column element to what you consider the \u201clast\u201d element\u2019s id Thanks for the reply :blush:. Is this same technique work for macOS target also? Yes, it does. We even using that technique in Photos in macOS Ventura :slightly_smiling_face: :star-struck: awesome By default UIHostingController configures a navigation bar. What is the best way to hide it? Overriding viewWillAppear to call setNavigationBarHidden does not always give the expected result. The best result I got was by overriding viewWillAppear and NOT calling super.viewWillAppear. Is there any risk with this method? I would not recommend overriding viewWillAppear and not calling super. Generally, I\u2019d recommend either using a navigationBarHidden(false) or the new toolbar(.hidden) modifier inside of your SwiftUI view. Or if you are managing a UINavigationController yourself, and you can\u2019t use those modifiers, you should be able to set the isNavigationBarHidden property to false yourself and the hosting controller will try to respect that. I\u2019ll call out though the release note in the first beta of iOS 16: > SwiftUI might incorrectly modify the isNavigationBarHidden property on UINavigationControllers not created by SwitftUI. It seems that this does not only concern iOS16... If you\u2019re having trouble with this on older releases, I\u2019d recommend trying to use .navigationBarHidden() for hiding the navigation bar. If you are unable to do that, then you could try subclassing the UINavigationController to avoid hiding or showing the navigation bar if SwiftUI incorrectly tries to mutate that property. Thank you <@U03HW7QCHK3> Subclassing UIHostingController is the solution for which I asked you advice. To prevent SwiftUI to set isNavigationBarHidden to true, we have to override viewWillAppear (and better yet: don't call super.viewWillAppear) Why do you say that it is not recommended? Question about SwiftUI. We have background task that may generate errors and application shows alert when this error appears. If we use alert(\u2026) modifier on parent view, it works. But when parent view displays some child view, parent view can\u2019t show this alert and app shows warning in console \u201cAttempt to present <Alert on <ParentView which is already presenting <ChildView\". As workaround we can use identical alert(\u2026) modifiers for parent and ALL child views (sheets, popovers, action sheets, etc.). But in this case we still have warning in console. How can we display \u201cglobal\u201d alerts in SwiftUI without warnings in console? Or perhaps we can ignore these warnings? This looks like a bug, could you please file a feedback with the reproducing project so we can take a closer look? Should i attach sample here? Please attach it to the feedback report you create. Thanks for the answer! It doesn\u2019t look like a bug, more like a limitation. We need to show alert in unknown period of time. We may show it from .sheet or from the parent view. In parent view it will not be displayed if sheet is already on the screen. Is there a way to know when a drop has been cancelled with .onDrag and the DropDelegate ? While dragging I want to change the opacity of the view for the duration of the drag. I tried providing a preview view and using onDisappear but it isn't called either. There is dropExited but the user can still be dragging. There is no API for this, and as you noted, the drop session state in DropDelegate is different than the drag session state. We'd love to hear your use case for this, so could you file a feedback report asking for this functionality? How would I go about making a Grid where all the cells are the same size based on the cell that needs the largest size to appropriately display its content? To do this today you would need to do something more custom like creating a custom Layout. Though you might be able to to implement the custom Layout so that it calls into Grid itself, reusing its internal logic. Hi, I\u2019ve done something similar recently in iOS 15. For this I created a child size reader (you can search for that) and then keep track of the largest size. Lastly, simply apply the size to grid items using .frame. Not sure if you\u2019re already doing that and simply wanted a neater solution for iOS 16\u2026 The auto-magical conversion from UiNavigationBars to NSToolbar in Catalyst is an awesome addition, but I\u2019m not sure how it works with multiple columns. I have an app with UISplitViewController and 3 columns. All of the view controllers in each columns have their own buttons, and the one in the secondary view controller has a search bar. Which of these should appear in the Mac toolbar? Ideally they would all appear in their own \u201csection\u201d of the toolbar, like they do on iPad. But in testing, only the buttons from the secondary VC appear \u2014 no buttons from the other columns and no search bar. Is that expected? Is there a way to separate toolbar like we can using NSToolbar? The translation does bias towards supporting the secondary section (in the API its called content ) primarily as thats is where we expect o have the most \u201croom\u201d in the toolbar. We do have some issues with properly supporting split views in the current beta, but it would be great to get feedback on what you are seeing and what you might expect I'd especially appreciate the feedback because I'm surprised that your search bar isn't migrating, and would like to see your example Ah my appologies \u2014 I said \u201csecondary\u201d but meant \u201csupplementary\u201d. My supplementary column has a search field. All columns have buttons. When run on Mac, the buttons from my supplementary column appear in the toolbar (at the far right, no columns in the toolbar) but no other buttons or the search field appear. yea, I think we don\u2019t translate search from anything other than the detail column currently. We may have bugs here as well, so please send your samples so we can make sure your translation is fantastic! When we use UIHostingController, should we register it to responder chain? by presentation or adding as a child view controller. Otherwise what would be happening? for example just retaining UIHostingController and then just adding view to the parent view. I cover this in the \u201cUse SwiftUI with UIKit\u201d talk! Its right at the beginning. You just need to add it as a child viewcontroller // Add the hosting controller as a child view controller viewController.addChild(hostingController) viewController.view.addSubview(hostingController.view) hostingController.didMove(toParent: viewController) https://developer.apple.com/wwdc22/10072 At the end I cover some of the limitations of UIHostingController and speak to why its necessary to embed in as a childViewController oh I missed that video! I would check it. Thanks! I wonder if I could create a UIView that supports rendering SwiftUI view like UIHostingConfiguration for supporting older iOS version. https://github.com/muukii/CompositionKit/pull/7|https://github.com/muukii/CompositionKit/pull/7 Awesome talk <@U03HL05BUJG>! What would your recommended approach be for small leaf node SwiftUI views in UIKit, which would not usually be full VCs? Even given the pitfalls of separating the hosting view, we find that we need to do this in ui libraries where clients expect UIViews, not view controllers. Thanks! (FB10019256) Is there a way in the SwiftUI document model to access updateChangeCount directly, in order to decouple document saving from the undo stack? (Certain operations don't make sense to undo, but do change the document's file state.) (FB9974481) Hi Ron! For now, there is no API available in SwiftUI for updateChangeCount . But also I want to thank you for filing the feedback, it is important to know which functionality is anticipated. How do i get points on my lineMark in swift charts? (See Image for comparison) Left is current, right is design MAKING PROGRESS, thank you for your help!!! how can I add the little circle dots for the data points? This is my code currently, Unbelievably short and easy (edited) You can either add a PointMark on top of the LineMark or by adding a symbol to the linemark LineMark(x: .value(\"xvalue\", point.x), y: .value(\"yvalue\", point.y)) .symbol(Circle()) My \u201cUse SwiftUI in UIKit\u201d sample project actually has an example of this https://developer.apple.com/documentation/uikit/views_and_controls/using_swiftui_with_uikit as does I believe some of the Swift Charts talks sample code too! <@U03HL05BUJG> what if i want to add the numerical values (eg: 15) above that PointMark? TextMark? as an annotation? Yes Let me find the docs for that\u2026 It should be an annotation modifer on the mark! .annotation(position: .top, alignment: .leading) https://developer.apple.com/documentation/charts/visualizing_your_app_s_data the swift charts example project has an example of this! You are the best! Got it! Anywhere I can follow you on social media? Ahh sorry I don\u2019t have any social media! Is it possible in SwiftUI on Mac to use a modifier with a mouse click like \u2318-leftClick? I like to select non-adjacent items like in Photos and Finder. Gestures have .modifiers(_:) modifier, which allows restricting that gesture to only respond when the modifier is active. So in this case you could use a Tap gesture with .modifiers(.command) For complex views, I often define subviews inside computed vars to keep my body block more readable. Especially for components that don't need to be reused elsewhere in the app, so they don't seem to warrant a reusable struct. Example: struct MyView: View { var body: some View { someText someButton } private var someText: some View { Text(\"Hello\") } private var someButton: some View { Button(\"Press\") {} } } I've heard that this can be bad for performance - is that true? And does using @ViewBuilder on some computed vars have any impact? SwiftUI\u2019s traversal of your view tree isn\u2019t impacted at all by whether you chose to use new structs, or computed properties, for small pieces of views, etc. So this is totally reasonable. Using @ViewBuilder here also shouldn\u2019t have a performance impact. I would highly recommend doing so! The only thing that you should pay attention to is your mechanisms for invalidating your views will be a bit less fine-grained, as you\u2019re making a larger part of your view hierarchy dependent on the sources of truth specified by MyView . Make sure you\u2019re paying attention to what pieces of your view depend on which pieces of data, and when you see computed properties that have completely disparate dependencies from the rest of the view, you consider breaking those out. Are the new navigation bar styles ( https://developer.apple.com/documentation/uikit/uinavigationitem/3987969-style)|https://developer.apple.com/documentation/uikit/uinavigationitem/3987969-style) supported on SwiftUI? Howdy! You can find the answer to this in this slack thread: https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654786985779439 Hi all, thanks for all the great work this year. What approaches would you recommend to start integrating SwiftUI into existing codebases that support iOS 14/15? I'd suggest trying to adopt it in a view controller first, and replace the existing view controller with a UIHostingController and try to recreate the existing layout. A great place to start is a settings view, where you can use Form to put a list of fields, switches, sliders, and more. In continuation of my question about NavigationStack https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654791464924449|https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654791464924449 Can I somehow show an alert or make some operation when a user clicks the back button in the toolbar? Or now it isn\u2019t possible too? For example, stop a task or warn a user that there are some not saved changes and wait for the user\u2019s response. Is there a way to specify the tabbing order of text fields, akin to textView.nextKeyView (FB10020959). My app has a combination of SwiftUI TextField Views and NS/UITextView wrapping in NS/UIViewRepresentable, and I've seen some odd cases where the default tabbing order is quite unintuitive. By default the key view loop follows the same default order as AppKit \u2014 leading to trailing, then top to bottom \u2014 do please file a feedback if you find a situation where that\u2019s not the case. There is support for customizing that order by describing the sections that separate your UI using focusSection() ( https://developer.apple.com/documentation/swiftui/view/focussection()?changes=_4 ) focusSection would have been perfect, but my bug is occurring on the iOS app. It does seem to be following leading to trailing, top to bottom in terms of the entire screen, however, the View with logically grouped TextField\u2019s can happen to be in a column next to unrelated NSTextViews far away, but along the same leading to trailing line horizontally. Ah, thanks for clarifying, and I see in the feedback that you provided a test flight \u2014 thank you! We\u2019ll check that out and follow up if we have any more questions on that Is it possible to have a self-sizing list in SwiftUI similar to how LazyVStack works? List has support for drag and drop through onMove while LazyVStack does not. List doesn't size itself based on the height of the content like LazyVStack. You'll need to give it a fixed height using the .frame(height: ...) modifier. List has support for reordering, swipe actions, and other features that LazyVStack does not. We'd love to hear about your use case in a feedback report. Specifically let us know what kind of experience you're trying to achieve, but can't. I have a couple of apps written in SwiftUI in the past two years that are broken now because they rely on lists and the background color can\u2019t be changed anymore using the UITableView.appearance() API. How can we change the default background color for List in SwiftUI? Please see our earlier post in the channel here: https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654793028152169 Awesome! Thanks for your reply and sorry for not paying attention :D I really hope I won't have to throw away my years worth of code for such a small detail :P No worries :slightly_smiling_face: Is there SwiftUI equivalent of new UINavigationItem.backAction? There is no equivalent in SwiftUI, but a feedback would be appreciated! How can I adjust the width or thickness of a linemark in swift charts? you can use a LineStyle! https://developer.apple.com/documentation/charts/chartcontent/linestyle(_:) .lineStyle(StrokeStyle(lineWidth: 3)) We have an app that has a 50+ components built using UIKit. We are trying to use SwiftUI by reusing our existing components but we are facing multiple challenges. 1. It seems that UIViewRepresentable doesn't work out of the box with views that are built using auto-layout. We have a generic container (IntrinsicContentHeight<View) that tries to calculate the size of the UIKit view and return it to the intrinsicContentSize. Is there a better solution? 2. We noticed performance issues using UIViewRepresentable views in SwiftUI Lists. It seems that Lists always re-create the UIViewRepresentable views and never recycle cells. Are we doing anything wrong? 3. Our design system slightly modifies the line-height of our texts. We have a solution using NSAttributedString that works well for UIKit but haven't found a solution for SwiftUI. Text supports line spacing but it can only increase the default line spacing, it cannot decrease it (passing negative values does nothing). Is there anything we can do to customize the line height of a Text in SwiftUI? I only have an answer for the first question. We have new API that allows you to more finely control how your UIViewRepresentable interacts with the SwiftUI layout system. Please see: https://developer.apple.com/documentation/swiftui/uiviewrepresentable/sizethatfits(_:uiview:context:)-9ojeu Thank you, Raj! I will check if we can build a solution for iOS 16 with sizeThatFits and our other container for older versions. For your last question, can you please file a feedback with your request? In particular, it would be good to know what kind of customization you need for line spacing/line height? It it going to be a fixed value, or certain percentage, or a delta applied, or a value relative to Dynamic Type sizes? Thanks! Value relative to the dynamic type size sounds like the thing that will work for me. I will make sure to file feedback to give more details. Thank you for answering, Paul! Is there any thought to making UITextInteraction spell-checking API public? UITextInteraction respects the spellCheckingType property of UITextInputTraits. If the value is UITextSpellCheckingTypeDefault or UISpellCheckingTypeYes, spell checking occurs as expected. Unfortunately, UITextInteraction uses the private UITextReplacement class to handle spell checking. When it encounters a misspelled word, it correctly presents a menu with the spelling options. However, when you tap the correction, it attempts to call the private replace: method with a sender of private class UITextReplacement. The replacementText property of the sender contains the misspelling. The code works fine, but using this private method and property earns you a rejection from app review, so it isn\u2019t possible to use the spell-checking features of UITextInteraction. This seems like an oversight on our part. Have you filed a feedback request for this? FB10136384 Thanks. I think the expectation is that replaceRange:withText: should be invoked instead of the private replace: method. In the meantime, you should try implementing shouldChangeTextInRange:replacementText: and see if that gets called before/after the call to replace . That would be a way to get the replacement text without using private API. Did anything change in UITextInteraction with respect to dictation? On iOS 16, dictation in a custom view that supports UITextInteraction always fails. The same (shipping) code works on iOS 15. Changes were made to dictation to support the new \"modeless\" dictation feature. If this has broken a custom text view implementation, it was unintentional and a feedback request would be appreciated. Will do. I notice that dictation seems to be failing in Pages too, but not Notes old this (old) iPad. Is there a way to use app-specific UIActivities with SwiftUI\u2019s new ShareLink? This might be a better question for the <#C03HX19UNCQ|>. The new find stuff is great \u2014 it took 30 seconds to integrate with my UITextView. On Catalyst, the find panel seems to have some issues, namely that clicking on the buttons doesn\u2019t work \u2014 the search field is focused and I can type it in, but clicking doesn\u2019t do anything. Is that a known issue with beta 1 or is there something I should investigate on my end? This is a known issue (but feedback requests still appreciated). A workaround currently is to call layoutSubviews on the UITextView after the find panel appears. A bit late to the party, due to a lab appointment, so still working my way through the video. Can you advise: is there an API to enable me to add custom data detectors in text editing in my app? For instance, I'd like to be able to have a data detector that adds text decoration or other features to any time someone inserts the name of a person in their text, from a list of names of people already in the app (say, several thousand people). Is there an API/mechanism for this? Unfortunately, that is not something that is supported today. Please file a feedback request for us to look at in the future :slightly_smiling_face: Obviously not as nice as a dedicated API , but you could do it very manually, by watching the delegate methods of UITextView for text changes, and then maybe overlaying things or editing the text to add links. Honestly, it\u2019s been a while since I last checked this, but is there an API in UIKit these days that I can query to check whether an external hardware keyboard is present? There is no API for this, it's not really something we think developers should have to account for, generally. Do you have a specific use case in mind? (We usually encourage developers to use the keyboard layout guide for laying out UI instead). I once worked on a hex editor for iPad as a side project. I wanted to show a custom inputView with a hex keypad when no hardware keyboard is attached, but wanted to use the default keyboard when there is a hardware keyboard present so that there is no on-screen keyboard shown. I think I filed a feedback about this after a WWDC20 lab session, let me find the ID FB7777442 This year\u2019s enhancements kinda make me want to continue working on it, but since there are no guarantees in iOS/iPadOS when an app is (not) terminated in the background, I still don\u2019t feel too confident about it It would be helpful to know if a hardware keyboard is attached because you can optimize what functions are offered in the assistant. In screenwriting, for example, the tab key is important enough to add to the bar if you don't have a keyboard attached. Oh gosh, that was me you talked to! Well, thanks for the reminder, I will try my best. Oh yes, I totally missed that sentence :smile: One case we're hitting where we would love to check for HW keyboard - in our custom text editing view we offer intelligent word completion. When HW keyboard is connected we'd like the prompt to reference the \"tab\" key. When it's not connected, we'd like to instead tell the user to swipe right. Your hex editor needs to run in the background indefinitely? Why? Let\u2019s say you are moving around large chunks of data stored on a slow USB drive. If the OS decides to kill the app while the app is still busy writing data, data corruption occurs, I guess? Maybe this is not an issue with journaled file systems, but what about plain old FAT32? The UIFindInteraction doesn't seem to fire off a new query on my session when the user removes all text from the search box. Is this intended? Is there another way of listening for this so we can clear our custom text highlights? You should get a call to invalidateFoundResults instead. Thank you for answering the last minute Q! I totally thought invalidateFoundResults was only there to be called by us, not the system. Ah, indeed, the documentation is wrong. Thanks for pointing that out! Hi, In my CarPlay app, In templateApplicationScene(_:didConnect:) I get the provided interfaceController and store a reference to it, but most of the times when I try to access interfaceController.carTraitCollection The app crashes because carTraitCollection is Uninitialized , so it makes creating image for different display scale (2x, 3x) or different display styles (dark, light) impossible. I tried to make sure that I always access it from main thread but still have the same problem. This code always crashes for me: // CarPlay connected func templateApplicationScene( _ templateApplicationScene: CPTemplateApplicationScene, didConnect interfaceController: CPInterfaceController ) { self.interfaceController = interfaceController; self.interfaceController?.delegate = self; print(interfaceController.carTraitCollection) } Thanks for the report! We\u2019re aware of the issue and we\u2019re tracking it. Your app shouldn\u2019t be crashing, though - in a Swift app, you\u2019ll want to handle this variable potentially being nil, perhaps by assuming a default screen scale (2x) if the trait collection isn\u2019t yet available. Thanks for the answer. I do that in my real app, I assume 2x for images and dark for the style, but is there any specific point in the life cycle that it will be initialized for sure? It should be non-nil if your app has been launched on the CarPlay screen, but it\u2019ll likely be nil if your app has not been launched in CarPlay. Please also give this a try in iOS 16 seed 1; this should be improved! how can I know that app is launch in the CarPlay? The CPInterfaceControllerDelegate callbacks will inform your app when it has been launched in CarPlay (and disconnected from CarPlay) The same one you have above :slightly_smiling_face: Thanks, and the value will remain non-nil after the first appearance on the app on CarPlay screen? Sorry, do you mean the trait collection, the scene, something else? The trait collection should not be nil, but this is an issue we are tracking :slightly_smiling_face: The value will get updated at some point after the sceneDidConnect callback, but unfortunately we don't have a delegate callback to inform your app when this happens. As Jonathan said, we are aware of this issue, but please feel free to file a feedback on this! > yes, I mean the interfaceController.carTraitCollection We already have filed a feedback in December 2020: It is FB8926706 > Thanks <@U03HVCNN2DU> , do you mean that interfaceController.carTraitCollection can also be nil when I get the CPInterfaceControllerDelegate > call back as Jonathan said? Yes, that is the issue we\u2019re describing here :slightly_smiling_face: Yes, the bug is that sometimes the trait collection is not initialized at the time of the scene-connect delegate callback, but it will eventually get initialized Thank you Kevin and Jonathan. another related question that I noticed, looking at another thread, <@U03HJA80GLS> When using UIImageAsset, the pickup of the correct asset also depends on the interfaceController.carTraitCollection which is nil at some point, so if we use an image asset to create the template when the carTraitCollection is nil we will get the wrong asset, or no asset at all. Is there any workaround for this? Hi, I've a question about the Car Play simulator . I added arm64 in the excluded architecture to run in the iPhone simulator for my M1 mac. Now I get this error in carplay simulator \u201cTerminating app due to uncaught exception 'NSInvalidArgumentException', reason: 'Unsupported object <CPTabBarTemplate: \u2026. \u201c , I think it crashes because of arm64 in the excluded architecture. Can you please help ? Hi! This is most likely caused by a library or third-party dependency in your app that does not have an arm64 slice. We\u2019d recommend you work with your vendor(s) as needed to get an updated version of all of your dependencies. Here\u2019s a similar thread with some more detail: https://developer.apple.com/forums/thread/698332 You might also try running your app on-device with the new CarPlay Simulator app on your Mac, available under the Additional Tools downloads on the developer site. Thank you so much. I downloaded that tool and planning to use the CarPlay simulator. When your CarPlay app is running but your screen on your device is locked, is your app considered \"in the foreground?\" For example, can I run something like a Timer if I'm just within the CarPlay UI? When an app is launched in CarPlay and visible on the CarPlay display, the corresponding UIScene (specifically a CPTemplateApplicationScene ) is foregrounded. As with UIScene in general, your app's overall application state is the highest activation state of all of its connected scenes. Thanks! Do push notifications (non-time sensitive) work like normal when in the CarPlay UI? As in, can I process them and do everything I would expect to do just like when my app is open on my device? Yes, user notifications work in CarPlay but the app must be allowed in CarPlay. When requesting notifications authorization, include the allowInCarPlay option: https://developer.apple.com/documentation/usernotifications/unnotificationcategoryoptions/1649281-allowincarplay And then make sure to post the notification with that category. Separate out notifications intended for CarPlay into their own category to avoid other notifications your app might post that aren\u2019t relevant while driving from appearing in CarPlay. For more information see this video starting around 14:20: https://developer.apple.com/videos/play/wwdc2017/719 Thanks! However, notifications are limited to certain app categories and for those categories (such as VoIP and Messaging apps) the notification is handled using a Siri intent. <@U03J83E86Q0> So for example, in a quick food ordering app, can we get notifications say for the status of your order just like we currently do on device? I might have missed anything in the docs about which entitlements could receive notifications Quick food ordering does not support notifications In no way at all? So, what would be standard on a device to receive a notification when there is an update to the status of your order so that I can update the UI in my app\u2026that is not supported? If not, what mechanisms are available for the CarPlay app to get say an updated status? App icon badges will appear in CarPlay, that\u2019s one way to communicate that your app has new information. Definitely file feedback too if you think it could be useful for your app! Will do. Although this may be an entire project killer at this point. Does the new CarPlay Simulator app only function with devices running iOS 16? \u2013 I couldn't get it to actually display CarPlay visuals from my plugged-in iOS 15.5 iPhone (No error messages; just \u2026nothing happening) CarPlay Simulator is supported for iPhones with iOS 15.2 and above. Can you share more information about what model Mac you are using to run CarPlay Simulator? Ah. I\u2019m on: \u2022 MacBook Air (M1, 2020) \u2022 macOS 12.4 (21F79) For reference: this is what I see when I launch the app, with my unlocked iPhone (15.5) connected. Buttons are responsive to clicks, but \u2026nothing I wildly click causes CarPlay visuals to appear. [Here\u2019s hoping I\u2019m not being really silly here! :crossed_fingers::skin-tone-3:] Same issue for me. iPhone 12 mini on iOS 15.4.1; Macbook Pro 13\" i5 2020 (MacBookPro16,2) on macOS 12.4 Can you try the steps in the CarPlay Simulator Help \"Troubleshooting CarPlay Simulator\"? In particular note if your iPhone has Personal Hotspot enabled. We are also tracking one known issue that we expect to have a fix for relatively soon. None of the steps in the Troubleshooting guide worked for me, including adding CarPlay Simulator to the Firewall settings I'm sorry the steps didn't resolve the issue. This sounds like something we need some more information on. Could you please submit feedback for this issue with both macOS and iOS sysdiagnose? It's also worth trying the latest beta of iOS 15 - 15.6 beta 2, it includes a fix to an issue that can lead to this problem. (FYI: CarPlay Simulator just kicked into life for me once I\u2019d turned off Personal Hotspot :+1::skin-tone-3:) I've been snapshotting a SwiftUI View ( UIHostingController = UIImage ) to create Point of Interest images. This works in the iOS Simulator (with CarPlay external display), but I've found on a real car head-unit that: - when my app is launched from CarPlay (and hasn't been launched on the device), - or the device is locked, I just get an empty image rendered when run via CarPlay. This seems to be related to the 'on-device app' being inactive, despite the CarPlay app being active. I'm hitting walls in my knowledge of iOS app activation states\u2013\u2013 is there there some way (API) you can think of that I could call in my CarPlay side of things, to 'wake up' (:man-shrugging::skin-tone-3:) the 'on-device app'? Double check your scene delegates and that you are doing your drawing work on the carplay scene/display otherwise please file a feedback with a small sample app replicating the issue you are seeing <@U03HBMBDJNS> When you say \u2018doing your drawing work on the carplay scene/display\u2019, I\u2019m not quite sure what you mean? I\u2019m currently calling the CPPointOfInterestTemplate \u2019s setPointsOfInterest method, and the drawing code, inside a DispatchQueue.main.async call (because that was the first thing I tried when having UI-related issues), if that\u2019s of any relevance? (This code path runs in response to the delegate\u2019s didChangeMapRegion and also CLLocationManager \u2019s didUpdateLocations ) (Re: work in the right place -> do you know if there\u2019s a specific queue/etc I should be calling drawing code via, when in CarPlay, for it to function?) This is probably a question better answered by the SwiftUI team, but filing a feedback with a sample app would be helpful for us to understand if this use case is expected to work. Noted, thanks :relaxed: And, to answer your question about 'waking up' the app on the phone, there is no means of doing this. Apps are only launched (given scenes) by the system (generally this is because the user launched the app in that environment) For a generic notification that shows up when your app is not in the foreground...are those just time-sensitive notifications or are those restricted to only certain types of entitlements? Hi! We only allow notifications for certain categories such as navigation apps, parking apps, and EV charging apps. Notifications are also allowed, but further restricted for some categories, (such as VoIP and Messaging apps) where the notification is handled using a Siri intent. When requesting notifications authorization for user notifications, include the allowInCarPlay option: https://developer.apple.com/documentation/usernotifications/unnotificationcategoryoptions/1649281-allowincarplay And make sure to post the notification with that category. Separate out notifications intended for CarPlay into their own category to avoid other notifications your app might post that aren\u2019t relevant while driving from appearing in CarPlay. Is there any way to have two different icons for each tab? one for when the tab is selected and one for when the tab is not selected? I have tried to change the tab icon when the template appears/disappears, but it does not work. You should get a CPTabBarTemplateDelegate callback when the active tab changes, but any changes to the tab image will only be picked up when the tab bar template is created OR reloaded with a new set of tabs, via updateTemplates: . This would be a great feedback to file! Thank, I will file a feedback Can a CarPlay app run in the background, for location updates, and trigger a local notification? (i.e. location services background mode) (Or do CarPlay apps not have access to the same functionality when 'closed'?) Use case = an EV charging app, which launches Maps to navigate to a charging station, but monitors location updates in the background, and \u2013 when the user is a few miles from the charging station location \u2013 checks the station's availability again; to notify the user if the station is no longer available. Background app activity policies are the same in CarPlay. If your app already supports a background location service such as an arrival geofence for that charger the app will continue to support that behavior when launched or backgrounded in CarPlay as well. Yay! :partying_face: Do I have any access to car information through carplay? Can you please clarify the use case you are trying to address here? I was hoping for mileage information, for trip durations, and just noting this mileage is being recorded as a business trip or a personal trip. Unfortunately that information isn't passed over, no. What is the expected image size for the section header button (introduced in iOS 15)? Hello! CPMaximumListSectionImageSize will give you the maximum size of the section header image. In the CPListSection header, note the following on the initializer that includes the headerImage : > @note The maximum size of the section header image is given by CPMaximumListSectionImageSize . Using the value you retrieve from CPMaximumListSectionImageSize will always be correct for the version of iOS on your user\u2019s device > CPMaximumListSectionImageSize > returns something like 20*20 but the image looks larger than that. when I provide a system images, the size looks good on simulator and different head units, but when I provide my image (and I size the image based on CPMaximumListSectionImageSize ) the image appears with different sizes on different head units, some times it is larger that it should be, some times very smaller. ) and my other question is that what should happens if I provides an image with wrong size? Will CarPlay resize that image to fit? What if I provide a pdf or svg image? <@U03HBMB6TPG> would you please take a look at these questions? Your image will be resized to the maximum size if it's too large, but it won't be scaled up if its too small. It's worth mentioning that you should size images to the car\u2019s display scale, using carTraitCollection Thanks <@U03HBMB6TPG> any idea how something like this can happen? Thank you for the note <@U03JBFATXM2>! We\u2019re aware of the issue and are tracking it Is there any workaround for this at the moment <@U03HBMB6TPG>? In the meantime, if your app can resize the image to CPMaximumListSectionImageSize first, it should result in the right size on the car screen > Sorry may be my question was not clear enough, I was asking about the section header button from the beginning, not the section header image. Is CPMaximumListSectionImageSize also applies to the header button? <@U03HBMB6TPG>? You\u2019re right! That button also needs to be resized. CPMaximumListSectionImageSize should be a good starting point there too. We\u2019ll definitely look into this for the CPButton, CPButtonMaximumImageSize is provided Thank you for the answers <@U03HBMB6TPG>. Is there any estimate for when this will be fixed, will it be fixed for iOS 16 release? No problem <@U03JBFATXM2>, thank you for developing with us! We\u2019re investigating the issue, but can't share a timeframe at this time In iOS Simulator, with the additional CarPlay external display options enabled via defaults , there's Load and Save functionality for simulation config (screen dimensions, scale, etc) Are there any defaults that can be set to enable load/save functionality in the new CarPlay Simulator app? (Use case: swapping between different 'cars' for testing) Unfortunately this isn't a currently supported feature, but please file a feedback request for this! Noted :+1::skin-tone-3: FB10165659 (Support loading+saving of CarPlay Simulator configuration) When it comes to color of icons and images, it seems as those need to already be the color you want them to be for displaying. It doesn't seem that I can tint them when displaying them in the CarPlay UI. Is that correct or am I doing something wrong? Hi! It will depend on the specific UI element you\u2019re looking at. Some elements, like nav bar leading and trailing buttons, are tinted by the system, while images in your list items and grid templates should support any custom colors you provide Maybe you can share which elements you\u2019re looking at? Thanks! ok. I\u2019m specifically thinking of like icons in the list and grid template. I\u2019ve tried the way we do it in the iPhone app with tinting, and that just hasn\u2019t seemed to work so far. I most recently tried withTintColor for example. I was trying to avoid running them through something like UIGraphicsImageContext Maybe another thing to try is to use UIImageAsset wherever possible, which should let you provide a day/night variant of each image (for day/night mode in CarPlay) If things aren\u2019t looking right though, please file a feedback issue and we will look into it :slightly_smiling_face: thanks! <@U03HJA80GLS> When using UIImageAsset, the pickup of the correct asset also depends on the interfaceController.carTraitCollection which is nil at some point, so if we use an image asset to create the template when the carTraitCollection is nil we will get the wrong asset. Is there any workaround for this? Thanks! That\u2019s worth a separate feedback for sure. I don\u2019t think we have any workaround to share right now, but it\u2019s under investigation. For testing CarPlay with iOS Simulator, it appears that the CarPlay window shown with iOS Simulator is not responding to external inputs on Apple silicon. Is that a known issue? Hi there \u2014 we're aware of an issue with inputs on the Xcode Simulator and are looking into it. Cool thanks ^^ Just want to make sure I didn\u2019t do something dumb :sweat_smile: Any chance for an answer previous question regarding the ability to share text from one CarPlay app to another? Hi! Maybe you can share more about your particular use case? In general, lots of third party services offer libraries/frameworks for these kinds of text operations, which could be an option for you. Another possibility is if you\u2019re looking for a specific app, you can use URL schemes. There you can open a URL on your template application scene, which should cause the desired app to launch in CarPlay (with the payload you provide), but again that\u2019ll be specific to the particular app you\u2019re integrating with. <@U03HJA80GLS> Yeah, I just didn\u2019t save the question and it probably was lost among other CarPlay questions\u2026 Here\u2019s the case: The user is in the car and he wants to send an email via some mailing app. He can do it using voice commands - dictate some text and requests to send. We\u2019d like to provide the user an ability to dictate that text in our app for CarPlay, which will perform some manipulations with the text (grammar checks, punctuation fixes, etc.) and \u201creturn\u201d it to the user, so that he could send it to another app (e.g. Mail app) using some voice command. Is it possible to implement such an app and would it be possible to let the user send it to another app - either using a direct command or using some copy/paste flow? See the CarPlay App Programming Guide for the supported categories of apps in CarPlay: https://developer.apple.com/carplay/documentation/CarPlay-App-Programming-Guide.pdf Technically, we can fit under the communication category ) But is there gonna be a way for the user to use Siri and ask her to send our text to someone using, for example, iMessage?","title":"uiframeworks"},{"location":"wwdc22/uiframeworks-lounge.html#uiframeworks-lounge-qas","text":"","title":"uiframeworks-lounge QAs"},{"location":"wwdc22/uiframeworks-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/uiframeworks-lounge.html#was-something-changed-on-uidraginteraction-when-i-tried-doing-local-drag-and-drop-move-in-my-app-the-drop-gets-cancelled-if-my-finger-is-not-perfectly-still-when-i-lift-it-bug","text":"That may be bug. Feel free to file a feedback and join a UIFrameworks lab where we can look into this together! It would also be interesting to know if you have any conflicting gesture recognizers. If you do have any other gesture recognizers attached, I would suggest disabling all of them and enabling them one at a time to see which one is causing the conflict. That\u2019s a very good suggestion <@U03HELXCTGV>! I have a soup of gesture recognizers. Might be a UISwipeGR! Any chance we can lower the duration of the lift gesture (without doing some ugly hacks)? I know the duration is reduced if it\u2019s dragged with a pointer.","title":"Was something changed on UIDragInteraction? When I tried doing local drag and drop (move) in my app, the drop gets cancelled if my finger is not perfectly still when I lift it. Bug?"},{"location":"wwdc22/uiframeworks-lounge.html#does-the-browser-style-navigation-barcontroller-manage-the-forward-navigation-stack-automatically-and-if-so-is-it-accessible-for-say-a-custom-swipe-forward-gesture-a-la-safari","text":"The browser mode doesn\u2019t manage a forward navigation stack, so you\u2019ll have to bring your own implementation (and a gesture if you like). It may also be more flexible to bring your own back/forward stack for this case, as if your app adapts to compact modes you may want to fallback to a more traditional navigation style (the navigation bar will do so automatically) Looking at the UINavigationItem docs, I see there's a new backAction property but there's not a corresponding one for going forwards. So is the forwards button itself something I'd also need to add myself? typically for forward/backward navigation we would recommend using UIBarButtonItem s for that (in the leading position) the backAction gets translated slightly differently, and would (in browser mode) produce a typical back button rather than what you likely want for \u201cgo back\u201d in the browsing stack its certainly a subtlety here, but this is what Files does and what we would probably say is best I see. So putting my own bar button item in the leading position would place it after the builtin back button but before the title And to match the appearance, it seems like the back button is just using the chevron.left SF symbol in the default size/config the default is chevron.backward (which is left in LTR). If you specify this as the image for a UIBarButtonItem UIKit will supply the proper symbol config to match so to sum up, to get File\u2019s style, set 2 items with chevron.backward & chevron.forward and manage your own stack (replacing content as necessary). Then you can reorganize when in compact to a more traditional UI Awesome, thank you!","title":"Does the browser-style navigation bar/controller manage the forward navigation stack automatically, and if so, is it accessible for, say, a custom swipe forward gesture a la Safari?"},{"location":"wwdc22/uiframeworks-lounge.html#thanks-for-the-great-session-love-uikit-so-much-in-a-uikit-app-if-you-enable-mac-catalyst-with-the-mac-idiom-is-it-possible-to-opt-into-native-macos-swiftui-by-default-it-uses-the-ios-style-for-form-for-example-which-isnt-ideal-when-youre-building-ui-thatll-only-be-used-in-your-mac-app-thanks","text":"No, you can either use SwiftUI in native macOS (AppKit) mode, or Catalyst (UIKit) mode, but not both.","title":"Thanks for the great session! Love UIKit so much. In a UIKit app if you enable Mac Catalyst with the Mac idiom, is it possible to opt into native macOS SwiftUI? By default it uses the iOS style for Form for example which isn't ideal when you're building UI that'll only be used in your Mac app. Thanks!"},{"location":"wwdc22/uiframeworks-lounge.html#thank-you-for-such-a-great-improvments-as-of-ios-15-when-youre-using-estimated-size-of-nscollectionlayoutsections-boundarysupplementaryitems-you-cant-simultaneously-use-its-visibleitemsinvalidationhandler-properly-does-that-fixed-in-ios-16","text":"Hi, we still don\u2019t support mutating items via visibleItemsInvalidationHandler in Compositional Layouts that are self-sizing. If you\u2019d like to talk about your specific use case, I\u2019m happy to talk about alternatives, in either our Q&A later today (Pacific Time), or in a 1:1 Lab session. Probably a 1-1 lab might be better for this question. Thank you! 1-1 lab sounds good","title":"Thank you for such a great improvments! As of iOS 15, when youre using .estimated() size of NSCollectionLayoutSections boundarySupplementaryItems you can`t simultaneously use its visibleItemsInvalidationHandler properly. Does that fixed in iOS 16?"},{"location":"wwdc22/uiframeworks-lounge.html#will-multiple-windows-be-supported-in-ios-right-now-we-need-to-use-uikit-lifecycle-if-we-want-to-show-hud-that-overlays-the-entire-app-for-example-httpswwwfivestarsblogarticlesswiftui-windowshttpswwwfivestarsblogarticlesswiftui-windows","text":"Hi there, do you mean supported in SwiftUI ? I think this might be a better question for the <#C03HX19UNCQ|>. Hi <@U03HELXCTGV>, thank you, yes you're right","title":"Will multiple windows be supported in iOS? Right now we need to use UIKit lifecycle if we want to show HUD that overlays the entire app. For example: https://www.fivestars.blog/articles/swiftui-windows/|https://www.fivestars.blog/articles/swiftui-windows/"},{"location":"wwdc22/uiframeworks-lounge.html#wed-like-to-improve-our-app-framework-to-have-a-better-desktop-class-experience-for-that-wed-like-to-utilize-uicontextmenuinteraction-and-the-new-uieditmenuinteraction-how-should-we-decide-where-to-use-one-or-the-other-can-context-menus-on-ipados-be-presented-in-place-like-on-macos-without-a-targeted-preview","text":"UIEditMenuInteraction powers the new light-weight edit menu. With edit menu interaction, on right-click on iPadOS (and macOS), it\u2019ll also automatically present the more compact context menu without a targeted preview. Use Edit menu interaction in contexts where you don\u2019t need a preview, or where the menu shouldn\u2019t block the content (i.e. text, or some canvas type view like Keynote). Thank you! If I understand correctly, would you recommend using Edit menu to present actions for inserting objects onto a canvas like in Keynote (after a long press or a right click), because a context menu would block the content? UIEditMenuInteraction also has an arrow you can point at something, unlike UIContextMenuInteraction . Thank you, Glen! And from what I\u2019ve seen so far, Edit menu can be triggered programmatically as a result of a user action or custom gesture. To make sure: can context menus be triggered in that way as well? Anyway, thank you both for taking the time to answer my question! We do not allow for programmatic activation of context menus, and only for the lightweight edit menu. Got it. Thank you for clarifying this!","title":"We'd like to improve our app &amp; framework to have a better desktop-class experience. For that, we'd like to utilize UIContextMenuInteraction and the new UIEditMenuInteraction. How should we decide where to use one or the other? Can context menus on iPadOS be presented \"in place\", like on macOS, without a targeted preview?"},{"location":"wwdc22/uiframeworks-lounge.html#in-ios-15-compositional-layout-with-orthogonal-scrolling-behaviour-with-a-single-section-in-non-full-screen-size-also-scrolls-vertically-what-is-the-correct-way-to-prevent-vertical-scrolling-for-single-horizontal-section","text":"If it is the outer (collection) view that is scrolling, you can set alwaysBounceVertical = false on it to prevent the scrolling. However if it is just a single section you may be better of not using an orthogonal section but just a horizontally scrolling collection view. This might be a great question to discuss in the labs later this week to find a solution that works best for you. Thanks, other solution we tried to go back with the flow layout but we then loose the nice things about the declarative relation as in compositional layout.","title":"In iOS 15, compositional layout with orthogonal scrolling behaviour with a single section in non-full screen size also scrolls vertically, what is the correct way to prevent vertical scrolling for single horizontal section?"},{"location":"wwdc22/uiframeworks-lounge.html#in-new-uicalendarview-is-it-possible-to-change-the-layout-like-in-the-ios-calender-app","text":"That isn\u2019t possible at the moment. Please file a feedback for us if this is something you\u2019d like to see us add in the future. Sure, thanks.","title":"In new UICalendarView, is it possible to change the layout? Like in the iOS Calender app?"},{"location":"wwdc22/uiframeworks-lounge.html#since-setvalueforkey-on-uidevice-is-not-longer-supported-to-force-orientation-is-there-are-step-by-step-explanation-of-how-to-force-landscape-orientation-for-1-uiviewcontroller-only","text":"You can call +[UIViewController attemptRotationToDeviceOrientation] and return an updated UIInterfaceOrientationMask from -[UIViewController supportedInterfaceOrientations] . It\u2019s also important to note that setValue:forKey: to access and modify internal state is never a supported way to do things. You might get lucky but you should really consider this as not being supported and should file feedback to request API for the feature you are trying to access. Thanks, I somehow missed attemptRotationToDeviceOrientation so I\u2019m going to give it a try in a sample project. If that doesn\u2019t work i\u2019m going to see if I can join a lab. I\u2019ve always avoided using setValue:forKey: but a client was stubborn and demanded a fullscreen button that turned the orientation so I had too. But thanks I\u2019m going to give it a try :slightly_smiling_face: Hey <@U03JRR4R3CY> we have also such a button and the same problem now. After the button press we support that the user can rotate back with a physical orientation change. It's not a nice solution in my opinion, but it seems to work to temporarily override supportedInterfaceOrientation, call attemptRotationToDeviceOrientation() and then reset supportedInterfaceOrientation to all normally allowed orientations again. I started making a prototype here. Maybe it's useful for you too.","title":"Since setValue:forKey: on UIDevice is not longer supported to force orientation. Is there are step by step explanation of how to force landscape orientation for 1 UIViewController only?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-customize-the-colors-of-the-chevron-thats-shown-in-the-uinavigationbar-when-returning-a-uimenu-in-the-new-titlemenuprovider-function-on-uinavigationitem","text":"The chevron will adapt automatically for light/dark mode, but there's no way to specify custom colors. If you feel that this is necessary for your app, please file a feedback report! Ouch, that\u2019s unfortunate! But thanks for answering my question :pray:","title":"Is there a way to customize the colors of the chevron that\u2019s shown in the UINavigationBar when returning a UIMenu in the new titleMenuProvider function on UINavigationItem?"},{"location":"wwdc22/uiframeworks-lounge.html#the-session-mentioned-nswindowcollectionbehavior-but-how-will-stage-manager-interact-with-nswindowlevel-more-specifically-if-i-have-a-window-in-my-app-that-i-globally-float-above-all-other-windows-by-setting-the-level-to-nswindowlevelscreensaver-will-that-window-still-appear-in-stage-manager","text":"Window levels still work the same way in Stage Manager, in that they\u2019ll impact the z-ordering of your window with respect to other windows on screen. Otherwise, its relationship to other windows in center stage (like whether it\u2019s considered \u201cfloating\u201d or \u201cauxiliary\u201d) is dictated by its collection behavior. Thank you Jeff!","title":"The session mentioned NSWindow.collectionBehavior, but how will Stage Manager interact with NSWindow.level? More specifically, if I have a window in my app that I globally float above all other windows by setting the level to NSWindow.Level.screenSaver, will that window still appear in Stage Manager?"},{"location":"wwdc22/uiframeworks-lounge.html#id-really-love-to-learn-some-more-about-extensionkitextensionframework-and-extension-uis-are-there-any-sessions-that-cover-these-and-if-not-what-would-be-a-good-loungelab-to-ask","text":"There aren\u2019t any videos or labs specifically about ExtensionKit, but there\u2019s some new documentation that explains how it (plus ExtensionFoundation) work together to host UI and non-UI extensions https://developer.apple.com/documentation/extensionkit I\u2019ve been checking out that documentation! It looks awesome. It appears that there may need to be some additional host configuration (perhaps in the Info.plist) to register extension points. Is that right, and is that documented anywhere? The documentation specifically says \u201cA Mac app can also declare its own extension points so that other apps can extend the Mac app\u2019s functionality.\u201c, but I cannot find any details on how. It looks like the details on that might be under-documented in the current seed. Could you write up a docs feedback using the Feedback Assistant app? Filed FB10061654. Thank you so much! Are there any good labs for asking about these frameworks? There aren\u2019t any labs about ExtensionKit this year. One thing I did notice, though, which might be catching you \u2014 if your documentation viewer is set to Objective-C, you won\u2019t see the Swift-only API in ExtensionKit and ExtensionFoundation. Ah yes, look at that! I was using the Swift view though. It looks like I\u2019ll have to wait for updated docs - as far as I can tell, it is impossible to configure a hosting app without some additional information. Hi <@U03JKSLQ7J6>, There is some additional extension point configuration required. The \"Host\" app must declare an extension point by installing am .appextensionpoint file in the app bundle's Extensions directory. This is a minimal extension point declaration file: <@U03JNQ1ABFC> oh wow this is incredibly helpful, I\u2019m really excited to check this out!","title":"I'd really love to learn some more about ExtensionKit/ExtensionFramework and extension UIs. Are there any sessions that cover these, and if not, what would be a good lounge/lab to ask?"},{"location":"wwdc22/uiframeworks-lounge.html#if-i-have-an-app-thats-primarily-written-in-appkit-is-there-any-way-to-use-swiftui-to-define-toolbar-items-from-within-an-appkit-window-or-does-the-window-itself-need-to-be-created-by-swiftui-too-for-that-to-work","text":"You can attach your own NSView hierarchies to NSToolbarItems. Within these, you can use the NSHostingController or NSHostView classes offered by SwiftUI to embed a SwiftUI view hierarchy in an AppKit view hierarchy. It is correct that the SwiftUI .toolbar() modifier only works with windows that are created and managed by SwiftUI. But, as Chris said, you can bring SwiftUI content into your NSToolbar using a hosting view. Got it, thanks!","title":"If I have an app that's primarily written in AppKit, is there any way to use SwiftUI to define toolbar items from within an AppKit window? Or does the window itself need to be created by SwiftUI too for that to work?"},{"location":"wwdc22/uiframeworks-lounge.html#assuming-i-am-not-doing-anything-custom-or-low-level-involving-metal-are-there-any-appkit-specific-apis-to-be-aware-of-to-fully-support-promotion-displays","text":"macOS 12 added these new APIs to NSScreen. // Variable Rate Refresh @interface NSScreen () /** The maximum frames per second this screen supports. */ @property (readonly) NSInteger maximumFramesPerSecond API_AVAILABLE(macos(12.0)); /** The minimum refresh interval this screen supports, in seconds. This is the shortest amount of time a frame will be present on screen. minimumRefreshInterval and maximumRefreshInterval will be the same for displays that do not support variable refresh rates. */ @property (readonly) NSTimeInterval minimumRefreshInterval API_AVAILABLE(macos(12.0)); /** The maximum refresh interval this screen supports, in seconds. minimumRefreshInterval and maximumRefreshInterval will be the same for displays that do not support variable refresh rates. */ @property (readonly) NSTimeInterval maximumRefreshInterval API_AVAILABLE(macos(12.0)); /** The update granularity of the screen's current mode, in seconds. The display will update at the next boundary defined by the granularity, after the minimum refresh interval has been reached. When 0, the display can update at any time between the minimum and maximum refresh rate intervals of the screen. Fixed refresh rate screen modes will return the refresh interval as the update granularity (e.g. 16.66ms for 60Hz refresh rates), meaning updates only occur at refresh rate boundaries. */ @property (readonly) NSTimeInterval displayUpdateGranularity API_AVAILABLE(macos(12.0)); /** The time at which the last framebuffer update occurred on the display, in seconds since startup that the system has been awake. */ @property (readonly) NSTimeInterval lastDisplayUpdateTimestamp API_AVAILABLE(macos(12.0)); @end If you are managing your own refresh timer, these can be helpful. But if you\u2019re just doing normal AppKit stuff, you don\u2019t need to take any action. NSAnimation functionality, for example, will \u201cdo the right thing.\u201d","title":"Assuming I am not doing anything custom or low-level involving Metal, are there any AppKit-specific APIs to be aware of to fully support ProMotion displays?"},{"location":"wwdc22/uiframeworks-lounge.html#in-jeffs-appkit-session-he-mentioned-heightofrow-timing-will-be-different-in-ventura-for-dynamic-row-heights-does-it-still-work-with-auto-layout-auto-resized-row-heights-i-dont-implement-heightofrow","text":"Row height estimation as discussed in the session only apply to NSTableViews whose delegate implements heightOfRow. Auto-layout based rows will continue work as before. Excellent. Thanks. Sorry for the double post.","title":"In Jeff\u2019s AppKit session, he mentioned heightOfRow  timing will be different in Ventura for dynamic row heights. Does it still work with auto-layout auto-resized row heights? I don\u2019t implement heightOfRow."},{"location":"wwdc22/uiframeworks-lounge.html#hi-again-another-question-that-myke-said-would-be-best-for-this-venue-still-working-with-swiftui-xcode-134-under-macos-monterey-tried-to-restrict-orientation-for-deployed-app-to-iphoneportrait-orientation-by-using-the-check-box-in-generaldeployment-orientation-but-it-still-tries-to-work-when-going-to-landscape-leftright-in-the-process-it-refuses-to-scroll-in-those-orientations-and-when-i-return-to-portrait-the-headersui-elements-are-scrunched-this-is-not-solved-until-i-send-the-app-to-background-and-open-the-app-again-any-ideas-on-how-to-tackle-this-would-be-appreciated-thanks","text":"You have a portrait-only iPhone app deployed to the Mac App Store, and on the Mac, the app is rotatable, in spite of being portrait-only? That should not happen. Please use Feedback Assistant to file a bug report! Good catch! Apologies for the confusion, and incorrect question submission: this is an iPhone app still in development, and has not been deployed to the Mac App Store. Ah, thank you for clarifying! Is there a lounge where SwiftUI/iOS questions could be submitted? I obviously lost my way\u2026 We do have swiftui-lounge for SwiftUI questions, but this might be a more generic UIKit question, so you're not in the wrong place, here. For context, is the entire app SwiftUI? > Is there a lounge where SwiftUI/iOS questions could be submitted? There's a SwiftUI Q&A later today: https://developer.apple.com/wwdc22/110393 Apologies for the delayed response: it is entirely in SwiftUI. The original version of the app was in UIKit, and the problem is not present there. Thank you for the info on the SwiftUI Q&A lounge! np! We're following up with some SwiftUI folks on the expectation here <@U03J21EKNSE>, I just created a super simple SwiftUI test project and scoped it to portrait only on the iPhone in \"Deployment Info -> iPhone Orientation\" and the app doesn't rotate when I rotate the device physically If you have more information on this we'd love to hear it Otherwise, if you can reduce this to a sample project and file a feedback we'd like to look into it! Hello again Steve M. Thanks for doing this. Were you using Xcode 14 for this test project? I am seeing this in Xcode 13.4. I am having trouble downloading 14 (not surprising with everyone trying to download!). I will search for info on how to submit feedback (never done it before); hope to get it fixed! Thank you for the follow up; have a good afternoon! I was using Xcode 14. And for Feedback, there's a handy page here: https://developer.apple.com/bug-reporting/ Thank you: incentive to dare use a beta version of Xcode (another first time!)","title":"Hi again! Another question that Myke said would be best for this venue. Still working with SwiftUI, Xcode 13.4 under macOS Monterey: tried to restrict orientation for deployed app to iPhone/portrait orientation by using the check box in GeneralDeployment orientation, but it still tries to work when going to landscape left/right. In the process, it refuses to scroll in those orientations, and when I return to portrait the headers/UI elements are \u201cscrunched.\u201d This is not solved until I send the app to background, and open the app again. Any ideas on how to tackle this would be appreciated, thanks!"},{"location":"wwdc22/uiframeworks-lounge.html#does-nstableview-support-dynamic-row-heights-for-multi-column-tables-when-i-tried-that-if-i-had-multiple-columns-it-didnt-resize-the-row-to-the-tallest-cells-content","text":"Automatic row height rows should size to fit the tallest cell's content. If that's not working for you, we'd love you to write a feedback, ideally with a sample project. Ok will do. Thanks.","title":"Does NSTableView support dynamic row heights for multi-column tables? When I tried that, if I had multiple columns, it didn't resize the row to the tallest cell's content."},{"location":"wwdc22/uiframeworks-lounge.html#how-can-nswindowcollectionbehavior-be-set-in-a-swiftui-app-and-if-it-can-be-does-that-apply-to-the-entire-app-or-only-a-specific-windowgroupdocumentgroup","text":"There isn\u2019t currently a way to request different collection behaviors from SwiftUI \u2014 so that would be an excellent enhancement request in Feedback Assistant!","title":"How can NSWindow.collectionBehavior be set in a SwiftUI app? And if it can be, does that apply to the entire app or only a specific WindowGroup/DocumentGroup?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-present-the-menu-of-a-uieditmenuinteraction-thats-associated-with-an-instance-of-uitextinput-im-working-on-a-text-editor-where-im-programmatically-showing-the-menu-previously-i-could-use-uimenucontrollersharedshowmenufromrect-whats-the-equivalent-with-uieditmenuinteraction-to-show-a-menu-with-all-the-standard-editing-actions","text":"Yep, UIEditMenuInteraction has a method called presentEditMenu(with:configuration) , which can be used in the same way. The default menu will contain the standard edit actions. You can override the menu if you'd like via menuForConfiguration on UIEditMenuInteractionDelegate. <@U03HELXCTGV> Thanks for your replies. I tried grabbing the UIEditMenuInteraction from UITextInput\u2019s interactions and present it with presentEditMenu(with:) . However, I\u2019ll need to pass a configuration with an ID. When supplying nil as ID, the menu isn\u2019t shown because it doesn\u2019t have any command and/or actions. If I pass UITextSelectionView.SelectionCommands as ID it works but that feels a bit wrong. I\u2019ve only gotten this ID by happenstance because my console showed some errors earlier. Is this the recommended way of doing it? You should install your own UIEditMenuInteraction; if nothing is showing when you call presentEditMenu(with:) , then there\u2019s a good chance that nothing is responding YES to canPerformAction:withSender: so I\u2019d verify that your actions can be performed. Also, if you just want to add/remove/change some menu items for the edit menu that comes up on a UITextInput like text view etc., you can override buildMenuWithBuilder: on any suitable view controller in your hierarchy and use UIMenuBuilder to change the menu.","title":"Is there a way to present the menu of a UIEditMenuInteraction that\u2019s associated with an instance of UITextInput? I\u2019m working on a text editor where I\u2019m programmatically showing the menu. Previously I could use UIMenuController.shared.showMenu(from:rect:). What\u2019s the equivalent with UIEditMenuInteraction to show a menu with all the standard editing actions?"},{"location":"wwdc22/uiframeworks-lounge.html#xcode-1341-m1-max-macbook-pro-macos-124-a-pure-swiftui-app-run-on-my-mac-mac-catalyst-choosing-a-signinwithapplebutton-added-in-a-view-crashes-the-app-with-_authenticationservices_swiftuisigninwithapplebuttonswift303-fatal-error-attempting-to-present-asauthorizationcontroller-from-a-swiftui-view-not-in-a-hierarchy-this-should-not-be-possible-please-file-feedback-anyone-ware-of-this-and-what-might-be-done-to-fix-this","text":"This sounds like a bug in SwiftUI. Please, file a Feedback request and attach this sample project Already did (FB10033240). Was hoping for some additional clues :slightly_smiling_face: This currently prevents me from updating an active app in the store with SignInWithApple functionality :confused: Thanks for the feedback number, I\u2019m taking a look now On the new SDKs, I\u2019m not reproducing a crash with the attached sample app. Have you tried with the new Xcode beta yet? not yet. Lacking a second M1 config to install beta on without compromising my working production enviroment. I\u2019ve added some detail to the bug, and also sent it to the AuthenticationServices team to double check. I don\u2019t have an immediate workaround, but from what I\u2019m seeing this should be resolved in the newer SDKs, so please update us once you have the chance to test if you\u2019re still encountering issues <@U03JH6HKC66> You can run the Xcode 14 beta alongside your existing Xcode install without issue. Thank you. I will try to get the beta SDKs running on my system asap so i can double check. Would be a shame if it could only be resolved with the new SDKs as it will be a while until i can upload to AppStore with them. <@U03J22A0C4S> i was under the impression i would need macOS 13 for Xcode 14. But seems i misread that. It's the other way around. Yeah, runs fine under 12.4. 13 is incompatible with Xcode 13. (Hopefully not permanently.) <@U03HHJP6N3C> i tried with Xcode 14.0 / macOS 12.4 I can still reproduce this behavior with Xcode 14.0 / macOS 12.4. Make sure to add and run Supported Destination Mac Catalyst . Designed for iPad is working fine. iOS Deployment Target 15.5 / macOS 12.3 (any higher settings give me a \"The app is incompatible with the current version of macOS\" on macOS 12.4). Thank you for the additional info, after changing some project settings I am reproducing the crash. Looking\u2026 Based on the crash backtrace, this does seem to be a bug with the SignInWithAppleButton in Mac Catalyst. I\u2019ve gotten it to the right folks, we don\u2019t have a workaround for you right away but we\u2019ll update the bug as we look into it. Thank you for bringing this to our attention! Thank you. I am happy you were able to reproduce the bug so i am sure it\u2019s not something i am doing wrong. Looking forward for a fix / updates.","title":"Xcode 13.4.1 / M1 Max MacBook Pro / macOS 12.4 a pure SwiftUI app run on \"My Mac (Mac Catalyst)\"   choosing a SignInWithAppleButton() added in a view crashes the app with:  _AuthenticationServices_SwiftUI/SignInWithAppleButton.swift:303: Fatal error: Attempting to present ASAuthorizationController from a SwiftUI view not in a hierarchy. This should not be possible, please file feedback.  Anyone ware of this and what might be done to fix this?"},{"location":"wwdc22/uiframeworks-lounge.html#does-uiframeworks-mean-not-swift-ui-is-this-code-for-non-swiftui-uiframeworks","text":"If this is a pure SwiftUI question you should use the SwiftUI lounges. If this is a question about how to use UIKit/Appkit with SwiftUI this might be a good lounge to ask your question. K. Point I am making is general, that maybe using \"UIFrameworks\" as a way of saying \"Not Swift UI\" is not clear. I think UIKit could get its own room (IDK what AppKit people would want). Apple is probably going to be making this distinction for a while so maybe think about what you call \"UIFrameworks but not SwiftUI\"","title":"Does \"UIFrameworks\" mean not Swift UI? Is this code for non-SwiftUI UIFrameworks?"},{"location":"wwdc22/uiframeworks-lounge.html#starting-from-ios-16-buttons-on-the-screen-dont-receive-touches-when-a-uimenucontroller-nay-a-menu-of-a-uieditmenuinteraction-is-presented-when-attempting-to-select-a-button-on-the-screen-thats-outside-of-the-menu-the-menu-will-be-dismissed-but-the-selected-button-will-not-receive-the-touches-this-is-a-change-from-ios-15-is-this-intentional-or-a-bug-in-beta-1-the-behavior-in-ios-16-poses-a-problem-in-my-app-that-lets-users-select-some-text-in-which-case-the-uimenucontroller-may-be-shown-and-act-on-that-selected-text-by-selecting-a-button-on-the-screen","text":"Hey hey! This is definitely a bug and we are tracking this internally already. Awesome! Glad to hear that. Thanks for your reply <@U03H31CT6S3> Yeah np! And if you find any issues, please feel free to file a feedback for us to address as soon as we can! Filing feedback on issues like this is very helpful! It\u2019s filed as FB10080311","title":"Starting from iOS 16, buttons on the screen don\u2019t receive touches when a UIMenuController (nay, a menu of a UIEditMenuInteraction) is presented. When attempting to select a button on the screen that\u2019s outside of the menu, the menu will be dismissed but the selected button will not receive the touches. This is a change from iOS 15. Is this intentional or a bug in beta 1?  The behavior in iOS 16 poses a problem in my app that lets users select some text, in which case the UIMenuController may be shown, and act on that selected text by selecting a button on the screen."},{"location":"wwdc22/uiframeworks-lounge.html#is-the-new-uihostingconfiguration-bound-only-to-collection-and-table-view-cells-or-is-this-api-designed-to-be-used-in-other-places-where-i-might-inject-swiftui-views-in-the-middle-of-custom-uikit-view-hierarchies","text":"UIHostingConfiguration is designed to be used with UICollectionView and UITableView cells. Watch the video \u201cUse SwiftUI with UIKit\u201d coming on Thursday later this week to learn more about it, and other ways to integrate SwiftUI into your UIKit app! https://developer.apple.com/wwdc22/10072 Thank you! So if I\u2019m vending a mostly UIKit-based framework and I want my users to be able to customize/replace a part of the hierarchy with SwiftUI, embedding a child UIHostingController continues to be the most recommended way to support this? UIHostingController looked to be a much better fit\u2026 :smile: UIHostingController is the most general-purpose API to embed SwiftUI inside your UIKit app. Because it is a view controller, you can use it anywhere you can present or embed a view controller in UIKit, and all SwiftUI features are supported inside of it (including ones that require a connection to the view controller hierarchy in UIKit). UIHostingController also has some new features in iOS 16, so be sure to watch the \u201cUse SwiftUI with UIKit\u201d video to learn more about all of this later this week. Noted, I\u2019ll be sure to do so. Thanks again for answering my questions!","title":"Is the new UIHostingConfiguration bound only to collection and table view cells? Or is this API designed to be used in other places where I might \"inject\" SwiftUI views in the middle of custom UIKit view hierarchies?"},{"location":"wwdc22/uiframeworks-lounge.html#most-sf-symbols-appear-to-have-transparent-background-colours-in-appkit-is-there-a-way-to-get-a-white-background-colour-so-that-coloured-symbols-look-good-on-nspopupbuttons-right-now-when-the-selection-is-hovering-over-an-nsmenuitem-with-a-coloured-sf-symbol-for-the-image-the-symbol-colour-doesnt-change-to-make-it-viewable-for-example-if-your-accent-colour-is-yellow-and-the-nsmenuitems-image-is-yellow-it-basically-disappears-from-view-yellow-on-yellow","text":"In general, we don\u2019t alter color images for states like menu highlight, because your image might be something like an icon which doesn\u2019t make sense to draw with a white foreground. However, I think your SF Symbol example is a good case where we could be smarter, since we know more about the underlying symbol image. That would be an excellent enhancement request for us. Excellent. I\u2019ll submit feedback for it. When setting to multi-coloured SF Symbols, NSPopupButton just completely removes all the colours. But explicitly setting colours for them using pallet configuration does show the colours, but doesn\u2019t invert or change the highlight state when traversing the popup button. Here\u2019s my popup button example with SF Symbols. NSMenu does have a delegation mechanism that can tell you when an item has been highlighted. Try implementing -menu:willHighlightItem: , and altering the highlighted item. Since these are symbols, you could use -[NSImage imageWithSymbolConfiguration:] to get a new image with a different configuration, and set that to the highlighted item\u2019s image property. Just be sure to restore the original image back to the previously-highlighted menu item and when the menu closes ( -menuDidClose: ).","title":"Most SF Symbols appear to have transparent background colours. In AppKit, is there a way to get a white background colour so that coloured symbols look good on NSPopupButtons? Right now when the selection is hovering over an NSMenuItem with a coloured SF Symbol for the image, the symbol colour doesn't change to make it viewable. For example, if your accent colour is yellow and the NSMenuItem's image is yellow, it basically disappears from view (yellow on yellow)."},{"location":"wwdc22/uiframeworks-lounge.html#is-it-possible-to-access-swiftui-environmentmodifiers-inside-uiviewrepresentables","text":"Yes. The context passed to make/updateUIView(context:) has an environment property you can read from. https://developer.apple.com/documentation/swiftui/uiviewrepresentable/makeuiview(context:) Or you can use @Environment in your representable struct. Thanks :blush:","title":"Is it possible to access swiftui environment/modifiers inside UIViewRepresentables?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-known-issue-with-keyboard-shortcuts-in-uitextview-where-if-you-type-command-b-then-press-b-straight-after-the-b-key-press-is-not-registered-its-when-the-keystroke-is-the-same-letter-as-the-previous-command-key-combination-i-thought-it-was-our-bug-originally-but-havent-made-any-headway-into-whats-causing-it","text":"Is this a new issue in iOS 16? Are you able to reproduce this in a sample app? This is not a known issue at the moment Are you still holding the command key on the second 'B' press? This is an issue in iOS 15 (not tried in 16). Definitely not still holding command. Sample app is our next step, although we hit it in two apps so far (but with some shared code so the bug could be in there). Unfortunately I\u2019m not able to reproduce this in a UITextView with allowsEditingTextAttributes enabled. Hitting \u2318B toggles boldface on the selected text, then hitting B replaces the now-bold text into a \u201cb\u201d. If you\u2019re able to reproduce this in a sample app, and file a bug report with the report attached (and post the report here), that would help us investigate the issue further!","title":"Is there a known issue with keyboard shortcuts in UITextView where if you type command-B then press \u2018B\u2019 straight after, the \u2018B\u2019 key press is not registered? (It\u2019s when the keystroke is the same letter as the previous command key combination.)  I thought it was our bug originally, but haven\u2019t made any headway into what\u2019s causing it."},{"location":"wwdc22/uiframeworks-lounge.html#what-should-i-do-when-i-encounter-performance-issue-when-i-have-hundreds-of-cashapelayers-ink-strokes-inside-a-huge-uiscrollview-content-view-should-i-consider-rasterization-solutions-such-as-catiledlayer","text":"To start, it might be better to try and see if you can flatten it and use a single shape layer instead of several separate ones. You should be able to combine the shapes together by using things like CGPathCreateCopyByUnioningPath . Depending on what you're trying to do, it might even be better to try and use a Metal shader instead. I encourage you to schedule a lab appointment so we can look at it together! <@U03HELXCTGV> Actually having one giant shape layer with huge amount of points is performing worse. I had to split a long stroke into multiple CAShapeLayers to prevent frame drops. I\u2019ve never used Metal before, it seems daunting. Where should I start? I\u2019m simply trying to add a lot of bezier paths into the canvas. You can try shouldRasterize = YES , which can be set on the shape layer or some super layer. Sometimes that will avoid the perf issues. You might want to try using SwiftUI Canvas, which you can embed in your UIKit app if you need to. SwiftUI Canvas uses Metal to render the draw commands you issue it, so it\u2019s very performant. https://developer.apple.com/documentation/SwiftUI/Canvas Caveat: shouldRasterize does slow down animation though, so keep it on only when your UI is at rest. It won\u2019t give you as much control as using Metal directly, but if you just need simple bezier paths, it might be a good higher level fit. Actually it\u2019s for inking, so I\u2019ll need to be able to dynamically add points, as well as moving them around. You may also want to try making a custom UIView subclass, override drawRect() , and use CoreGraphics to draw the ink strokes instead. This will allow you to only draw the strokes that are visible (in the rect parameter passed in to drawRect ). Yeah, so you would bracket your addition/animation/moving code with shouldRasterize = NO and shouldRasterize = YES . Does scrolling in the scroll view count as animation? Also, if I turn on shouldRasterize , things get blurry after I zoom in. How do I trigger a rerender? It shouldn't, but YMMV. The idea is that if nothing changes within the shape layer, it should be safe to rasterize w/o negative impact. I think you should be able to call setNeedsDisplay Scrolling does not count as animation. Animation would be if you were animating the individual CAShapeLayer between different bezier paths. Animating the position of CAShapeLayers with shouldRasterize enabled should also be okay. So, I have a giant container UIView (inside a scrollview), containing a ton of small but overlapping CAShapeLayers. What\u2019s the difference between turning on shouldRasterize on the container view vs. the individual shapelayers? I would guess that rasterizing the container view would be most performant, because scrolling would only be moving the one rasterized bitmap, rather than having to recomposite multiple bitmaps. But you'll really have to try this out to see if it works for you. Another thing you can try is setting drawsAsynchronously = true on the shape layers. This will allow the layers to perform the actual drawing on background threads, which may be faster, but since it won't be synchronized with the main loop, you might \"see\" the strokes being drawn independent of the current CA transaction. I think setting shouldRasterize on only the container view will rasterize all of the overlapping CAShapeLayers into one bitmap. Setting shouldRasterize on the only the individual CAShapeLayers will rasterize each bezier path separately, and CA will not cache the composition of them all together. I\u2019m not sure which of these is more or less performant, and the answer might depend on your implementation. Say I have a 10000pt height container view with two tiny shape layers on top-left and bottom-right, does setting shouldRasterize on the container view create a giant bitmap that will kill the app due to memory pressure? When I worked practically on these sorts of issues, I would have to tweak shouldRasterize depending on what was being displayed. E.g. turn it off for large shapes, turn it on for small shapes. You may even have to figure out which shapes could be combined in a rasterization e.g. by putting them in a common layer and setting the flag there. The main advantage of CAShapeLayer here (vs some manual rasterization like drawing into a context) is good memory use for large, unrasterized shapes and inbuilt animation between states. I noticed that with a lot of CAShapeLayer , backboardd gets saturated causing the frame drops, while none of my app\u2019s CPU and GPU are high. CALayers (including CAShapeLayer) are rendered with Metal in another process (as you\u2019ve observed), so that\u2019s why you\u2019re seeing that process get saturated, not your own. > Say I have a 10000pt height container view with two tiny shape layers on top-left and bottom-right, does setting shouldRasterize on the container view create a giant bitmap that will kill the app due to memory pressure? I think the memory pressure would be in another process, not your app (since CAShapeLayer is rendered out of process), but in general I think this would cause memory pressure on the system. So I don\u2019t think you want to use shouldRasterize on large layers. Is there a way to debug memory use by this rendering process? That\u2019s a good question, and I\u2019m not sure. Can you put these questions together in a developer forum post? If you paste the link to a post here, I can forward it on to the Core Animation team. Hmm, I\u2019m afraid I don\u2019t have time right now to write a post. Is there a lab I can go to to talk to the CA team? Is it Core Graphics Lab or UIKit lab? I\u2019m afraid not. A forum post is the best way to communicate with the Core Animation team. Yes, we have labs on Thursday and Friday. Make sure to sign up!","title":"What should I do when I encounter performance issue when I have hundreds of CAShapeLayers (ink strokes) inside a huge UIScrollView content view? Should I consider rasterization solutions such as CATiledLayer?"},{"location":"wwdc22/uiframeworks-lounge.html#whats-the-simplest-way-to-edit-the-airdropped-files-thumbnail-and-filename-for-the-ios-sharesheet-is-there-an-updated-or-modern-best-practice-version-of-the-sample-code-httpsdeveloperapplecomlibraryarchivesamplecodesc2273introductionintrohtmlapple_refdocuiddts40013842httpsdeveloperapplecomlibraryarchivesamplecodesc2273introductionintrohtmlapple_refdocuiddts40013842","text":"Provide a LPLinkMetadata object to the activity view controller. The easiest way to do this is using a UIActivityItemsConfiguration object. let sharedItem = NSItemProvider(contentsOf: url) let aic = UIActivityItemsConfiguration(itemProviders: [sharedItem]) let previewImage = UIImage(...) let previewImageProvider = NSItemProvider(object: previewImage) let lm = LPLinkMetadata() lm.URL = url lm.imageProvider = previewImageProvider aic.linkMetadata = lm let avc = UIActivityViewController(activityItemsConfiguration: aic) // present avc","title":"What's the simplest way to edit the AirDropped file's thumbnail and filename for the iOS sharesheet?  Is there an updated or modern best practice version of the sample code: https://developer.apple.com/library/archive/samplecode/sc2273/Introduction/Intro.html#//apple_ref/doc/uid/DTS40013842|https://developer.apple.com/library/archive/samplecode/sc2273/Introduction/Intro.html#//apple_ref/doc/uid/DTS40013842"},{"location":"wwdc22/uiframeworks-lounge.html#hi-how-can-i-change-some-properties-of-nswindow-such-as-the-titlebar-color-when-using-catalyst","text":"Use the UITitlebarTitleVisibilityHidden option on UITitlebar.titleVisibility to disable the titlebar material, window title, and its toolbar. You can then provide your own view underneath. The window\u2019s safe area insets still apply and the area beyond those insets remain draggable. Note that the UITitlebar cannot have an NSToolbar in order for this to work. Also feel free to file feedback requesting enhancements if you wish to do this while maintaining a window toolbar! Therefore there is no way I can keep the window toolbar so that the user can still see (for example) the window title (and modify the background color at the same time)? That is correct, there is no way to do that by default. But if you set the titlebar visibility hidden, since you then manage your own content in that region you can add a UILabel with the window title Design-wise, on macOS we do generally prefer to use the standard toolbar/titlebar backing for consistency across the system. It plays an important role in communicating which window is currently frontmost. There are exceptions, like the new Weather app, but they\u2019re fairly exceptional. It\u2019s totally OK to go for something custom (we love creative designs!) but if you do, you\u2019re also taking responsibility for those affordances. <@U03HB0LK3FY> Thanks for the suggestion! In fact, I was trying to keep the titlebar in order to give the Catalyst app a more \u201cmacOS\u201d feel, but without simulating the default appearance with custom labels, since it almost never looks quite the same as the system","title":"Hi, how can I change some properties of NSWindow (such as the titlebar color) when using Catalyst?"},{"location":"wwdc22/uiframeworks-lounge.html#i-saw-an-app-that-shows-the-multitasking-dots-at-the-top-of-the-screen-but-still-hides-the-reset-of-the-status-area-i-could-not-figure-out-how-to-do-that-do-you-have-a-suggestion-on-how-best-to-do-this","text":"I think returning true from an override of prefersStatusBarHidden on a view controller in your app will achieve this effect. Ah, yes, that does it. Thank you!","title":"I saw an app that shows the multitasking dots at the top of the screen, but still hides the reset of the status area. I could not figure out how to do that. Do you have a suggestion on how best to do this?"},{"location":"wwdc22/uiframeworks-lounge.html#uihostingconfiguration-question-i-was-expecting-onappear-calls-to-be-made-on-the-hosted-views-as-cells-move-on-screen-but-it-seems-that-they-line-up-with-i-assume-the-reused-cell-population-if-i-adjust-your-useswiftuiwithuikit-demo-to-increase-the-cell-counts-and-add-a-print-on-onappear-i-see-fewer-logs-than-i-would-expect-to-see-and-no-logging-once-i-scroll-through-all-of-the-content-curious-if-this-expected-behavior-before-or-a-function-of-this-being-beta","text":"Great question, the intent is for onAppear used inside UIHostingConfiguration to behave like it does in a SwiftUI List , however it\u2019s a known issue in the current iOS 16 beta that it isn\u2019t called with the expected timing :thumbsup: Fantastic, thank you!","title":"UIHostingConfiguration question - I was expecting onAppear calls to be made on the hosted Views as cells move on screen, but it seems that they line up with (I assume?) the reused cell population.  If I adjust your UseSwiftUIWithUIKit demo to increase the cell counts and add a print on onAppear I see fewer logs than I would expect to see.  And no logging once I scroll through all of the content.  Curious if this expected behavior before, or a function of this being beta?"},{"location":"wwdc22/uiframeworks-lounge.html#i-would-like-to-allow-users-to-configure-command-keys-for-menus-in-my-app-however-i-have-not-found-a-way-to-change-the-command-keys-or-add-remove-menus-after-the-initial-builder-phase-occurs-via-the-appdelegate-as-a-test-i-built-my-menus-and-then-used-a-dispatch-after-to-remove-the-view-menu-after-4-seconds-im-reusing-the-same-builder-that-was-passed-in-however-nothing-happens-is-the-expected-is-there-a-way-to-update-the-command-keys-after-launch","text":"Updating the main menu needs to happen synchronously in the -buildMenuWithBuilder: method. If you want to update the main menu later, you need to call UIMenuSystem.main.setNeedsRebuild(), and that\u2019ll call buildMenuWithBuilder again (lazily). awesome! Also note that on macOS / Catalyst, it is best to avoid a dynamically changing menu bar structure. All items should be in the same location at all times, so they're easy to discover and their location is easy to remember. Instead of adding/removing items, use validation to disable items when they are not applicable.","title":"I would like to allow users to configure command keys for menus in my app. However, I have not found a way to change the command keys (or add / remove  menus) after the initial builder phase occurs (via the AppDelegate). As a test, I built my menus, and then used a Dispatch after to remove the View menu after 4 seconds. I'm reusing the same builder that was passed in. However, nothing happens. Is the expected? Is there a way to update the command keys after launch?"},{"location":"wwdc22/uiframeworks-lounge.html#catalyst-question-i-would-like-create-a-custom-cursor-that-is-dependent-on-settings-in-the-ui-that-only-works-inside-a-specific-view-eg-a-tool-normally-on-mac-that-would-be-done-with-an-nstrackingarea-since-uipointerinteraction-is-not-supported-on-mac-how-should-i-go-about-doing-this","text":"UIPointerInteraction should work with Mac Catalyst, and allow you to customize the cursor using NSCursor API when hovering inside a view using its UIPointerInteractionDelegate methods. Can you share some more detail about where you are seeing this not work? If this isn\u2019t working, please file a feedback with a sample project so we can take a closer look! OK - I'm a little confused by what you wrote. I have code which uses UIPointerInteraction and implements the delegate's func pointerInteraction(_ interaction: UIPointerInteraction,styleFor region: UIPointerRegion) On iOS, I get the desired cursor: let cursor = UIBezierPath(ovalIn: rect) return UIPointerStyle(shape: UIPointerShape.path(cursor)) That does not do anything on Mac. It sounds like I should conditionalize inside the delegate method and call NSCursor? Ah, I should use willEnter /willExit and call NSCursor. Ah, yes. UIPointerStyle is available on Mac Catalyst to reduce conditionalization, but won\u2019t affect the NSCursor appearance the same way it does on iPad. And yep, that was just the answer I was typing out! Thank you - the thing I noticed is that if the cursor is already in the view, then the cursor doesn't get set (e.g., if you have the mouse over something and you click, which then displays a new view which now encompasses the cursor. In that case, the methods for willEnter gets called but the cursor doesn't update for some reason. I have to move the cursor out of the view and back in (I have logging to show that). If that's not expected, I can make a project + feedback. This is definitely at least something we\u2019d like to take a look at, so please file a feedback! FB10082348 submitted.","title":"Catalyst question: I would like create a custom cursor that is dependent on settings in the UI that only works inside a specific view (e.g., a tool). Normally on Mac, that would be done with an NSTrackingArea. Since UIPointerInteraction is not supported on Mac, how should I go about doing this?"},{"location":"wwdc22/uiframeworks-lounge.html#is-it-now-possible-to-create-custom-app-specific-extensions-that-are-not-pre-defined-by-apple-or-am-i-misreading-the-extensionkit-docs-if-this-is-possible-is-there-a-good-session-to-watch-this-week","text":"Here is a previous question on ExtensionKit. Seems like there is no session on that this year, though. https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654627443299689 > Is it now possible to create custom app-specific extensions that are not pre-defined by Apple or am I misreading the ExtensionKit docs? It is possible for macOS apps to define an extension point. Other 3rd party apps can then embed AppExtensions that target the extension point. > If this is possible, is there a good session to watch this week? There are no sessions planned for ExtensionKit this week. (edited) Great news! This is something I'm excited for even if it only works with macOS. Are these extensions App Store compatible? Yes.","title":"Is it now possible to create custom app-specific extensions that are not pre-defined by Apple or am I misreading the ExtensionKit docs? If this is possible, is there a good session to watch this week?"},{"location":"wwdc22/uiframeworks-lounge.html#probably-it-is-more-core-services-but-i-cant-find-anything-like-that-so-we-have-a-case-where-ipad-is-the-heart-of-a-system-or-rather-ipads-in-the-aircraft-cockpit-it-is-one-of-the-most-powerful-and-flexible-units-now-we-need-two-or-more-devices-to-communicate-pilot-co-pilot-cabin-crew-yet-regulators-have-very-specific-need-that-one-device-is-a-master-and-manages-synchronization-with-others-we-found-that-ipad-is-not-providing-things-like-services-eg-httpsoaprestsocket-server-that-would-be-running-no-matter-if-app-is-open-or-closed-pilots-are-using-many-apps-for-different-purposes-is-that-achievable-in-any-way-clients-would-love-to-see-ipad-only-solution-but-we-are-limited-now-to-use-windows-machine-acting-as-server-and-ipad-devices-are-just-clients-with-quite-similar-functionality","text":"This is a very great question. Right now it is not generally possible to have code running indefinitely while an app is in the background. We have special endpoints for things like voip but do not support generic background processes. I\u2019d suggest filing feedback and also reaching out to evangelism about potential use cases that you might have for this. Also a disclaimer: this question has legal implications since you are mentioning aviation which is explicitly mentioned in the iPadOS license agreement under 7.5. So the answer above is purely based on the technical feasibility as we can not give you legal advice here. <@U03JFDDG9QD> Wondering if Multipeer Connectivity framework would work for you. Multipeer is odd cause there's no real \u201chost\u201d and \u201cclient\u201d\u2026. You could manage a host using communicated meta data though. I also think you disconnect from the network when the app gets backgrounded too\u2026 You might want to take a look at CoreBluetooth, as it explicitly allows an app to continue scanning for peripherals while backgrounded. See this doc This is not a recommendation for your application. Please contact developer relations to explore legal and other implications. In case you're still checking your thread - talk to the folks at ditto.live who are ex-Realm engineers doing a lot of work with in-plane communications.","title":"Probably it is more core services but I can't find anything like that, so: We have a case where iPad is the heart of a system. Or rather iPads. In the aircraft cockpit it is one of the most powerful and flexible units. Now - we need two or more devices to communicate (pilot, co-pilot, cabin crew...). Yet regulators have very specific need that one device is a master and manages synchronization with others.  We found that iPad is not providing things like services (e.g. http/soap/rest/socket server) that would be running no matter if app is open or closed (pilots are using many apps for different purposes) - is that achievable in any way?  Clients would love to see iPad-only solution but we are limited now to use windows machine acting as server and iPad devices are just clients (with quite similar functionality)."},{"location":"wwdc22/uiframeworks-lounge.html#looks-great-i-assume-apps-that-already-have-a-more-overflow-menu-should-migrate-the-items-to-additionaloverflowitems-is-that-correct","text":"for iOS 16 I would say definitely! In part it will help you a lot in Stage Manager, as you won\u2019t have to do as much to respond to size changes there are a few different ways to interact with it. centerItemGroups will flow into it first, but so will trailing items (as a group via the older rightBarButtonItems API, and by-group in the new trailingItemGroups API). there is also the additionalOverflowItems that can be used to add content directly to the overflow button\u2019s menu \u2013 we think this will be rare, but can be useful for specific use cases. Thank you. And from the sample code, it looks like the optional group does NOT flow into the overflow button's menu. optional groups should flow if there isn\u2019t enough room but items that are not in the customization currently go no where (not directly accessible). We\u2019re considering some control over that behavior however. That was what I was getting at. For apps with lots of commands, that would be helpful. Would you like a radar :slightly_smiling_face: always helpful! Will do. Excellent presentation, David. Thank you.","title":"Looks great! I assume apps that already have a More (overflow) menu should migrate the items to additionalOverflowItems. Is that correct?"},{"location":"wwdc22/uiframeworks-lounge.html#this-may-not-be-a-question-with-a-straightforward-answer-but-a-degree-of-consensus-in-the-developer-community-prior-to-the-current-beta-releases-has-been-that-using-swiftui-inside-catalyst-is-the-most-difficult-path-and-it-would-be-better-to-target-macos-natively-directly-with-swiftui-in-trying-to-give-the-best-desktop-class-experience-on-ipad-would-it-be-your-recommendation-to-using-a-native-swiftui-app-if-that-is-an-option-or-a-catalyst-app-with-swiftui-components","text":"Poorly worded as of course what I mean is desktop-class experience on iPad while also targeting macOS as well... Catalyst support is continuously improving. I think that it really depends on on which works best for your App. If you have the luxury to try both you might want to compare the outcomes of each. Basically use whatever works best for your concrete situation. There is no one size fits all solution. The range here can be from an app 100% written in UIKit to 100% written in SwiftUI. And it could be a macOS version written in AppKit, using Catalyst or SwiftUI. In what state is your current code base? Is this an iPad app you're transitioning to the Mac? > Poorly worded as of course what I mean is desktop-class experience on iPad while also targeting macOS as well... Oops, just saw this What stood out for me in the What's New in SwiftUI session was the advances in Forms. Previously, Forms on macOS seemed to require some, platform specific additions to get nice looking layouts without using Catalyst. With iOS 16 and macOS 13, it looks like you can get great looking Form layouts with nearly the same SwiftUI views. Really nice! Late to the party to see these additional replies. :slightly_smiling_face: My app code base is currently all UIKit, pretty tidy, ready to be modular and move to the next ideal UI architecture.","title":"This may not be a question with a straightforward answer, but... A degree of consensus in the developer community prior to the current beta releases has been that using SwiftUI inside Catalyst is the most difficult path, and it would be better to target macOS natively directly with SwiftUI. In trying to give the best desktop-class experience on iPad, would it be your recommendation to using a native SwiftUI app, if that is an option, or a Catalyst app with SwiftUI components?"},{"location":"wwdc22/uiframeworks-lounge.html#are-there-any-particular-reasons-that-you-can-reveal-for-why-theres-no-general-uihostingview-only-uihostingcontroller-whereas-appkit-has-nshostingview-available","text":"This is intentional, because our architecture relies on a UIView being a part of a view controller hierarchy (and thus, being \"owned\" by a view controller). Think of things like modal presentations, responder chain, etc. This is partially touched on in Use SwiftUI with UIKit as well! https://developer.apple.com/wwdc22/10072 Thank you!","title":"Are there any particular reasons (that you can reveal) for why there\u2019s no general UIHostingView, only UIHostingController, whereas AppKit has NSHostingView available?"},{"location":"wwdc22/uiframeworks-lounge.html#will-swiftui-slowly-replace-uikit-im-surprised-that-there-is-still-a-lot-of-content-focused-on-uikit-this-year-this-video-is-mostly-about-uikit-code-and-i-was-wondering-how-i-can-implement-them-in-swiftui-like-the-editor-style-toolbar","text":"SwiftUI and UIKit are two different layers of abstraction, and one is not supplanting the other. When deciding what to use for your app, you should pick whatever is the best tool for the job. Taylor K also had a great answer to this: https://wwdc22.slack.com/archives/C03HX19UNCQ/p1654645179571169?thread_ts=1654645084.628639&cid=C03HX19UNCQ To add to this, for a lot of developers this means a mix of UIKit and SwiftUI, but it can also mean to start a new app that is written 100% in SwiftUI or UIKit. For the last bit of your question, there will be a couple SwiftUI talks later in the week that go into the new iPad features: https://developer.apple.com/videos/play/wwdc2022/110343/ https://developer.apple.com/videos/play/wwdc2022/10058","title":"Will SwiftUI slowly replace UIKit? I'm surprised that there is still a lot of content focused on UIKit this year. This video is mostly about UIKit code and I was wondering how I can implement them in SwiftUI, like the editor style toolbar?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-any-reason-why-live-text-feature-is-not-supported-in-ios-keyboard-extensions-using-the-api-either-crashes-the-extension-or-just-prints-error-logs-depending-on-the-ios-version","text":"Have you filed a feedback for this yet? It would be helpful to have the crash logs so we can take a look at what happened - if you have a feedback ID I can pass it on to the appropriate people Keep in mind also that the operating system maintains a much stricter memory allowance for keyboard extensions, so it's pretty likely that your extension is being killed by the OS for using too much memory. <@U03HELXCTGV> 100% it didn't. I was testing it on the template project + I have nightmares where I see our keyboard crashing in 2022 with the same OOM crash, using more than 66 Mb :rolling_on_the_floor_laughing::rolling_on_the_floor_laughing::rolling_on_the_floor_laughing: ... so I won't confuse it :innocent: In case you're interested about details of why it affects me and my team so much, the feedback ID is FB10034138 . Would really appreciate for any response on that. <@U03H31CU1C7> sure. FB10022377 There's no system diagnose, but you won't need it, cz there's a test project attached to the ticket. Thanks in advance.","title":"Is there any reason why Live Text feature is not supported in iOS Keyboard extensions?  Using the API either crashes the extension or just prints error logs, depending on the iOS version."},{"location":"wwdc22/uiframeworks-lounge.html#for-uieditmenuinteraction-all-the-examples-in-the-sessions-showed-apppending-items-to-the-menu-is-it-possible-to-have-our-custom-items-at-the-beginning-or-even-mixed-in-with-the-suggested-ones-if-we-appropriately-shuffle-them-into-the-returned-menu","text":"You can! suggestedActions is just an array, so you can prepend to it, append to it, or just return a UIMenu with an entirely different set of children if you want.","title":"For UIEditMenuInteraction, all the examples in the sessions showed APPPENDING items to the menu. Is it possible to have our custom items at the beginning or even mixed in with the 'suggested' ones if we appropriately shuffle them into the returned menu?"},{"location":"wwdc22/uiframeworks-lounge.html#what-sessions-can-we-watch-to-start-learning-more-about-live-activities-for-the-ios-home-screen","text":"Please schedule a 1-1 lab and we will probably be able to better answer this question. Or come to our QA again tomorrow.","title":"What sessions can we watch to start learning more about Live Activities for the iOS home screen?"},{"location":"wwdc22/uiframeworks-lounge.html#when-an-ipad-is-connected-to-a-low-dpi-external-monitor-will-apps-run-at-a-1x-scale-factor-with-1x-assets-used-or-will-they-stay-at-2x-with-some-sort-of-downscaling","text":"1x external displays will always be reflected as 2x today. So you do not need to worry about providing 1x assets Thanks! Np!","title":"When an iPad is connected to a low dpi external monitor, will apps run at a 1x scale factor with 1x assets used? Or will they stay at 2x with some sort of downscaling?"},{"location":"wwdc22/uiframeworks-lounge.html#for-uimenuelements-if-i-change-the-uimenuelementattributeshidden-attribute-while-its-visible-will-it-disappear","text":"Similarly, if I change the title/image, do they update if already visible? No, it won't. The only way to update a visible menu is using UIContextMenuInteraction.updateVisibleMenu() if you're using UIContextMenuInteraction directly, or by setting the button's menu property to a new value if you're using a UIButton . Menus are copied when consumed by the system, so any instance you may be holding onto is no longer directly connected to what's visible. (off to Feedback Assistant)","title":"For UIMenuElements, if I change the UIMenuElementAttributesHidden attribute while it's visible, will it disappear?"},{"location":"wwdc22/uiframeworks-lounge.html#not-sure-if-this-is-the-right-time-for-this-but-in-ios-16-uidevicename-has-changed-to-only-return-the-model-name-eg-iphone-instead-of-matts-internet-communicator-i-think-in-one-of-the-sessions-i-heard-that-there-is-an-entitlement-to-get-the-old-behaviour-but-i-cant-find-any-reference-to-it-anywhere","text":"The entitlement is com.apple.developer.device-information.user-assigned-device-name . You can find out more about it in the \u201cWhat\u2019s new in privacy\u201d video. There are some restrictions around what you can do with it (more in that video). In my case it would be on supervised devices, so hopefully not too restrictive","title":"Not sure if this is the right time for this, but... In iOS 16 UIDevice.name has changed to only return the model name (E.g \"iPhone\" instead of \"Matt's internet communicator\"). I think in one of the sessions I heard that there is an entitlement to get the old behaviour, but I can't find any reference to it anywhere."},{"location":"wwdc22/uiframeworks-lounge.html#what-is-the-behavior-of-uidevicename-would-it-return-iphone-13-pro-or-just-iphone-if-you-dont-have-the-entitlement-to-get-the-devices-name-thanks","text":"Just \"iPhone\" or \"iPad\". Yeah, it returns the same thing as UIDevice.model. I\u2019m trying to remember. Is there a method that returns the internal model number i.e \u201ciPhone10,6\u201d Off the top of my head, I think the answer for UIDevice and this is no, but I'm checking No, I don't believe that is something UIDevice will return It would be worth filing a feedback report if you need more fine-grained model delineation, with a description of what you intend to use it for. It\u2019s useful for us to hear things like that first-hand and have documented requests to consider. I believe there are lower level unix functions that will return this (perhaps sysctl or uname ) It would be useful for setting up a support request message prefilled with the user's device type (along with app version, etc) Yes my use case matches that of Lowell :slightly_smiling_face: FB5524321 UIDevice API desired to get model name as it appears in Settings 48331816 yeah - I think in the past I used sysctl - can\u2019t remember the key though Searching seems to indicate both uname and sysctl hw.model key might return this, but I haven't verified that at all Might be worth a try at least Seems its via utsname and uname : https://stackoverflow.com/questions/26028918/how-to-determine-the-current-iphone-device-model (Assuming that code still works in iOS 16)","title":"What is the behavior of UIDevice.name? Would it return iPhone 13 Pro or just iPhone if you don't have the entitlement to get the device's name? Thanks!"},{"location":"wwdc22/uiframeworks-lounge.html#navigation-item-center-groups-does-it-make-sense-to-swap-them-completely-based-on-some-factor-like-object-selection-state-if-so-should-one-change-uinavigationitemcustomizationidentifier-then-any-other-considerations","text":"It can, and thats a valid use for multiple customization identifiers on a single UINavigationItem. We should continue to properly save & restore state when you update. I\u2019m pretty sure we\u2019ll update things properly regardless of the relative ordering of you setting .centerItemGroups vs .customizationIdentifier, but if you find that not to be the case file a report if you just want to conditionally make functionality available though, you might look at using the hidden property we just added to UIBarButtonItem and UIBarButtonItemGroup Ohh, I missed hidden , seems useful. But I think for completely different modes, using different customization identifiers might make more sense. I will try to implement it and maybe on tomorrows labs (assuming I will get my ticket :wink:) I will share my experience. As a side note: would be great if one could present \u201cvertical\u201d edit menu from UIBarButtonItems , even if initialised via touch. you mean the EditMenuInteraction ? Yes. I\u2019m sure there is a way to initialise it from UIBarButtonItem , but my note was specifically about always using vertical style in such case. I don\u2019t think we\u2019ve really thought that much about it actually, usually you want the edit interaction to be directly on whatever thing is being edited. I think there are ways you might be able to get something useful like that, but I\u2019d want to talk to the engineer that worked on edit menu interaction So, another thing to chat about on labs. Thank you David :slightly_smiling_face:","title":"Navigation item center groups: does it make sense to swap them completely based on some factor, like object selection state? If so, should one change UINavigationItem.customizationIdentifier then? Any other considerations?"},{"location":"wwdc22/uiframeworks-lounge.html#do-you-think-its-a-good-idea-trying-to-convert-modals-to-use-structured-concurrency-on-ios-so-instead-of-usual-callback-the-caller-would-await-on-result-property","text":"Yes! For modal flows that return a result via a delegate, I think using structured concurrency to await the result instead is an interesting approach to try.","title":"Do you think it's a good idea trying to convert modals to use structured concurrency on iOS? So instead of usual callback the caller would await on result property."},{"location":"wwdc22/uiframeworks-lounge.html#do-uimenuelementattributeskeepsmenupresented-and-uimenuelementsize-translate-to-samesimilar-behaviour-in-mac-catalyst","text":"Since macOS menus are modal, UIMenuElementAttributesKeepsMenuPresented has no effect on the Mac. Small or medium UIMenuElementSize s will produce standard (full width) menu elements since that design paradigm doesn't exist on the Mac. I thought so, but was worth asking :slightly_smiling_face: Not having UIMenuElementAttributesKeepsMenuPresented -like behaviour seems particularly problematic, since it enables great UX on iPad. With no direct Mac behaviour, one must look for completely different solution there. Yeah, that's totally understandable. It's just incompatible with how Mac menus work currently.","title":"Do UIMenuElementAttributesKeepsMenuPresented and UIMenuElementSize translate to same/similar behaviour in Mac Catalyst?"},{"location":"wwdc22/uiframeworks-lounge.html#i-am-wondering-how-many-of-these-api-are-available-in-a-native-swiftui-app-i-am-taking-my-ios-app-to-both-ipados-and-macos-in-a-current-redesign-which-will-require-ios-16can-require-macos-13-but-i-need-to-decide-between-using-catalyst-vs-going-swiftui-native","text":"If your writing a pure SwiftUI app, then you should compile it native for each platform. e.g. disabling the traffic light buttons when not applicable. does your SwiftUI app rely on UIKit for something? The question is the opposite, really. It\u2019s currently a UIKit app. I am at the point of needing to decide\u2014do I ship it for macOS with Catalyst, or given I\u2019m doing a major redesign anyway, do I migrate it to be a SwiftUI app. I am wondering\u2014are any of the features/capabilities shown here in deploying as a Catalyst app that are not available on macOS using a native SwiftUI app? This would be a good question for the SwiftUI lounge. OK If you already have a UIKit app that\u2019s working well, proceeding with a Catalyst app would definitely be the quickest path to Mac, and you can always start to mix and match SwiftUI alongside at the same time if you aren\u2019t ready to commit fully. As far as how these properties are exposed in SwiftUI, I\u2019m not sure off the top of my head, but there are additional SwiftUI Q&As that should be able to provide a concrete answer for you","title":"I am wondering how many of these API are available in a native SwiftUI app?   I am taking my iOS app to both iPadOS and macOS in a current redesign, which will require iOS 16/can require macOS 13, but I need to decide between using Catalyst vs. going SwiftUI native."},{"location":"wwdc22/uiframeworks-lounge.html#we-have-a-callkit-voip-app-that-supports-ipad-along-with-iphone-and-we-want-to-use-it-as-a-mac-app-for-apple-silicon-macs-we-have-two-issues-1-on-mac-there-is-no-request-for-microphone-access-the-microphone-request-as-it-is-on-iosipados-is-not-compatible-on-macos-we-should-add-different-permission-requests-according-the-available-platform-ios-vs-macos-2-on-mac-there-is-no-callkit-ui-for-the-incoming-calls-the-expectation-here-is-to-just-run-our-app-and-make-the-apps-windows-the-front-window-without-any-callkit-ui-view-or-status-bar-indicator-thank-you","text":"PushKit will deliver the VoIP push, and it's delivered to the app, which is launched if needed. But on macOS, CallKit will not provide any UI. It is up to your app to provide its own UI when it receives the payload. I'm not certain about the microphone access question. That might be something best asked in one of the upcoming labs for AVAudioSession, AVKit, or AVFoundation. Which API are you using to attempt microphone access? <@U03J217TL5R>? Oh, I see! So, the launch of the app is what expected on VoIP push reception without UI. Then, we are OK with that! We should only see the microphone access issue\u2026 Hmm, I don't have access in our code right now (out of office time zone here). But, as far as remember we use AVAudioSession and we check if category == PLAY_AND_RECORD in order to check microphone availability\u2026 There are two upcoming AVAudioSession labs: June 9th from 11-2 PDT and June 10th from 1-3 PDT. That would be the best place to get your microphone access question answered. Thank you guys, <@U03HB0DARPG> and <@U03HB0DC1PY> I'll check the labs and I'll be prepared with the exact microphone request code that we use! :wave:","title":"We have a CallKit VOIP app that supports iPad (along with iPhone) and we want to use it as a Mac app for Apple silicon Macs. We have two issues:  1) On Mac there is no request for microphone access. The microphone request as it is on iOS/iPadOS is not compatible on macOS? We should add different permission requests according the available platform (iOS vs macOS)?  2) On Mac there is no CallKit UI for the incoming calls? The expectation here is to just run our app and make the app's windows the front window without any CallKit UI view or status bar indicator?  Thank you!!"},{"location":"wwdc22/uiframeworks-lounge.html#im-trying-to-hide-my-uinavigationbar-bar-with-slide-however-the-hiding-animation-is-broken-it-disappears-in-one-go-the-show-animation-slides-in-correctly-though-is-this-a-known-issue","text":"I would suspect that your doing something that UIKit isn\u2019t really prepared to manage properly \u2013 can you paste in the code your using? even without an example, my rough guess here would be that if your setting UINavigationController.isNavigationBarHidden we are probably just pulling the navigation bar out of the hierarchy before your animation begins. If you are calling it with animated:true then we may just be doing things that preclude your animation entirely. And if your setting UINavigationBar.isHidden then that won\u2019t be animatable I'm hiding the status bar simultaneously to get a full screen experience. When I remove the status bar animation, the nav bar animates fine. FB8980917 has a sample app. I pulled up the sample from that issue, it seems to be working in iOS 16, have you tried it there? (I haven\u2019t checked on anything older at the moment, so not sure if this is a new change or not)","title":"I'm trying to hide my UINavigationBar bar with .slide. However, the hiding animation is broken, it disappears in one go. The show animation slides in correctly though. Is this a known issue?"},{"location":"wwdc22/uiframeworks-lounge.html#are-there-any-ways-to-support-bulleted-lists-unordered-and-ordered-via-the-uitextview-or-what-is-the-preferred-approached-here-thanks","text":"This wasn\u2019t available in UIKit until this year, but in the betas you\u2019ll see NSTextList (a long-standing feature available only in AppKit until now). Check out this video tomorrow for more: https://developer.apple.com/wwdc22/10090 This is great! Thank you!","title":"Are there any ways to support bulleted lists (unordered and ordered) via the UITextView? Or what is the preferred approached here. Thanks"},{"location":"wwdc22/uiframeworks-lounge.html#im-currently-trying-to-use-a-uicellaccessory-with-a-custom-view-in-a-uicollectionviewlistcell-but-i-get-a-degenerate-layout-feedback-loop-when-expanding-a-list-section-is-there-something-i-need-to-be-doing-differently","text":"It's possible. Every layout feedback loop is different. I strongly recommend signing up for a lab where an engineer can do some joint debugging with you. If you can't get a lab, please try the developer forums. Meanwhile, I'll see if I can help you a little here. Do you have logs from the layout feedback loop debugger? I have a feedback with sample project for this issue as I believe it is an unexpected behavior. If someone would be willing to take a look it would be much appreciated! FB9207332 Awesome! I love a sample project! Me too! Thanks for your response! <@U03JU8F54C8> I'm not reproducing the LFL on an iPad with your sample app. Of course, I'm working on an unreleased build, and it's possible we have a fix for the issue. On the other hand, maybe this is device specific, and I should be trying this on an iPhone instead? Oh! I'll check the screen recording\u2026 I'll try it on a phone. If we have a fix, I'll be duping your FB report Just opened my sample. It does happen for me on the iPhone sim in Xcode 14 beta 1. We experienced this issue intermittently on the iPad as well but my specific example may not exactly produce it. Were you able to reproduce on the iPhone simulator? Just also reproduced on a test device. Since I'd bet it's not encouraged to post files here, I'll post my debugger output to the feedback. Apologies that it's very long, I think I set the debugging threshold high. Ok. Just attached that file to the feedback. Thanks so much again for taking a look.","title":"I'm currently trying to use a UICellAccessory with a custom view in a UICollectionViewListCell, but I get a degenerate layout feedback loop when expanding a list section. Is there something I need to be doing differently?"},{"location":"wwdc22/uiframeworks-lounge.html#macos-extensionkit-question-does-an-extension-have-to-be-delivered-within-an-app-bundle-is-it-possible-to-build-a-standalone-appex-and-have-it-be-discoverable-by-an-hosting-application","text":"It is not possible to deliver an extension outside of an app bundle. Extensions must be contained in an app bundle in the Extensions directory. Ah hello! Thanks for your help yesterday! Would be awesome to not need to create wrapper apps to deliver extension-only payloads. I\u2019ll file a bug report. Will all of ExtensionKit\u2019s capabilities be available to apps delivered outside the App Store using Developer ID? Happy to help. Yes ExtensionKit\u2019s capabilities are supported apps delivered outside of the App Store. The host app and the extension must be code signed and the extension must be sandboxed. Ah this is great info - particularly about the sandboxing requirements for extensions! I\u2019m just getting started, so perhaps this is a silly question. But, I\u2019ve been playing with the extension Xcode template, and it is helpful it getting an idea of how to structure everything. I find myself doing awkward things to get at the NSXPCConnection and its properties, since it is only available from the AppExtensionConfiguration. I\u2019m just passing it right though back up to the main extension class. Does this sound like it makes sense? Is there any way for my main app to pass security-scoped bookmarks, or other privileged file-access mechanism back down to the extension, or will each extension that needs this kind of thing need to request access from the user itself? ExtensionKit doesn't provide specific API to grant file access to an extension. I'm not knowledgeable enough about security scoped bookmarks to comment at the moment. Ok that\u2019s totally understandable. But, could an extension create an open dialog for gaining file access? Yes, extensions can use NSOpenPannel Ok wonderful! And from my frantic internet searching just now, it does seem possible to share file accesses between apps and XPC services. So, I feel like that covers what I would need. I realize that I\u2019m perhaps monopolizing your time/attention. Please feel free to ignore as needed :smiley: Do you have any thoughts on the best way to communicate the NSXPCConnection from the configuration back up to the extension class? I\u2019m envisioning needing to use the remoteObject and exportedObject from extensions for bidirectional communication with the host app. WRT configuring XPC connections: We recommend that the app that defines the extension point provided a framework to extension developers. This framework should specialize the ExtensionKit protocols. This is a quick example of something such a framework might implement struct ExampleConfiguration&lt;E:ExampleExtension&gt;: AppExtensionConfiguration { let exportedObject: ExportedObject init(_ appExtension: E) { self.exportedObject = ExportedObject(appExtension) } /// Determine whether to accept the XPC connection from the host. func accept(connection: NSXPCConnection) -&gt; Bool { connection.exportedInterface = NSXPCInterface(with: TextTransformerXPCProtocol.self) connection.exportedObject = exportedObject connection.resume() return true } class ExportedObject: NSObject, TextTransformerXPCProtocol { let appExtension: E init(_ appExtension: E) { self.appExtension = appExtension } func transform(text: String, reply: (String) -&gt; Void) { let transformedText = appExtension.transform(text: text) reply(transformedText) } } } /// The AppExtension protocol to which this extension will conform. /// This is typically defined by the extension host in a framework. protocol ExampleExtension : AppExtension { func transform(text: String) -&gt; String } extension ExampleExtension { var configuration: ExampleConfiguration&lt;some ExampleExtension&gt; { // Return your extension's configuration upon request. return ExampleConfiguration(self) } } An extension implementation would look like this: @main final class ExampleAppExtension: ExampleExtension { func transform(text: String) -&gt; String { return text.uppercased() } } ok this makes total sense, and is really helpful. Thank you very much! Is there any way that I could accept-list locally-bundled extensions within the host app? Or, would those always need explicit user approval as well? Oh, and I assume EXAppExtensionBrowserViewController is the only way to approve extensions? Correct so even host-bundled extensions must be given explicit approval? Yes, you can file feedback if this will cause problems. My cursory testing with extensions seems to have a lower first-request latency than a plain XPC service, which would be really cool! Am I right about that? Extensions are launched when the host creates the AppExtensionProcess instance so it is already running by the time you send the first XPC message. If you include the time waiting for the AppExtensionProcess instance to be created I suspect the overall latency will be equivalent. I have not measured this. Ok this makes sense. I was also curious about the .appextensionpoint file. Aside from documentation, which I assume is forthcoming, it was interesting this wasn\u2019t incorporated into the app\u2019s Info.plist. Is there something here than made separate files a better fit? I have an answer for this: > Is there any way for my main app to pass security-scoped bookmarks, or other privileged file-access mechanism back down to the extension, or will each extension that needs this kind of thing need to request access from the user itself? You can expect Security scoped bookmarks to grant the extension access to files that the host app has access to. That is awesome news. Sandboxing was, so far, the thing giving me the most concern. Is the app-bundling requirement for extensions something that might change before Ventura ships? Extensions must be bundled in a container app's bundle. Beyond that I cant comment on the future direction of the platform. I understand. Thank you so much for your time, you\u2019ve been immensely helpful. I\u2019m incredibly excited about these APIs, and cannot wait to do some real work with them. Glad to help. I'm excited to see what people do with these APIs.","title":"macOS ExtensionKit question. Does an extension have to be delivered within an app bundle? Is it possible to build a standalone .appex and have it be discoverable by an hosting application?"},{"location":"wwdc22/uiframeworks-lounge.html#the-new-self-sizing-uitableview-and-uicollectionview-cells-look-fantastic-is-there-anything-similar-for-table-view-headers-and-footers-is-there-a-recommended-technique-to-have-headers-and-footers-resize-to-accomodate-dynamic-type-for-example","text":"The new selfSizingInvalidation functionality, including manually calling invalidateIntrinsicContentSize yourself, as well as the automatic .enabledIncludingConstraints mode for Auto Layout, work in UITableViewHeaderFooterView too! The feature is not just limited to cells :smiley: (And for UICollectionView, this includes self-sizing supplementary views too.) Thank you, that\u2019s great news!","title":"The new self-sizing UITableView and UICollectionView cells look fantastic. Is there anything similar for table view headers and footers? Is there a recommended technique to have headers and footers resize to accomodate dynamic type for example?"},{"location":"wwdc22/uiframeworks-lounge.html#i-saw-that-the-san-francisco-font-has-new-compressed-condensed-and-expanded-style-widths-however-i-do-i-access-these-styles-with-uifont-or-uifontdescriptor","text":"You can use UIFontDescriptor.TraitKey.width with a float value. We know that some suggested constants for the various \u2018standard\u2019 widths would be helpful - like the constants for \u2018standard\u2019 widths for the use with UIFontDescriptor.TraitKey.weight , so keep an eye out for them in future betas. Great, thanks. Are the float values for the width described anywhere? https://developer.apple.com/documentation/uikit/uifontwidthtrait?language=objc \u2026or probably more useful, https://developer.apple.com/documentation/uikit/uifontdescriptor/traitkey/1616684-width > The valid value range is from -1.0 to 1.0 . The value of 0.0 corresponds to the regular glyph spacing.","title":"I saw that the San Francisco font has new compressed, condensed and expanded style widths. However I do I access these styles with UIFont or UIFontDescriptor?"},{"location":"wwdc22/uiframeworks-lounge.html#would-you-be-able-to-share-some-insight-on-how-the-various-collection-views-are-implemented-within-the-calendar-app-the-paging-that-goes-on-endlessly-is-something-i-have-been-trying-to-recreate-for-a-while-but-its-tricky-any-hints-would-be-greatly-appreciated","text":"You can achieve endless scrolling by essentially adding and removing sections while scrolling and making sure to adjust the content offset when doing so. UIScrollView (and thus UICollectionView ) will keep its inertia and make it look seamless. If you have specific use cases this might be a great thing to discuss in a 1on1 lab. Thanks, i\u2019ll try applying for a lab again :crossed_fingers:","title":"Would you be able to share some insight on how the various Collection Views are implemented within the Calendar app? The paging that goes on endlessly is something I have been trying to recreate for a while but it's tricky. Any hints would be greatly appreciated :)"},{"location":"wwdc22/uiframeworks-lounge.html#ios-16-offers-a-great-new-api-addition-in-the-way-of-uihostingconfiguration-for-rendering-swiftui-views-in-list-collectionview-cells-however-wed-love-to-support-this-type-of-interop-on-ios-15-too-when-using-uihostingcontrollers-in-uicollectionview-list-cells-is-there-any-specific-considerations-that-should-be-taken-into-account-in-addition-to-managing-cell-reuse-issues","text":"Unfortunately there isn\u2019t an officially supported way to use SwiftUI inside of UIKit cells prior to iOS 16. Embedding a UIHostingController inside of cells is not recommended. Just to be clear, it's not recommended due to performance issues, right? <@U03HW7U6QF3> This is great to know, thank you! Is there any further light you can shed on why? As Lior comments, I am guessing due to performance overhead or reuse pitfalls? i love this new feature!!!! While performance is certainly a concern, the main issue is really around the nature of needing to embed a view controller (the UIHostingController) in each cell. Generally speaking, putting view controllers inside cells is discouraged in UIKit. This is because of how cell reuse works, as well as the way information propagates through the view controller and view hierarchy in UIKit. So using UIHostingController in cells creates problems and can result in some surprising bugs. (A common example we\u2019ve seen are issues with safe area information propagating into SwiftUI.) These are all things that the new UIHostingConfiguration API addresses! TIL Makes complete sense. So glad to see the new UIHostingConfiguration API and we'll be eager to adopt it once our minimum version is 16! Thanks to you and team for your hard work. Despite that advice not to use UIHostingController in cells, provided that you are using the supported UIHostingConfiguration API starting with iOS 16, if you are able to get something working with UIHostingController on previous iOS versions and you have tested it well enough yourself that you know it works and meets your needs, you can probably get away with that because UIKit & SwiftUI aren\u2019t changing anymore on those older iOS versions. And since you\u2019re using UIHostingConfiguration going forward, you won\u2019t run into any new issues with the unsupported UIHostingController implementation in the future. But again, this is not officially supported, so you are on your own, and you need to thoroughly test to make sure you can find an implementation that works well enough. <@U03HW7U6QF3> would these concerns also apply to general usage of UIHostingControllers for leaf node views, like buttons? VC pattern would generally not be used there in pure UIKit world, and UIHostingControllers force us into it. Are there perf concerns? Is leaf node SwiftUI in UIKit discouraged, or is it okay if we properly add the VCs in the VC hierarchy? Not really, the specific issues mentioned above are really about having view controllers inside cells (inside of a UIScrollView), due to cell reuse and scrolling performance. Don't tempt me too much Tyler! I seem to have a knack for finding myself in uncharted territory, and will occasionally go to great lengths to avoid making a xib or writing constraints! :joy: Thanks <@U03HW7U6QF3>! I will misuse this thread to once again request a UIHostingView for leaf node SwiftUI views in UIKit :smile: FB10019256 Feedback requests are greatly appreciated :pray: Be sure to check out the video \u201cUse SwiftUI with UIKit\u201d when it goes live tomorrow as well! https://developer.apple.com/videos/play/wwdc2022/10072/","title":"iOS 16 offers a great new API addition in the way of UIHostingConfiguration for rendering SwiftUI views in List CollectionView cells, however we\u2019d love to support this type of interop on iOS 15 too. \u2028When using UIHostingControllers in UICollectionView list cells, is there any specific considerations that should be taken into account in addition to managing cell reuse issues?"},{"location":"wwdc22/uiframeworks-lounge.html#what-happens-to-the-display-scale-when-connecting-to-external-displays-of-different-resolutions-thanks","text":"1x displays will be reflected as 2x when connected to the iPad Cool. What about 6k displays, are those still 2x? I believe so, but will check I don't know if it's okay to jump in here, and I may not be understanding the question. But, for non-M1 iPads, external displays are treated as 1x displays in my experience (I get a UIScreen that is 3840 x 2160 with a scale of 1x). I brought this up in my lab meeting today with Owen and I've filed a feedback report on it. I believe that is expected And there should be no change there between iOS 15 and iOS 16 OK - then I didn't understand the answer you had where 1x displays are reflected as 2x. You were talking with Owen - was this a question about Catalyst? Because that's very different. It was in the catalyst lab yes, but I'm running my catalyst app on iPad. My answer assumed the question was related to Stage Manager, I should have clarified Sorry, let me rephrase then. On iPadOS, when using stage mode on an external display, what does the system return to my app as the displayScale in UITraitCollection ? E.g. if I read it from my UIWindowScene \u2019s rootViewController ? 6k will come across as 2x, 1x displays will come across as 2x, and we don't support anything greater than 6k at the moment, so you should see 2x in this case thanks! np!","title":"What happens to the display scale when connecting to external displays of different resolutions? Thanks!"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-make-a-palette-window-in-a-catalyst-app-like-a-photoshop-palette-it-gets-events-but-its-not-a-true-document-window-and-it-has-a-smaller-title-bar-hud-appearance","text":"This is not currently possible. We would appreciate a feedback request detailing your use case though! Here it is:FB10114581","title":"Is there a way to make a palette window in a Catalyst app (like a Photoshop palette - it gets events, but it's not a true document window), and it has a smaller title bar / HUD appearance?"},{"location":"wwdc22/uiframeworks-lounge.html#in-the-past-ive-tried-incorporating-uitableview-uicollectionview-into-a-swiftui-app-using-uiviewrepresentable-for-the-advanced-drag-and-drop-features-after-experiencing-some-bugginess-with-the-tables-cell-reuse-i-was-advised-by-an-apple-engineer-to-build-a-uikit-or-appkit-app-a-year-later-with-this-latest-iteration-of-swiftui-do-apple-engineers-stand-by-that-guidance-or-has-swiftui-become-more-hospitable-to-complex-uikit-elements-like-uitableview-and-uicollectionview-for-apples-apps-that-are-comprised-mostly-of-swiftui-like-shortcuts-im-curious-if-you-could-share-which-framework-is-used-for-the-life-cycle-foundation-thanks-for-all-the-great-work","text":"Hi, we have a couple of sessions coming out tomorrow that will talk about this in some more detail: https://developer.apple.com/videos/play/wwdc2022/10072/|Use SwiftUI with UIKit and https://developer.apple.com/videos/play/wwdc2022/10075/|Use SwiftUI with AppKit . In addition, we also have a Q&A for SwiftUI & UIFrameworks tomorrow morning at 8 AM PDT. I\u2019d encourage you to ask again there, as well as sign up for a SwiftUI Lab to discuss specific use cases! Will do, thank you!!","title":"In the past I\u2019ve tried incorporating UITableView / UICollectionView into a SwiftUI app using UIViewRepresentable (for the advanced drag and drop features). After experiencing some bugginess with the table\u2019s cell reuse, I was advised by an Apple engineer to build a UIKit or AppKit app. A year later, with this latest iteration of SwiftUI, do Apple engineers stand by that guidance or has SwiftUI become more hospitable to complex UIKit elements like UITableView and UICollectionView? For Apple\u2019s apps that are comprised mostly of SwiftUI (like Shortcuts), I\u2019m curious if you could share which framework is used for the life cycle (foundation). Thanks for all the great work!"},{"location":"wwdc22/uiframeworks-lounge.html#documentation-mentions-uitextview-in-ios16-uses-textkit-2-ive-managed-to-disable-word-wrap-via-textcontainerwidthtrackstextview-false-and-setting-the-textcontainersize-to-a-value-wider-than-the-views-bounds-but-looks-like-uitextview-still-doesnt-support-horizontal-scrolling-nor-it-tracks-the-cursor-is-that-something-thats-expected-to-work-or-not-yet","text":"If I\u2019m understanding you correctly, I don\u2019t believe that this is intended to be supported. Did it used to work for you with TextKit 1? No, but I was under the impression that the new TextKit engine is going to support it. I suspect you have good reasons why you\u2019re not doing this, but if you need a one-line text input (no wrapping, scrolls) UITextField is your class. You could likely set up a UITextView to not scroll and be at your desired width, then place it inside a UIScrollView - but you would not get any sort of autoscrolling. \u2026but you could scroll yourself by setting content offset when the text view\u2019s reports to its delegate that the selection has changed. (Obviously your question is still rolling around in my head :slightly_smiling_face:) If this is something you really need and are unable to accomplish reasonably, feedback is always appreciated ( https://developer.apple.com/bug-reporting/ ). I\u2019d like the content to overflow beyond the view\u2019s bounds, without wrapping text, which is preferred in plain text (fixed-width font) editors (code editor, markdown, etc). Think of it as a soft-wrap feature, where the UITextView on an iPhone screen or a windowed iPad app is actually narrower and doesn\u2019t fit the 80-column text content. I noticed that the NSTextContainer allows for a wider content than the viewport\u2019s size, but the UITextView doesn\u2019t seem to implement auto-scrolling, as the caret moves beyond the text view\u2019s visible area. Makes sense. You could try implementing UITextViewDelegate.textViewDidChangeSelection(_:) and calling setContentOffset with the result of something calculated from caretRectForPosition: or the like (note that UITextView implements all of UITextInput , which has a bunch of methods about geometry). But you\u2019re definitely doing something that is not in the designed-for use cases. Thanks for the suggestion, I just tried it, but it seems that the widthTracksTextView is not supported on iOS/iPadOS. As soon as the UITextView \u2019s frame changes its size, the textCointainer \u2019s size is set and the lines are wrapped. I also filed the suggestion: FB10115560 Ah - yeah, you\u2019re right. We override those based on the \u2019scrollEnabled ' state. I will return to my \u2018non-scrollable text view inside a scroll view\u2019 suggestion then. But I realize it\u2019s clunky. Thanks for filing the feedback!","title":"Documentation mentions UITextView in iOS16 uses TextKit 2. I\u2019ve managed to disable word wrap via textContainer.widthTracksTextView = false and setting the textContainer.size to a value wider than the view\u2019s bounds, but looks like UITextView still doesn\u2019t support horizontal scrolling, nor it tracks the cursor. Is that something that\u2019s expected to work, or not yet?"},{"location":"wwdc22/uiframeworks-lounge.html#like-func-playerviewcontroller_-playerviewcontroller-avplayerviewcontroller-willtransitiontovisibilityoftransportbar-visible-bool-with-coordinator-avplayerviewcontrolleranimationcoordinator-there-is-no-callback-methods-to-track-info-pannel-visibility-whats-the-best-way-to-identify-when-info-pannel-appears-and-disappears","text":"This is a better question for AVKit. It looks like there's an AVKit lab at 3 PM on Thursday, June 9th: https://developer.apple.com/wwdc22/110548 Noted thanks! np!","title":"Like 'func playerViewController(_ playerViewController: AVPlayerViewController, willTransitionToVisibilityOfTransportBar visible: Bool, with coordinator: AVPlayerViewControllerAnimationCoordinator)` there is no callback methods to track info-pannel visibility. Whats the best way to identify when info-pannel appears and disappears?"},{"location":"wwdc22/uiframeworks-lounge.html#i-have-a-view-controller-that-has-multiple-uitextfields-when-im-in-one-of-those-text-fields-and-press-tab-on-an-ipad-it-focuses-on-subsequent-text-fields-however-when-i-run-via-catalyst-on-macos-tabbing-while-in-a-text-field-inserts-a-tab-character-instead-of-focusing-on-the-next-text-field-do-you-know-how-to-enable-tabbing-to-focus-on-the-next-text-field","text":"This was a bug that should be fixed in macOS Ventura Ok awesome, thanks! Is there a known workaround for this issue in macOS Big Sur? <@U03HELWUJN9> unfortunately there is not but you should check on latest betas. Thanks!","title":"I have a view controller that has multiple UITextFields. When I\u2019m in one of those text fields and press tab on an iPad, it focuses on subsequent text fields. However, when I run via Catalyst on macOS, tabbing while in a text field inserts a tab character instead of focusing on the next text field. Do you know how to enable tabbing to focus on the next text field?"},{"location":"wwdc22/uiframeworks-lounge.html#does-continuity-camera-support-a-file-promise-previously-only-the-data-itself-was-returned-with-kpasteboardtypefilepromisecontent","text":"NSFilePromiseReceiver should handle this type of pasteboard data as a file promise. how are you trying to call in the file promise? <@U03HL05FX6Y> Sorry for the wait, lab. We were doing it manually with the older APIs We had not yet adopted NSFilePromiseProvider/Receiver Our code dates to 2019 NSFilePromiseReceiver is pretty nice because it's compatible with any code using the other techniques At the time we implemented Continuity Camera the file promise was never \"kept\", only extracting the data in-process worked And we use file promises in other places, we're familiar with the technique Try it out with NSFilePromiseReceiver. If it's still failing, file a feedback report with a small sample project. ok","title":"Does Continuity Camera support a file promise? Previously only the data itself was returned with kPasteboardTypeFilePromiseContent."},{"location":"wwdc22/uiframeworks-lounge.html#hi-im-currently-trying-to-update-my-apple-watch-app-to-support-watchos-9-however-the-videoplayer-in-swiftui-is-crashing-when-playing-a-video-and-xcode-prints-something-to-do-with-volume-hud-class-name-changing-and-just-wondering-if-this-is-something-thats-changed-with-watchos-9-or-its-something-that-i-need-to-update-in-my-code-somewhere","text":"Can you ask in the <#C03GSLANZJT|> or the <#C03HX19UNCQ|>? Also, if you\u2019re able to provide them the crash log, that\u2019ll help investigate the issue! Yep I can do both, I'll make sure to be on my Mac too so I can get the log for them. Thanks both :)","title":"Hi,  I\u2019m currently trying to update my Apple Watch app to support watchOS 9 however the VideoPlayer in SwiftUI is crashing when playing a video and Xcode prints something to do with volume HUD class name changing and just wondering if this is something that\u2019s changed with watchOS 9 or it\u2019s something that I need to update in my code somewhere."},{"location":"wwdc22/uiframeworks-lounge.html#hi-in-my-carplay-app-in-templateapplicationscene_didconnect-i-get-the-provided-interfacecontroller-and-store-a-reference-to-it-but-most-of-the-times-when-i-try-to-access-interfacecontrollercartraitcollection-the-app-crashes-because-cartraitcollection-is-uninitialized-so-it-makes-creating-image-for-different-display-scale-2x-3x-or-different-display-styles-dark-light-impossible-i-tried-to-make-sure-that-i-always-access-it-from-main-thread-but-still-have-the-same-problem","text":"This is a better question for someone on the CarPlay team. There's a Q&A Digital Lounge on Friday at 1 PM Pacific time. Also, probably worth filing a feedback on it in case it isn't being tracked anywhere! Looks like we do have feedback tracking this and the CarPlay team is looking into it.","title":"Hi, In my CarPlay app, In templateApplicationScene(_:didConnect:) I get the provided interfaceController and store a reference to it, but most of the times when I try to access interfaceController.carTraitCollection The app crashes because carTraitCollection is Uninitialized, so it makes creating image for different display scale (2x, 3x) or different display styles (dark, light) impossible. I tried to make sure that I always access it from main thread but still have the same problem."},{"location":"wwdc22/uiframeworks-lounge.html#with-uiactivityviewcontroller-is-it-possible-to-provide-multiple-representations-of-the-same-object-a-url-and-text-eg-through-an-nsitemprovider-as-well-as-lplinkmetadata-uiactivityitemsconfigurationreading-is-the-only-way-ive-gotten-the-nsitemprovider-to-work-but-that-doesnt-support-link-metadata-and-uiactivityitemsource-supports-the-metadata-but-not-the-item-provider","text":"You can provide link metadata using UIActivityItemsConfigurationReading by implementing the activityItemsConfigurationMetadata(key:) function, and watching for the linkPresentationMetadata key. Remember also that you have the option of using the prebuilt UIActivityItemsConfiguration class, which has properties that you can set for these metadata. When I return LPLinkMetadata from the UIActivityItemsConfiguration.metadataProvider closure, it doesn't use the image, title, or subtitle in the metadata Strike that, the metadataProvider closure is never even called with the linkPresentationMetadata key <@U03H31CQZ0F> I've filed FB10116030 about this Thank you. I\u2019ll take a look.","title":"With UIActivityViewController, is it possible to provide multiple representations of the same object (a URL and text, e.g., through an NSItemProvider) as well as LPLinkMetadata?   UIActivityItemsConfigurationReading is the only way I've gotten the NSItemProvider to work, but that doesn't support link metadata. And UIActivityItemSource supports the metadata, but not the item provider."},{"location":"wwdc22/uiframeworks-lounge.html#in-catalyst-is-there-a-way-to-only-show-the-preview-for-a-link-when-dragging-a-link-in-a-uitextview-instead-of-the-whole-view-the-associated-uidraginteractiondelegate-methods-dont-seem-to-work","text":"Can you elaborate a bit more on your setup and what you're seeing? Are you installing your own UIDragInteraction on the UITextView ? Are you seeing different behavior on iOS vs macOS? If it's difficult to describe in Slack, I encourage you to sign up for one of the UIKit labs tomorrow or Friday. Yes, it's a custom UIDragInteraction and it works correctly on iOS If you drag on iOS, only the area around the link is highlighted, but if you drag on Catalyst, the entire view is highlighted Is there a reason you couldn't rely on the text view's built in drag interaction and customize the behavior via UITextDragDelegate ? I think it's because my text view has isEditable and isSelectable set to false. But also using the UITextDragDelegate methods doesn't work either.","title":"In Catalyst, is there a way to only show the preview for a link when dragging a link in a UITextView instead of the whole view? The associated UIDragInteractionDelegate methods don't seem to work."},{"location":"wwdc22/uiframeworks-lounge.html#with-the-new-self-sizing-cells-is-there-a-way-to-run-an-animation-alongside-a-cell-resize-animation-or-at-least-adding-a-custom-completion-handler-to-the-system-provided-resize-animation","text":"With the new selfSizingInvalidation feature (aka \u201cself-resizing cells\u201d), when a cell is resized with animation, the cell will also receive a full layout pass as part of the same animation. This means that any layout changes to subviews inside the cell\u2019s contentView will also animate by default alongside the resizing of the cell itself. There isn\u2019t an explicit way to manually attach animations to that resize animation when it\u2019s initiated directly by a cell via invalidateIntrinsicContentSize \u2014 do you have something specific in mind you\u2019re trying to do? You can of course continue to use the apply method on diffable data source (re-applying the current snapshot), or performBatchUpdates on the collection/table view directly if you are not using a diffable data source, to resize visible cells as you would prior to iOS 16. And both of those APIs do provide a completion handler you can use. I've got a text view that I'm hiding/showing that I want to appear to progressively expand as the cell grows. I've had some trouble doing this previously with the old beginUpdates/endUpdates, but it sounds like it should be doable with the self-resizing cells stuff Relatedly, the text view is in a stack view and the cell isn't automatically detecting the change when isHidden is changed on the text view (which should, AIUI, cause the stack view to change its internal constraints which should count as an auto layout change). I have to call invalidateIntrinsicContentSize myself to get the size to update. Could that be something on my end, or is it more likely a framework bug? > I\u2019ve got a text view that I\u2019m hiding/showing that I want to appear to progressively expand as the cell grows. I\u2019ve had some trouble doing this previously with the old beginUpdates/endUpdates, but it sounds like it should be doable with the self-resizing cells stuff The new self-resizing cells functionality does work great with multiline UITextView ( scrollEnabled = false ) inside of the cell contentView , when you want the cell to size-to-fit the text view as more text is added! I\u2019m still not sure what you had in mind around adding alongside animations or completions though. > Relatedly, the text view is in a stack view and the cell isn\u2019t automatically detecting the change when isHidden is changed on the text view (which should, AIUI, cause the stack view to change its internal constraints which should count as an auto layout change). I have to call invalidateIntrinsicContentSize myself to get the size to update. Could that be something on my end, or is it more likely a framework bug? If you have an easy place to call invalidateIntrinsicContentSize yourself, that\u2019s actually the best option in general combined with the default selfSizingInvalidation = .enabled . The .enabledIncludingConstraints mode can be more expensive, as it requires checking for size changes on any Auto Layout constraint changes. But based on your description this does sound like something that is supposed to work automatically when you have set .enabledIncludingConstraints , so if you don\u2019t see any obvious issues on your end, it would be great if you could submit a Feedback report with a small sample project attached for us to take a look at! https://feedbackassistant.apple.com It's not adding/removing text from the view that I'm doing, but hiding it wholesale. And since it's in a stack view, it was getting removed from the view hierarchy altogether\u2014without being animated, so I thought I'd need a custom animation to cover that up. Now I'm thinking it would work better to add a constraint setting the height to 0 to \"hide\" it without removing it, and thus letting it animate I can stick to calling that method myself, it's pretty easy to do, I was just hoping for a little magic, lol. I'll try to drum up a sample project for a feedback that repros the issue, though Gotcha \u2014 and yes a sample project would be great! \u201c\u2026when a cell is resized with animation, the cell will also receive a full layout pass as part of the same animation.\u201d Is this true for updates via performBatchUpdates as well? My recollection is that there were issues around the cell view instance getting swapped out which would end up killing the animation within the cell. Are cell views always preserved across empty batch updates, or am I misremembering? > My recollection is that there were issues around the cell view instance getting swapped out which would end up killing the animation within the cell. Are cell views always preserved across empty batch updates, or am I misremembering? For empty batch updates, as well as the new iOS 16 self-sizing invalidation feature, the existing cells won\u2019t be replaced or reused. Only when using the reload API to reload an item/section will the cell get replaced with a new one. Thanks Tyler! (Don\u2019t forget about the reconfigure API introduced in iOS 15 to update cells without replacing them, too!)","title":"With the new self-sizing cells, is there a way to run an animation alongside a cell resize animation (or at least, adding a custom completion handler to the system-provided resize animation)?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-get-a-notification-when-a-new-item-eg-image-is-added-to-uipasteboard-remotely-eg-through-universal-clipboard-currently-i-can-poll-changecount-to-detect-changes-but-i-receive-no-changednotification-unless-the-copy-originated-from-the-app-itself-im-trying-to-implement-a-feature-where-the-app-conveniently-offers-a-paste-button-when-theres-something-on-the-uipasteboard","text":"That would've been handy. We do the same but with users' texts, suggesting them a button that Creates document right from the pasteboard. But it probably doesn't work with universal clipboard. I'm not sure if it works for us in the iPad split view, but I assume we check pasteboard everytime our app becomes active. However, AFAIK, it is not possible with universal clipboard We generally discourage polling the pasteboard in any way, it can be resource-intensive. However, feel free to request a public notification for when the pasteboard changes, you can check for item count + metadata w/o triggering the paste alert. The new paste button does enable/disable itself when the something appears on the pasteboard though. Hmm, is it customizable? Would be grateful for a link on the session about that ) \u201cHowever, feel free to request a public notification for when the pasteboard change\u201d. Is there an API to do this? Did you mean I should file a feedback? Also, there seems to be an issue where the UIPasteboard.general.changeCount gets incremented by two every time I start editing a text view (without copying anything) https://developer.apple.com/wwdc22/10068?time=683 > Did you mean I should file a feedback? Yes. \u201cyou can check for item count + metadata w/o triggering the paste alert.\u201d When you said metadata, are there any existing ones I can look at (to detect duplicates) without showing the alert? Re: changeCount , we don't guarantee that it will increment +1 each time. Multiple processes etc. may end up bumping it up more. I see metadata = types of items. So e.g. -[UIPasteboard hasStrings] is a metadata query, but -[UIPasteboard strings] is a data query. So, assuming I want to detect when a new image is pasted elsewhere\u2026 hasStrings will not change (already true), and changeCount is an unreliable way of detecting new ones. So I\u2019m basically out of luck\u2026 You can also run pattern detection w/o triggering the paste alert, so e.g. looking for URL's and addresses via detectPatternsForPatterns:completionHandler: . This is also considered metadata and won't trigger the alert. changeCount will get incremented with a new image \u201c changeCount will get incremented with a new image\u201d. Yes, I\u2019m observing that. However since changeCount gets incremented no matter what the type of the content is, it seems like I cannot get a image-specific changeCount , is that correct? You can check if the data is an image and the change count is incremented. You\u2019re right! I have a bug in my code :man-facepalming:. Thanks!","title":"Is there a way to get a notification when a new item (e.g. image) is added to UIPasteboard remotely (e.g. through Universal Clipboard). Currently I can poll changeCount to detect changes, but I receive no changedNotification unless the copy originated from the app itself. I'm trying to implement a feature where the app conveniently offers a paste button when there's something on the UIPasteboard."},{"location":"wwdc22/uiframeworks-lounge.html#does-swiftui-support-the-new-toolbar-modes-for-ipad-or-do-you-need-to-use-uikit","text":"It does! Check out the toolbar role API: https://developer.apple.com/documentation/swiftui/view/toolbarrole(_:) And the secondary action toolbar item placement: https://developer.apple.com/documentation/swiftui/toolbaritemplacement/secondaryaction For more information, I go into these APIs in the 2 part of SwiftUI on iPad series: https://developer.apple.com/videos/play/wwdc2022/10058 https://developer.apple.com/videos/play/wwdc2022/10058 Thank you so much!","title":"Does SwiftUI support the new toolbar modes for iPad or do you need to use UIKit?"},{"location":"wwdc22/uiframeworks-lounge.html#hello-should-we-approach-swiftui-views-as-data-models-for-real-views-where-should-we-place-data-formatting-logic-if-swiftui-view-is-a-data-model-we-can-data-formatting-logic-there-how-can-we-test-our-data-formatting-logic-in-this-case","text":"The most convenient way to unit test your data formatting logic (like a Foundation FormatStyle) will be to factor the formatter out and assert against the String it produces I generally wouldn't consider a SwiftUI view to be a data model Factoring out the formatter might take the form of lifting into a model or view model Or it might just take the form of specifying it in a computed property on the View and running the assertion against that So, we can use property testing to assert data formatting directly inside SwiftUI Views. Is it good practice? How do you deal with this problem at Apple in big projects? Unit testing is a bit of an art and so I wouldn't say it's a necessarily a good or bad practice to assert directly against the SwiftUI view. I think the concerns are more pragmatic: how easy is it to assert against the SwiftUI view vs against a model. Does one way or the other require extra boilerplate? Does one way or the other lead to fewer false negatives? Thanks Kyle for your answers :raised_hands:","title":"Hello, should we approach SwiftUI views as data models for real views? Where should we place data formatting logic? If SwiftUI view is a data model we can data formatting logic there? How can we test our data formatting logic in this case?"},{"location":"wwdc22/uiframeworks-lounge.html#does-this-new-apis-are-backward-comaptible-with-uikit-project-or-just-only-ios-16","text":"In general all APIs that are introduced in new versions of our operating systems always require those versions to be used. However you can conditionalize your code to only use those new features when running on that OS and have your app back deploy to old versions without those features. Ok got it, but it might double the implementation and maintenance That, unfortunately, is a technical limitation. These features are implemented in frameworks that are shipping with the OS and that are heavily integrated with the OS, so they require the new OS to work. In a lot of cases you can just add the new features when running on the new OS and just do nothing on older versions, in which case it is enough to just check the current version you are running at run time before calling out to the new APIs. Ok got it thanks, I think we might need to wait until next two years lol :smile:","title":"Does this new APIs are backward comaptible with UIKit project or just only iOS 16?"},{"location":"wwdc22/uiframeworks-lounge.html#whats-the-naming-convention-youd-recommend-using-for-a-swiftui-view-with-or-without-view-suffix-i-think-ive-seen-both-in-apple-examples","text":"A good rule of thumb is that if the name we are picking has a clear visual representation, say Text , Image , Toggle , we omit the view suffix. In cases like ProgressView where Progress really would feel more like the data itself than its visual representation we add the View suffix. Specifically in the case of Progress we would clash with Foundation.Progress which will require developer to always fully qualify the type name which is not ideal. How does one ask a new question in this channel? I don't see a way to do it. That makes a lot of sense, thank you Luca! Clint \u2013 click the + in the bottom left and select \u201cAsk a question\u201d: Thanks for clarifying--the message next to that plus button says \"only certain people can post\" so that threw me off","title":"What\u2019s the naming convention you\u2019d recommend using for a SwiftUI view - with or without View suffix? I think I\u2019ve seen both in Apple examples."},{"location":"wwdc22/uiframeworks-lounge.html#once-again-here-goes-the-question-about-swiftui-views-in-tablecollection-cell-grin-if-wed-like-to-tinker-with-our-own-uihostingconfiguration-to-support-older-os-versions-what-would-be-the-recommended-way-to-update-cells-height-when-used-with-diffabledatasource-currently-the-most-common-scenario-i-see-is-a-pair-of-tableviewbeginupdates-tableviewendupdates-it-would-be-nice-to-recap-on-our-available-options","text":"On iOS 15 and earlier, you request an update to the size of self-sizing cells in UICollectionView and UITableView by: \u2022 If using diffable data source, re-applying the diffable data source\u2019s current snapshot with animatingDifferences: true \u2022 If not using diffable data source, performing empty batch updates on the collection or table view directly (that\u2019s the same as the begin/end updates you mentioned) Got it, thanks!","title":"Once again, here goes the question about SwiftUI views in Table/Collection cell. :grin: If we'd like to tinker with our own UIHostingConfiguration to support older OS versions, what would be the recommended way to update cell's height when used with DiffableDataSource? Currently, the most common scenario I see is a pair of  tableView.beginUpdates() tableView.endUpdates()  It would be nice to recap on our available options."},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-proper-way-to-implement-dynamic-menu-items-on-macos-with-commandmenu-menu-items-that-change-with-keyboard-modifier-keys-im-currently-polling-on-a-timer-but-that-is-unsatisfying-in-appkit-this-was-done-using-the-isalternate-property-fb9406583","text":"There is not a way to do this at the moment. Thanks for filing (and referencing) the feedback, we will route it to the appropriate component!","title":"Is there a proper way to implement dynamic menu items on macOS with CommandMenu? (Menu items that change with keyboard modifier keys.) I'm currently polling on a timer, but that is... unsatisfying. In AppKit, this was done using the isAlternate property.  (FB9406583)"},{"location":"wwdc22/uiframeworks-lounge.html#weve-experiencing-a-watchdog-termination-in-the-background-due-to-a-function-inside-uikit-sleeping-please-see-the-following-backtrace","text":"Thread 0 Crashed: 0 libsystem_kernel.dylib 0x00000002065bba2c __semwait_signal + 8 1 libsystem_c.dylib 0x00000001d9c700e4 nanosleep + 220 (nanosleep.c:104) 2 libsystem_c.dylib 0x00000001d9c70e14 usleep + 68 (usleep.c:52) 3 QuartzCore 0x00000001d292bc84 CABackingStoreCollectBlocking + 264 (x-misc.cpp:186) 4 UIKitCore 0x00000001d1184778 __35-[UIWindowScene _prepareForSuspend]_block_invoke + 60 (UIWindowScene.m:1273) 5 UIKitCore 0x00000001d1243b90 -[_UIContextBinder purgeContextsWithPurgeAction:afterPurgeAction:] + 472 (_UIContextBinder.m:221) 6 UIKitCore 0x00000001d12874ec -[UIWindowScene _prepareForSuspend] + 88 (UIWindowScene.m:1271) 7 UIKitCore 0x00000001d1ec935c __130-[UIApplication _updateStateRestorationArchiveForBackgroundEvent:saveState:exitIfCouldNotRestoreState:updateSnapshot:windowScene:]_block_invoke_2 + 228 (UIApplication.m:11237) 8 UIKitCore 0x00000001d128e4dc -[_UIAfterCACommitBlock run] + 72 (_UIAfterCACommitQueue.m:137) 9 UIKitCore 0x00000001d11a1864 -[_UIAfterCACommitQueue flush] + 192 (_UIAfterCACommitQueue.m:228) 10 libdispatch.dylib 0x00000001ce7eae6c _dispatch_call_block_and_release + 32 (init.c:1517) 11 libdispatch.dylib 0x00000001ce7eca30 _dispatch_client_callout + 20 (object.m:560) 12 libdispatch.dylib 0x00000001ce7faf48 _dispatch_main_queue_drain + 928 (inline_internal.h:2622) 13 libdispatch.dylib 0x00000001ce7fab98 _dispatch_main_queue_callback_4CF + 44 (queue.c:7770) 14 CoreFoundation 0x00000001ceb3d800 __CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__ + 16 (CFRunLoop.c:1795) 15 CoreFoundation 0x00000001ceaf7704 __CFRunLoopRun + 2532 (CFRunLoop.c:3144) 16 CoreFoundation 0x00000001ceb0abc8 CFRunLoopRunSpecific + 600 (CFRunLoop.c:3268) 17 GraphicsServices 0x00000001eac3e374 GSEventRunModal + 164 (GSEvent.c:2200) 18 UIKitCore 0x00000001d147a648 -[UIApplication _run] + 1100 (UIApplication.m:3511) 19 UIKitCore 0x00000001d11fbd90 UIApplicationMain + 364 (UIApplication.m:5064) 20 [REDACTED] 0x0000000104e87460 main + 112 (main.m:27) 21 dyld 0x00000001055e5ce4 start + 520 (dyldMain.cpp:879) Hi there! This looks like a potentially issue in CoreAnimation. If you haven't already, can you file a feedback with the entire crash log? Sure thing. Thanks! :slightly_smiling_face: Thank you! Apps generally have around 5 seconds to stop processing when they\u2019re backgrounded. If an app is still \u2018alive\u2019 and doing things at that point, it will be terminated to preserve system responsiveness. In this 5 seconds, we usually reconfigure your apps interface multiple times (rotating, switching to light/dark mode etc.) to take screenshots to use in the application switcher UI. What might be happening here is that your app is taking a long time to lay out for these screenshots. It may be finishing in just under five seconds - then this system code (which frees up memory used by CA that\u2019s not necessary while your app has nothing on screen) runs - and during that we hit the timeout. If your crash trace has a \u201ctermination reason\u201d that mentions \u201cwatchdog\u201d that\u2019s an indication that this might be happening. Thanks for reading my novel. <@U03HELXN8CV> That's super interesting! I never considered how much work goes into creating screenshots for the app switcher","title":"We've experiencing a watchdog termination in the background due to a function inside UIKit sleeping. Please see the following backtrace:"},{"location":"wwdc22/uiframeworks-lounge.html#my-doubt-is-regarding-drag-and-drop-on-swiftui-i-think-there-are-2-approaches-but-i-am-stuck-with-both-the-approaches-wwdc22-when-using-the-new-draggable-dropdestination-transferable-api-i-am-only-able-to-drag-single-items-i-am-unable-to-drag-multiple-items-in-a-list-i-have-filed-a-feedback-fb10128110-wwdc21-i-have-faced-a-couple-of-issues-for-drag-and-drop-introduced-in-wwdc21-ondrag-ondrop-itemidentifier-the-feedback-ids-are-fb9854301-fb9854569-fb9855245-fb9855532-fb9855567-fb9855575-note-all-feedbacks-include-a-sample-project-with-detail-steps-and-some-even-have-screenshots-and-videos-please-let-me-know-me-if-my-approach-is-wrong-or-if-i-am-missing-something-unfortunately-i-didnt-manage-to-get-a-swiftui-lab-session-got-declined-so-any-assistance-on-the-above-would-be-much-appreciated","text":"Hi <@U03JCHKCDB4>! First, thank you for filing the radars! Getting feedback on the new APIs and knowing which enhancements are anticipated is important. For now, I have one pointer that could be useful: for Lists and ForEach, we expect you to use the onInsert modifier. onDrag is designed for other types of views. FYI, onInsert on a Table with 0 rows did not work last year (FB9265795) on macOS, it seems the problem persists this year with macOS. Haven\u2019t test it in iOS, which now supports Tables too. > Thanks for the response, I am a bit confused here, so when should I use draggable ? Did you mean draggable instead of onDrag ? Yeah, I was talking about last year APIs. draggable is new this year, analogous to onDrag and making use of Transferable 1. Ok that clarifies one part, please bear with me as I slowly understand this. So draggable is the new approach however does it only support simple cases now (like single item drag for non-list and non-ForEach views)? 2. Also using draggable I noticed there was no way to generate preview , or specify the item is to be moved (no green + circle) instead of copied 3. So would slowly draggable grow powerful that would one day support all cases instead of ondrag ? <@U03HW7P2WTB> 4. Also I would be really over the moon, if someone could have a look at the above posted Feedback IDs, as it would help me ship my app which is a bit stuck. 1. Yes. 2. This is correct as well 3. Can\u2019t comment on that. 4. Sure! Can\u2019t promise anything, but thanks for bringing them up <@U03HW7P2WTB> \u2022 Thanks a lot!!! Fingers crossed \u2022 So in the meantime I think I will continue to use the older APIs ( onDrag / onInsert / onDrop / itemProvider ) \u2022 I however I am excited about the new APIs with Transferable looks pretty cool, fingers crossed again one day I get to use them. \u2022 Thanks for patiently answering all the questions, really appreciate it <@U03HVFXKB5L> I am a huge fan of your blog posts and app, has helped me tremendously including for drag and drop, most of my above questions were on the mac. Thanks a lot!!! Thanks <@U03JCHKCDB4>, it seems I have a lot of work ahead to update both the app and the blog :wink: So excited with some of the additions this year, specially Layout and NavigationPath. <@U03HW7P2WTB> Thank you so much for helping me get the lab appointment, really appreciate it, made my day!!! You guys have no clue how excited i was \u2026 I am really grateful to you guys for answering all my questions. I am so sorry i forgot note down his name \u2026 I have been watching so many videos to get his name to tag him on this post. Please let him know as well Sure! Thank you as well for filing the feedback and asking great questions! It was nice meeting you today.","title":"My doubt is regarding drag and drop on SwiftUI I think there are 2 approaches but I am stuck with both the approaches  WWDC22 - When using the new draggable, dropDestination, Transferable API, I am only able to drag single items. I am unable to drag multiple items in a list. I have filed a feedback FB10128110  WWDC21 - I have faced a couple of issues for drag and drop introduced in WWDC21 (onDrag, onDrop, itemIdentifier), the Feedback ids are FB9854301, FB9854569, FB9855245, FB9855532, FB9855567, FB9855575.  Note: All feedbacks include a sample project with detail steps and some even have screenshots and videos  Please let me know me if my approach is wrong or if I am missing something. Unfortunately I didn't manage to get a SwiftUI lab session (got declined), so any assistance on the above would be much appreciated"},{"location":"wwdc22/uiframeworks-lounge.html#are-there-any-major-limitations-to-be-aware-of-with-the-new-uihostingconfiguration-for-collection-view-cells-using-swiftui-that-may-not-be-addressed-in-the-session-video","text":"We do try to address the majority of the limitations in the video! Theres a spot at the end where we talk about how UIViewControllerRepresentable will not work there. The video also covers considerations around data flow. https://developer.apple.com/wwdc22/10072 Ok, still waiting for the session to become available. Thanks :grin:","title":"Are there any major limitations to be aware of with the new UIHostingConfiguration for collection view cells using SwiftUI, that may not be addressed in the session video?"},{"location":"wwdc22/uiframeworks-lounge.html#i-was-wondering-if-we-can-use-the-swiftui-view-protocol-with-the-new-any-keyword-and-if-itll-be-helpful-or-a-good-solution-in-some-example-use-case","text":"any View does not itself conform to View , so is not a replacement for AnyView today. Also any View is only defining an existential container that you can use in a type signature but you can\u2019t instantiate that type. There might be use cases for using any View to store that into collection, for example: [any View] . But most of the time, if you\u2019re in the situation where you think you need [AnyView] or [any View] , what you should likely do is invert the view dependency flow and have [AnyDataModel] or [any DataModel] instead, then create your views based on the type of data provided at runtime. Great, thank you for the detailed explanation! You are welcome! Was your question based on a concrete problem you are trying to solve? No, I just watched the Embrace Generics session today and this question immediately popped to mind :slightly_smiling_face:","title":"I was wondering if we can use the SwiftUI View protocol with the new 'any' keyword? And if it'll be helpful or a good solution in some example use-case?"},{"location":"wwdc22/uiframeworks-lounge.html#we-have-compositional-layout-with-collectionview-and-content-is-coming-from-remote-including-image-so-based-on-the-image-height-we-adjust-the-height-of-the-cells-so-until-we-determine-the-height-of-the-image-we-have-ideal-height-but-the-problem-is-user-can-keep-scrolling-down-while-images-are-getting-downloaded-cells-are-drawn-with-ideal-height-but-when-image-get-downloaded-we-fix-the-height-for-the-cell-during-which-we-see-collection-view-jittersjump-up-down-due-to-content-size-changes-any-ways-to-improve-this","text":"Here is the video Hi Pavan, there are a few different strategies you can take here, depending on how you\u2019re updating your cells. What you\u2019ll want to do is to apply a content offset to the scroll view that is the inverse of the increase in the size from cells above the currently visible area. This would need to be performed synchronously, however. The details here depend on the specifics of your layout definition, though. Another alternative is to use .estimated for your item definitions, Then, if their size changes, UICollectionView will automatically adjust the content offset to preserve the semantic scroll position. Hi Aditya, yes we are using estimated height and the above results with that, so what we are doing once we get actual size we are reloading the section to determine the actual height but during which content size changes hence collection view jumps I didn\u2019t understand the inversion logic you said, sorry Ah, reloading the section. That is unfortunately a known issue with UICollectionView currently, where we don\u2019t preserve the scroll position when items/sections are reloaded. I see, do you think this could be solved in SwiftUI using ScrollView + LazyVStack? Any other recommendation is to improve this UI will be great help since this just works fine in android where our customers complaints for iOS :disappointed: Have you tried reconfiguring the individual items instead? Use either the reconfigure method on diffable data source, https://developer.apple.com/documentation/uikit/nsdiffabledatasourcesnapshot/3804468-reconfigureitems or UICollectionView.reconfigureItems(at:) : https://developer.apple.com/documentation/uikit/uicollectionview/3801889-reconfigureitems reloading performs a delete + insert hmm i see, but our app supports from iOS 13 but this API looks like from iOS 15.0 unfortunately Prior to iOS 15, you can update the contents of the cells directly (by fetching them from UICollectionView.cellForItem(at:) , and call performBatchUpdates: with an empty block to update their visible sizes. Cells that are off screen, or not currently prepared for display (like prefetched cells) will return nil from cellForItem(at:) , and you can just specify the full-size image when they\u2019re dequeued the next time. > Cells that are off screen, or not currently prepared for display (like prefetched cells) will return nil from cellForItem(at:) , and you can just specify the full-size image when they\u2019re dequeued the next time. Do i need to return nil for this specific case? > Prior to iOS 15, you can update the contents of the cells directly (by fetching them from UICollectionView.cellForItem(at:) , and call performBatchUpdates: with an empty block to update So with this approach will collection view preserves the scroll offset? No, you\u2019d be calling cellForItem(at:) on the UICollectionView, so it will return nil if a prepared cell does not currently exist. > So with this approach will collection view preserves the scroll offset? Yes! That should preserve the scroll offset. aha it\u2019s just like below? ```psuedo code // loop over cells let cell = UICollectionView.cellForItem(at:...) cell.image.height = xxx // Call performBatchUpdates collectionView.performBatchUpdates { <empty> } yep! That should work! Definitely encourage you to try it out. Ok thanks a lot for the great tips let me give a try :slightly_smiling_face: Just curious to know, does this issue can be solved if we adapt SwiftUI from iOS 14.0? Also, an alternative approach to all of this would be\u2013if you had control over the server\u2013for the server to tell you what the size of the image is before-hand. If that\u2019s not possible, you could also try reading the first few bytes of the image data to get the size. unfortunately that would be hard to get the sizes from the server since we don\u2019t control the data instead merchants does in our platform, but anyways we could give a try with suggested approach first and see Fair. Feel free to file a feedback if you run into issues, perhaps even try to get a lab appointment. Yes i got one for tomorrow but will give a shot on this, thanks --- > #### Hello! How can I get started with UIKit? I currently know SwiftUI only (which, in my opinion, is amazing btw!), but I'd like to learn a little bit of UIKit too, because I might use it in some cases too in the future. Thanks for asking this question. It\u2019s always good to know both frameworks so you can leverage the power of both and choose whatever is best for the problem you are solving. There are great, brand new documentations this year for both SwiftUI and UIKit. You can find a guide to get started here: <https://developer.apple.com/tutorials/app-dev-training#uikit-essentials> Thank you very much! Really appreciate it. We also have a new sample app this year that we are using for the Desktop Class App talks, which is a very feature rich app that I would recommend checking out here: <https://developer.apple.com/documentation/uikit/app_and_environment/building_a_desktop-class_ipad_app> Thank you! Aside from that there are also tons of other resources on the internet that have a very high quality, so I\u2019d also suggest looking around a bit there and see what people recommend. I must say that the tutorials are amazing. I followed some of them to learn Swift and SwiftUI. Thank you for this great tutorials and documentation! --- > #### Using ShareLink in SwiftUI and Transferable, we will now be able to share custom types between users. How does this compare to sharing Core Data entities using a .shared CloudKit database? As I understand, the main difference is that sharing using Transferable copies the item from User A and pastes it for User B, so in the end there will be 2 instances of this item, with no synchronization. Any subsequent changes to any of the items will remain local for that user. In contrast, sharing using .shared CloudKit database means a single entity will be synchronized for both users. Am I on the right track? Are there some other key differences to be aware of? Hi Jan! &gt; How does this compare to sharing Core Data entities using a .shared CloudKit database? There is a difference: `ShareLink` is used for sharing here and now, not for persistent storage. Core Data is a database\u2013and its intended use is for storing items on the disk. &gt; As I understand, the main difference is that sharing using Transferable copies the item from User A and pastes it for User B It depends on which transport APIs we are using. For example, Drag and Drop allows sharing items within a single process or between different processes. Copy/paste works similarly. If we are using `FileRepresentation`, we can specify the `allowAccessingOriginalFile` parameter for `SentTransferredFiles` and tell the system if should make the copy, or not. `Transferable` doesn\u2019t provide any synchronization, just passes around data. Thank you <@U03HW7P2WTB>! So in a use case where we are sharing persistent data between users of the same app (for example, a recipe created by the user in a recipe app), is using ShareLink discouraged in favour of sharing using Core Data + CloudKit? I wouldn\u2019t say *discouraged*, it just has different purpose. `ShareLink` allows to present the share sheet or another system sharing interface, so users could send over the recipes to their friends in Messages, or pasted into Notes, etc. Does this make sense? Yes I think so - in this case, the recipe would have to be converted to something Messages/Notes can understand, like String or Image. Right? --- > #### Good morning! Thank you for a wonderful WWDC! I was trying to play around with the new PhotoPicker. I\u2019m able to initiate the picker but I\u2019m not quite sure how to assign the selected photo to an image in my SwiftUI view. Would you please provide a code sample? I was trying to look into the documentation but I couldn\u2019t find anything that would resolve my problem. Good morning Euguene. We're glad you enjoyed WWDC! Did you try out the code snippets from the What's new talk? That has an example covering that. Yup. Also didn't work on my machine The PhotoPicker won't dismiss itself. And also I'm not sure where/how to grab that image It's a bug. Check my thread here for a fix. <https://twitter.com/xmcgraw/status/1534557535495147520?s=21&amp;t=wihRQOSc4PXqFcu1fWt-dQ|https://twitter.com/xmcgraw/status/1534557535495147520?s=21&amp;t=wihRQOSc4PXqFcu1fWt-dQ> You can access the Data of the image (Transferable). --- > #### Having implemented several SwiftUI apps, I still consider its constraints concerning custom designs a challenge to communicate to designers. Apart from HIG or trying out, do you know any resources/tooling which helps to implement good design? Like are their Sketch export tools to generate code for components/symbols? I know there are Figma (and potentially Sketch) plugins for generating SwiftUI code from designs, however I haven't personally used them so I can't vouch for how well they work Apple also provides design tool plugins, which make it easier for designers to create mock ups that use stock system components and styles: <https://developer.apple.com/design/resources/> Of course, nothing is going to be as high fidelity as the designer jumping into SwiftUI directly. Many designers have found SwiftUI to be surprisingly approachable and there are a number of SwiftUI resources available on the internet targeted at teaching designers to code SwiftUI views. Yeah, stock components are well supported and I know it\u2019s in Apples interest to push in that direction (which I like), but reality bites more often with custom tweaks and they are sometimes really hard to solve\u2026 I wish PaintCode would be updated to export to SwiftUI\u2026 --- > #### Is there a way to coordinate between SwiftUI DragGesture with their UIKit counterparts in the UIGestureRecognizerDelegate like `shouldBegin` or `shouldRequireFailure`? Sorry, no, there's no interop between SwiftUI gestures and UIKit gestures. How about multiple SwiftUI DragGestures? <@U03HHJNTFM0> You can ensure exclusivity with this: <https://developer.apple.com/documentation/swiftui/gesture/exclusively(before:)> Does it work if there are two DragGestures applying to different views? No, the exclusivity is only for a single gesture application. Then my question would be how to coordinate them They seem to conflict each other, and optionally I want to disable one over another It's not supported. Please file a feedback request for this. Will do, thanks <@U03HHJNTFM0> <@U03HELT4EG5> --- > #### About SwiftUI\u2019s Layout: When creating a custom Layout, is it OK to use another Layout within the func like sizeThatFits or placeSubviews, including the built-in Layout (like HStack)? More specifically, is it possible to compose some HStack and VStack within my custom Layout? In that case I have to create a LayoutSubview by myself, is it possible? Yes this is an explicit use case the Layout protocol was designed to support Yes, that should be fine, although you'll need to manage the cache(s) of any intermediate layouts you use. (E.g. store them in your own layout's cache storage, and invalidate them as needed.) You can also create subsets of the subviews collection, to pass to the inner layouts. \u2026 use the LayoutSubviews subscript(bounds:) operator for that. Any examples of layouts inside layouts we could look at? I don't think there are any available examples of that yet. I\u2019ve made one that works but doesn\u2019t yet deal with cache completely. <https://gist.github.com/ryanlintott/d03140dd155d0493a758dcd284e68eaa> For example, I\u2019m implementing the (Vertical) Flow Layout. After doing some calculation and chunked the `subviews`, firstly I use those chunked subviews to make some `HStack`s, then I should embed these `HStack` into a `VStack` but I cannot convert these `HStack`s to `LayoutSubviews` that I can pass to `VStack`\u2019s `subviews`parameter, right? --- > #### Hi there! What's the recommended way to repeatedly fetch data in a SwiftUI app, (so that we don't push updates from a different thread)? In general, I would suggest to factor out the logic that fetches the data into its own type. You always want to execute this kind of side effect not on the main thread and the hop back onto the main thread to set the data on the model. Swift\u2019s actors are a great tool to encapsulate this kind of logic. So for example you could the model that is offered to the view be an `ObservableObject` that is bound to the main actor, and have a separate actor (hence running on a separate thread) that takes care of the fetching and report back new data to your observable object. do you have an example of that? Thanks! In my case, I had a model that conformed to ObservableObject, but somehow I still got the warning. Will look into it though! I would like to see sample code for this. Words do not describe the nuts and bolts of coding..... I think conformance to ObservableObject is not the issue here. What you want is for your function to be wrapped in the Main actor, so updates to UI are pushed on the main thread: @MainActor func getData() {} Yeah, I understood that the conformance to ObservableObject is not the problem, but rather the way I used to asynchronously fetch data to the model. Thanks! Here\u2019s a link about Swift Actors that i just found: <https://developer.apple.com/videos/play/wwdc2021/10133/> --- > #### Would you use SwiftUI or UIKit to implement a list that can change layout between a table and grid layouts? I currently have a UICollectionView with different layouts but I can't animate between the layout change. ~\u2026what\u2019s your iOS target version~ :smiling_imp: As always we recommend using the tool that is best for the job. UICollectionView does support animating layout changes via `setLayout:animated:` so there is no need to rewrite this in SwiftUI. If there are specific things you want to achieve that can only be achieved in SwiftUI, then of course converting it to that is a very valid option. <@U03JRNE4KJL> 15 and 16 This also might be a good question for a 1on1 lab if you are facing specific issues with collection view animating between layouts. If there is time available I'll try to grab one :sweat_smile: I forget if I was using `setLayout` before and what the crash was. I'm using compositional layout and returning a different `NSCollectionLayoutSection` based on if the layout should be a grid or list, and calling `self.collectionView.collectionViewLayout.invalidateLayout()` , but I can try that and see if it works, or maybe there is a better way to construct multiple layouts? Right, UICollectionView can\u2019t currently animate changes to the layout stemming from an invalidation. If you require an animation, I would call `setCollectionViewLayout(_:animated:completion:)` --- > #### Assume that I keep the whole app state in the state object in my app struct? This way I can make sure that all the views are in a consistent state and have a single source of truth. How can I tune performance because in this case, SwiftUI starts diffing the whole app view hierarchy on every single change of the app state? You\u2019re completely right that with a single observable object, you\u2019ll end up invalidating large parts of your view hierarchy whenever a change occurs. Invalidating even large amounts of the SwiftUI view hierarchy should be an inexpensive operation, as view descriptions are rather lightweight, but if you are running into performance issues here, there are a few things you can do to help. The first recommendation I have is to split out some of the values which are only relevant to a certain subset of views into their own observable object. This is likely going to get you the most performance win of any of these suggestions, but if you don\u2019t want to make that architectural change, there are still some things you can do: \u2022 Avoid marking non-published values of your `ObservableObject` published \u2022 (Assuming you\u2019re using `EnvironmentObject` to make sure your single `StateObject` can be accessed throughout your view hierarchy) ensuring you only declare dependencies on the `EnvironmentObject` in places it\u2019s needed \u2022 And if you still need to optimize further, writing a custom `objectWillChange` implementation for your `StateObject`s which only does invalidation when changes that should actually affect the UI occur (in cases, for example, where published values have multiple different representations that should display in the same manner). Thanks Sam :+1: --- > #### What is correct way to specify file type that allowed for Drag&amp;Drop from Finder. I mean if I use ```UTType.fileURL``` it allows to Drop any type of files, but if I specify ```UTType.image``` that not allow Drop at all. What is correct approach if I want allow Drop only images from Finder? Use the generic UTType.fileURL. And also override draggingEntered to inspect the exact file types on the pasteboard. Return true/false as appropriate. Also when inspecting the pasteboard, use the options to auto filter the list for you. ```swift func readObjects( forClasses classArray: [`AnyClass`], options: [`NSPasteboard`.`ReadingOptionKey` : Any]? = nil ) -&gt; [Any]? If I use SwiftUI DropDelegate.dropEntered(info:) how I can get access to the pasteboard from DropInfo object? Or there is another approach in this case? Let me chime in. DropInfo has a member called hasItemsConforming(to:) member, https://developer.apple.com/documentation/swiftui/dropinfo/hasitemsconforming(to:)-47irh , which allows to check for the content types you are interested in. Does this help? I try to use it, but unfortunately, it does not help in my case. Or maybe I just do something wrong. This method only checks the dropped content type. In my case it\u2019s UTType.fileURL but it does not check is this an image or an mp4 file. Is it iOS or Mac? macOS Then the first idea on top of my head is to try to initialize NSImage with the URL/Data from the NSItemProvider and see if it succeeds. If the app isn\u2019t restricted to images from Finder only, you could register for receiving NSImage.imageTypes Oh, great idea. I\u2019ll try to initialize NSImage from NSImageProvider Thank you Julia!","title":"We have compositional layout with CollectionView and content is coming from remote including image so based on the image height we adjust the height of the cells, so until we determine the height of the image we have ideal height, but the problem is user can keep scrolling down while images are getting downloaded cells are drawn with ideal height but when image get downloaded we fix the height for the cell during which we see collection view jitters/jump up down due to content size changes Any ways to improve this?"},{"location":"wwdc22/uiframeworks-lounge.html#im-trying-to-mimic-the-3-view-layout-in-xcode-pages-numbers-etc-using-swiftui-sidebar-document-inspector-i-can-use-the-navigationsplitview-for-the-sidebar-and-document-but-i-cant-find-a-way-to-animate-the-inspector-in-a-right-sidebar-the-swiftui-animation-usually-animates-all-the-child-views-in-the-inspectors-view-is-there-a-way-to-do-this-in-swiftui-on-the-mac","text":"Hi James - thanks for the question. One thought we had here is potentially using an HSplitView as the view for your detail, and having one of its children be the inspector. The animation not working is likely a bug, and we'd definitely appreciate a feedback about it. How are you trying to do the animation currently, if I may ask? I\u2019ve tried something like with previous code (NavigationView is apparently deprecated) _NavigationView {_ _SideBarView(survey: $document.survey)_ _HSplitView {_ _DetailView(document: $document)_ _.padding()_ _InspectorPane(survey: $document.survey)_ _.animation(.linear, value: uiState.showInspectorPane) // looks terrible_ _}_ _}_ I\u2019ve tried moving the .animation inside the InspectorPane() struct, but the effect is still bad. Ok, sorry you're hitting that. It would be helpful if you could include those details in the feedback. But this is the expected approach, but using NavigationSplitView? That feels like a good expression of this structure, yes Thanks.","title":"I'm trying to mimic the 3-view layout in Xcode, Pages, Numbers, etc.  using SwiftUI.  Sidebar | Document | Inspector  I can use the NavigationSplitView for the Sidebar and Document, but I can't find a way to animate the Inspector in a right sidebar. The SwiftUI animation usually animates all the child views in the Inspector's view. Is there a way to do this in SwiftUI on the Mac?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-allow-duplicate-commandmenu-item-keyboardshortcuts-in-appkit-macos-menus-this-works-but-not-in-swiftui-the-duplicate-shortcuts-are-never-active-at-the-same-time-they-depend-on-the-type-of-the-selection-focus-fb9931615","text":"Assigning the same shortcut to different menu items is not recommended, even for AppKit apps. The user should be able to predict the outcome of each shortcut. If the selection focus changes which menu item is invoked, that would be confusing. The context (math vs text selection) is different enough to avoid confusion, but more importantly the change from AppKit to SwiftUI forces a UI change in the new app version which breaks current users muscle memory. I see you've already filed feedback, thanks. I don't suppose that the two different actions are similar enough that you could combine them into the same menu item, with the same shortcut? The Copy menu item is a good example of something that applies in a variety of different circumstances, and is strongly responder chain / focus driven... In the one case where I had overloaded a shortcut, no. (Command-I for italics in text, as people are very used to. In math, formatting is determined by content, so there is no Italic action. Command-I was overloaded for Isolate, and had been for 30 years, even before the app supported text blocks at all. It is a very, very old application.) I have rewritten 35 years of code in Swift and SwiftUI with the goal of maximum bug-for-bug compatibility. While there are inevitably going to be differences, I would like to make the transition as seamless as possible for users, many of whom have been using the software for decades. Ron, in your AppKit version, did you use the same selector for each item that had a matching command key? I\u2019m asking just to understand the approach that worked for you in the old codebase.","title":"Is there a way to allow duplicate CommandMenu item .keyboardShortcuts? In AppKit macOS menus, this works, but not in SwiftUI.  The duplicate shortcuts are never active at the same time - they depend on the type of the selection focus. (FB9931615)"},{"location":"wwdc22/uiframeworks-lounge.html#swiftui-layout-isnt-possible-for-dynamic-content-right-since-each-subview-need-to-be-there-at-calculation-time","text":"It could depend what you mean by dynamic content The SwiftUI layouts you write yourself, like the system provided HStack and VStack, require all children up front However, often we use \"dynamic\" within the team to refer to content driven by a ForEach And that is absolutely supported with custom SwiftUI Layouts If the subviews change size the layout should respond to those size changes. The main thing that SwiftUI Layout doesn't support today is the lazy/incremental layouts that don't need to load all views up front There is not a way to build your own LazyHStack and LazyVStack on top of SwiftUI Layout... or at least not one that is as efficient in lazy loading content But as <@U03JLPYRCFK> alluded to, the SwiftUI framework automatically manages dependencies between views and will recall your layout's sizeThatFits or placeSubviews whenever anything needs to be recomputed (including child size changes) Thanks for the answer, <@U03HELTEP9T>. I mean something like Lazy in my question, which you already answered :grin: So, if the Layouts could\u2019ve been lazy, that means that the views in the Layout would\u2019ve been loaded only when needed (when they appeared on screen)? I\u2019m asking this to make sure I understood the concept of lazy grids. <@U03JRP87THN> That's correct. A lazy Layout theoretically wouldn't have access to all of its children up front from sizeThatFits or placeSubviews, and so that limits some of the things it can calculate up front. Paul covers this well at the beginning of the \"Compose custom layouts with SwiftUI\" talk from yesterday: https://developer.apple.com/videos/play/wwdc2022/10056/ Thank you very much for the clarification!","title":"SwiftUI Layout isn't possible for dynamic content, right? Since each subview need to be there at calculation time."},{"location":"wwdc22/uiframeworks-lounge.html#when-using-swiftui-with-core-data-should-we-setup-the-predicate-and-sort-descriptors-for-a-fetchrequest-in-the-view-init-or-in-the-onappear-of-the-view-when-theyre-based-on-object-passed-from-the-containing-view-i-noticed-some-issues-when-using-the-onappear-because-items-are-displayed-then-removed-with-animation","text":"If you want to be sure that the FetchRequest is setup before the view does any work you might want to set that up in the initializer. I think it would be worth to file a feedback regarding the issue that you are seeing with onAppear . How to update FetchRequest for searchbar changes? On this point in particular, I think we deserve a more SwiftUI-esque layer for communicating with Core Data. Turning the process of fetching data into a View is not very intuitive and breaks MVVM Is there a best practice for Core Data and Swift UI app project on http://developer.apple.com|developer.apple.com ? <@U03JRQ81NEL> here I can help. You pass the searchValue to your view with FetchRequest. Then in the init, you add an NSPredicate specifying that only results that contain searchValue should be shown, and then create a FetchRequest with that predicate Thanks <@U03J7BQQNPJ>. I'll fill a feedback. Do you confirm that the FetchRequest is reset when the containing View is redrawn? For example, if the SubView customises the predicate or sortDescriptor (in a context where the SubView apply some filters). The FetchRequest is not tied to the View lifecycle contrary to State or StateObject.","title":"When using SwiftUI with Core Data, should we setup the predicate and sort descriptors for a FetchRequest in the View init or in the onAppear of the View, when they\u2019re based on object passed from the containing View? I noticed some issues when using the onAppear because items are displayed then removed (with animation)."},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-specific-session-this-week-that-discusses-app-extensions-and-the-new-appextension-api-httpsdeveloperapplecomdocumentationextensionfoundationappextensionhttpsdeveloperapplecomdocumentationextensionfoundationappextension-i-tried-searching-and-found-nothing","text":"Hi! There\u2019s no session related to AppExtensions, but if you have specific questions about it, feel free to submit those and we\u2019ll respond as best we can :slightly_smiling_face: Just an introduction and overview. Is there some sample code. The docs are pretty sparse. For example, will this API allow me as Developer A to open up a part of my app and UI to Developer B to extend, or is it limited to extending Apple apps? Currently we have no documentation beyond the published API reference.","title":"Is there a specific session this week that discusses App Extensions, and the new AppExtension API (https://developer.apple.com/documentation/extensionfoundation/appextension)?|https://developer.apple.com/documentation/extensionfoundation/appextension)? I tried searching and found nothing."},{"location":"wwdc22/uiframeworks-lounge.html#in-swiftui-is-there-a-way-to-listen-for-navigation-events-or-would-that-have-to-be-custom-from-uikit-delegates","text":"Thanks for the question, Martin. With the new iOS 16 NavigationStack, you can bind a path to the stack. Then you can use the .onChange() modifier to watch for changes to the path. What about non navigation events like dismissal of sheets or full screen modals? Is there a stateful way to be informed of when these events are done so we can say present a follow up confirmation or pop the navigation stack back? Hey. David. Those dismiss events will reset the binding that caused the modal presentation to appear. So the same recommendation works there. You can use onChange() to observe that binding too. What if you\u2019re programmatically changing those bindings? If I set an isPresented var to false to dismiss a modal, how would I be able to know when the modal is actually gone? If I change the navigation backing state right away to pop a view or try and set a sheet as not to display , I\u2019m usually presented with inconsistent UI behavior or a message saying that I\u2019m trying to present / dismiss in an invalid state. Ah, I see. I\u2019m usually stuck doing something like asyncAwait after and then playing with the timeframe and that\u2019s, well, gross. The delay hack is one approach. Yah, and I does work for sure Another approach people seem to have some success with is using a common sheet modifier for a variety of presentations. Then you can drive the sheet with an optional enum state. Set it to nil to dismiss, or switch between different non-nil values to present different sheets. We\u2019re also made some improvements in iOS 16 to make the invalid state issue much less common. For example, we try to delay the presentation of the next sheet until the previous one is gone. So you might find that things just work now. :slightly_smiling_face: Yah, and that does work, but when you mixing sheets, full screen modals (leveraging into uirepresentable) and navigation stack changes, the interaction doesn\u2019t line up nicely. OK, I\u2019ll have to test around and file some feedbacks if I still find cases. Thanks I\u2019d love a Feedback with details of your use case. Sure; I\u2019ll see if I can extract out the sample cases. Thank you!","title":"In SwiftUI, is there a way to listen for navigation events or would that have to be custom from UIKit delegates?"},{"location":"wwdc22/uiframeworks-lounge.html#do-ids-in-a-foreach-just-need-to-be-unique-within-that-foreach-or-globally-for-instance-using-the-new-charts-all-my-data-is-keyed-by-date-and-there-are-several-places-where-i-have-adjacent-foreaches-that-include-mostly-the-same-dates","text":"The IDs must be unique within the container, so for example if you have a List with two ForEach's inside it, they must generate unique IDs within that List.","title":"Do ids in a ForEach just need to be unique within that ForEach or globally? For instance using the new Charts, all my data is keyed by date and there are several places where I have adjacent ForEaches that include mostly the same dates."},{"location":"wwdc22/uiframeworks-lounge.html#i-am-new-to-swiftui-what-is-the-best-way-to-switch-between-views-for-example-depending-on-the-condition-of-a-state-variable-how-can-i-switch-between-a-homeview-signinview-and-signupview","text":"I\u2019ve personally done this before with a parent \u201ccontainer\u201d view, an enum to represent the different possible views, and an @State var that I use in a switch statement to swap out views. I have @Binding vars in all my child views so they can programatically change what\u2019s shown in the parent. However, with programatic navigation in NavigationStack that\u2019d probably be the preferred way to do this, check out the https://developer.apple.com/videos/play/wwdc2022/10054/|SwiftUI cookbook for navigation session by <@U03HW7P0HQR> for more You can actually use switch statements directly in SwiftUI! e.g. enum Screen { case home case signIn case signUp } ... @State private var selectedScreen: Screen ... var body: some View { switch selectedScreen { case .home: HomeView() case .signIn: SignInView() case .signUp: SignUpView() } } Yeah that\u2019s how I\u2019ve done it But I mostly did that because I couldn\u2019t use a NavigationStack on macOS before NavigationView is what I used before the new navigations were introduced. Depending on your needs, NavigationStack is also a great tool to use for this, directly manipulating the current path :+1: But if you just need to swap views in and out, a switch works just fine. For example, it\u2019s common to use a switch to choose the detail view of a NavigationSplitView based on the current selection. Thank you so much!","title":"I am new to SwiftUI, what is the best way to Switch between Views? For example, depending on the condition of a @State variable how can I switch between a HomeView, SignInView and SignUpView?"},{"location":"wwdc22/uiframeworks-lounge.html#textkit-2-question-just-watching-the-video-about-how-glyph-apis-are-no-longer-available-i-have-code-that-capitalises-characters-at-glyph-rendering-time-by-finding-the-appropriate-character-range-for-the-glyph-range-converting-to-upper-case-and-re-generating-glyphs-for-the-new-string-its-important-to-me-to-not-change-the-contents-of-my-text-storage-to-have-the-upper-case-characters-this-should-behave-like-a-display-time-attribute-how-would-you-go-about-doing-this-in-textkit-2","text":"We don\u2019t have any TextKit 2 experts around this morning - but there\u2019s a lab tomorrow morning at 10AM PDT - that would be a great place to ask this. The TextKit folks are very friendly :-) Look for \u201cTextKit 2, Fonts, and SFSymbols Adoption lab\u201d. I will ask there! I'd love to know the answer to this too","title":"TextKit 2 question: just watching the video about how glyph APIs are no longer available.   I have code that capitalises characters at glyph rendering time, by finding the appropriate character range for the glyph range, converting to upper case, and re-generating glyphs for the new string.   It\u2019s important to me to not change the contents of my text storage to have the upper case characters \u2014 this should behave like a display time attribute.   How would you go about doing this in TextKit 2?"},{"location":"wwdc22/uiframeworks-lounge.html#swiftui-when-using-self_printchanges-in-the-body-of-views-what-should-we-pay-close-attention-to-is-self-printed-often-harmful-what-are-the-numbers-44-we-see-when-using-fetchrequest-in-the-view","text":"@Self self is not inherently harmful. The purpose of Self._printChanges() is not to indicate that something is wrong but rather as a tool to match your expectation with what is happening at runtime. What you want to be on the look out for is that you don\u2019t do unnecessary invalidation which might cause performance issue. If you see @ followed by any number it means that we couldn\u2019t find the field (property) name, so printed the byte offset of the swift view struct field instead. Thanks <@U03J7BQQNPJ> for the precisions! Helpful. I'm seeing lots of @Self printed when a View has an EnvironmentObject dependency (a Core Data object), and the object publishes a change. For example, a List of Core Data objects (owned) related to the EnvironmentObject (owner). If I update an owned object from the List, the relationship between the owner and the owned is updated, the owner EnvironmentObject publishes an update. This break the animation that should be displayed when updating the owned item in the List.","title":"SwiftUI: When using Self._printChanges() in the body of Views, what should we pay close attention to? Is @Self printed often harmful? What are the numbers @44 we see when using FetchRequest in the View."},{"location":"wwdc22/uiframeworks-lounge.html#hi-im-still-having-an-issue-with-generic-view-models-in-swiftui-im-having-trouble-to-understand-what-should-i-choose-between-observableobject-or-stateobject-when-injecting-a-generic-view-model-into-a-swiftui-view-im-commonly-doing-this-swift-protocol-viewmodelprotocol-struct-myviewltviewmodel-viewmodelprotocol-view-observedobject-var-viewmodel-viewmodel-let-myviewmodel-myviewmodel-let-myview-myviewviewmodel-myviewmodel-in-this-case-the-viewmodel-property-should-be-stateobject-or-observableobject","text":"I may add some precision to my code example. The ViewModelProtocol conforms to ObservableObject . And MyViewModel should be a class conforming to ViewModelProtocol This particular example should be @ObservedObject https://developer.apple.com/wwdc22/10072 . There are a few nuances that are explained in this year's Use SwiftUI with UIKit and last year's Data Essentials sessions. With @StateObject , MyView would own the model, and is at liberty to make a copy of it when initialized. That means that the myViewModel variable would become \"disconnected\" from the @StateObject . https://developer.apple.com/videos/play/wwdc2020/10040|Data Essentials in SwiftUI Thanks for the quick reply ! What is a common case where @StateObject should be useful ? I feel like when I\u2019m using it, I can\u2019t mock any data because the model can\u2019t be injected so it makes unit test or screenshots automation harder If your view wants to own an ObservableObject view model, and the lifetime of that object should match the lifetime of the identity of the view, you should use StateObject. You can initialize a StateObject with an external value using this code in your initializer: _viewModel = StateObject(wrappedValue: existingModel) Note that the value passed to that initializer is accessed once and can't change over time. Thank you. It makes sense. With your solution, @StateObject allows me to mock the data passed to the view (example: if the view displays a list of books, I can pass the mocked book list to the view initializer and then instantiate the StateObject ). But it does not allow me to mock the state of the view (example : if the view model controls wether or not the view is in editing mode) Edit : typo","title":"Hi ! I\u2019m still having an issue with generic view models in SwiftUI. I\u2019m having trouble to understand what should I choose between @ObservableObject or @StateObject when injecting a generic view model into a SwiftUI view.  I\u2019m commonly doing this: swift protocol ViewModelProtocol { }  struct MyView&amp;lt;ViewModel: ViewModelProtocol: View {     @ObservedObject var viewModel: ViewModel }  let myViewModel = MyViewModel() let myView = MyView(viewModel: myViewModel)  In this case, the viewModel property should be @StateObject or @ObservableObject ?"},{"location":"wwdc22/uiframeworks-lounge.html#using-swiftui-cells-in-uikit-is-a-fantastic-feature-in-ios-16-however-that-will-not-work-if-my-app-is-still-supporting-ios-prior-to-ios-13-right","text":"The use of new APIs requires iOS 16, so unfortunately you can not use these in iOS 15 or before. However you can use them in an app that is available on iOS 15 and before conditionally when the user is running iOS 16. Thanks <@U03HELWUJN9> So I will always need to wrap my code in the if @available block Correct. That is the recommended way to do this. Thanks for confirming <@U03HELWUJN9> :pray::skin-tone-2:","title":"Using SwiftUI cells in UIKit is a fantastic feature in iOS 16, however that will not work if my app is still supporting iOS prior to iOS 13. Right ?"},{"location":"wwdc22/uiframeworks-lounge.html#hi-there-this-might-not-be-a-ui-specific-question-sorry-for-that-but-id-like-to-know-the-answer-to-use-the-new-swiftui-apis-how-can-i-make-sure-i-support-older-ios-versions-for-example-ios-15-in-my-swiftui-app-while-still-using-the-new-features-for-ios-16-users-are-there-any-macros-for-that","text":"You can conditionalize your code based on whether iOS 16 APIs are available like this: if #available(iOS 16, *) { // iOS 16 code } else { // iOS 15 and earlier code } Thanks! This is exactly what I was searching for. Is there any difference between if #available and @available ? Also note that Xcode will see that you are targeting an older version and will offer to put that check for you automatically. Either as if #available , or prefix your entire function or struct with @available(iOS 16.0, *) I saw someone talk about @available in another thread, the question above this one. Thank you! @available is used to annotate your declarations to mark them as requiring a newer iOS version. For example, if you have a method that takes in an object/struct only available on iOS 16, then you need to annotate the method as @available(iOS 16, *) #available is a runtime check to be used in a condition. @available is an annotation for a method that makes that method unavailable. If your iOS 16 only code is run in a method that is only available in iOS 16 then the compiler won\u2019t complain. So you can use a mixture of both. Thank you! Right, the compiler will then complain if you try to call that method on an older iOS version. Frameworks like UIKit and SwiftUI use this to prevent new API introduced in iOS 16 from being called on iOS 15, for example. Interesting! Thank you very much, I really appreciate it! Usually one way suffice, but sometimes, you can use both together: struct ContentView: View { var body: some View { VStack { if #available(macOS 13.0, *) { MyNewView() } else { // Fallback on earlier versions MyOldView() } } } } @available(macOS 13.0, *) struct MyNewView: View { var body: some View { Grid { GridRow { Text(\"Hello\") } GridRow { Image(systemName: \"hand.wave\") } } } } struct MyOldView: View { var body: some View { VStack { Text(\"Hello\") Image(systemName: \"hand.wave\") } } }","title":"Hi there! This might not be a UI-specific question (sorry for that), but I'd like to know the answer to use the new SwiftUI APIs.  How can I make sure I support older iOS versions, for example iOS 15 in my SwiftUI app, while still using the new features for iOS 16 users? Are there any macros for that?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-of-disabling-scrolling-on-a-list-or-would-the-best-option-be-to-use-uihostingconfiguration-inside-of-a-uicollectionview-thats-wrapped-in-a-uiviewrepresentable-so-that-i-can-disable-scrolling-on-the-collection-view-itself","text":"Use the new scrollDisabled modifier. https://developer.apple.com/documentation/swiftui/menu/scrolldisabled(_:) Ah, I completely missed that, nice!","title":"Is there a way of disabling scrolling on a List, or would the best option be to use UIHostingConfiguration inside of a UICollectionView that's wrapped in a UIViewRepresentable so that I can disable scrolling on the collection view itself?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-an-analog-to-appkits-flangschanged-notification-to-change-the-appearance-of-swiftui-controls-based-on-keyboard-modifiers-im-currently-polling-on-a-timer-to-do-this-but-that-is-unsatisfying-fb9601140","text":"Hi Ron, You could try using something like this: import AppKit import SwiftUI class KeyboardModifierMonitor: ObservableObject { @Published var eventModifiers = EventModifiers() var localEventMonitor: Any? var globalEventMonitor: Any? init() { localEventMonitor = NSEvent.addLocalMonitorForEvents( matching: [.flagsChanged], handler: { [weak self] event in self?.eventModifiers = EventModifiers(event.modifierFlags) return event } ) globalEventMonitor = NSEvent.addGlobalMonitorForEvents( matching: [.flagsChanged], handler: { [weak self] event in self?.eventModifiers = EventModifiers(event.modifierFlags) } ) } deinit { localEventMonitor.map { NSEvent.removeMonitor($0) } globalEventMonitor.map { NSEvent.removeMonitor($0) } } } extension EventModifiers { init(_ flags: NSEvent.ModifierFlags) { self.init() if flags.contains(.capsLock) { insert(.capsLock) } if flags.contains(.shift) { insert(.shift) } if flags.contains(.control) { insert(.control) } if flags.contains(.option) { insert(.option) } if flags.contains(.command) { insert(.command) } if flags.contains(.numericPad) { insert(.numericPad) } if flags.contains(.function) { insert(.function) } } } which you would then use in your controls like so: public struct MyAmazingButton: View { @StateObject fileprivate var eventMonitor = KeyboardModifierMonitor() public var body: some View { let modifiers = eventMonitor.eventModifiers let isOptionHeld = modifiers.contains(.option) let isShiftHeld = modifiers.contains(.shift) &lt;...&gt; Nice! Will do. Oh darn! I did try exactly that a year ago. The problem is that while the macOS menu is down, the message is not delivered. The behavior in AppKit of opening a menu, then trying the modifier keys to see what changes while the menu is down is lost. EventMonitors don't fire during tracking loops, and while a menu is open, AppKit is tracking that via an event tracking loop. You can submit a feedback request to get modifier changes during event tracking. But what it really sounds like is that you want a way to provide alternate menu items via SwiftUI. Should I file a separate fb to get modifer events during event tracking, or is FB9601140 sufficient since all I really want is alternate menu items via SwiftUI. <@U03HZ42MBV3> FB9601140 does't discuss menus at all. Observing modifier changes to update what's in the menu isn't the right way to go about it. Please file a FB requesting what you really need, which is a way to specify alternate menu items in SwiftUI. Sorry, wrong number, juggling too many open issues. that should have been FB9406583 Ah, yes. That radar covers it. Thanks!","title":"Is there an analog to AppKit's flangsChanged notification, to change the appearance of SwiftUI controls based on keyboard modifiers? I'm currently polling on a timer to do this, but that is unsatisfying. (FB9601140)"},{"location":"wwdc22/uiframeworks-lounge.html#uinavigationbarappearances-have-different-behavior-between-ios-13-and-ios-15-how-can-we-get-the-behavior-of-ios-13-in-the-current-version-of-ios-where-the-navigation-bar-doesnt-disappear-upon-scrolling-and-can-take-a-background-color-easily-and-wouldnt-it-make-sense-if-a-breaking-change-is-introduced-to-document-the-code-necessary-to-restore-the-previous-behavior","text":"The primary change from iOS 13 to iOS 15 is that the scrollEdgeAppearance always applies regardless of if you have a collapsible section or not (large title or collapsing search bar) so if you want the pre iOS 15 behavior, just configure the scrollEdgeAppearance the same as the standardAppearance , which should just be 1 extra line of code to assign your configured standardAppearance to the scrollEdgeAppearance . I tried that but it didn't work -- it wouldn't take the named color. can you post the code your using? *let* nav = UINavigationController(rootViewController: spotListVC!) *self*.window!.rootViewController = nav `*let* spotBlue = UIColor(named: String(\"PrimaryBlue\"))` *`if`* `*#available*(iOS 15.0, *) {` `// iOS 15 and above` `*let* navigationBarAppearace = UINavigationBarAppearance()` `navigationBarAppearace.configureWithOpaqueBackground()` `navigationBarAppearace.backgroundColor = spotBlue` `navigationBarAppearace.titleTextAttributes = [.foregroundColor: UIColor.lightText]` `nav.navigationItem.standardAppearance = navigationBarAppearace` `nav.navigationItem.scrollEdgeAppearance = navigationBarAppearace` `*let* buttonAppearance = UIBarButtonItemAppearance()` `buttonAppearance.normal.titleTextAttributes = [.foregroundColor: UIColor.white]` `nav.navigationItem.standardAppearance?.buttonAppearance = buttonAppearance` `nav.navigationItem.compactAppearance?.buttonAppearance = buttonAppearance` `} *else* { // iOS 14 and below` `// Fallback on earlier versions` `*let* navigationBarAppearace = UINavigationBar.appearance()` `navigationBarAppearace.tintColor = spotBlue` `navigationBarAppearace.barTintColor = spotBlue` `navigationBarAppearace.titleTextAttributes = [NSAttributedString.Key.foregroundColor:UIColor.white]` `nav.navigationBar.tintColor = UIColor.white` `}` Try adding let _ = nav.navigationBar.standardAppearance and seeing if that resolves it. I think you might be hitting a bug when exclusively using per-item appearance only Ok, let me check No joy. Was there a particular place to add that line? no where in particular no. do you have a feedback request on this? or can you attach a sample (assuming we can do that here!) When I add these lines: nav.navigationItem.compactAppearance = navigationBarAppearace nav.navigationBar.standardAppearance = navigationBarAppearace it will engage the blue background upon scrolling... But it will disappear as soon as the tableView reaches the top edge again. And no, I don't have any feedback request -- this is my first interaction regarding this topic. you would want to set the scrollEdgeAppearance as well \u2013 the compactAppearance is used in landscape on smaller phones only (and realistically unless you need to customize compactAppearance separately from standardAppearance it is rarely necessary to customize it at all) Oops, I think I added standard twice. I think I just found it. I wasn't distinguishing between navigationBar and navigationItem Sometimes I think a handy little diagram to go along with the explanations would go a long way toward making some of this documentation easier to follow because it can be easy to miss a subtle word change like that. Thanks for your help! no problem! One last minor issue in the button on the navBar. The backButton word is the proper color, but the \"<\" chevron is not. What controls that? Right now I have: *let* navigationBarAppearace = UINavigationBarAppearance() navigationBarAppearace.configureWithOpaqueBackground() navigationBarAppearace.backgroundColor = spotBlue navigationBarAppearace.titleTextAttributes = [.foregroundColor: UIColor.white] `nav.navigationItem.standardAppearance = navigationBarAppearace` `nav.navigationItem.scrollEdgeAppearance = navigationBarAppearace` `nav.navigationItem.compactAppearance = navigationBarAppearace` `nav.navigationBar.standardAppearance = navigationBarAppearace` `nav.navigationBar.scrollEdgeAppearance = navigationBarAppearace` `nav.navigationBar.compactAppearance = navigationBarAppearace` *let* buttonAppearance = UIBarButtonItemAppearance() buttonAppearance.normal.titleTextAttributes = [.foregroundColor: UIColor.white] `nav.navigationItem.standardAppearance?.buttonAppearance = buttonAppearance` `nav.navigationItem.scrollEdgeAppearance?.buttonAppearance = buttonAppearance` `nav.navigationItem.compactAppearance?.buttonAppearance = buttonAppearance` `nav.navigationBar.standardAppearance.buttonAppearance = buttonAppearance` `nav.navigationBar.scrollEdgeAppearance?.buttonAppearance = buttonAppearance` `nav.navigationBar.compactAppearance?.buttonAppearance = buttonAppearance` I think thats still controlled by the navigation bar\u2019s tintColor only. its unfortunate, but feedback is always welcome I don't see that as a possible setting of the UINavigationBarAppearance correct, you\u2019d have to set it on the navigation bar directly. thats part of the unfortunate-ness :cry: Ah, I just did that... Ok, so now I get it Whew... Glad you guys were here this week! :smile: its definitely come up a few times that having that available in the appearance objects would be super helpful, just hasn\u2019t been high enough priority to get it done yet. Feedback is definitely appreciated in this area though!","title":"UINavigationBarAppearances have different behavior between iOS 13 and iOS 15.  How can we get the behavior of iOS 13 in the current version of iOS (where the navigation bar doesn't disappear upon scrolling and can take a background color easily)?  And wouldn't it make sense if a breaking change is introduced to document the code necessary to restore the previous behavior?"},{"location":"wwdc22/uiframeworks-lounge.html#with-menubarextra-is-it-possible-to-have-a-primary-action-thats-triggered-when-the-menu-bar-item-is-clicked-and-then-a-separate-menu-thats-shown-when-the-item-is-option-clicked","text":"Hi - thanks for the question. This isn't something we have support for at the moment, but a feedback with any details you can provide would certainly be helpful. FB10134356!","title":"With MenuBarExtra, is it possible to have a primary action that's triggered when the menu bar item is clicked and then a separate menu that's shown when the item is option clicked?"},{"location":"wwdc22/uiframeworks-lounge.html#swiftui-and-environmentobject-do-we-have-to-pass-the-object-every-time-we-use-a-navigationlink-my-app-crashes-when-viewa-adds-an-environmentobject-post-when-displaying-viewb-and-viewb-navigates-to-viewc-where-viewc-accesses-the-post-object-cf-httpsdeveloperapplecomforumsthread707659httpsdeveloperapplecomforumsthread707659","text":"I think the answer on the Dev Forums is the correct one, does it solve this problem? > Your TagView requires BOTH a post and a tag but you are only passing a tag environment object... I posted an answer but it seems the post failed. So writing it again here: the TagView can only be accessed from a PostView where the post is in the environment (because it's the purpose of the PostView to display the Post and details about it). So why isn't the Post object available from the Environment in the context of the TagView if I push a View from this PostView? <@U03HL00QL68> Navigation destinations inherit their environment from the stack in which they appear, not from the context in which they are presented. So the navigation environment of a previous view on the stack doesn\u2019t affect the subsequent view. I think you could move your .navigationDestination(for: Tag.self) into PostView , then propagate both the tag and the post into the TagView from there. Thanks <@U03HW7P0HQR> this works (I was doing that). But I\u2019m still not sure I understand exactly: only EnvironmentObject set on the NavigationStack { }.environmentObject(store) are available for all views presented by NavigationLinks in the Stack? If I pass an object from within the Stack (like in my example), then it's not really in the Environment for views presented from within the Stack? Right. The parent environment is the one for the whole stack. By which I mean the surrounding NavigationStack , not the views on the stack. (Tough to describe this without pictures.) Thanks <@U03HW7P0HQR> for the explanations. But in my example, I pass the post as an EnvironmentObject when a Post is selected. And it's directly available in the PostView. So this post object is passed in the Environment but not for pushed Views from the PostView. I was expected the object to be available in all the \u201cflow\u201d (cf screenshot). Or this flow. I thought that if a Cart object created in the UserLogin View (probably in a NavigationStack) and passed in the Environment from this UserLogin View, it was accessible in the PaymentDetails in the same Stack. But this is only true if the Cart is passed before the NavigationStack? A view can\u2019t have two parent environments. Consider the two ways we could propagate the environment: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 NavigationStack \u2502 \u2502 NavigationStack \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u251c\u2500\u25b6\u2502 Root View \u2502 \u2514\u2500\u25b6\u2502 Root View \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u25bc \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u251c\u2500\u25b6\u2502 Pushed View \u2502 \u2502 Pushed View \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u25bc \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500\u25b6\u2502 Pushed View \u2502 \u2502 Pushed View \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 We chose the one on the left and leave it to developers to propagate any additional information as needed. This reduces the number of dependencies.","title":"SwiftUI and EnvironmentObject: do we have to pass the object every time we use a NavigationLink?  My app crashes when ViewA adds an EnvironmentObject Post when displaying ViewB, and ViewB navigates to ViewC where ViewC accesses the Post object. Cf: https://developer.apple.com/forums/thread/707659|https://developer.apple.com/forums/thread/707659"},{"location":"wwdc22/uiframeworks-lounge.html#as-of-monterey-list-supports-drag-and-drop-reordering-of-items-on-macos-and-iosipados-but-it-seems-that-other-types-eg-hstack-vstack-etc-support-drag-and-drop-reordering-of-items-on-macos-only-in-catalyst-apps-in-any-other-kind-of-macos-app-the-same-swiftui-code-that-works-as-expected-in-iosipados-and-catalyst-doesnt-work-ie-a-drag-cannot-be-started-at-all-the-code-compiles-just-fine-but-it-doesnt-actually-work-how-can-i-support-drag-and-drop-reordering-of-items-or-sub-views-in-swiftui-code-in-hstacks-vstacks-etc-in-non-catalyst-macos-apps","text":"Hi John! Could you please file a feedback via Feedback Assistant? It would also be great if you could include a code snippet and describe the desired behavior vs what you see. Here's some code that allows a drag to be started on macOS in Catalyst but not on macOS outside Catalyst: https://pastebin.com/VvJgDq0x Hm, have you tried to provide the contents to the NSItemProvider instead of nil ? It is possible that empty NSItemProvider s are treated slightly differently on different platforms. Changing that line to this worked. Thanks! *return* NSItemProvider(item: item *as* NSSecureCoding, typeIdentifier: \"public.text\")","title":"As of Monterey, List supports drag-and-drop reordering of items on macOS and iOS/iPadOS, but it seems that other types (e.g., HStack, VStack, etc.) support drag-and-drop reordering of items on macOS only in Catalyst apps. In any other kind of macOS app, the same SwiftUI code that works as expected in iOS/iPadOS and Catalyst doesn't work (i.e., a drag cannot be started at all). The code compiles just fine, but it doesn't actually work.  How can I support drag-and-drop reordering of items (or sub-Views) in SwiftUI code in HStacks, VStacks, etc. in non-Catalyst macOS apps?"},{"location":"wwdc22/uiframeworks-lounge.html#hi-is-there-any-recommended-way-of-making-controls-inside-a-swiftuis-form-look-more-native-on-macos-in-an-app-made-with-catalyst-would-i-have-to-have-an-appkit-bundleplugin-that-renders-those-swiftui-views","text":"To enable macOS-native UI in your Catalyst app, go to your target's settings: General \u2192 Deployment Info \u2192 Mac Catalyst Interface, and switch from \"Scaled to Match iPad\" to \"Optimize for Mac\". Controls in SwiftUI will automatically adapt to be more Mac-like in that mode. Also check out the new Form styles, to have the controls arranged in a traditional Mac columnar layout. From what I understand, this will put Mac controls into the iOS style Form - it will not enable the macOS style Form. Unless this is changing this year, there\u2019s no way to get native macOS SwiftUI in Catalyst. FB9994506: Ability to use macOS SwiftUI in iOS app using Catalyst optimized for Mac idiom This year, there is the new FormStyle support, with FormStyle.columns available on all platforms, including Catalyst. That creates a Form layout like the default on native macOS with trailing aligned labels next to the leading aligned controls Form { Picker(\"Notify Me About:\", selection: $notifyMeAbout) { Text(\"Direct Messages\").tag(NotifyMeAboutType.directMessages) Text(\"Mentions\").tag(NotifyMeAboutType.mentions) Text(\"Anything\").tag(NotifyMeAboutType.anything) } Toggle(\"Play notification sounds\", isOn: $playNotificationSounds) Toggle(\"Send read receipts\", isOn: $sendReadReceipts) Picker(\"Profile Image Size:\", selection: $profileImageSize) { Text(\"Large\").tag(ProfileImageSize.large) Text(\"Medium\").tag(ProfileImageSize.medium) Text(\"Small\").tag(ProfileImageSize.small) } .pickerStyle(.inline) } .formStyle(.columns) https://developer.apple.com/documentation/swiftui/formstyle/columns Woooo I love that for us thanks!! <@U03HW7NMP6D> Is there any possibility of creating my own form style? For example if I wanted to backport the new .columns style back to macOS Monterey. FormStyle is only public this year, so not able to be backported. You would have to create your own Form replacement as a whole to do something like that Awesome, thanks for your help!","title":"Hi, is there any recommended way of making controls inside a SwiftUI\u2019s Form look more native on macOS in an app made with Catalyst? Would I have to have an AppKit bundle/plugin that renders those SwiftUI views?"},{"location":"wwdc22/uiframeworks-lounge.html#perhaps-a-bit-of-a-simple-question-for-such-a-space-but-i-am-really-wondering-the-best-way-to-get-started-with-swiftui-i-have-done-the-apple-app-dev-course-where-i-build-scrumdinger-ive-perused-the-documentation-and-im-very-familiar-with-c-bash-and-6502-assembly-i-build-games-for-the-nes-however-im-having-a-bit-of-a-hard-time-remembering-the-syntax-mostly-though-i-guess-im-looking-for-a-place-to-start-my-own-development-for-ideas-of-things-to-implement-to-get-better-at-swift-i-have-always-wanted-to-at-one-point-in-my-life-work-for-apple-like-since-i-saw-the-iphone-come-out-in-2007-im-19-now-sorry-if-this-has-been-a-bit-all-over-the-place-im-just-so-excited-to-be-here","text":"Hi Jakob! > However, I'm having a bit of a hard time remembering the syntax. Mostly though, I guess I'm looking for a place to start my own development, for ideas of things to implement to get better at Swift. I'd recommend not worrying about learning and getting better at Swift or SwiftUI. Instead I'd recommend finding something you're excited to build and... just building it. I've found when you do that, you end up learning the language syntax and framework APIs without even trying :slightly_smiling_face: When you say building it, what parts of it? That is a great suggestion though, thanks! Just building an app or a game for yourself using Swift/SwiftUI, like it sounds like you're already doing for NES! Don't worry about \"writing the code the right way\" to start. Just worry about getting something working Ahhh gotcha, I read your response wrong, thank you. I have a big problem with putting more effort into looking like I'm doing work rather than actually doing it, so it's refreshing to hear that I'm not really correct for that","title":"Perhaps a bit of a simple question for such a space, but I am really wondering the best way to get started with SwiftUI. I have done the Apple App Dev course where I build scrumdinger, I've perused the documentation, and I'm very familiar with c, bash, and 6502 assembly (I build games for the NES). However, I'm having a bit of a hard time remembering the syntax. Mostly though, I guess I'm looking for a place to start my own development, for ideas of things to implement to get better at Swift. I have always wanted to at one point in my life work for Apple, like since I saw the iPhone come out in 2007, I'm 19 now. Sorry if this has been a bit all over the place, I'm just so excited to be here."},{"location":"wwdc22/uiframeworks-lounge.html#can-ontapgesture-be-used-more-than-once-on-a-single-view-like-single-click-to-select-double-click-to-open-in-a-window-itemview-ontapgestureperform-selectitem-ontapgesturecount-2-perform-openiteminwindow","text":"onTapGesture can indeed be used more than once but you need to get them in the right order, so that the double-tap fails before the single-tap succeeds. Something like this: ItemView() .onTapGesture(count: 2) { ... } .onTapGesture { ... } Thanks. I'll give it a try.","title":"Can onTapGesture be used more than once on a single view? \u2028Like single-click to select, double-click to open in a window?  ItemView()   .onTapGesture(perform: selectItem) \u2028  .onTapGesture(count: 2, perform: openItemInWindow)"},{"location":"wwdc22/uiframeworks-lounge.html#what-is-the-recommended-way-to-conditionalize-code-for-view-modifiers-that-are-only-available-on-newer-versions-of-ios-for-example-if-i-have-a-view-and-previously-was-using-the-regular-ontapgesture-modifier-on-ios-15-but-when-running-on-ios-16-i-want-to-instead-use-the-new-version-that-provides-location","text":"We recommend factoring out the common parts and then using if #available checks to use the relevant modifier. Something like this: let common = commonViewParts if #available(iOS 16.0, macOS 13.0, *) { return common.newModifier() } else { return common.oldModifier() } Thanks. It would be great to have a cleaner way to do this in the future, especially as new modifiers get added in future versions of the operating systems (imagine an app targeting iOS 18, that had conditional branches for 18, 17, 16, and 15) Please do file some feedback with this request! Just filed FB10135113 thank you!","title":"What is the recommended way to conditionalize code for view modifiers that are only available on newer versions of iOS? For example, if I have a View, and previously was using the regular onTapGesture modifier on iOS 15, but when running on iOS 16, I want to instead use the new version that provides location?"},{"location":"wwdc22/uiframeworks-lounge.html#hi-i-asked-this-yesterday-and-was-told-to-ask-the-swiftui-team-and-include-a-log-so-here-i-am-im-trying-to-update-my-watchos-app-to-support-watchos-9-however-when-trying-to-play-a-video-using-videoplayer-the-app-crashes-immediately-and-the-console-shows-unable-to-find-class-nacvolumecontroller-is-there-any-around-this-or-should-i-file-a-feedback","text":"HI <@U03HZ3L98TF> that sounds like a potential issue in the Seed would you mind filling a feedback on it and posting the feedback number here so we can take a look for you! Thank you! Any additional details on how you\u2019re calling the videoPlayer or even a small sample project if it repros will be super helpful! Hi, Thank you I'm just making a small project and filing a feedback\u2026. I should have something soon Error reproduced, filing feedback Small additional request from the watch team if its not too much of a hassle is to get a sysdiagnose from a watch when it crashes. Instructions here: https://download.developer.apple.com/iOS/watchOS_Logs/sysdiagnose_Logging_Instructions.pdf Sorry for late reply busy multitasking, the link says I don't have permission to view it\u2026 I am logged in and have a paid developer membership. Not sure why hmm https://developer.apple.com/bug-reporting/profiles-and-logs/?platform=watchos its the sysdiagnose instructions from this page Yep that appears to have worked Thank you right I'll get the log and finish the feedback","title":"Hi, I asked this yesterday and was told to ask the SwiftUI team and include a log so here I am\u2026 I\u2019m trying to update my watchOS app to support watchOS 9 however when trying to play a video using VideoPlayer the app crashes immediately and the console shows \u201cUnable to find class NACVolumeController\u201d. Is there any around this or should I file a feedback?"},{"location":"wwdc22/uiframeworks-lounge.html#when-it-comes-to-concurrency-and-swiftui-how-are-you-managing-creationinjections-of-your-actor-objectsbusiness-layer-so-that-your-views-have-access-to-them-or-do-you-have-any-specific-recommendations-based-on-the-newer-swift-concurrency-model-ive-been-reviewing-through-some-of-the-2021-videos-in-conjunction-with-the-new-discussions-this-year-and-i-tend-to-see-shared-in-the-sample-code-which-would-mean-a-singleton-which-id-frankly-rather-avoid-we-been-injecting-interactors-through-the-environment-or-environment-object-but-this-has-issues-because-using-stateobjects-we-dont-necessarily-have-access-to-the-environment-object-during-init-when-weve-been-trying-to-binding-subscriptions-to-our-published-vars-if-we-use-task-would-we-have-access-to-enviroment-at-the-time-the-task-closure-is-run-in-this-way-we-could-probably-set-access-to-our-common-actors-through-the-environment-or-environmentobject-and-then-set-up-flows-that-just-bind-to-state-via-an-async-sequence-is-there-a-way-to-know-when-environment-is-actual-set-for-a-view-so-we-can-respond-to-in-within-a-stateobject-to-set-up-subscription-bindings-either-through-publishers-or-async-sequences","text":"For pretty much any property wrapper in SwiftUI its value become available just before the body of the view is called. If you have ever implemented a custom DynamicProperty that is exactly when update() is called. Yes, by the time the .task closure run your environment is available and you can capture its value. I just want to remind you that you should not conform an actor to ObservableObject because the expectation is that all the ObservableObject instance are isolated to the main thread. Yah, of course the ObservableObject would not be an actor. The actor is that business layer object that we need to call in to perform operations and that\u2019s what I\u2019m trying to figure out how the best way for a view to reach out to them would be. I don\u2019t like making actors singletons as I\u2019d rather have them injected or passed to the View so we can have a test version, etc. Thank you! I\u2019ve found that because there is some unknown amount of time between init() of a view or especially the init() of the @StateObject of view and when the environment is set that it\u2019s hard to know when to run configuration code related to this injection. It\u2019s only safe to access any of the property wrapper from within body","title":"When it comes to concurrency and SwiftUI, how are you managing creation/injections of your actor objects/business layer so that your views have access to them or do you have any specific recommendations based on the newer Swift concurrency model. I've been reviewing through some of the 2021 videos in conjunction with the new discussions this year and I tend to see .shared in the sample code, which would mean a singleton which I'd frankly rather avoid. We been injecting interactors through the @Environment or @Environment object but this has issues because using @StateObjects we don't necessarily have access to the @Environment object during init when we've been trying to binding subscriptions to our @Published vars. If we use .task {} would we have access to @Enviroment at the time the task closure is run? In this way we could probably set access to our common actors through the @Environment or @EnvironmentObject and then set up flows that just bind to @State via an async Sequence. Is there a way to know when @Environment is actual set for a view so we can respond to in within a @StateObject to set up subscription bindings either through publishers or async sequences?"},{"location":"wwdc22/uiframeworks-lounge.html#the-offercoderedemptionispresentedoncompletion-modifier-seems-very-specific-and-maybe-should-have-been-something-exported-from-storekit-going-forward-will-specialized-ui-like-this-always-be-added-to-core-swiftui-or-will-we-start-seeing-other-frameworks-implement-swiftui-modifiers","text":"offerCodeRedemption(isPresented:onCompletion:) is actually exported from StoreKit like you're asking. We're curious to learn what gave you the impression that it was exported from SwiftUI directly (and if that's a bad thing)? Interesting! The documentation here: https://developer.apple.com/documentation/swiftui/view/offercoderedemption(ispresented:oncompletion:)?changes=latest_minor makes it look like it is part of core SwiftUI. I don't see anything that indicates it is part of StoreKit. Ya that is very subtle. Thanks for pointing that out","title":"The offerCodeRedemption(isPresented:onCompletion:) modifier seems very specific and maybe should have been something exported from StoreKit.    Going forward will specialized UI like this always be added to core SwiftUI or will we start seeing other frameworks implement SwiftUI modifiers."},{"location":"wwdc22/uiframeworks-lounge.html#is-it-possible-with-swiftuis-new-navigationstack-to-hide-the-tabbar-of-a-tabview-when-the-destionation-view-appears-with-the-existing-navigationview-it-was-possible-but-not-so-easy-to-handle-the-navigation-title-and-buttons","text":"Take a look at the new toolbar visibility accepting modifier. This is new in SwiftUI and allows configuring the hiding or showing of different bars like the navigation bar, or the tab bar. ContentView() .toolbar(.hidden, in: .tabBar) See https://developer.apple.com/documentation/swiftui/presentedwindowcontent/toolbar(_:in:)","title":"Is it possible with SwiftUIs new NavigationStack to hide the tabbar of a TabView when the destionation view appears. With the existing NavigationView it was possible but not so easy to handle the navigation title and buttons."},{"location":"wwdc22/uiframeworks-lounge.html#when-using-charts-to-draw-a-line-graph-is-there-a-way-to-get-the-last-point-to-be-at-the-trailing-edge-of-the-chart-view-im-finding-that-it-always-has-the-trailing-edge-as-a-multiple-of-values-used-on-the-x-axis","text":"You can use the .chartXAxis(content:) modifier passing an AxisContentBuilder that either completely customizes the x-axis, or you could first try out this initializer of AxisMarks passing true for those roundLowerBound and roundUpperBound /// Automatically determines the values for the markers, /// approximating the target number of values. public static func automatic( desiredCount: Int? = nil, roundLowerBound: Bool? = nil, roundUpperBound: Bool? = nil ) -&gt; Values Thank you. I'll have a look at using that.","title":"When using Charts to draw a line graph, is there a way to get the last point to be at the trailing edge of the chart view? I\u2019m finding that it always has the trailing edge as a multiple of values used on the x axis."},{"location":"wwdc22/uiframeworks-lounge.html#swiftui-in-uikit-what-is-the-recommended-way-of-animating-changes-to-swiftui-view-size-when-inside-a-uihostingcontroller-the-problem-im-facing-is-that-while-the-swiftui-view-itself-will-animate-and-with-the-new-sizing-options-the-uihostingcontroller-will-do-intrinsic-content-size-invalidation-automatically-the-hosting-view-bounds-change-isnt-animated-it-just-jumps-this-is-a-problem-especially-when-other-uiviews-are-constrained-to-the-hosting-view-with-auto-layout-is-there-any-way-to-solve-this-other-than-manually-tracking-swiftui-size-changes-and-triggering-a-separate-layout-pass-animation-of-the-hosting-view-superview","text":"Could you please file a feedback for this? Sure! Does that mean there's no solution to this at this time, or you just need an example? Both!","title":"SwiftUI in UIKit: What is the recommended way of animating changes to SwiftUI view size when inside a UIHostingController? The problem I\u2019m facing is that while the SwiftUI view itself will animate, and with the new sizing options the UIHostingController will do intrinsic content size invalidation automatically, the hosting view bounds change isn\u2019t animated, it just jumps. This is a problem especially when other UIViews are constrained to the hosting view with Auto Layout. Is there any way to solve this other than manually tracking SwiftUI size changes and triggering a separate layout pass + animation of the hosting view superview?"},{"location":"wwdc22/uiframeworks-lounge.html#i-wrote-small-testing-navigationstack-struct-contentview-view-var-body-some-view-navigationstack-vstack-navigationlinkvalue-123-textclick-me-toolbar-toolbaritem-button-label-textdone-navigationtitletitle-navigationdestinationfor-intself-value-in-detailview-toolbarhidden-in-windowtoolbar-struct-detailview-view-var-body-some-view-textdetail-view-navigationtitletitle-toolbar-toolbaritem-button-label-textnew-button-but-when-i-click-on-click-me-button-the-back-button-automatically-appear-in-the-toolbar-how-i-can-hide-or-customize-this-back-button-ps-its-on-macos","text":"It isn't possible to hide the back button, but a feedback asking for this could help us gauge additional interest for it :pray: Especially whether you'd like to hide or customize Thank you! I will post a feedback report. Can I somehow show an alert or make some operation when a user clicks the back button in the toolbar? Or now it isn\u2019t possible too? For example to stop a task or to warning a user that there is some unsave changes and wait for user response.","title":"I wrote small testing NavigationStack struct ContentView: View {     var body: some View {         NavigationStack {             VStack {                 NavigationLink(value: 123) {                     Text(\"Click Me\")                 }             }             .toolbar {                 ToolbarItem {                     Button {                      } label: {                         Text(\"Done\")                     }                 }             }             .navigationTitle(\"Title\")             .navigationDestination(for: Int.self) { value in                 DetailView()             }         } //        .toolbar(.hidden, in: .windowToolbar)     } }  struct DetailView: View {     var body: some View {         Text(\"Detail View\")             .navigationTitle(\"Title\")             .toolbar {                 ToolbarItem {                     Button {                                              } label: {                         Text(\"New Button\")                     }                 }             }     } }  But when I click on 'Click Me' button the back button automatically appear in the toolbar. How I can hide or customize this back button? PS: It's on macOS"},{"location":"wwdc22/uiframeworks-lounge.html#im-trying-to-have-ondelete-for-delete-and-swipeactions-for-other-actions-it-is-not-working-this-way-any-idea-the-thing-is-i-try-to-have-delete-as-part-of-swipeactions-however-i-cant-find-the-way-to-set-up-the-delete-animation-like-the-one-ondelete-has","text":"The moment you add .swipeActions it up to you to define the delete action (SwiftUI will stop synthesize that for you in the swipe action drawer). You want to create a button with a destructive role to achieve the same result: Button(role: .destructive) { delete() } label: { Label(\"Delete\", systemImage: \"trash\") } <@U03J7BQQNPJ> On the iPad, In a list when a swipe action is used which shows a confirmation dialogue, the popup is not shown on the correct cell. For Example cell 5 is swiped, confirmation dialogue points to a different cell cell. Feedback FB10026540 <@U03JCHKCDB4> thank you for taking the time to file a feedback. <@U03J7BQQNPJ>, thanks for the reply. I do have destructive button like you mentioned. However, it doesn't have the animation like the one onDelete has. Any idea how to set it up? <@U03J7BQQNPJ> Regarding feedback FB10026540 I have updated the project and added more details on my observation. Thanks a ton for the immediate response!!!","title":"I'm trying to have .onDelete for delete and .swipeActions for other actions. It is not working this way. Any idea? The thing is I try to have delete as part of .swipeActions. However, I can't find the way to set up the delete animation like the one .onDelete has."},{"location":"wwdc22/uiframeworks-lounge.html#in-swiftui-when-you-had-a-list-view-with-items-that-were-continuously-synced-with-a-server-and-you-displayed-the-details-view-of-a-selected-item-then-the-app-jumps-back-to-the-main-list-are-navigationstacks-solving-this-problem","text":"Hi, Arnfried. Thanks for the question! Generally the popping back problem is because there is a state change that\u2019s invalidating the view that contains the NavigationView . This causes SwiftUI to discard the NavigationView and create a new one, popping you back to the root. Check out Demystify SwiftUI from #wwdc21 https://developer.apple.com/wwdc21/10022 for more details on view invalidation and identity. With NavigationStack , you can bind a path, so the popping back shouldn\u2019t happen. But you should still investigate why the identity is changing. That\u2019s likely a source of performance problems, since lots of extra work will happen. In the new navigation API, that new work will be to replace the entire navigation hierarchy. Isn\u2019t it better to stop syncing List with a server in time if you are in the detail view? That\u2019s really up to the specific app design. Many times that would be right, but some apps might want the root view to be always correct immediately when the user pops the stack. Or maybe the backend doesn\u2019t vend the details separately. Hi Alexey, stopping the sync is in our case not possible. As Curt C mentioned, it can als be a state change that causes the back popping. It is really the NavigationLink as he said. I tried to set identifiers for the link itself or the destination view but the effect is still there. But I will test it with the new NavigationStack. Maybe this solves the problem. Explicitly setting an identifier isn\u2019t a solution if the structural identity of the view containing the NavigationStack is also changing. Explicit identifiers can be used to force an identity change, but not to prevent one. In practice the identity of a view is a combination of its structural and explicit identity.","title":"In SwiftUI when you had a List view with items that were continuously synced with a server and you displayed the details view of a selected item then the app jumps back to the main list. Are NavigationStacks solving this problem?"},{"location":"wwdc22/uiframeworks-lounge.html#were-using-an-attributed-string-with-the-labelcolor-on-a-nsstatusitems-button-however-when-displaying-this-on-an-inactive-display-the-color-isnt-dimmed-as-expected-is-there-a-way-to-have-the-text-appear-dim-when-on-an-inactive-display-using-this-approach","text":"Hey <@U03JKFBJG69>! Status items should automatically look great on secondary displays, with no extra work needed for apps to support this appearance. :sunglasses: :thinking_face: However, if you\u2019re not seeing that is the case for a status item that you\u2019re using or creating, please provide this feedback through http://feedbackassistant.apple.com|feedbackassistant.apple.com . A sample project and the version of macOS would be greatly helpful in reproducing this issue. Feel free to send me a link to that feedback you\u2019ve filed, in this thread. :+1: Thank you :slightly_smiling_face: Is there other content in the status item? For example, does the status item\u2019s button have a (templated) image? Yes, the status item\u2019s button has a image which is a template image Ah ha. Thanks for that info! :slightly_smiling_face: <@U03HEM646TX> Filed as FB10144807 I can reproduce this on macOS 12.4. You should be able to reproduce it via the following code snippet: import Cocoa @main class AppDelegate: NSObject, NSApplicationDelegate { let statusItem = NSStatusBar.system.statusItem(withLength: NSStatusItem.variableLength) func applicationDidFinishLaunching(_ aNotification: Notification) { let useTemplateImage = true // set this to false and notice how the text appears for the status item on an inactive display statusItem.button?.imagePosition = .imageRight statusItem.button?.attributedTitle = NSAttributedString(string: \"Hello World\", attributes: [NSAttributedString.Key.foregroundColor : NSColor.labelColor]) statusItem.button?.image = NSImage(named: useTemplateImage ? NSImage.quickLookTemplateName : NSImage.statusAvailableName) } }","title":"We're using an attributed string with the labelColor on a NSStatusItems button. However, when displaying this on an inactive display the color isn't dimmed as expected. Is there a way to have the text appear dim when on an inactive display using this approach?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-recreate-the-appearance-of-the-widgets-on-the-lock-screen-using-uivisualeffectview-in-an-app-using-either-swiftui-or-uikit-im-specifically-interested-in-recreating-the-appearance-of-the-circular-widget","text":"A variety of the gauge styles used in circular complications on the Lock Screen are available in app as well! You can access them using any of the SwiftUI gauge styles with an \u201caccessory\u201d prefix. Please be aware though that those styles are really only intended for Lock Screen / widgets and contexts in your app that should have a similar look and feel, so please be thoughtful about where you use them. <@U03HKVDCL7N> Thanks for your reply. I realise that my question was poorly phrased. I\u2019m looking for a way to recreate the vibrancy that the Lock Screen widgets use. Would that be done using a UIVisualEffectView? with UIVisualEffectView you can try the UIVibrancyEffect s created with a UIBlurEffectStyleSystem*Material , but I\u2019m not sure that will necessarily match what the lock screen is doing","title":"Is there a way to recreate the appearance of the widgets on the Lock Screen using UIVisualEffectView in an app using either SwiftUI or UIKit? I'm specifically interested in recreating the appearance of the circular widget."},{"location":"wwdc22/uiframeworks-lounge.html#the-documentation-for-makecoordinator-states-creates-the-custom-instance-that-you-use-to-communicate-changes-from-your-view-to-other-parts-of-your-swiftui-interface-could-you-explain-what-that-means-it-doesnt-make-sense-to-me-because-normally-the-coordinator-is-an-nsobject-that-is-the-delegate-to-a-uiview-i-have-seen-some-examples-of-coordinator-classes-that-take-in-an-binding-that-was-passed-in-to-the-uiviewrepresentable-and-then-into-the-coordinator-is-that-what-this-means","text":"The Coordinator you return can be any type, including an NSObject subclass. That coordinator will then be passed into makeUIView and you can assign it as the delegate of your view. I explained this in my session \"Use SwiftUI with AppKit\", starting at around 12:38 into the video. https://developer.apple.com/videos/play/wwdc2022/10075/ Thanks <@U03HB1A8VRU> I just watched it. I've never seen that technique of initing the coordinator with self and also setting self in updateNSView. Seems overly complex to me but I'll try it and see how it compares with just forwarding the @Binding. I think it would be useful if the documentation for makeCoordinator was updated to say - instance that communicates changes back to the UIViewRepresentable struct. Passing a binding works great as well, but doesn't scale as well if you have multiple pieces of state to pass through. You're welcome to do it either way!","title":"The documentation for makeCoordinator states: \"Creates the custom instance that you use to communicate changes from your view to other parts of your SwiftUI interface.\" could you explain what that means? It doesn't make sense to me because normally the coordinator is an NSObject that is the delegate to a UIView. I have seen some examples of Coordinator classes that take in an @Binding, that was passed in to the UIViewRepresentable and then into the Coordinator. Is that what this means?"},{"location":"wwdc22/uiframeworks-lounge.html#theres-no-way-of-posting-images-on-the-question-but-i-ll-add-it-once-the-question-gets-posted-using-swiftui-i-m-trying-to-use-swiftchart-to-add-color-underneath-my-line-graph-i-was-wondering-if-theres-a-good-resource-on-how-i-can-do-that","text":"Yes, let's see what you're trying to do! Check out AreaMark ! The design is on the right for my Stair Climber app (join beta here https://stairmasterclimber.com ) and what i got is on the left On it right now I forget the exact modifier to specify the color underneath the area mark, it may be foregroundStyle passing a gradient in this case probably from orange to clear :OOOOO YOU ARE A GENIUS :bow: And is there a way to add the border color? Which border color are we talking? Oops i mean the line color On the top of the area Ah this is one of my favorite parts of Charts Swift Charts are amazing! So you can have multiple marks in one chart You see the line on top of the area? Would I basically just need to add a line chart on top? :open_mouth: There may be a modifier specifically for the line color of an AreaMark so, I'll check with some Swift Charts engineers, but in the meantime, it's still instructive to add a LineMark plotting the same data after the AreaMark The line mark will be superimposed on top of the area mark So literally you have two charts on top of each other, but conceptually, it looks like a single styled chart I hope you're adding HealthKit integration ;) YES I AM Health Kit & Game Kit Well if you've successfully gamified Stair Stepping, that sounds awesome StairMasterClimber, because elevators are so old-school We have 30 people so far, I would be honored 100 privacy safe and no ads Sounds really interesting! :partying_face: alright HealthKit and Swift Charts :raised_hands::skin-tone-2: going to be awesome You\u2019re a genius sir. Almost there, just need to add the gradient!! <@U03HL00QL68> One last question, how can I add the little circle dots for the data points? Do i need to create a new question for that? This is my code currently, Unbelievably short and easy","title":"There's no way of posting images on the question. But I ll add it once the question gets posted. Using SwiftUI, I m trying to use SwiftChart to add color underneath my line graph. I was wondering if there's a good resource on how I can do that?"},{"location":"wwdc22/uiframeworks-lounge.html#whats-the-recommended-way-of-using-a-property-wrapper-such-as-focusstate-such-that-it-will-compile-targeting-ios-14-for-example-afaik-you-cant-add-an-availability-annotation","text":"You are correct that you can\u2019t apply availability to stored properties in Swift. There are a few techniques you can use to work around this. Most commonly, you can factor out the portion of your UI that uses this property into it\u2019s own View , and apply availability to that entire view. However, the best way to structure the code really depends on your specific use case, and we agree that this can be tricky to handle in some situations. If you have a specific use case you\u2019re okay with sharing, please file feedback. Real-world code examples are incredibly helpful when designing future improvements to the language and the framework. Thanks for the reply, would be helpful if it was possible to directly init a FocusState","title":"What's the recommended way of using a property wrapper such as @FocusState such that it will compile targeting iOS 14 for example? AFAIK you can't add an availability annotation"},{"location":"wwdc22/uiframeworks-lounge.html#swift-ui-question-how-to-identify-the-visibility-in-index-or-id-inside-a-tablecolumn-view-for-paginate-remote-content","text":"Hi Ratnesh! Similar to techniques of paginating content in Lists, you can also use onAppear for TableColumn views in Table as well using the same technique. You\u2019d want to compare the id of the column element to what you consider the \u201clast\u201d element\u2019s id Thanks for the reply :blush:. Is this same technique work for macOS target also? Yes, it does. We even using that technique in Photos in macOS Ventura :slightly_smiling_face: :star-struck: awesome","title":"SWIFT-UI question:  How to identify the visibility (in index or id) inside a TableColumn View for paginate remote content."},{"location":"wwdc22/uiframeworks-lounge.html#by-default-uihostingcontroller-configures-a-navigation-bar-what-is-the-best-way-to-hide-it-overriding-viewwillappear-to-call-setnavigationbarhidden-does-not-always-give-the-expected-result-the-best-result-i-got-was-by-overriding-viewwillappear-and-not-calling-superviewwillappear-is-there-any-risk-with-this-method","text":"I would not recommend overriding viewWillAppear and not calling super. Generally, I\u2019d recommend either using a navigationBarHidden(false) or the new toolbar(.hidden) modifier inside of your SwiftUI view. Or if you are managing a UINavigationController yourself, and you can\u2019t use those modifiers, you should be able to set the isNavigationBarHidden property to false yourself and the hosting controller will try to respect that. I\u2019ll call out though the release note in the first beta of iOS 16: > SwiftUI might incorrectly modify the isNavigationBarHidden property on UINavigationControllers not created by SwitftUI. It seems that this does not only concern iOS16... If you\u2019re having trouble with this on older releases, I\u2019d recommend trying to use .navigationBarHidden() for hiding the navigation bar. If you are unable to do that, then you could try subclassing the UINavigationController to avoid hiding or showing the navigation bar if SwiftUI incorrectly tries to mutate that property. Thank you <@U03HW7QCHK3> Subclassing UIHostingController is the solution for which I asked you advice. To prevent SwiftUI to set isNavigationBarHidden to true, we have to override viewWillAppear (and better yet: don't call super.viewWillAppear) Why do you say that it is not recommended?","title":"By default UIHostingController configures a navigation bar. What is the best way to hide it?  Overriding viewWillAppear to call setNavigationBarHidden does not always give the expected result.  The best result I got was by overriding viewWillAppear and NOT calling super.viewWillAppear.  Is there any risk with this method?"},{"location":"wwdc22/uiframeworks-lounge.html#question-about-swiftui-we-have-background-task-that-may-generate-errors-and-application-shows-alert-when-this-error-appears-if-we-use-alert-modifier-on-parent-view-it-works-but-when-parent-view-displays-some-child-view-parent-view-cant-show-this-alert-and-app-shows-warning-in-console-attempt-to-present-alert-on-parentview-which-is-already-presenting-childview-as-workaround-we-can-use-identical-alert-modifiers-for-parent-and-all-child-views-sheets-popovers-action-sheets-etc-but-in-this-case-we-still-have-warning-in-console-how-can-we-display-global-alerts-in-swiftui-without-warnings-in-console-or-perhaps-we-can-ignore-these-warnings","text":"This looks like a bug, could you please file a feedback with the reproducing project so we can take a closer look? Should i attach sample here? Please attach it to the feedback report you create. Thanks for the answer! It doesn\u2019t look like a bug, more like a limitation. We need to show alert in unknown period of time. We may show it from .sheet or from the parent view. In parent view it will not be displayed if sheet is already on the screen.","title":"Question about SwiftUI. We have background task that may generate errors and application shows alert when this error appears. If we use alert(\u2026) modifier on parent view, it works. But when parent view displays some child view, parent view can\u2019t show this alert and app shows warning in console \u201cAttempt to present &lt;Alert on &lt;ParentView which is already presenting &lt;ChildView\".  As workaround we can use identical alert(\u2026) modifiers for parent and ALL child views (sheets, popovers, action sheets, etc.). But in this case we still have warning in console.  How can we display \u201cglobal\u201d alerts in SwiftUI without warnings in console? Or perhaps we can ignore these warnings?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-know-when-a-drop-has-been-cancelled-with-ondrag-and-the-dropdelegate-while-dragging-i-want-to-change-the-opacity-of-the-view-for-the-duration-of-the-drag-i-tried-providing-a-preview-view-and-using-ondisappear-but-it-isnt-called-either-there-is-dropexited-but-the-user-can-still-be-dragging","text":"There is no API for this, and as you noted, the drop session state in DropDelegate is different than the drag session state. We'd love to hear your use case for this, so could you file a feedback report asking for this functionality?","title":"Is there a way to know when a drop has been cancelled with .onDrag and the DropDelegate? While dragging I want to change the opacity of the view for the duration of the drag. I tried providing a preview view and using onDisappear but it isn't called either. There is dropExited but the user can still be dragging."},{"location":"wwdc22/uiframeworks-lounge.html#how-would-i-go-about-making-a-grid-where-all-the-cells-are-the-same-size-based-on-the-cell-that-needs-the-largest-size-to-appropriately-display-its-content","text":"To do this today you would need to do something more custom like creating a custom Layout. Though you might be able to to implement the custom Layout so that it calls into Grid itself, reusing its internal logic. Hi, I\u2019ve done something similar recently in iOS 15. For this I created a child size reader (you can search for that) and then keep track of the largest size. Lastly, simply apply the size to grid items using .frame. Not sure if you\u2019re already doing that and simply wanted a neater solution for iOS 16\u2026","title":"How would I go about making a Grid where all the cells are the same size based on the cell that needs the largest size to appropriately display its content?"},{"location":"wwdc22/uiframeworks-lounge.html#the-auto-magical-conversion-from-uinavigationbars-to-nstoolbar-in-catalyst-is-an-awesome-addition-but-im-not-sure-how-it-works-with-multiple-columns-i-have-an-app-with-uisplitviewcontroller-and-3-columns-all-of-the-view-controllers-in-each-columns-have-their-own-buttons-and-the-one-in-the-secondary-view-controller-has-a-search-bar-which-of-these-should-appear-in-the-mac-toolbar-ideally-they-would-all-appear-in-their-own-section-of-the-toolbar-like-they-do-on-ipad-but-in-testing-only-the-buttons-from-the-secondary-vc-appear-no-buttons-from-the-other-columns-and-no-search-bar-is-that-expected-is-there-a-way-to-separate-toolbar-like-we-can-using-nstoolbar","text":"The translation does bias towards supporting the secondary section (in the API its called content ) primarily as thats is where we expect o have the most \u201croom\u201d in the toolbar. We do have some issues with properly supporting split views in the current beta, but it would be great to get feedback on what you are seeing and what you might expect I'd especially appreciate the feedback because I'm surprised that your search bar isn't migrating, and would like to see your example Ah my appologies \u2014 I said \u201csecondary\u201d but meant \u201csupplementary\u201d. My supplementary column has a search field. All columns have buttons. When run on Mac, the buttons from my supplementary column appear in the toolbar (at the far right, no columns in the toolbar) but no other buttons or the search field appear. yea, I think we don\u2019t translate search from anything other than the detail column currently. We may have bugs here as well, so please send your samples so we can make sure your translation is fantastic!","title":"The auto-magical conversion from UiNavigationBars to NSToolbar in Catalyst is an awesome addition, but I\u2019m not sure how it works with multiple columns. I have an app with UISplitViewController and 3 columns. All of the view controllers in each columns have their own buttons, and the one in the secondary view controller has a search bar. Which of these should appear in the Mac toolbar? Ideally they would all appear in their own \u201csection\u201d of the toolbar, like they do on iPad. But in testing, only the buttons from the secondary VC appear \u2014 no buttons from the other columns and no search bar. Is that expected? Is there a way to separate toolbar like we can using NSToolbar?"},{"location":"wwdc22/uiframeworks-lounge.html#when-we-use-uihostingcontroller-should-we-register-it-to-responder-chain-by-presentation-or-adding-as-a-child-view-controller-otherwise-what-would-be-happening-for-example-just-retaining-uihostingcontroller-and-then-just-adding-view-to-the-parent-view","text":"I cover this in the \u201cUse SwiftUI with UIKit\u201d talk! Its right at the beginning. You just need to add it as a child viewcontroller // Add the hosting controller as a child view controller viewController.addChild(hostingController) viewController.view.addSubview(hostingController.view) hostingController.didMove(toParent: viewController) https://developer.apple.com/wwdc22/10072 At the end I cover some of the limitations of UIHostingController and speak to why its necessary to embed in as a childViewController oh I missed that video! I would check it. Thanks! I wonder if I could create a UIView that supports rendering SwiftUI view like UIHostingConfiguration for supporting older iOS version. https://github.com/muukii/CompositionKit/pull/7|https://github.com/muukii/CompositionKit/pull/7 Awesome talk <@U03HL05BUJG>! What would your recommended approach be for small leaf node SwiftUI views in UIKit, which would not usually be full VCs? Even given the pitfalls of separating the hosting view, we find that we need to do this in ui libraries where clients expect UIViews, not view controllers. Thanks! (FB10019256)","title":"When we use UIHostingController, should we register it to responder chain? by presentation or adding as a child view controller. Otherwise what would be happening? for example just retaining UIHostingController and then just adding view to the parent view."},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-in-the-swiftui-document-model-to-access-updatechangecount-directly-in-order-to-decouple-document-saving-from-the-undo-stack-certain-operations-dont-make-sense-to-undo-but-do-change-the-documents-file-state-fb9974481","text":"Hi Ron! For now, there is no API available in SwiftUI for updateChangeCount . But also I want to thank you for filing the feedback, it is important to know which functionality is anticipated.","title":"Is there a way in the SwiftUI document model to access updateChangeCount directly, in order to decouple document saving from the undo stack? (Certain operations don't make sense to undo, but do change the document's file state.) (FB9974481)"},{"location":"wwdc22/uiframeworks-lounge.html#how-do-i-get-points-on-my-linemark-in-swift-charts-see-image-for-comparison","text":"Left is current, right is design MAKING PROGRESS, thank you for your help!!! how can I add the little circle dots for the data points? This is my code currently, Unbelievably short and easy (edited) You can either add a PointMark on top of the LineMark or by adding a symbol to the linemark LineMark(x: .value(\"xvalue\", point.x), y: .value(\"yvalue\", point.y)) .symbol(Circle()) My \u201cUse SwiftUI in UIKit\u201d sample project actually has an example of this https://developer.apple.com/documentation/uikit/views_and_controls/using_swiftui_with_uikit as does I believe some of the Swift Charts talks sample code too! <@U03HL05BUJG> what if i want to add the numerical values (eg: 15) above that PointMark? TextMark? as an annotation? Yes Let me find the docs for that\u2026 It should be an annotation modifer on the mark! .annotation(position: .top, alignment: .leading) https://developer.apple.com/documentation/charts/visualizing_your_app_s_data the swift charts example project has an example of this! You are the best! Got it! Anywhere I can follow you on social media? Ahh sorry I don\u2019t have any social media!","title":"How do i get points on my lineMark in swift charts? (See Image for comparison)"},{"location":"wwdc22/uiframeworks-lounge.html#is-it-possible-in-swiftui-on-mac-to-use-a-modifier-with-a-mouse-click-like-leftclick-i-like-to-select-non-adjacent-items-like-in-photos-and-finder","text":"Gestures have .modifiers(_:) modifier, which allows restricting that gesture to only respond when the modifier is active. So in this case you could use a Tap gesture with .modifiers(.command)","title":"Is it possible in SwiftUI on Mac to use a modifier with a mouse click like \u2318-leftClick? I like to select non-adjacent items like in Photos and Finder."},{"location":"wwdc22/uiframeworks-lounge.html#for-complex-views-i-often-define-subviews-inside-computed-vars-to-keep-my-body-block-more-readable-especially-for-components-that-dont-need-to-be-reused-elsewhere-in-the-app-so-they-dont-seem-to-warrant-a-reusable-struct-example-struct-myview-view-var-body-some-view-sometext-somebutton-private-var-sometext-some-view-texthello-private-var-somebutton-some-view-buttonpress-ive-heard-that-this-can-be-bad-for-performance-is-that-true-and-does-using-viewbuilder-on-some-computed-vars-have-any-impact","text":"SwiftUI\u2019s traversal of your view tree isn\u2019t impacted at all by whether you chose to use new structs, or computed properties, for small pieces of views, etc. So this is totally reasonable. Using @ViewBuilder here also shouldn\u2019t have a performance impact. I would highly recommend doing so! The only thing that you should pay attention to is your mechanisms for invalidating your views will be a bit less fine-grained, as you\u2019re making a larger part of your view hierarchy dependent on the sources of truth specified by MyView . Make sure you\u2019re paying attention to what pieces of your view depend on which pieces of data, and when you see computed properties that have completely disparate dependencies from the rest of the view, you consider breaking those out.","title":"For complex views, I often define subviews inside computed vars to keep my body block more readable.  Especially for components that don't need to be reused elsewhere in the app, so they don't seem to warrant a reusable struct. Example:  struct MyView: View {     var body: some View {         someText         someButton     }     private var someText: some View {         Text(\"Hello\")     }     private var someButton: some View {         Button(\"Press\") {}     } }  I've heard that this can be bad for performance - is that true? And does using @ViewBuilder on some computed vars have any impact?"},{"location":"wwdc22/uiframeworks-lounge.html#are-the-new-navigation-bar-styles-httpsdeveloperapplecomdocumentationuikituinavigationitem3987969-stylehttpsdeveloperapplecomdocumentationuikituinavigationitem3987969-style-supported-on-swiftui","text":"Howdy! You can find the answer to this in this slack thread: https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654786985779439","title":"Are the new navigation bar styles (https://developer.apple.com/documentation/uikit/uinavigationitem/3987969-style)|https://developer.apple.com/documentation/uikit/uinavigationitem/3987969-style) supported on SwiftUI?"},{"location":"wwdc22/uiframeworks-lounge.html#hi-all-thanks-for-all-the-great-work-this-year-what-approaches-would-you-recommend-to-start-integrating-swiftui-into-existing-codebases-that-support-ios-1415","text":"I'd suggest trying to adopt it in a view controller first, and replace the existing view controller with a UIHostingController and try to recreate the existing layout. A great place to start is a settings view, where you can use Form to put a list of fields, switches, sliders, and more.","title":"Hi all, thanks for all the great work this year.   What approaches would you recommend to start integrating SwiftUI into existing codebases that support iOS 14/15?"},{"location":"wwdc22/uiframeworks-lounge.html#in-continuation-of-my-question-about-navigationstack-httpswwdc22slackcomarchivesc03h9k1jyjyp1654791464924449httpswwdc22slackcomarchivesc03h9k1jyjyp1654791464924449-can-i-somehow-show-an-alert-or-make-some-operation-when-a-user-clicks-the-back-button-in-the-toolbar-or-now-it-isnt-possible-too-for-example-stop-a-task-or-warn-a-user-that-there-are-some-not-saved-changes-and-wait-for-the-users-response","text":"","title":"In continuation of my question about NavigationStack https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654791464924449|https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654791464924449  Can I somehow show an alert or make some operation when a user clicks the back button in the toolbar? Or now it isn\u2019t possible too? For example, stop a task or warn a user that there are some not saved changes and wait for the user\u2019s response."},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-specify-the-tabbing-order-of-text-fields-akin-to-textviewnextkeyview-fb10020959-my-app-has-a-combination-of-swiftui-textfield-views-and-nsuitextview-wrapping-in-nsuiviewrepresentable-and-ive-seen-some-odd-cases-where-the-default-tabbing-order-is-quite-unintuitive","text":"By default the key view loop follows the same default order as AppKit \u2014 leading to trailing, then top to bottom \u2014 do please file a feedback if you find a situation where that\u2019s not the case. There is support for customizing that order by describing the sections that separate your UI using focusSection() ( https://developer.apple.com/documentation/swiftui/view/focussection()?changes=_4 ) focusSection would have been perfect, but my bug is occurring on the iOS app. It does seem to be following leading to trailing, top to bottom in terms of the entire screen, however, the View with logically grouped TextField\u2019s can happen to be in a column next to unrelated NSTextViews far away, but along the same leading to trailing line horizontally. Ah, thanks for clarifying, and I see in the feedback that you provided a test flight \u2014 thank you! We\u2019ll check that out and follow up if we have any more questions on that","title":"Is there a way to specify the tabbing order of text fields, akin to textView.nextKeyView (FB10020959). My app has a combination of SwiftUI TextField Views and NS/UITextView wrapping in NS/UIViewRepresentable, and I've seen some odd cases where the default tabbing order is quite unintuitive."},{"location":"wwdc22/uiframeworks-lounge.html#is-it-possible-to-have-a-self-sizing-list-in-swiftui-similar-to-how-lazyvstack-works-list-has-support-for-drag-and-drop-through-onmove-while-lazyvstack-does-not","text":"List doesn't size itself based on the height of the content like LazyVStack. You'll need to give it a fixed height using the .frame(height: ...) modifier. List has support for reordering, swipe actions, and other features that LazyVStack does not. We'd love to hear about your use case in a feedback report. Specifically let us know what kind of experience you're trying to achieve, but can't.","title":"Is it possible to have a self-sizing list in SwiftUI similar to how LazyVStack works? List has support for drag and drop through onMove while LazyVStack does not."},{"location":"wwdc22/uiframeworks-lounge.html#i-have-a-couple-of-apps-written-in-swiftui-in-the-past-two-years-that-are-broken-now-because-they-rely-on-lists-and-the-background-color-cant-be-changed-anymore-using-the-uitableviewappearance-api-how-can-we-change-the-default-background-color-for-list-in-swiftui","text":"Please see our earlier post in the channel here: https://wwdc22.slack.com/archives/C03H9K1JYJY/p1654793028152169 Awesome! Thanks for your reply and sorry for not paying attention :D I really hope I won't have to throw away my years worth of code for such a small detail :P No worries :slightly_smiling_face:","title":"I have a couple of apps written in SwiftUI in the past two years that are broken now because they rely on lists and the background color can\u2019t be changed anymore using the UITableView.appearance() API.  How can we change the default background color for List in SwiftUI?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-swiftui-equivalent-of-new-uinavigationitembackaction","text":"There is no equivalent in SwiftUI, but a feedback would be appreciated!","title":"Is there SwiftUI equivalent of new UINavigationItem.backAction?"},{"location":"wwdc22/uiframeworks-lounge.html#how-can-i-adjust-the-width-or-thickness-of-a-linemark-in-swift-charts","text":"you can use a LineStyle! https://developer.apple.com/documentation/charts/chartcontent/linestyle(_:) .lineStyle(StrokeStyle(lineWidth: 3))","title":"How can I adjust the width or thickness of a linemark in swift charts?"},{"location":"wwdc22/uiframeworks-lounge.html#we-have-an-app-that-has-a-50-components-built-using-uikit-we-are-trying-to-use-swiftui-by-reusing-our-existing-components-but-we-are-facing-multiple-challenges-1-it-seems-that-uiviewrepresentable-doesnt-work-out-of-the-box-with-views-that-are-built-using-auto-layout-we-have-a-generic-container-intrinsiccontentheightview-that-tries-to-calculate-the-size-of-the-uikit-view-and-return-it-to-the-intrinsiccontentsize-is-there-a-better-solution-2-we-noticed-performance-issues-using-uiviewrepresentable-views-in-swiftui-lists-it-seems-that-lists-always-re-create-the-uiviewrepresentable-views-and-never-recycle-cells-are-we-doing-anything-wrong-3-our-design-system-slightly-modifies-the-line-height-of-our-texts-we-have-a-solution-using-nsattributedstring-that-works-well-for-uikit-but-havent-found-a-solution-for-swiftui-text-supports-line-spacing-but-it-can-only-increase-the-default-line-spacing-it-cannot-decrease-it-passing-negative-values-does-nothing-is-there-anything-we-can-do-to-customize-the-line-height-of-a-text-in-swiftui","text":"I only have an answer for the first question. We have new API that allows you to more finely control how your UIViewRepresentable interacts with the SwiftUI layout system. Please see: https://developer.apple.com/documentation/swiftui/uiviewrepresentable/sizethatfits(_:uiview:context:)-9ojeu Thank you, Raj! I will check if we can build a solution for iOS 16 with sizeThatFits and our other container for older versions. For your last question, can you please file a feedback with your request? In particular, it would be good to know what kind of customization you need for line spacing/line height? It it going to be a fixed value, or certain percentage, or a delta applied, or a value relative to Dynamic Type sizes? Thanks! Value relative to the dynamic type size sounds like the thing that will work for me. I will make sure to file feedback to give more details. Thank you for answering, Paul!","title":"We have an app that has a 50+ components built using UIKit. We are trying to use SwiftUI by reusing our existing components but we are facing multiple challenges. 1. It seems that UIViewRepresentable doesn't work out of the box with views that are built using auto-layout. We have a generic container (IntrinsicContentHeight&lt;View) that tries to calculate the size of the UIKit view and return it to the intrinsicContentSize. Is there a better solution?  2. We noticed performance issues using UIViewRepresentable views in SwiftUI Lists. It seems that Lists always re-create the UIViewRepresentable views and never recycle cells. Are we doing anything wrong?  3. Our design system slightly modifies the line-height of our texts. We have a solution using NSAttributedString that works well for UIKit but haven't found a solution for SwiftUI. Text supports line spacing but it can only increase the default line spacing, it cannot decrease it (passing negative values does nothing). Is there anything we can do to customize the line height of a Text in SwiftUI?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-any-thought-to-making-uitextinteraction-spell-checking-api-public-uitextinteraction-respects-the-spellcheckingtype-property-of-uitextinputtraits-if-the-value-is-uitextspellcheckingtypedefault-or-uispellcheckingtypeyes-spell-checking-occurs-as-expected-unfortunately-uitextinteraction-uses-the-private-uitextreplacement-class-to-handle-spell-checking-when-it-encounters-a-misspelled-word-it-correctly-presents-a-menu-with-the-spelling-options-however-when-you-tap-the-correction-it-attempts-to-call-the-private-replace-method-with-a-sender-of-private-class-uitextreplacement-the-replacementtext-property-of-the-sender-contains-the-misspelling-the-code-works-fine-but-using-this-private-method-and-property-earns-you-a-rejection-from-app-review-so-it-isnt-possible-to-use-the-spell-checking-features-of-uitextinteraction","text":"This seems like an oversight on our part. Have you filed a feedback request for this? FB10136384 Thanks. I think the expectation is that replaceRange:withText: should be invoked instead of the private replace: method. In the meantime, you should try implementing shouldChangeTextInRange:replacementText: and see if that gets called before/after the call to replace . That would be a way to get the replacement text without using private API.","title":"Is there any thought to making UITextInteraction spell-checking API public? UITextInteraction respects the spellCheckingType property of UITextInputTraits. If the value is UITextSpellCheckingTypeDefault or UISpellCheckingTypeYes, spell checking occurs as expected. Unfortunately, UITextInteraction uses the private UITextReplacement class to handle spell checking. When it encounters a misspelled word, it correctly presents a menu with the spelling options. However, when you tap the correction, it attempts to call the private replace: method with a sender of private class UITextReplacement. The replacementText property of the sender contains the misspelling. The code works fine, but using this private method and property earns you a rejection from app review, so it isn\u2019t possible to use the spell-checking features of UITextInteraction."},{"location":"wwdc22/uiframeworks-lounge.html#did-anything-change-in-uitextinteraction-with-respect-to-dictation-on-ios-16-dictation-in-a-custom-view-that-supports-uitextinteraction-always-fails-the-same-shipping-code-works-on-ios-15","text":"Changes were made to dictation to support the new \"modeless\" dictation feature. If this has broken a custom text view implementation, it was unintentional and a feedback request would be appreciated. Will do. I notice that dictation seems to be failing in Pages too, but not Notes old this (old) iPad.","title":"Did anything change in UITextInteraction with respect to dictation? On iOS 16, dictation in a custom view that supports UITextInteraction always fails. The same (shipping) code works on iOS 15."},{"location":"wwdc22/uiframeworks-lounge.html#is-there-a-way-to-use-app-specific-uiactivities-with-swiftuis-new-sharelink","text":"This might be a better question for the <#C03HX19UNCQ|>.","title":"Is there a way to use app-specific UIActivities with SwiftUI\u2019s new ShareLink?"},{"location":"wwdc22/uiframeworks-lounge.html#the-new-find-stuff-is-great-it-took-30-seconds-to-integrate-with-my-uitextview-on-catalyst-the-find-panel-seems-to-have-some-issues-namely-that-clicking-on-the-buttons-doesnt-work-the-search-field-is-focused-and-i-can-type-it-in-but-clicking-doesnt-do-anything-is-that-a-known-issue-with-beta-1-or-is-there-something-i-should-investigate-on-my-end","text":"This is a known issue (but feedback requests still appreciated). A workaround currently is to call layoutSubviews on the UITextView after the find panel appears.","title":"The new find stuff is great \u2014 it took 30 seconds to integrate with my UITextView. On Catalyst, the find panel seems to have some issues, namely that clicking on the buttons doesn\u2019t work \u2014 the search field is focused and I can type it in, but clicking doesn\u2019t do anything. Is that a known issue with beta 1 or is there something I should investigate on my end?"},{"location":"wwdc22/uiframeworks-lounge.html#a-bit-late-to-the-party-due-to-a-lab-appointment-so-still-working-my-way-through-the-video-can-you-advise-is-there-an-api-to-enable-me-to-add-custom-data-detectors-in-text-editing-in-my-app-for-instance-id-like-to-be-able-to-have-a-data-detector-that-adds-text-decoration-or-other-features-to-any-time-someone-inserts-the-name-of-a-person-in-their-text-from-a-list-of-names-of-people-already-in-the-app-say-several-thousand-people-is-there-an-apimechanism-for-this","text":"Unfortunately, that is not something that is supported today. Please file a feedback request for us to look at in the future :slightly_smiling_face: Obviously not as nice as a dedicated API , but you could do it very manually, by watching the delegate methods of UITextView for text changes, and then maybe overlaying things or editing the text to add links.","title":"A bit late to the party, due to a lab appointment, so still working my way through the video. Can you advise: is there an API to enable me to add custom data detectors in text editing in my app? For instance, I'd like to be able to have a data detector that adds text decoration or other features to any time someone inserts the name of a person in their text, from a list of names of people already in the app (say, several thousand people). Is there an API/mechanism for this?"},{"location":"wwdc22/uiframeworks-lounge.html#honestly-its-been-a-while-since-i-last-checked-this-but-is-there-an-api-in-uikit-these-days-that-i-can-query-to-check-whether-an-external-hardware-keyboard-is-present","text":"There is no API for this, it's not really something we think developers should have to account for, generally. Do you have a specific use case in mind? (We usually encourage developers to use the keyboard layout guide for laying out UI instead). I once worked on a hex editor for iPad as a side project. I wanted to show a custom inputView with a hex keypad when no hardware keyboard is attached, but wanted to use the default keyboard when there is a hardware keyboard present so that there is no on-screen keyboard shown. I think I filed a feedback about this after a WWDC20 lab session, let me find the ID FB7777442 This year\u2019s enhancements kinda make me want to continue working on it, but since there are no guarantees in iOS/iPadOS when an app is (not) terminated in the background, I still don\u2019t feel too confident about it It would be helpful to know if a hardware keyboard is attached because you can optimize what functions are offered in the assistant. In screenwriting, for example, the tab key is important enough to add to the bar if you don't have a keyboard attached. Oh gosh, that was me you talked to! Well, thanks for the reminder, I will try my best. Oh yes, I totally missed that sentence :smile: One case we're hitting where we would love to check for HW keyboard - in our custom text editing view we offer intelligent word completion. When HW keyboard is connected we'd like the prompt to reference the \"tab\" key. When it's not connected, we'd like to instead tell the user to swipe right. Your hex editor needs to run in the background indefinitely? Why? Let\u2019s say you are moving around large chunks of data stored on a slow USB drive. If the OS decides to kill the app while the app is still busy writing data, data corruption occurs, I guess? Maybe this is not an issue with journaled file systems, but what about plain old FAT32?","title":"Honestly, it\u2019s been a while since I last checked this, but is there an API in UIKit these days that I can query to check whether an external hardware keyboard is present?"},{"location":"wwdc22/uiframeworks-lounge.html#the-uifindinteraction-doesnt-seem-to-fire-off-a-new-query-on-my-session-when-the-user-removes-all-text-from-the-search-box-is-this-intended-is-there-another-way-of-listening-for-this-so-we-can-clear-our-custom-text-highlights","text":"You should get a call to invalidateFoundResults instead. Thank you for answering the last minute Q! I totally thought invalidateFoundResults was only there to be called by us, not the system. Ah, indeed, the documentation is wrong. Thanks for pointing that out!","title":"The UIFindInteraction doesn't seem to fire off a new query on my session when the user removes all text from the search box. Is this intended? Is there another way of listening for this so we can clear our custom text highlights?"},{"location":"wwdc22/uiframeworks-lounge.html#hi-in-my-carplay-app-in-templateapplicationscene_didconnect-i-get-the-provided-interfacecontroller-and-store-a-reference-to-it-but-most-of-the-times-when-i-try-to-access-interfacecontrollercartraitcollection-the-app-crashes-because-cartraitcollection-is-uninitialized-so-it-makes-creating-image-for-different-display-scale-2x-3x-or-different-display-styles-dark-light-impossible-i-tried-to-make-sure-that-i-always-access-it-from-main-thread-but-still-have-the-same-problem-this-code-always-crashes-for-me-carplay-connected-func-templateapplicationscene-_-templateapplicationscene-cptemplateapplicationscene-didconnect-interfacecontroller-cpinterfacecontroller-selfinterfacecontroller-interfacecontroller-selfinterfacecontrollerdelegate-self-printinterfacecontrollercartraitcollection","text":"Thanks for the report! We\u2019re aware of the issue and we\u2019re tracking it. Your app shouldn\u2019t be crashing, though - in a Swift app, you\u2019ll want to handle this variable potentially being nil, perhaps by assuming a default screen scale (2x) if the trait collection isn\u2019t yet available. Thanks for the answer. I do that in my real app, I assume 2x for images and dark for the style, but is there any specific point in the life cycle that it will be initialized for sure? It should be non-nil if your app has been launched on the CarPlay screen, but it\u2019ll likely be nil if your app has not been launched in CarPlay. Please also give this a try in iOS 16 seed 1; this should be improved! how can I know that app is launch in the CarPlay? The CPInterfaceControllerDelegate callbacks will inform your app when it has been launched in CarPlay (and disconnected from CarPlay) The same one you have above :slightly_smiling_face: Thanks, and the value will remain non-nil after the first appearance on the app on CarPlay screen? Sorry, do you mean the trait collection, the scene, something else? The trait collection should not be nil, but this is an issue we are tracking :slightly_smiling_face: The value will get updated at some point after the sceneDidConnect callback, but unfortunately we don't have a delegate callback to inform your app when this happens. As Jonathan said, we are aware of this issue, but please feel free to file a feedback on this! > yes, I mean the interfaceController.carTraitCollection We already have filed a feedback in December 2020: It is FB8926706 > Thanks <@U03HVCNN2DU> , do you mean that interfaceController.carTraitCollection can also be nil when I get the CPInterfaceControllerDelegate > call back as Jonathan said? Yes, that is the issue we\u2019re describing here :slightly_smiling_face: Yes, the bug is that sometimes the trait collection is not initialized at the time of the scene-connect delegate callback, but it will eventually get initialized Thank you Kevin and Jonathan. another related question that I noticed, looking at another thread, <@U03HJA80GLS> When using UIImageAsset, the pickup of the correct asset also depends on the interfaceController.carTraitCollection which is nil at some point, so if we use an image asset to create the template when the carTraitCollection is nil we will get the wrong asset, or no asset at all. Is there any workaround for this?","title":"Hi, In my CarPlay app, In templateApplicationScene(_:didConnect:) I get the provided interfaceController and store a reference to it, but most of the times when I try to access interfaceController.carTraitCollection The app crashes because carTraitCollection is Uninitialized, so it makes creating image for different display scale (2x, 3x) or different display styles (dark, light) impossible. I tried to make sure that I always access it from main thread but still have the same problem.  This code always crashes for me: // CarPlay connected     func templateApplicationScene(         _ templateApplicationScene: CPTemplateApplicationScene,         didConnect interfaceController: CPInterfaceController     ) {         self.interfaceController = interfaceController;         self.interfaceController?.delegate = self;           print(interfaceController.carTraitCollection) }"},{"location":"wwdc22/uiframeworks-lounge.html#hi-ive-a-question-about-the-car-play-simulator-i-added-arm64-in-the-excluded-architecture-to-run-in-the-iphone-simulator-for-my-m1-mac-now-i-get-this-error-in-carplay-simulator-terminating-app-due-to-uncaught-exception-nsinvalidargumentexception-reason-unsupported-object-cptabbartemplate-i-think-it-crashes-because-of-arm64-in-the-excluded-architecture-can-you-please-help","text":"Hi! This is most likely caused by a library or third-party dependency in your app that does not have an arm64 slice. We\u2019d recommend you work with your vendor(s) as needed to get an updated version of all of your dependencies. Here\u2019s a similar thread with some more detail: https://developer.apple.com/forums/thread/698332 You might also try running your app on-device with the new CarPlay Simulator app on your Mac, available under the Additional Tools downloads on the developer site. Thank you so much. I downloaded that tool and planning to use the CarPlay simulator.","title":"Hi, I've a question about the Car Play simulator . I added arm64 in the excluded architecture to run in the iPhone simulator for my M1 mac.  Now I get this error in  carplay simulator \u201cTerminating app due to uncaught exception 'NSInvalidArgumentException', reason: 'Unsupported object &lt;CPTabBarTemplate: \u2026. \u201c , I think it crashes because of arm64 in the excluded architecture. Can you please help ?"},{"location":"wwdc22/uiframeworks-lounge.html#when-your-carplay-app-is-running-but-your-screen-on-your-device-is-locked-is-your-app-considered-in-the-foreground-for-example-can-i-run-something-like-a-timer-if-im-just-within-the-carplay-ui","text":"When an app is launched in CarPlay and visible on the CarPlay display, the corresponding UIScene (specifically a CPTemplateApplicationScene ) is foregrounded. As with UIScene in general, your app's overall application state is the highest activation state of all of its connected scenes. Thanks!","title":"When your CarPlay app is running but your screen on your device is locked, is your app considered \"in the foreground?\"  For example, can I run something like a Timer if I'm just within the CarPlay UI?"},{"location":"wwdc22/uiframeworks-lounge.html#do-push-notifications-non-time-sensitive-work-like-normal-when-in-the-carplay-ui-as-in-can-i-process-them-and-do-everything-i-would-expect-to-do-just-like-when-my-app-is-open-on-my-device","text":"Yes, user notifications work in CarPlay but the app must be allowed in CarPlay. When requesting notifications authorization, include the allowInCarPlay option: https://developer.apple.com/documentation/usernotifications/unnotificationcategoryoptions/1649281-allowincarplay And then make sure to post the notification with that category. Separate out notifications intended for CarPlay into their own category to avoid other notifications your app might post that aren\u2019t relevant while driving from appearing in CarPlay. For more information see this video starting around 14:20: https://developer.apple.com/videos/play/wwdc2017/719 Thanks! However, notifications are limited to certain app categories and for those categories (such as VoIP and Messaging apps) the notification is handled using a Siri intent. <@U03J83E86Q0> So for example, in a quick food ordering app, can we get notifications say for the status of your order just like we currently do on device? I might have missed anything in the docs about which entitlements could receive notifications Quick food ordering does not support notifications In no way at all? So, what would be standard on a device to receive a notification when there is an update to the status of your order so that I can update the UI in my app\u2026that is not supported? If not, what mechanisms are available for the CarPlay app to get say an updated status? App icon badges will appear in CarPlay, that\u2019s one way to communicate that your app has new information. Definitely file feedback too if you think it could be useful for your app! Will do. Although this may be an entire project killer at this point.","title":"Do push notifications (non-time sensitive) work like normal when in the CarPlay UI? As in, can I process them and do everything I would expect to do just like when my app is open on my device?"},{"location":"wwdc22/uiframeworks-lounge.html#does-the-new-carplay-simulator-app-only-function-with-devices-running-ios-16-i-couldnt-get-it-to-actually-display-carplay-visuals-from-my-plugged-in-ios-155-iphone-no-error-messages-just-nothing-happening","text":"CarPlay Simulator is supported for iPhones with iOS 15.2 and above. Can you share more information about what model Mac you are using to run CarPlay Simulator? Ah. I\u2019m on: \u2022 MacBook Air (M1, 2020) \u2022 macOS 12.4 (21F79) For reference: this is what I see when I launch the app, with my unlocked iPhone (15.5) connected. Buttons are responsive to clicks, but \u2026nothing I wildly click causes CarPlay visuals to appear. [Here\u2019s hoping I\u2019m not being really silly here! :crossed_fingers::skin-tone-3:] Same issue for me. iPhone 12 mini on iOS 15.4.1; Macbook Pro 13\" i5 2020 (MacBookPro16,2) on macOS 12.4 Can you try the steps in the CarPlay Simulator Help \"Troubleshooting CarPlay Simulator\"? In particular note if your iPhone has Personal Hotspot enabled. We are also tracking one known issue that we expect to have a fix for relatively soon. None of the steps in the Troubleshooting guide worked for me, including adding CarPlay Simulator to the Firewall settings I'm sorry the steps didn't resolve the issue. This sounds like something we need some more information on. Could you please submit feedback for this issue with both macOS and iOS sysdiagnose? It's also worth trying the latest beta of iOS 15 - 15.6 beta 2, it includes a fix to an issue that can lead to this problem. (FYI: CarPlay Simulator just kicked into life for me once I\u2019d turned off Personal Hotspot :+1::skin-tone-3:)","title":"Does the new CarPlay Simulator app only function with devices running iOS 16? \u2013 I couldn't get it to actually display CarPlay visuals from my plugged-in iOS 15.5 iPhone (No error messages; just \u2026nothing happening)"},{"location":"wwdc22/uiframeworks-lounge.html#ive-been-snapshotting-a-swiftui-view-uihostingcontroller-uiimage-to-create-point-of-interest-images-this-works-in-the-ios-simulator-with-carplay-external-display-but-ive-found-on-a-real-car-head-unit-that-when-my-app-is-launched-from-carplay-and-hasnt-been-launched-on-the-device-or-the-device-is-locked-i-just-get-an-empty-image-rendered-when-run-via-carplay-this-seems-to-be-related-to-the-on-device-app-being-inactive-despite-the-carplay-app-being-active-im-hitting-walls-in-my-knowledge-of-ios-app-activation-states-is-there-there-some-way-api-you-can-think-of-that-i-could-call-in-my-carplay-side-of-things-to-wake-up-man-shruggingskin-tone-3-the-on-device-app","text":"Double check your scene delegates and that you are doing your drawing work on the carplay scene/display otherwise please file a feedback with a small sample app replicating the issue you are seeing <@U03HBMBDJNS> When you say \u2018doing your drawing work on the carplay scene/display\u2019, I\u2019m not quite sure what you mean? I\u2019m currently calling the CPPointOfInterestTemplate \u2019s setPointsOfInterest method, and the drawing code, inside a DispatchQueue.main.async call (because that was the first thing I tried when having UI-related issues), if that\u2019s of any relevance? (This code path runs in response to the delegate\u2019s didChangeMapRegion and also CLLocationManager \u2019s didUpdateLocations ) (Re: work in the right place -> do you know if there\u2019s a specific queue/etc I should be calling drawing code via, when in CarPlay, for it to function?) This is probably a question better answered by the SwiftUI team, but filing a feedback with a sample app would be helpful for us to understand if this use case is expected to work. Noted, thanks :relaxed: And, to answer your question about 'waking up' the app on the phone, there is no means of doing this. Apps are only launched (given scenes) by the system (generally this is because the user launched the app in that environment)","title":"I've been snapshotting a SwiftUI View (UIHostingController = UIImage) to create Point of Interest images. This works in the iOS Simulator (with CarPlay external display), but I've found on a real car head-unit that: - when my app is launched from CarPlay (and hasn't been launched on the device), - or the device is locked, I just get an empty image rendered when run via CarPlay.  This seems to be related to the 'on-device app' being inactive, despite the CarPlay app being active. I'm hitting walls in my knowledge of iOS app activation states\u2013\u2013 is there there some way (API) you can think of that I could call in my CarPlay side of things, to 'wake up' (:man-shrugging::skin-tone-3:) the 'on-device app'?"},{"location":"wwdc22/uiframeworks-lounge.html#for-a-generic-notification-that-shows-up-when-your-app-is-not-in-the-foregroundare-those-just-time-sensitive-notifications-or-are-those-restricted-to-only-certain-types-of-entitlements","text":"Hi! We only allow notifications for certain categories such as navigation apps, parking apps, and EV charging apps. Notifications are also allowed, but further restricted for some categories, (such as VoIP and Messaging apps) where the notification is handled using a Siri intent. When requesting notifications authorization for user notifications, include the allowInCarPlay option: https://developer.apple.com/documentation/usernotifications/unnotificationcategoryoptions/1649281-allowincarplay And make sure to post the notification with that category. Separate out notifications intended for CarPlay into their own category to avoid other notifications your app might post that aren\u2019t relevant while driving from appearing in CarPlay.","title":"For a generic notification that shows up when your app is not in the foreground...are those just time-sensitive notifications or are those restricted to only certain types of entitlements?"},{"location":"wwdc22/uiframeworks-lounge.html#is-there-any-way-to-have-two-different-icons-for-each-tab-one-for-when-the-tab-is-selected-and-one-for-when-the-tab-is-not-selected-i-have-tried-to-change-the-tab-icon-when-the-template-appearsdisappears-but-it-does-not-work","text":"You should get a CPTabBarTemplateDelegate callback when the active tab changes, but any changes to the tab image will only be picked up when the tab bar template is created OR reloaded with a new set of tabs, via updateTemplates: . This would be a great feedback to file! Thank, I will file a feedback","title":"Is there any way to have two different icons for each tab? one for when the tab is selected and one for when the tab is not selected? I have tried to change the tab icon when the template appears/disappears, but it does not work."},{"location":"wwdc22/uiframeworks-lounge.html#can-a-carplay-app-run-in-the-background-for-location-updates-and-trigger-a-local-notification-ie-location-services-background-mode-or-do-carplay-apps-not-have-access-to-the-same-functionality-when-closed-use-case-an-ev-charging-app-which-launches-maps-to-navigate-to-a-charging-station-but-monitors-location-updates-in-the-background-and-when-the-user-is-a-few-miles-from-the-charging-station-location-checks-the-stations-availability-again-to-notify-the-user-if-the-station-is-no-longer-available","text":"Background app activity policies are the same in CarPlay. If your app already supports a background location service such as an arrival geofence for that charger the app will continue to support that behavior when launched or backgrounded in CarPlay as well. Yay! :partying_face:","title":"Can a CarPlay app run in the background, for location updates, and trigger a local notification? (i.e. location services background mode)  (Or do CarPlay apps not have access to the same functionality when 'closed'?)  Use case = an EV charging app, which launches Maps to navigate to a charging station, but monitors location updates in the background, and \u2013 when the user is a few miles from the charging station location \u2013 checks the station's availability again; to notify the user if the station is no longer available."},{"location":"wwdc22/uiframeworks-lounge.html#do-i-have-any-access-to-car-information-through-carplay","text":"Can you please clarify the use case you are trying to address here? I was hoping for mileage information, for trip durations, and just noting this mileage is being recorded as a business trip or a personal trip. Unfortunately that information isn't passed over, no.","title":"Do I have any access to car information through carplay?"},{"location":"wwdc22/uiframeworks-lounge.html#what-is-the-expected-image-size-for-the-section-header-button-introduced-in-ios-15","text":"Hello! CPMaximumListSectionImageSize will give you the maximum size of the section header image. In the CPListSection header, note the following on the initializer that includes the headerImage : > @note The maximum size of the section header image is given by CPMaximumListSectionImageSize . Using the value you retrieve from CPMaximumListSectionImageSize will always be correct for the version of iOS on your user\u2019s device > CPMaximumListSectionImageSize > returns something like 20*20 but the image looks larger than that. when I provide a system images, the size looks good on simulator and different head units, but when I provide my image (and I size the image based on CPMaximumListSectionImageSize ) the image appears with different sizes on different head units, some times it is larger that it should be, some times very smaller. ) and my other question is that what should happens if I provides an image with wrong size? Will CarPlay resize that image to fit? What if I provide a pdf or svg image? <@U03HBMB6TPG> would you please take a look at these questions? Your image will be resized to the maximum size if it's too large, but it won't be scaled up if its too small. It's worth mentioning that you should size images to the car\u2019s display scale, using carTraitCollection Thanks <@U03HBMB6TPG> any idea how something like this can happen? Thank you for the note <@U03JBFATXM2>! We\u2019re aware of the issue and are tracking it Is there any workaround for this at the moment <@U03HBMB6TPG>? In the meantime, if your app can resize the image to CPMaximumListSectionImageSize first, it should result in the right size on the car screen > Sorry may be my question was not clear enough, I was asking about the section header button from the beginning, not the section header image. Is CPMaximumListSectionImageSize also applies to the header button? <@U03HBMB6TPG>? You\u2019re right! That button also needs to be resized. CPMaximumListSectionImageSize should be a good starting point there too. We\u2019ll definitely look into this for the CPButton, CPButtonMaximumImageSize is provided Thank you for the answers <@U03HBMB6TPG>. Is there any estimate for when this will be fixed, will it be fixed for iOS 16 release? No problem <@U03JBFATXM2>, thank you for developing with us! We\u2019re investigating the issue, but can't share a timeframe at this time","title":"What is the expected image size for the section header button (introduced in iOS 15)?"},{"location":"wwdc22/uiframeworks-lounge.html#in-ios-simulator-with-the-additional-carplay-external-display-options-enabled-via-defaults-theres-load-and-save-functionality-for-simulation-config-screen-dimensions-scale-etc-are-there-any-defaults-that-can-be-set-to-enable-loadsave-functionality-in-the-new-carplay-simulator-app-use-case-swapping-between-different-cars-for-testing","text":"Unfortunately this isn't a currently supported feature, but please file a feedback request for this! Noted :+1::skin-tone-3: FB10165659 (Support loading+saving of CarPlay Simulator configuration)","title":"In iOS Simulator, with the additional CarPlay external display options enabled via defaults, there's Load and Save functionality for simulation config (screen dimensions, scale, etc)  Are there any defaults that can be set to enable load/save functionality in the new CarPlay Simulator app? (Use case: swapping between different 'cars' for testing)"},{"location":"wwdc22/uiframeworks-lounge.html#when-it-comes-to-color-of-icons-and-images-it-seems-as-those-need-to-already-be-the-color-you-want-them-to-be-for-displaying-it-doesnt-seem-that-i-can-tint-them-when-displaying-them-in-the-carplay-ui-is-that-correct-or-am-i-doing-something-wrong","text":"Hi! It will depend on the specific UI element you\u2019re looking at. Some elements, like nav bar leading and trailing buttons, are tinted by the system, while images in your list items and grid templates should support any custom colors you provide Maybe you can share which elements you\u2019re looking at? Thanks! ok. I\u2019m specifically thinking of like icons in the list and grid template. I\u2019ve tried the way we do it in the iPhone app with tinting, and that just hasn\u2019t seemed to work so far. I most recently tried withTintColor for example. I was trying to avoid running them through something like UIGraphicsImageContext Maybe another thing to try is to use UIImageAsset wherever possible, which should let you provide a day/night variant of each image (for day/night mode in CarPlay) If things aren\u2019t looking right though, please file a feedback issue and we will look into it :slightly_smiling_face: thanks! <@U03HJA80GLS> When using UIImageAsset, the pickup of the correct asset also depends on the interfaceController.carTraitCollection which is nil at some point, so if we use an image asset to create the template when the carTraitCollection is nil we will get the wrong asset. Is there any workaround for this? Thanks! That\u2019s worth a separate feedback for sure. I don\u2019t think we have any workaround to share right now, but it\u2019s under investigation.","title":"When it comes to color of icons and images, it seems as those need to already be the color you want them to be for displaying.  It doesn't seem that I can tint them when displaying them in the CarPlay UI. Is that correct or am I doing something wrong?"},{"location":"wwdc22/uiframeworks-lounge.html#for-testing-carplay-with-ios-simulator-it-appears-that-the-carplay-window-shown-with-ios-simulator-is-not-responding-to-external-inputs-on-apple-silicon-is-that-a-known-issue","text":"Hi there \u2014 we're aware of an issue with inputs on the Xcode Simulator and are looking into it. Cool thanks ^^ Just want to make sure I didn\u2019t do something dumb :sweat_smile:","title":"For testing CarPlay with iOS Simulator, it appears that the CarPlay window shown with iOS Simulator is not responding to external inputs on Apple silicon. Is that a known issue?"},{"location":"wwdc22/uiframeworks-lounge.html#any-chance-for-an-answer-previous-question-regarding-the-ability-to-share-text-from-one-carplay-app-to-another","text":"Hi! Maybe you can share more about your particular use case? In general, lots of third party services offer libraries/frameworks for these kinds of text operations, which could be an option for you. Another possibility is if you\u2019re looking for a specific app, you can use URL schemes. There you can open a URL on your template application scene, which should cause the desired app to launch in CarPlay (with the payload you provide), but again that\u2019ll be specific to the particular app you\u2019re integrating with. <@U03HJA80GLS> Yeah, I just didn\u2019t save the question and it probably was lost among other CarPlay questions\u2026 Here\u2019s the case: The user is in the car and he wants to send an email via some mailing app. He can do it using voice commands - dictate some text and requests to send. We\u2019d like to provide the user an ability to dictate that text in our app for CarPlay, which will perform some manipulations with the text (grammar checks, punctuation fixes, etc.) and \u201creturn\u201d it to the user, so that he could send it to another app (e.g. Mail app) using some voice command. Is it possible to implement such an app and would it be possible to let the user send it to another app - either using a direct command or using some copy/paste flow? See the CarPlay App Programming Guide for the supported categories of apps in CarPlay: https://developer.apple.com/carplay/documentation/CarPlay-App-Programming-Guide.pdf Technically, we can fit under the communication category ) But is there gonna be a way for the user to use Siri and ask her to send our text to someone using, for example, iMessage?","title":"Any chance for an answer previous question regarding the ability to share text from one CarPlay app to another?"},{"location":"wwdc22/watchos-lounge.html","text":"watchos-lounge QAs by FeeTiki How should we update our complications going forward? Are complication updates from iPhone still supported? Nothing has changed with the ClockKit API in this release. Your app can request timeline reloads via CLKComplicationServer.sharedInstance.reloadTimeline(for: ...) And a similar mechanism exists if you move to WidgetKit based complications WidgetCenter.shared.reloadTimelines(ofKind: ...) I see, thanks! So I assume the data flow is that it\u2019s still up to the Watch app to react from transferCurrentComplicationUserInfo , store the data somehow, and trigger an update \u2014 in watchOS 9 or later using the WidgetKit API, in earlier watchOS versions using the CLK API, right? If you want to simplify, you can call both and the system will only reload your widgets if you\u2019re running on watchOS 9 or later and have adopted WidgetKit based complications. After migrating to WidgetKit, your ClockKit based complications will no longer be run. What is the best way to log Watch applications, especially in production builds? Installing logging profiles can be problematic for non-tech users and only lasts for a couple of days Hi Enrico! Are you asking what is the best way to capture logs from your customers when filing Feedback with Apple, or what is the best way to capture logs for your own use? The second case is what i\u2019m interested in most. The point is what method would you advise to receive logs from builds that have no debugger attached Hi, Enrico. As far as I know, logging profiles are the only way to get logs for your application on production builds. But maybe somebody else knows of something obscure that I don't! That\u2019s the only way I know of also! In my own apps, I have a logging system that then pushes logs over WatchConnectivity back to the phone with a debugging view. Thanks Brian. Is it true that they unfortunately expire after a few days? Any more detail on that? The profiles expire, but that doesn\u2019t stop you from having your own logging. :slightly_smiling_face: I believe the intention is to expire the profiles because often a user will forget they have it enabled and that can have power and performance implications. If this helps at all, here is my code https://gist.github.com/aaronpearce/ae5a287d99fce09b4507e708d6aa329e Uses a watchconnectivity library called Communicator. If we go for watchOS 9 only is it then possible for Watch Series 3 users to load the old Binary of the App from the \"Watch App Store\"? Hi Sebastian. I\u2019m not positive, but I don\u2019t think this will work reliably, especially if your watch app is independent. Is there an API you want to use that would require you to go minOS 9.0? I mean if we update our app to watchOS 9 only in autumn, would it be possible for the Watch Series 3 users to download an old Version of our Watch App that supports watchOS 8, like it is on the App Store, where users with older devices can still download the latest supported Version of an App? Yep, I understood the question. :slightly_smiling_face: My question back to you was: is there a reason you want to go watchOS 9 only? And in a related question, is there anywhere besides Google search - where we developers can get meaningful statistics on WatchOS (or iOS/iPadOS) adoption rates? We would like to go for watchOS 9 only to get rid of the 32bit Binary for Watch Series 3. This increases the App size of the iOS app or am I wrong? watchOS apps are already thinned for the platform they\u2019re delivered to, so this isn\u2019t something you need to worry about! both the architectures and the assets are thinned for the device class and size. :slightly_smiling_face: But the Watch Binary is still a part of the iOS Binary right? If a user with a Series 3 downloads the iOS App is then in the Binary a 32bit Version of the Watch App only or is the 64bit Version also included? The watchOS binary is not downloaded to the phone if the paired watch is running watchOS 6.0 or later. It does not make your iOS app any bigger. Ahh good to know thank you :raised_hands: Can you point me in the direction of the Share Sheet documentation? Is it SwiftUI only? The documentation is here: https://developer.apple.com/documentation/swiftui/sharelink , and there will be a related session available tomorrow: Meet Transferable. ShareLink is SwiftUI only. Is the size for a Watch App still limited to 75mb? Yes it is. Hello! There is no DatePicker for watchOS. Could creating a List with sections be an alternative? Thanks for your time. Can you please clarify what you mean by \"List with sections\"? A list with sections sounds interesting! You can also use multiple Pickers. Please do file a feedback about DatePicker (or any other enhancements you may think of) Yep, a date picker is definitely something that we've been missing for a while. Thank you Brian D and Matthew K. I had almost built the app with 3 separate pickers in UIKit, but had problems keeping track of the different number of day requirements (we don\u2019t want anyone saying \u201cJune 31\u201d. I have a watchOS lab tomorrow, but wanted to trim down my question (15 min is a short time for the messy situation I \u2026 I am in (but I will definitely take it!) Moving from UIKit to SwiftUI gave me different problems\u2026 life is interesting! By \u201cList with Sections\u201d I meant a list where I could send user to separate detail views for the entries, or list elements (apologies for the incorrect term) that could hold TextFields. But that is not great in watchOS. BTW: what happened to number pads in watchOS 8? There's a couple examples of date pickers in some of our watchOS apps. For example, Reminders has a version of a date picker when editing. They reset the day picker's value when the month picker changes, to prevent a user from picking an invalid date. Also, please file a feedback about number pads. And explain the use case, what you need out of the number pad. Thanks; will check Reminders, and type feedback. The number pad is to enter year and a second field for a Date calculation; will elaborate in the feedback report. thank you! if you can, follow up here when you have the feedback request number, that would be great. Hello! Trying to find out if Scott D received the message sent last night: my watch session did not get approved, so I just submitted two feedback requests: FB10115599 (mentioning the need for a DatePicker in watchOS) and FB10115881 (mentioning watchOS8 needs a number keyboard). Thanks for your time; have a good day! Thank you for following up! Does callkit automatically grant us access to the microphone on the watch and if so are we able to access the microphone in third party apps without using callkit using a plist value for example? No, CallKit does not provide access to the microphone automatically. The user would need to grant the permission. Thank you, does this mean we can get microphone access in other applications? Previously we couldn't to my knowledge but just wondering if this has changed recently. AVAudioRecorder is available on watchOS. https://developer.apple.com/documentation/avfaudio/avaudiorecorder Looks like what I am looking for! Thank you very much for quick response and your help Of course! Thank you for you questions! I have developed an app to submit files from the apple watch to the iPhone and from the iPhone to the watch. The file transfer from the Apple watch to the iPhone is successful, but the file transfer from the iPhone to the watch is not. The iPhone is receiving its own sent file. What am I doing wrong? Ziqiao Chen from apple couldn't find the problem in the DTS Lab Session. Hi Paul. It sounds like something is going wrong there. If Ziqiao wasn\u2019t able to help, I think we\u2019ll need more information. Please follow the instructions to install logging profiles and file feedback. Include a code snippet and we\u2019ll get it figured out. I've opened a TSI Thanks I have an existing iPhone+Watch app, when I open it in Xcode it runs great, but there is a warning to update to recommended settings... when I update to recommended settings I get an error related to \"Found WatchKit 2.0 app... WhatchKit http://App.app|App.app but it does not have a WKWatchKitApp or WKApplication key set to true in its Info.plist\" and am not totally sure how to best remediate this error after letting Xcode update to recommended settings? Hi Matthew! Updating to recommended settings attempted to convert your existing watch project to a single-target app, and missed a setting. Most likely you just need to add WKApplication to 1 in your Watch App target\u2019s Info.plist. if that doesn\u2019t do the trick, we can go into a bit more detail about your project to try to figure it out. :slightly_smiling_face: Cool. I went to my Watch App targets Info.plist and added WKApplication value 1 . I then got an error saying I have both WKApplication and WKWatchKitApp and should only have one of those keys. Should I remove WKWatchKitApp from the Info.plist? let\u2019s back up a bit, in that case. Is your app storyboard-based or SwiftUI (App) Lifecycle? SwiftUI if you\u2019re using App , you need to annotate your App type with @main , e.g.: @main struct MyApp: App { var body: some Scene { WindowGroup { NavigationStack { ContentView() } } } } In my WatchApp.swift file found in my WatchKit Extension I do have my App type annotated with @main , is that what you are referring to? yep. we might need to look at your project together, then. can you sign up for one of the watchOS labs and note in your question that you were referred from the digital lounge? sure do you have a link to the labs sign up so i make sure i go to the right place? https://developer.apple.com/wwdc22/labs/ to submit your request; pick the watchOS lab on Wednesday or Friday. https://developer.apple.com/wwdc22/110495 (Wed) https://developer.apple.com/wwdc22/110496 (Fri) thanks so much! just sent a request for Friday Do you have any tips to run a bigger App with support for Complications and Siri Intents on a real Apple Watch with the debugger? Currently I have to be lucky to get it to work. Here's a couple tips! Like any wireless connection, the connection between Apple Watch and iPhone is dependent on network conditions. When debugging, Apple Watch automatically prefers a WiFi connection to the phone to improve the speed. However, that has some requirements: \u2022 The iPhone and Watch are on the same WiFi network. \u2022 Consider that some Apple Watches only work on 2.4Ghz networks. \u2022 Some networks disable peer to peer connections, which will prevent WiFi from working. \u2022 Network congestion can play a part in causing issues. \u2022 Make sure the Apple Watch is sufficiently charged (we recommend keeping the watch on the charging puck while debugging) If WiFi is otherwise unavailable, we use Bluetooth for the connection which will be slower. Thank you very much. Will check again whether I can adjust the network somehow better when debugging with a real device. Is there a way to trigger a sysdiagnose on a watch (or iPhone) on a watch simulator , sorry my question was cut There is! :sunglasses: Instructions on how to do so are available on https://developer.apple.com/bug-reporting/profiles-and-logs/?platform=watchos For the simulator, you can use xcrun from your mac. xcrun simctl sysdiagnose Thanks <@U03HHJK2FJ6> that\u2019s very helpful! How this command behaves if i have multiple simulators open simultaneously? (maybe a watch sim paired with an iphone sim) The help for simctl will give some pointers. https://developer.apple.com/forums/thread/679842|This answer from the developer forums suggests using the --uuid parameter. What happens if a WidgetKit-backed complication update is running while I request another reload of the \"widgets\"? Is the second request dropped, or enqueued to the end of the current \"reload\"? Thanks! The second is enqueued. If you add a third\u2026 we\u2019ll take a peek at the requests and decide which one to keep This is because a reload might be in-progress just as you get new data, and so your app requests another reload. It\u2019d be mean for us to ignore that second one, since you had no control over when the first reload actually happens But if you give us a third reload request\u2026, well that\u2019s greedy and we will throw one away :) There was a cool WidgetKit talk in WWDC 2021 about reloads and budgets thanks! I mean, if the user makes 3 changes in the app, that does mean that we honestly need to update 3 times I suppose you\u2019d recommend throttling? There\u2019s nothing wrong with requesting a third time, because you don\u2019t control when the reloads will happen. We\u2019ll be sure to keep one of the reload requests queued up, and when budgets allow, we\u2019ll reload your widget with whatever is the current latest content We\u2019ll do throttling too. Depends on several factors such as how frequently your widget is viewed, and there is a daily budget. https://developer.apple.com/documentation/widgetkit/keeping-a-widget-up-to-date has more Is it possible to display location associated with a workout that occurred in a third-party watchOS app that was then written to the Fitness app? Just like with workouts from the Workout app. You can record the user\u2019s route for some types of workouts; details here: https://developer.apple.com/documentation/healthkit/workouts_and_activity_rings/creating_a_workout_route/ You as the developer would be responsible for the visual representation of this data. :slightly_smiling_face: Thank you! Seems this wouldn't fit for just a pin of a location, like where a tennis workout happened. Unless I'm missing something, I can file this in Feedback Assistant. Right, Tennis is not a workout for which you can collect a route, but if you request location permissions, you might be able to capture the user\u2019s location at the time the workout was started and/or ended, which would be sufficient for a pin. Can we establish a Socket connection for server-based apps? And also - can we use QUIC for client-server connections on WatchOS? Sockets are only available for streaming audio. You can use the Network framework for the connection. Thank you for adding CallKit support on WatchOS. In case if user receives voip call on iphone, and he didn't have an watchOS version of this app on his Watch - can he receive incoming call alert or notification on watch in that case? Or we need to have an watchOSApp? No, if the user doesn't have the watchOS version of the app installed on their Watch, they would not be notified of the incoming call. So, am I right that watchOS now will support VoIP notifications for such cases? <@U03HB0UR9RU> sorry, i forgot to mention :disappointed: I am not sure if I understand the question correctly. Just to clarify, the Watch would receive the incoming call notification if the corresponding watchOS app is installed and it would not, if it is not installed. Do the new WidgetKit complications support all existing Watch faces? Or do we still need to handle the older complication families separately using CLKComplicationTemplateWhatever ? WidgetKit supports all watch faces that previously used the \"graphic\" complication templates. Luckily, we also updated a bunch of watch faces to use that \"graphic\" style in watchOS 9! Once you start using WidgetKit for complications, the system will always prefer using those complications even if your app provides complications via ClockKit. There's some great sessions this year that goes over the migration path between ClockKit and WidgetKit. Thanks! Is there a good list of which faces now use graphic complication? That's been a little tricky to piece together in the past. All the faces in watchOS 9 support graphic complications. The talk on Thursday, Go further with Complications in WidgetKit, goes into more detail about this. Hello everyone! How can we schedule local notifications to be displayed on watchOS apps with a companion iOS app and a custom notification view? I mean, if a notification is scheduled on the iPhone, does this get shown on the Watch using our custom notification view - or do we have to provide additional configuration to \"share\" notifications across the two devices? If you have a custom WKNotificationScene in your watch app, it will be used to display the notification that\u2019s automatically forwarded to the watch by the system. (IOW, the system handles coordination between the watch and the phone.) the local notification scheduled from your iPhone will automatically display on the watch when appropriate, you only need a WKNotificationScene if you want to customize the UI of the notification on the watch Thank you, Lori and Jose - that\u2019s amazing! That\u2019s the clarification I was precisely looking for. Great! :smile: Are we allowed to instantiate a CBPeripheralManager on a watch application and let the watch act as a Btle peripheral? No. The Watch app can instantiate a CBCentralManager and communicate with peripherals, but it cannot be a peripheral. I know this is likely to change with AppIntents/AppShortcuts - but is there a way to get shortcuts into the watchOS simulator so they can be tested? Currently they can only be added via iOS, and they don't sync across (presumably because they need iCloud) Hi Quentin! Can you file Feedback about this? (Or if you have, can you post the FB number here?) Will do for this specifically. It\u2019s related to this though: FB9980324 Submitted: FB10081835 If we convert our Intent definition files to App Intents, can we deploy to watchOS 8 and earlier and have existing user-created shortcuts continue working? This would best answered at the SiriKit lounge. Hello! Can anyone suggest an alternative to a DatePicker (still absent from watchOS)? Multiple wheel pickers are a possibility, especially since people would not want to type month/day/year in their Watch. Dictation is another possibility I\u2019m considering; Steppers would have too many numbers for days\u2026 thanks in advance for the input! Yeah, I think multiple pickers would be the way to go, but don\u2019t be afraid to experiment and find something that fits your app\u2019s needs. Hi Mayra! Yes, for now multiple pickers is probably the best way to implement a date picker manually. We will absolutely take your Feedback about wanting a DatePicker for watchOS, tho! In the Feedback, tell us the combinations of dates/use cases you\u2019re most interested in! Thank you! The multiple picker implementation is almost 95% done in WatchKit/UIKit, but I am moving it to SwiftUI to see if it\u2019s \u201ceasier\u201d. The main problem I am encountering is matching the checks needed to make sure days offered are correct with respect to year (leap or not) and 30/31 days for the rest of the months. I hope the new capabilities in watchOS9 and SwiftUI will help me finish! Homework accepted: will file tonight! I'd have thought the watch screen size is just about big enough for the new date picker option on swiftui, would be great to see it there Same here, but I\u2019ve been waiting for something like it for two years\u2026 I\u2019m sure it will happen right after I hit my last bug. And the official implementation will be better (of course)! This looks to be a great year for watchOS. From the simplified app target to the improvements brought through SwiftUI, I already know I'll be building some better apps this year. Thank you! That is so awesome to hear. Thanks Josh! Thank you for the presentation! Is there anyway to know what watchOS versions my users are on? My app supports watchOS6, but if no one is using that OS I would like to remove code that is specific to that older version. Thank you! I think what we really need is someone to write some analytics packages for watchOS. :wink: It\u2019s possible to query the version number from the watch itself, but iirc there isn\u2019t a way to get that info from the phone. WKInterfaceDevice.systemVersion is how to get the info. For a cross-platform / structured data solution there's also: https://developer.apple.com/documentation/foundation/processinfo/1410906-operatingsystemversion Ok thank you. Yes, that would be nice to have analytics package for watchOS! It would be nice to see the break down on AppStore connect for watchOS like you can for iOS :blush: Definitely file that Feedback!! What's the cool magnifying glass thing that's in your presentation when you are telling us where to go to find more information about SwiftUI? Is there a story there? That is a hand lens for examining rocks in the field. I got that one when I was at university getting my degree. After being a geologist for a few years, I\u2019ve spent most of my career as a software engineer. But all of my math, physics, chemistry, biology, and communications serve me well. Howdy :face_with_cowboy_hat: First of all, many thanks for the marvellous updates this year and such a great sessions! My question is regarding Portrait watch face. I've been playing around with different images and what pique my curiosity was the fact that the same image might be used with depth effect on the new iOS Lock Screen while not supporting Portrait on the watch. It is correct to assume that the reason is different screen dimensions and Depth Effect requires some padding around the subject to work? Starting with watchOS 9, the Portraits face does support photos from your entire photos library (not only those taken in Portrait mode), just like the new lock screen. However, I think you're right in your analysis of screen size/shape, padding around the subject, etc. In order for the depth effect to work, there needs to be the right amount of overlap between the foreground subject and the time and the variables you mentioned that are different between the phone and watch would affect that amount. Gotcha, thanks <@U03HHHK57K5>! Of course! will the watch ui testing be able to handle complications in clockkit as well as widgetkit soon? It's hard to automate testing for the watch but i need it the most on complications You should be able to test the SwiftUI views that make up your WidgetKit complications and parts of your ClockKit complications as you would with any other SwiftUI view! If there's some specific support you're looking for in regards to having a more widget-like environment, please file a feedback and be sure to include specific details about what we can improve on! :slightly_smiling_face: ok! yeah! i sorely need ui testing for complications (they multiply like tribbles) - will file feedback if necessary. good to know!!! :tada: Hi all! When developing for watchOS in XCode, we currently use two targets: Apple Watch and Apple Watch Extension. To be honest, I'm still pretty confused about why this is needed, and I feel like both targets use my assets some of the time. Do upcoming releases simplify this workflow into a single target and make it more similar to iOS/macOS development? hi Jan! When the original Apple Watch was released, it was pretty revolutionary\u2014but not very powerful (at least compared to the current models!). Thus in watchOS 1, the code for third-party WatchKit apps ran on iPhone, as an extension to the companion iOS app, and drove the UI on the Apple Watch. The code had to be in two targets, because they went to different devices! And the WatchKit Extension target truly was an extension. This changed in watchOS 2, when the contents of both targets moved to Apple Watch. We are so excited to simplify the developer experience this year by merging the targets in Xcode, and by offering a new Watch App target that makes your options clearer up front. There are problems with syncing data (application context, messages, ...) between the Watch and iOS simulator ( https://developer.apple.com/forums/thread/682915).|https://developer.apple.com/forums/thread/682915). Are these problems fixed with Xcode 14? We gave some guidance about this on Tuesday: https://wwdc22.slack.com/archives/C03GSLANZJT/p1654637598881109 Debugging tip that may help @Simon, based on a few years now of working on a very complex iMessage app extension. Not quite as hard as WatchKit comms but Messages simulator debugging is pretty fragile too. Thanks to a friend pushing me, I made the bulk of my complex logic able to compile cleanly on Mac and run unit tests there. (I have a document model that rivals Keynote in complexity, so encode/decode & DOM manipulation is significant - 500 tests now). That saves me massive time & so can focus on just UI testing on the devices. We use a WKExtendedRuntimeSession (SmartAlarm Mode). When the user dismisses the alarm, we want to send a request to our server. This happens in the func extendedRuntimeSession(_: WKExtendedRuntimeSession, didInvalidateWith _: WKExtendedRuntimeSessionInvalidationReason, error: Error?) delegate function. Do we need to use a network background session for that? The normal async URLsession data task does not finish? Background URLSessions are almost always the right strategy on watchOS, and in this specific case as well. :slightly_smiling_face: Can SwiftUI/Swift be use to create a WatchOS custom keyboard? If so, can you point me to the resources/doc(s) that covers this topic? Thx Hi Nolan! This isn\u2019t supported on watchOS yet, but we\u2019ll happily take a Feedback enhancement request! I just sent a Feedback enhancement request; can I expect this functionality by next week? :laughing: If we go with the new app target for an independent Watch app which doesn't contain an iPhone app, will we be able to convert it to a iPhone app with an independent watch app? This can be done! 1. Add the iOS App Target 2. On the General settings for the iOS App Target, add the Watch App to the Frameworks, Libraries, and Embedded Content list. 3. On the Build Settings, search for Bundle Identifier. Set the WatchKit Companion App Bundle Identifier to the Bundle Identifier of the Watch App. 4. Delete the iOS Stub target that was originally created when you created the Watch Only app. 5. Delete the iOS Stub from the Watch App scheme Build. Are there any changes to a complication's daily reload budget when using WidgetKit instead of ClockKit? In particular, the WidgetKit docs say that reloads aren't counted when the containing app has an active audio or navigation session. Does this also apply on watchOS? Yes, audio and navigation sessions also give you unmetered refreshes on watchOS! The timeline reload budget is roughly the same, but the Background App Refresh and runtime budgets are different; the widget extension is metered separately from your main app with WidgetKit <@U03HHJ4LRU2> Thanks! Would the watchOS complication get unmetered refreshes if the companion iOS application had an active audio or navigation session? Or only if the watchOS app had an active session? Only on the same device, there's no sharing between them. In our application we've got a screen with recently played content which pulled from repositories/api client in iOS counterpart app. But when the iOS app is not running then Watch application will never load that screen. The data exchange works through WatchConnectivity and in documentation it says \"Sending a message from a watchOS app wakes up its paired iOS app if it's reachable.\" But if we \"kill\" iOS application the messages will be never delivered (as far as we see the mentioned screen above doesn't loads). Is there any other way to wake it up? Maybe you have any advice how to debug it? I\u2019d suggest using the WatchConnectivity\u2019s Application Context to communicate the information from iOS to Watch. The Application Context is intended to be there when the app launches and the values can be updated if something changes. For more information about the different parts of WatchConnectivity and when you might use each one (and other data communication technologies that might be useful), check out this session: https://developer.apple.com/videos/play/wwdc2021/10003/ . I apologize if this is an overly basic question. When the watch series 6 came out I was very excited about the oxygen sensor. Outside of spot checks I was interested in righting and app to gather oxygen data overnight along with other parameters to provide an additional dataset for people with obstructive sleep apnea to share with their doctors. Is their a way to modify the polling rate on the oxygen sensor to pull more frequently (if so what\u2019s the limit) and what\u2019s the best way to approach this? Please excuse my ignorance I\u2019m new to Apple Watch and a novice at software development(this is more of a passion project for me not for a job/income and I\u2019m intending to have this app be free and as a conversation starter for people and their physicians who are either diagnosed or think they may have sleep apnea and thus their doctor can conducted additional questioners and sleep studies to make an actual diagnoses or reassessment) Hi, Ahmed. Unfortunately there's currently no way to do this. But I'd suggest you file a feedback requesting the ability to do this. <@U03JYB15E2G> I would also benefit from this feature. If you file a feedback, please send it to me. Thank you will do! Hi all! SwiftUI on watchOS now appears to have a keyboard with QuickType. Can we provide values for QuickType to suggest specific strings? For example, let's say the user is searching in a long list of fruits. By passing the fruits data to QuickType, we could tell the system to suggest specific fruits whose name property matches the user-typed string, so that typing \"a\" does not suggest \"and\" but \"Apple\". Right now we can pass suggestions to .searchable, but the suggestions are not shown while the user is typing. Instead, they appear as an overlay to List once the user finished typing and returns to the view. Also, .searchable does not bring up the keyboard right away, unlike TextFieldLink. Hi again <@U03JMMN8659>! Just to reiterate from yesterday when we talked in the swiftui-lounge, if you can file a feedback request with your use case that would be great. Some of us were also in the swiftui-lounge for watch specific questions Ah, hello <@U03HHHXDL03>! I will file feedback, I just want to exhaust all that is already available before requesting new features :blush: as you see this is a followup to yesterday's thread. Since then, I looked at .searchable as you suggested. Hence my question above. .searchable is nice, but it does not suggest results to the user on watchOS while the user is typing We would love to have a Feedback about that! Alright, sent as FB10135103 :slightly_smiling_face: great thanks! thank you <@U03HHHXDL03>! I am currently working on an iOS app that stores data on device in core data. What way would be best to add an optional Watch app, that could be used along side it. (Prefer not to use cloud storage if possible). Similar to how workout functions, but the workout could be started on either iPhone, or Watch. Thanks for keeping the questions open for devs working GMT time :) Hi Steven! I\u2019m curious about why you want to avoid using cloud storage. If you used Core Data with CloudKit, then you\u2019d get the data synchronization there. This session might be helpful in looking at various strategies: https://developer.apple.com/videos/play/wwdc2021/10003/ . Sometimes you might need to combine strategies to get the right experience. Hi Anne, thanks for replying. I am hoping as far as possible to allow the user complete control over their data (for privacy), So I was working on the assumption that the most private is on device, then iCloud. So in my mind I had the idea of allowing the user to select either to store data only on their device, or on iCloud. I assumed that with Watch connectivity the data would stay on the Phone, with the Watch having access to it. Thanks for the link to the video, I will going and watch it now. Thanks again. So many of us are a lot more conscious now about how our data is being stored and shared, so it\u2019s great that you\u2019re considering this so strongly. :heart: It\u2019s a balancing act. You want to protect everyone\u2019s privacy and also ensure that they don\u2019t lose data should something happen to their phone. Options are always good. I hope the video helps, and definitely read about privacy and security with iCloud. Once you know all the options, you can make the best choice to provide the level of privacy you want to deliver. That video was great, just at the right level too. I see what you mean about maybe needing to think about combining strategies, and also the trade off between privacy and usability. I want to expose a TextField to users on my Watch app but only allow numeric entry. Can I show just a number pad keyboard for input? We do not support numeric keyboards at this time. Could you file a feedback with your use case? Hello! I posted a feedback about this same thing yesterday (FB ), and had been referred to 3 months ago in the Developer Forum. The developer that typed that comment said he had filed a report about the lack of number keyboard in watchOS 8 in August 2021 under the number FB9568384 (searched but couldn\u2019t find it as active). Thanks for filing! Duplicates are VOTES. :slightly_smiling_face: Hi, I enjoy the exciting WWDC. I have one question about WatchKit. Can you tell that you are wearing a Apple Watch? I can know the charging status with WKInterfaceDeviceBatteryState. How can the app tell if a user wears an Apple Watch on their wrist? Thank you! Hi Yoichi! It would be helpful to understand the use case better before responding. If threads are locked by the time you see this, can you please file Feedback and describe what you\u2019re trying to do? watchOS does support LocalAuthentication , so if your case is about, say, requiring that the watch be unclocked or that the user enter their passcode before showing data, you should be good. I've a game design which relies heavily on the digital crown (conceptually). Am I right in thinking that WKCrownSequencer will allow me to directly capture the rotations so I can map them into manipulating the state of the game, rather than it only being able to change focused pickers? (It's been a fair while since I dived into WK & gave up as early versions were too constraining) You're correct that you can use the digital crown to drive much more than pickers. But we would recommend looking at the https://developer.apple.com/documentation/swiftui/form/digitalcrownrotation(_:)|digitalCrownRotation in SwiftUI instead. The new ShareLink to open a share sheet on watchOS (finally!) is great. Is there a way to make my app a sharing destination on the Watch, like an equivalent to Share and Action Extensions on iOS? Hi Christopher, I'm so glad you like the new ShareLink API! There is not a way to make your app a sharing destination in watchOS 9.0, but please file Feedback with some details about your use case! Question about subscriptions on Apple Watch. I would like to have a single subscription to unlock content in my app across Apple devices. According to this ( https://developer.apple.com/support/universal-purchase/)|https://developer.apple.com/support/universal-purchase/) support doc: \"To offer your app on iPhone and Apple Watch, create an iOS app with a watchOS counterpart in Xcode and upload the apps to App Store Connect from the same Xcode project. Watch-only apps cannot be part of a universal purchase . But during the WWDC22 talk about multiplatform apps: \"Both my iOS and macOS app products use the same bundle identifier by default, which is awesome, because that means when I publish them to the App Store, they will be made available for Universal Purchase. So folks who buy my iOS app will also get my Mac app automatically .\" So, question: can we have a single subscription for content in an iOS app, and an independent watchOS app? :face_with_monocle: Hi Jan. Yes, your watch app is considered part of your iOS app for App Store purposes, but it can still be independent. Check the Runs Independently of iOS Companion App box in the Xcode project editor to make your watchOS app independent. Thank you <@U03H96N55U5> :pray: so it does not matter (for universal subscription) if the watch app targets have their own bundle identifiers that differ from the iOS app? If I recall correctly, the watchOS app should have the same root identifier as the iOS app. E.g., com.fancyapps.mymealtracker (iOS) and com.fancyapps.mymealtracker.watchkitapp (watchOS) yep, it does Multi-line text support in TextField is very exciting, but doesn\u2019t seem to work in watchOS 9 beta 1. Will it be supported? And if so, will the keyboard editor support new lines? Currently all newlines are removed from any existing text (collapsing it to a single line) and there\u2019s no way to type a return. <@U03HVF46TEJ> Could you please file a feedback request with your use case? and the details about new lines here as well? Sure, will do. Thanks! Checking an analogy here. I've been working for years on an iMessage app extension which also has fragile debugging on simulator. Something that helps hugely is I built a unit test suite that can run on Mac so all my complex data handling logic gets heavily exercised in fast builds there, leaving the on-device testing to being UI-only. Is this a useful strategy for WatchOS? Kinda a comment-as-question that might help @Simon & others with the sync issues, and am asking for my own benefit as I restart a Watch project. This is certainly a reasonable analogy (and not even where something is fragile). The more you make your model testable and test that directly, the more efficient you can make your testing. You can also potentially keep your quality high (assuming your tests are well-designed, of course). :slightly_smiling_face: UI tests will take longer to develop and run. Having good model tests allows you to focus your UI tests on testing the actual UI. I have a complex document model for encoding games that does things like particle emitters in 1/20th the Apple formats. Lots of bit fields. You bet my tests are exhaustive 500 of them, about 25% of code is test code when I'm working on the core model! TDD Purists will disagree with some of my torture tests though but I can refactor with impunity :smile: When using WatchKit and the ExtensionDelegate, we could access the root interface controller to change the snapshot controller for instance. Is there a way to do this using SwiftUI lifecycle? Hi <@U03JPJ277SQ>! New in watchOS 9 is the https://developer.apple.com/documentation/swiftui/backgroundtask/|BackgroundTask API, which has a snapshot task type. When you get the task, you can replace the root view of your app when a snapshot is requested from the system. In watchOS 8 and previous, you can use the https://developer.apple.com/documentation/swiftui/wkextensiondelegateadaptor|WKExtensionDelegateAdaptor to implement handle(_ backgroundTasks: Set&lt;WKRefreshBackgroundTask&gt;) One more note: If you\u2019ve updated your app to the newest recommended settings from Xcode, you\u2019ll no longer have a watchOS extension and you can use https://developer.apple.com/documentation/swiftui/wkapplicationdelegateadaptor|WKApplicationDelegateAdaptor instead of WKExtensionDelegateAdaptor. Oh I didn\u2019t know that. Thanks. Can you use SpriteKit on WatchOS with SwiftUI? Is SpriteKit limited on Watches? I can't find any listed - most stuff goes back to WatchOS2 or 3. You can embed SpriteKit into your SwiftUI views using https://developer.apple.com/documentation/spritekit/spriteview|SpriteView . Thanks, partly answering my own question - https://developer.apple.com/documentation/spritekit/sknode/controlling_user_interaction_on_nodes neatly side-steps mentioning WatchOS > SKNode subclasses UIResponder in iOS and tvOS, and NSResponder in macOS, allowing nodes to respond to user interaction events such as touches and mouse movements are there any other significant missing bits of SpriteKit on Watch? Thanks for extending the ask questions workflow (left town right after WWDC \ud83e\udee0). Are there any conceptual limitations to building an app with only 1 input method (one finger) and 40mm-45mm screens? With Apple Watches getting more and more independent from iPhones, how far should we take apps (gaming, social media, etc.)? Also, watchOS supports SceneKit but not Metal (at least not in the Xcode template), is there a reason for this conceptually or just technically? Hi Ethan! Lots of developers are doing pretty amazing things on Apple Watch\u2014I\u2019ve played some games that were so immersive that I had to rest my arm on a table to keep my shoulder from getting tired. :slightly_smiling_face: Speaking of your arm getting tired tho, that is a thing, and he primary use case for watchOS apps is quick interactions and glanceable experiences. As to one-finger interactions, Assistive Touch actually makes it possible to interact with an Apple Watch without having any fingers at all! Awesome! Makes the platform feel full of potential discovery. I haven't developed much for watchOS but with SwiftUI getting better I'm excited In terms of support for Metal, please do file feedback! Assistive touch might be one of my favorite Apple Watch features. Absolutely incredible! Will do! I really want Metal to come to the watch because the Ray traced Unity Lights showed how capable the watch can be That watch face really is incredible! I already ask a similar question, but can I use the NSUbiquitousKeyValueStore to sync data between different Complications / Widgets on multiple devices? Example: I tell Siri on my phone that I ate a Donut. I store then an Integer in NSUbiquitousKeyValueStore with the Amount of Donuts I ate. On my Watch I have a Complication that should show the amount of Donuts. Would NSUbiquitousKeyValueStore a good approach to sync data between Siri Intents and Watch Complications? No. KVS is for syncing configuration/preferences, not data. From what you\u2019re describing, you don\u2019t need data transfer at all. Your app can store the fact that you ate the donut (your intent could be invoked by something like, \u201chey Siri, tell MyDietTracker that I ate a donut,\u201d and MyDietTracker records that you ate the donut. The complication can be updated based on that data when the app is running, or using background app refresh. We want to use fall detection on the Watch to be informed when a fall occurs. Will the system continue to handle fall events natively and call the rescure services when we register as a fall detection delegate? Or are we from then on alone responsible for handling the fall event? Hi, Simon. I have verified that if you use CMFallDetectionManager to observe fall events this does not affect the default system behavior. Your app will be briefly woken up in response to fall events so that it can do a small amount of work, like sending a notification, or something similar. You can also request recent fall event samples from HealthKit. Keep in mind that you will need an entitlement to use CMFallDetectionManager . https://developer.apple.com/documentation/coremotion/cmfalldetectionmanager Here's the documentation for the HealthKit fall detection samples: https://developer.apple.com/documentation/healthkit/hkquantitytypeidentifier/1615592-numberoftimesfallen For that, you need to authorize HealthKit access to that sample type. Testing SwiftUI on watchOS - is there documentation or a video you would recommend for getting started with this? Thank you! The best way to test your SwiftUI app on watchOS is to make your model layer testable and write thorough tests for that with XCTest. This will allow you to concentrate you UI testing on Watch to the UI itself. You can build a test suite for your model layer and run it to catch any regressions. You can find documentation for XCTest here: https://developer.apple.com/documentation/xctest , and there are a lot of great WWDC session videos about designing and writing tests. Cool thank you. I have my model in a swift package and wrote tests for it with XCTest. Just need to figure out UI tests now ! :blush: Is there a component, or a way to animate marquee style (horizontal) scrolling for text, like in the Now Playing UI? Back in SwiftUI v1 and watchOS 6 I couldn\u2019t figure it out, has something been added since? There is nothing specifically built for this exact use-case. But I've built similar things using Canvas along with a TimelineView and an TimelineAnimationSchedule Ok, thanks for the info. I\u2019ll have a look at those newer APIs to see if I can implement it myself. Or you could use SpriteKit as in https://www.hackingwithswift.com/forums/swift/opening-credits-crawl-like-star-wars/287 (at the end) Dear WatchOS team: It is so easy to download a WatchOS app through the Apple-watch app on the iPhone as a companion app. Just great. But when our team tried to download an App direct through the Apple Watch Appstore the experience was quite different. Searching for the App, switching to Passcode mode, trying to enter the AppleID - not what we want to customers to endure. Any other way to download an independent Apple Watch App? Watch-only (and independent Watch) apps can be found on the iOS App Store, and installation can be triggered from there, yes. I honestly install my Watch apps from the App Store on Apple Watch, tho. At this point I have over 90 third-party watchOS apps on my watch; most of them were installed automatically because I have the companion iOS app, but I often search for apps directly on my watch because it\u2019s pretty fast to download from there\u2014and sometimes I only want the watchOS app. For example, I only ever use Insight Timer on my watch, so I install it from the App Store on Apple Watch. :slightly_smiling_face: Thanks for the insight!:slightly_smiling_face::de: In watchos 7, the CoreBluetoooth framework worked great in the background, allowing for a constantly updated app interface on the watch. Especially if this application communicates via BLE with a peripheral that has more than 20 characteristics. There are problems with the BLE connection, and then the application has to connect to the periphery every time and read the initial values of the characteristics. This process could take up to 10 seconds for my app. Starting with watchos 8, the application on the watch, going into the background, stops communicating with the periphery. How will the CoreBluetooth framework change in the future to properly design my application on the watch? Hi Sergey! Starting in watchOS 9.0, bluetooth peripherals that your app connects to will stay connected even when the application is suspended. In addition, there are some nifty new features like Timely Alerts which will wake your application in the background if a characteristic marked as Notify on Change is changed! I recommend watching the \"Get Timely Alerts from Bluetooth Devices on watchOS\" video, new this year to learn about all the details of the new capabilities! https://developer.apple.com/videos/play/wwdc2022/10135/ :+1: What's the best way to sending fast messages with less enery using with WatchConnectivity? Does sending as Data instead of Dictionary performs better? Do you recommend sending a big payload as smaller parts or at once? Also is there any payload size limit? Hi, Emre. There are a couple of parts to this question, but I think the most important part is whether it's better to do a large amount of work spread out over a longer period of time or to do it all at once. There are a couple of great WWDC talks that address this. One that really helped me to get into the right mindset about this was this one: https://developer.apple.com/videos/play/wwdc2015/707/ Doing a task on a mobile device requires spinning up whatever hardware is needed to do the task. That can be a CPU, or wireless radios. All of those things require power to run, and turning them on and off frequently is a very quick way to burn down the user's battery. So the best thing to do is to do as much work as you can in as short a time as possible and then let the device go back to sleep. There's also a great talk from last year about watch connectivity: https://developer.apple.com/wwdc21/10003 That one was done by <@U03HL004760>! Thank you! :pray: With the new sleep stage? is it possible to monitor and reading from realtime in extending runtime of what stage the user is in? Or all these data are still calculated and write to healthkit when the user wake up It is not possible to monitor sleep stage in realtime. I would suggest you file a Feedback. Really would support sleep stages in real time! Is there a way to send a message from the iPhone application to wake up the associated AppleWatch application ? Note: the AppleWatch application is already started, I just need to wake it up. Hi Frederic! What do you mean that the application is already started, and you just need to wake it up? Can you elaborate on your use case? Yes. The application is a workout application with a workout in progress. I just would like the iPhone application to send it a message in order to update some data that are only available on the iPhone so that when the user raise his/her hand, he/she can see the updated data immediately. OK, so it\u2019s not really about \u201cwaking\u201d the watch or the app (it\u2019s already running); you\u2019re looking for data to be transferred immediately. Yes, but when the Watch application is in the inactive state (watch screen is off). OK then no, there is no way for you to wake the screen programmatically. You can still send data over, and the user will see it when they raise their wrist (if that\u2019s how you\u2019ve designed the UI, of course). I\u2019m assuming you\u2019ve already seen <@U03HL004760>\u2019s great talk https://developer.apple.com/videos/play/wwdc2021/10003/ :slightly_smiling_face: Thanks! I did miss this WWDC21 video and from the code examples it seems that i will be able to implement my use case. thanks again! How can I make sure that my CoreData (CloudKitContainer) objects sync over correctly from watchOS to iOS? When does the watchOS app start the sync? Or is CoreData with iCloud maybe not the best way to sync Objects? It\u2019s one of the most recurring complaint from my users at the moment. Sorry if it\u2019s not detailed enough but I\u2019m a bit lost, A modern example of CoreData support between watchOS and iOS would be amazing :smile: Hi Hidde! So nice to meet you in person on Monday. :blush: CoreData sync via iCloud is pretty much automatic, but keep in mind that when syncing happens is based on system and network conditions. Usually data is available pretty quickly, but sometimes it can take longer. It\u2019s helpful to design your experience with the expectation that data might not arrive immediately. I can relate to the question. A lot of my support questions were related to \u2018stale\u2019 data (which would correct itself in just a few seconds upon launch of the Watch app). One thing that helped is communicating proper expectations to my users. Overall, CoreData+CloudKit was super convenient for both myself and my users. I just had to temper their expectations: Yes, your data is safe. You just need to be patient if you\u2019re doing a fresh launch of the app. Yep! And it is possible to design experiences that don\u2019t leave users waiting and wondering. :slightly_smiling_face: It\u2019s just a little different than designing iOS experiences, which more developers have experience with. :joy: I will probably relate to this soon, since I want to expand my app to watchOS. <@U03HMDG985D> could this be solved with a loading indicator (e.g. progress view) while the data is being loaded? Can we even detect that data after a fresh launch is still being loaded? If you\u2019re doing the CloudKit yourself, I think you could. If you\u2019re using NSPersistentCloudKitContainer , there is no way to know when the syncing begins (though you might be able to tell when it ends via the associated events notifications). In my case, I\u2019m using the persistent cloud kit container, so I can\u2019t provide that UI to my users. (I tried; it wasn\u2019t consistent.) Which is where informing/instructing my user was more valuable. Thanks, that's very insightful. I am using NSPersistentCloudKitContainer. So I guess I will go with a simple disclaimer. Can we even detect that the user already has some data in CloudKit, so that the disclaimer will not be shown to users who are totally new to the app? Wouldn\u2019t that map directly to the presence of data in your persistent container? I suppose. But if I just set up a disclaimer to show once container receives data, the disclaimer will also appear if a new user creates some data what is the data that y\u2019all are waiting for? fwiw I was thinking along the lines of relying on data that\u2019s already on the watch (because it\u2019s generated by the watch, or stored there) whenever possible, and augmenting with data from other sources when it becomes available. this isn\u2019t always possible, of course; if you have a reminders/task or grocery list app across platforms, the user is going to expect those to sync, and in that case spinners and other syncing UI might be approrpriate. And even if your app has no local data on first launch and you show your \u2018new user\u2019 UI, once your local Core Data store syncs up, you could update your UI. So the disclaimer would go away. And maybe the disclaimer can include wording that, if they have used the app on another device with the same Apple ID, that they can expect to see data soon. Hi! :wave: We\u2019re using HKLiveWorkoutBuilder for tracking indoor swimming. But the workoutBuilderDidCollectEvent for the distance datatype is coming in delayed and resulting in an inaccurate data. Usually 2 or more laps of distance (~50m) are missing. Could you maybe help me with this issue? Hi, <@U03J7AWBLDT> and I'm sorry you're running into trouble with your swimming app. This sounds very much like something the Health team would like to look at. Can you please file feedback about this? Thanks! How can we hook into assistive touch? (Sorry if covered in videos, due 16hr time diff & being busy elsewhere haven't seen them yet). I've been in love with the feature since a staffer at local Apple Store was wizzing through but I can't find an API, just UIKit stuff. You can! The \u201c https://developer.apple.com/videos/play/wwdc2022/10052/|What\u2019s New in SwiftUI \u201d video shows how you can use the accessibilityQuickAction API to configure what will happen when the user makes an assistive touch gesture. Also, here's the documentation link for this. https://developer.apple.com/documentation/swiftui/view/accessibilityquickaction(style:content:) It's a new API available for watchOS only in watchOS9. The user does need to have accessibility quick actions enabled via Settings -> Accessibility -> Quick Actions -> On. It's also limited to detecting the double pinch gesture only. I'd also found last years's video https://developer.apple.com/videos/play/wwdc2021/10223/ but no matching docs, was looking at the code. AssistiveTouch works with the same accessibility infrastructure as VoiceOver. Following best practices for https://developer.apple.com/documentation/swiftui/view-accessibility|Accessibility in SwiftUI will make your app work well with AssistiveTouch. Is it possible to know that the Watch App will exceed the 75mb limit before it has been submitted and processed in App Store connect? I don\u2019t have a great recommendation for how to check this in advance. But in general, are you having issues with this when submitting your app? I know we all want to be good Watch app citizens and some devices are quite limited on storage. If you\u2019re using a lot of storage, people might get frustrated and uninstall your app. Have you considered ways to reduce the size of your app and still deliver a great experience? I know this is a constant challenge. if you make the decision to make a Watch only app - does that mean you have to distribute it separately or can it still be sold with an iPhone app. Functionally they could be used separately but buying the iPhone app would provide the watch app too? Is that the definition of \"companion app\"? Hi Dan! You should only make a watch-only app if you don\u2019t have an iOS app with the same or related functionality already. Functionally a watch app that can run independently of its iOS companion app operates as if it is watch only if its companion is not installed, as in my Insight Timer use case from an earlier thread (I only install Insight Timer on the watch, and never install the companion iOS app). that\u2019s a lot of words, I realize. :stuck_out_tongue: If you have an iOS app, and your watchOS app offers the same or related functionality, they should be considered \u201cthe same app\u201d and submitted together. In iMessage apps, another restriction is lack of access to IAP for an extension-only app. So you're strongly encouraged there to have a \"real\" parent app. hi Andrew, that\u2019s unrelated Watch apps are not extensions of iOS apps. are you saying Watch-only apps have store IAP access? and Watch apps do support IAP! yes! yay! Thanks for clarifying the architecture - I started with WatchOS when they were much more tightly coupled. yeah! I have a little bit on the history in a previous thread; let me find it. Thanks! I think watch-only would be rare. https://wwdc22.slack.com/archives/C03GSLANZJT/p1654725594769139?thread_ts=1654725447.910529&cid=C03GSLANZJT Watch-only is less common, certainly, but there are still several hundred watch-only apps in the App Store. :slightly_smiling_face: Ah yes - I do have one called Geneva Moon that shows you moon phases and can locate the moon with the compass! Very fun Yeah! Watch-only apps are also a great way to learn SwiftUI, especially for students. You can create small and useful applications without having to worry about also building an iOS app. How does the connection to Xcode work? I don\u2019t think I\u2019ve tried since Xcode 12 - but it seemed like getting a real device to connect was difficult. It still has to go through a paired iPhone, right? yes, plug in the iPhone via lightning cable (don\u2019t try to do WiFi debugging on the phone and then also debug the watch app!) you don\u2019t want to do more hops than you need to. :slightly_smiling_face: Matthew had some good advice here: https://wwdc22.slack.com/archives/C03GSLANZJT/p1654634872820659?thread_ts=1654634863.147179&cid=C03GSLANZJT I\u2019m looking forward to Xcode 14 connecting to my iPhone better - I had to do my share of rebooting to get my iPhone connected with Xcode 13. And the AppleTV is easier to deploy to through TestFlight than directly across my house :dizzy_face: Thanks so much for those tips\u2026 now that I\u2019m thinking about it - I was using a S2 watch so that wasn\u2019t probably very quick either. I\u2019ll have to try with Xcode 14 and my S7! :rocket: Yes! That should help. Series 7 supports 5Ghz WiFi as well. :crossed_fingers: you should have a better time. omg yes please use a Series 6 or later if you have one! It got to be a challenge to see how long my S2 could last - it was something like 5 years! I still have a Series 2 that I use to test various scenarios, but I definitely don\u2019t debug on it. :slightly_smiling_face: Back when developing against watchOS 6, Watchdog killed my app after the binary grew too big. Removing some dependencies solved the issue. But I want to ask: is this size limit documented somewhere, and has it been raised in the last few releases? <@U03JFUBCW85> Could you elaborate on what issue you see? What are the error messages / symptoms when the problem occurs? or... rather, what were they when you were hitting the issue in watchOS 6. Spoiler alert: It\u2019s not the binary size that\u2019s the problem here. :slightly_smiling_face: So this was happening during the watchOS 6 beta period. Haven\u2019t tested out the limit since. As I remember it, Watchdog killed the app, but maybe I\u2019m misremembering and it was happening in App Store Connect? Like was just mentioned here: https://wwdc22.slack.com/archives/C03GSLANZJT/p1654795299375599 Not sure anymore I\u2019m afraid. So Sebastian is asking about the size limit when submitting to the App Store (because that\u2019s where the 75MB limit applies). I see, that\u2019s probably what I was hitting as well then. I\u2019m a bit hazy on the details, as it\u2019s been so long. give it another whirl! :slightly_smiling_face: I will, for sure. Thanks for the info. These lounges are really great! What's the best way to wake up iOS app from Watch app to keep it running? My iOS app is connecting to a server by socket and keep sending real time data to the Watch app. Is sending short messages to iPhone with an interval good way? Thank you Hi, Emre. Sending a message via WatchConnectivity from your Watch App will wake the iOS app. However, you can't due this forever, as the messaging will be limited eventually. Can you tell us a bit more about what you're trying to do here? Sorry to say we have to wrap up now, so we\u2019ll be turning threading off.","title":"watchos"},{"location":"wwdc22/watchos-lounge.html#watchos-lounge-qas","text":"","title":"watchos-lounge QAs"},{"location":"wwdc22/watchos-lounge.html#by-feetiki","text":"","title":"by FeeTiki"},{"location":"wwdc22/watchos-lounge.html#how-should-we-update-our-complications-going-forward-are-complication-updates-from-iphone-still-supported","text":"Nothing has changed with the ClockKit API in this release. Your app can request timeline reloads via CLKComplicationServer.sharedInstance.reloadTimeline(for: ...) And a similar mechanism exists if you move to WidgetKit based complications WidgetCenter.shared.reloadTimelines(ofKind: ...) I see, thanks! So I assume the data flow is that it\u2019s still up to the Watch app to react from transferCurrentComplicationUserInfo , store the data somehow, and trigger an update \u2014 in watchOS 9 or later using the WidgetKit API, in earlier watchOS versions using the CLK API, right? If you want to simplify, you can call both and the system will only reload your widgets if you\u2019re running on watchOS 9 or later and have adopted WidgetKit based complications. After migrating to WidgetKit, your ClockKit based complications will no longer be run.","title":"How should we update our complications going forward? Are complication updates from iPhone still supported?"},{"location":"wwdc22/watchos-lounge.html#what-is-the-best-way-to-log-watch-applications-especially-in-production-builds-installing-logging-profiles-can-be-problematic-for-non-tech-users-and-only-lasts-for-a-couple-of-days","text":"Hi Enrico! Are you asking what is the best way to capture logs from your customers when filing Feedback with Apple, or what is the best way to capture logs for your own use? The second case is what i\u2019m interested in most. The point is what method would you advise to receive logs from builds that have no debugger attached Hi, Enrico. As far as I know, logging profiles are the only way to get logs for your application on production builds. But maybe somebody else knows of something obscure that I don't! That\u2019s the only way I know of also! In my own apps, I have a logging system that then pushes logs over WatchConnectivity back to the phone with a debugging view. Thanks Brian. Is it true that they unfortunately expire after a few days? Any more detail on that? The profiles expire, but that doesn\u2019t stop you from having your own logging. :slightly_smiling_face: I believe the intention is to expire the profiles because often a user will forget they have it enabled and that can have power and performance implications. If this helps at all, here is my code https://gist.github.com/aaronpearce/ae5a287d99fce09b4507e708d6aa329e Uses a watchconnectivity library called Communicator.","title":"What is the best way to log Watch applications, especially in production builds? Installing logging profiles can be problematic for non-tech users and only lasts for a couple of days"},{"location":"wwdc22/watchos-lounge.html#if-we-go-for-watchos-9-only-is-it-then-possible-for-watch-series-3-users-to-load-the-old-binary-of-the-app-from-the-watch-app-store","text":"Hi Sebastian. I\u2019m not positive, but I don\u2019t think this will work reliably, especially if your watch app is independent. Is there an API you want to use that would require you to go minOS 9.0? I mean if we update our app to watchOS 9 only in autumn, would it be possible for the Watch Series 3 users to download an old Version of our Watch App that supports watchOS 8, like it is on the App Store, where users with older devices can still download the latest supported Version of an App? Yep, I understood the question. :slightly_smiling_face: My question back to you was: is there a reason you want to go watchOS 9 only? And in a related question, is there anywhere besides Google search - where we developers can get meaningful statistics on WatchOS (or iOS/iPadOS) adoption rates? We would like to go for watchOS 9 only to get rid of the 32bit Binary for Watch Series 3. This increases the App size of the iOS app or am I wrong? watchOS apps are already thinned for the platform they\u2019re delivered to, so this isn\u2019t something you need to worry about! both the architectures and the assets are thinned for the device class and size. :slightly_smiling_face: But the Watch Binary is still a part of the iOS Binary right? If a user with a Series 3 downloads the iOS App is then in the Binary a 32bit Version of the Watch App only or is the 64bit Version also included? The watchOS binary is not downloaded to the phone if the paired watch is running watchOS 6.0 or later. It does not make your iOS app any bigger. Ahh good to know thank you :raised_hands:","title":"If we go for watchOS 9 only is it then possible for Watch Series 3 users to load the old Binary of the App from the  \"Watch App Store\"?"},{"location":"wwdc22/watchos-lounge.html#can-you-point-me-in-the-direction-of-the-share-sheet-documentation-is-it-swiftui-only","text":"The documentation is here: https://developer.apple.com/documentation/swiftui/sharelink , and there will be a related session available tomorrow: Meet Transferable. ShareLink is SwiftUI only.","title":"Can you point me in the direction of the Share Sheet documentation? Is it SwiftUI only?"},{"location":"wwdc22/watchos-lounge.html#is-the-size-for-a-watch-app-still-limited-to-75mb","text":"Yes it is.","title":"Is the size for a Watch App still limited to 75mb?"},{"location":"wwdc22/watchos-lounge.html#hello-there-is-no-datepicker-for-watchos-could-creating-a-list-with-sections-be-an-alternative-thanks-for-your-time","text":"Can you please clarify what you mean by \"List with sections\"? A list with sections sounds interesting! You can also use multiple Pickers. Please do file a feedback about DatePicker (or any other enhancements you may think of) Yep, a date picker is definitely something that we've been missing for a while. Thank you Brian D and Matthew K. I had almost built the app with 3 separate pickers in UIKit, but had problems keeping track of the different number of day requirements (we don\u2019t want anyone saying \u201cJune 31\u201d. I have a watchOS lab tomorrow, but wanted to trim down my question (15 min is a short time for the messy situation I \u2026 I am in (but I will definitely take it!) Moving from UIKit to SwiftUI gave me different problems\u2026 life is interesting! By \u201cList with Sections\u201d I meant a list where I could send user to separate detail views for the entries, or list elements (apologies for the incorrect term) that could hold TextFields. But that is not great in watchOS. BTW: what happened to number pads in watchOS 8? There's a couple examples of date pickers in some of our watchOS apps. For example, Reminders has a version of a date picker when editing. They reset the day picker's value when the month picker changes, to prevent a user from picking an invalid date. Also, please file a feedback about number pads. And explain the use case, what you need out of the number pad. Thanks; will check Reminders, and type feedback. The number pad is to enter year and a second field for a Date calculation; will elaborate in the feedback report. thank you! if you can, follow up here when you have the feedback request number, that would be great. Hello! Trying to find out if Scott D received the message sent last night: my watch session did not get approved, so I just submitted two feedback requests: FB10115599 (mentioning the need for a DatePicker in watchOS) and FB10115881 (mentioning watchOS8 needs a number keyboard). Thanks for your time; have a good day! Thank you for following up!","title":"Hello! There is no DatePicker for watchOS. Could creating a List with sections be an alternative? Thanks for your time."},{"location":"wwdc22/watchos-lounge.html#does-callkit-automatically-grant-us-access-to-the-microphone-on-the-watch-and-if-so-are-we-able-to-access-the-microphone-in-third-party-apps-without-using-callkit-using-a-plist-value-for-example","text":"No, CallKit does not provide access to the microphone automatically. The user would need to grant the permission. Thank you, does this mean we can get microphone access in other applications? Previously we couldn't to my knowledge but just wondering if this has changed recently. AVAudioRecorder is available on watchOS. https://developer.apple.com/documentation/avfaudio/avaudiorecorder Looks like what I am looking for! Thank you very much for quick response and your help Of course! Thank you for you questions!","title":"Does callkit automatically grant us access to the microphone on the watch and if so are we able to access the microphone in third party apps without using callkit using a plist value for example?"},{"location":"wwdc22/watchos-lounge.html#i-have-developed-an-app-to-submit-files-from-the-apple-watch-to-the-iphone-and-from-the-iphone-to-the-watch-the-file-transfer-from-the-apple-watch-to-the-iphone-is-successful-but-the-file-transfer-from-the-iphone-to-the-watch-is-not-the-iphone-is-receiving-its-own-sent-file-what-am-i-doing-wrong-ziqiao-chen-from-apple-couldnt-find-the-problem-in-the-dts-lab-session","text":"Hi Paul. It sounds like something is going wrong there. If Ziqiao wasn\u2019t able to help, I think we\u2019ll need more information. Please follow the instructions to install logging profiles and file feedback. Include a code snippet and we\u2019ll get it figured out. I've opened a TSI Thanks","title":"I have developed an app to submit files from the apple watch to the iPhone and from the iPhone to the watch. The file transfer from the Apple watch to the iPhone is successful, but the file transfer from the iPhone to the watch is not. The iPhone is receiving its own sent file.  What am I doing wrong? Ziqiao Chen from apple couldn't find the problem in the DTS Lab Session."},{"location":"wwdc22/watchos-lounge.html#i-have-an-existing-iphonewatch-app-when-i-open-it-in-xcode-it-runs-great-but-there-is-a-warning-to-update-to-recommended-settings-when-i-update-to-recommended-settings-i-get-an-error-related-to-found-watchkit-20-app-whatchkit-httpappappappapp-but-it-does-not-have-a-wkwatchkitapp-or-wkapplication-key-set-to-true-in-its-infoplist-and-am-not-totally-sure-how-to-best-remediate-this-error-after-letting-xcode-update-to-recommended-settings","text":"Hi Matthew! Updating to recommended settings attempted to convert your existing watch project to a single-target app, and missed a setting. Most likely you just need to add WKApplication to 1 in your Watch App target\u2019s Info.plist. if that doesn\u2019t do the trick, we can go into a bit more detail about your project to try to figure it out. :slightly_smiling_face: Cool. I went to my Watch App targets Info.plist and added WKApplication value 1 . I then got an error saying I have both WKApplication and WKWatchKitApp and should only have one of those keys. Should I remove WKWatchKitApp from the Info.plist? let\u2019s back up a bit, in that case. Is your app storyboard-based or SwiftUI (App) Lifecycle? SwiftUI if you\u2019re using App , you need to annotate your App type with @main , e.g.: @main struct MyApp: App { var body: some Scene { WindowGroup { NavigationStack { ContentView() } } } } In my WatchApp.swift file found in my WatchKit Extension I do have my App type annotated with @main , is that what you are referring to? yep. we might need to look at your project together, then. can you sign up for one of the watchOS labs and note in your question that you were referred from the digital lounge? sure do you have a link to the labs sign up so i make sure i go to the right place? https://developer.apple.com/wwdc22/labs/ to submit your request; pick the watchOS lab on Wednesday or Friday. https://developer.apple.com/wwdc22/110495 (Wed) https://developer.apple.com/wwdc22/110496 (Fri) thanks so much! just sent a request for Friday","title":"I have an existing iPhone+Watch app, when I open it in Xcode it runs great, but there is a warning to update to recommended settings... when I update to recommended settings I get an error related to \"Found WatchKit 2.0 app... WhatchKit http://App.app|App.app but it does not have a WKWatchKitApp or WKApplication key set to true in its Info.plist\" and am not totally sure how to best remediate this error after letting Xcode update to recommended settings?"},{"location":"wwdc22/watchos-lounge.html#do-you-have-any-tips-to-run-a-bigger-app-with-support-for-complications-and-siri-intents-on-a-real-apple-watch-with-the-debugger-currently-i-have-to-be-lucky-to-get-it-to-work","text":"Here's a couple tips! Like any wireless connection, the connection between Apple Watch and iPhone is dependent on network conditions. When debugging, Apple Watch automatically prefers a WiFi connection to the phone to improve the speed. However, that has some requirements: \u2022 The iPhone and Watch are on the same WiFi network. \u2022 Consider that some Apple Watches only work on 2.4Ghz networks. \u2022 Some networks disable peer to peer connections, which will prevent WiFi from working. \u2022 Network congestion can play a part in causing issues. \u2022 Make sure the Apple Watch is sufficiently charged (we recommend keeping the watch on the charging puck while debugging) If WiFi is otherwise unavailable, we use Bluetooth for the connection which will be slower. Thank you very much. Will check again whether I can adjust the network somehow better when debugging with a real device.","title":"Do you have any tips to run a bigger App with support for Complications and Siri Intents on a real Apple Watch with the debugger? Currently I have to be lucky to get it to work."},{"location":"wwdc22/watchos-lounge.html#is-there-a-way-to-trigger-a-sysdiagnose-on-a-watch-or-iphone","text":"on a watch simulator , sorry my question was cut There is! :sunglasses: Instructions on how to do so are available on https://developer.apple.com/bug-reporting/profiles-and-logs/?platform=watchos For the simulator, you can use xcrun from your mac. xcrun simctl sysdiagnose Thanks <@U03HHJK2FJ6> that\u2019s very helpful! How this command behaves if i have multiple simulators open simultaneously? (maybe a watch sim paired with an iphone sim) The help for simctl will give some pointers. https://developer.apple.com/forums/thread/679842|This answer from the developer forums suggests using the --uuid parameter.","title":"Is there a way to trigger a sysdiagnose on a watch (or iPhone)"},{"location":"wwdc22/watchos-lounge.html#what-happens-if-a-widgetkit-backed-complication-update-is-running-while-i-request-another-reload-of-the-widgets-is-the-second-request-dropped-or-enqueued-to-the-end-of-the-current-reload-thanks","text":"The second is enqueued. If you add a third\u2026 we\u2019ll take a peek at the requests and decide which one to keep This is because a reload might be in-progress just as you get new data, and so your app requests another reload. It\u2019d be mean for us to ignore that second one, since you had no control over when the first reload actually happens But if you give us a third reload request\u2026, well that\u2019s greedy and we will throw one away :) There was a cool WidgetKit talk in WWDC 2021 about reloads and budgets thanks! I mean, if the user makes 3 changes in the app, that does mean that we honestly need to update 3 times I suppose you\u2019d recommend throttling? There\u2019s nothing wrong with requesting a third time, because you don\u2019t control when the reloads will happen. We\u2019ll be sure to keep one of the reload requests queued up, and when budgets allow, we\u2019ll reload your widget with whatever is the current latest content We\u2019ll do throttling too. Depends on several factors such as how frequently your widget is viewed, and there is a daily budget. https://developer.apple.com/documentation/widgetkit/keeping-a-widget-up-to-date has more","title":"What happens if a WidgetKit-backed complication update is running while I request another reload of the \"widgets\"? Is the second request dropped, or enqueued to the end of the current \"reload\"? Thanks!"},{"location":"wwdc22/watchos-lounge.html#is-it-possible-to-display-location-associated-with-a-workout-that-occurred-in-a-third-party-watchos-app-that-was-then-written-to-the-fitness-app-just-like-with-workouts-from-the-workout-app","text":"You can record the user\u2019s route for some types of workouts; details here: https://developer.apple.com/documentation/healthkit/workouts_and_activity_rings/creating_a_workout_route/ You as the developer would be responsible for the visual representation of this data. :slightly_smiling_face: Thank you! Seems this wouldn't fit for just a pin of a location, like where a tennis workout happened. Unless I'm missing something, I can file this in Feedback Assistant. Right, Tennis is not a workout for which you can collect a route, but if you request location permissions, you might be able to capture the user\u2019s location at the time the workout was started and/or ended, which would be sufficient for a pin.","title":"Is it possible to display location associated with a workout that occurred in a third-party watchOS app that was then written to the Fitness app? Just like with workouts from the Workout app."},{"location":"wwdc22/watchos-lounge.html#can-we-establish-a-socket-connection-for-server-based-apps-and-also-can-we-use-quic-for-client-server-connections-on-watchos","text":"Sockets are only available for streaming audio. You can use the Network framework for the connection.","title":"Can we establish a Socket connection for server-based apps? And also - can we use QUIC for client-server connections on WatchOS?"},{"location":"wwdc22/watchos-lounge.html#thank-you-for-adding-callkit-support-on-watchos-in-case-if-user-receives-voip-call-on-iphone-and-he-didnt-have-an-watchos-version-of-this-app-on-his-watch-can-he-receive-incoming-call-alert-or-notification-on-watch-in-that-case-or-we-need-to-have-an-watchosapp","text":"No, if the user doesn't have the watchOS version of the app installed on their Watch, they would not be notified of the incoming call. So, am I right that watchOS now will support VoIP notifications for such cases? <@U03HB0UR9RU> sorry, i forgot to mention :disappointed: I am not sure if I understand the question correctly. Just to clarify, the Watch would receive the incoming call notification if the corresponding watchOS app is installed and it would not, if it is not installed.","title":"Thank you for adding CallKit support on WatchOS. In case if user receives voip call on iphone, and he didn't have an watchOS version of this app on his Watch - can he receive incoming call alert or notification on watch in that case? Or we need to have an watchOSApp?"},{"location":"wwdc22/watchos-lounge.html#do-the-new-widgetkit-complications-support-all-existing-watch-faces-or-do-we-still-need-to-handle-the-older-complication-families-separately-using-clkcomplicationtemplatewhatever","text":"WidgetKit supports all watch faces that previously used the \"graphic\" complication templates. Luckily, we also updated a bunch of watch faces to use that \"graphic\" style in watchOS 9! Once you start using WidgetKit for complications, the system will always prefer using those complications even if your app provides complications via ClockKit. There's some great sessions this year that goes over the migration path between ClockKit and WidgetKit. Thanks! Is there a good list of which faces now use graphic complication? That's been a little tricky to piece together in the past. All the faces in watchOS 9 support graphic complications. The talk on Thursday, Go further with Complications in WidgetKit, goes into more detail about this.","title":"Do the new WidgetKit complications support all existing Watch faces? Or do we still need to handle the older complication families separately using CLKComplicationTemplateWhatever?"},{"location":"wwdc22/watchos-lounge.html#hello-everyone-how-can-we-schedule-local-notifications-to-be-displayed-on-watchos-apps-with-a-companion-ios-app-and-a-custom-notification-view-i-mean-if-a-notification-is-scheduled-on-the-iphone-does-this-get-shown-on-the-watch-using-our-custom-notification-view-or-do-we-have-to-provide-additional-configuration-to-share-notifications-across-the-two-devices","text":"If you have a custom WKNotificationScene in your watch app, it will be used to display the notification that\u2019s automatically forwarded to the watch by the system. (IOW, the system handles coordination between the watch and the phone.) the local notification scheduled from your iPhone will automatically display on the watch when appropriate, you only need a WKNotificationScene if you want to customize the UI of the notification on the watch Thank you, Lori and Jose - that\u2019s amazing! That\u2019s the clarification I was precisely looking for. Great! :smile:","title":"Hello everyone! How can we schedule local notifications to be displayed on watchOS apps with a companion iOS app and a custom notification view? I mean, if a notification is scheduled on the iPhone, does this get shown on the Watch using our custom notification view - or do we have to provide additional configuration to \"share\" notifications across the two devices?"},{"location":"wwdc22/watchos-lounge.html#are-we-allowed-to-instantiate-a-cbperipheralmanager-on-a-watch-application-and-let-the-watch-act-as-a-btle-peripheral","text":"No. The Watch app can instantiate a CBCentralManager and communicate with peripherals, but it cannot be a peripheral.","title":"Are we allowed to instantiate a CBPeripheralManager on a watch application and let the watch act as a Btle peripheral?"},{"location":"wwdc22/watchos-lounge.html#i-know-this-is-likely-to-change-with-appintentsappshortcuts-but-is-there-a-way-to-get-shortcuts-into-the-watchos-simulator-so-they-can-be-tested-currently-they-can-only-be-added-via-ios-and-they-dont-sync-across-presumably-because-they-need-icloud","text":"Hi Quentin! Can you file Feedback about this? (Or if you have, can you post the FB number here?) Will do for this specifically. It\u2019s related to this though: FB9980324 Submitted: FB10081835","title":"I know this is likely to change with AppIntents/AppShortcuts - but is there a way to get shortcuts into the watchOS simulator so they can be tested? Currently they can only be added via iOS, and they don't sync across (presumably because they need iCloud)"},{"location":"wwdc22/watchos-lounge.html#if-we-convert-our-intent-definition-files-to-app-intents-can-we-deploy-to-watchos-8-and-earlier-and-have-existing-user-created-shortcuts-continue-working","text":"This would best answered at the SiriKit lounge.","title":"If we convert our Intent definition files to App Intents, can we deploy to watchOS 8 and earlier and have existing user-created shortcuts continue working?"},{"location":"wwdc22/watchos-lounge.html#hello-can-anyone-suggest-an-alternative-to-a-datepicker-still-absent-from-watchos-multiple-wheel-pickers-are-a-possibility-especially-since-people-would-not-want-to-type-monthdayyear-in-their-watch-dictation-is-another-possibility-im-considering-steppers-would-have-too-many-numbers-for-days-thanks-in-advance-for-the-input","text":"Yeah, I think multiple pickers would be the way to go, but don\u2019t be afraid to experiment and find something that fits your app\u2019s needs. Hi Mayra! Yes, for now multiple pickers is probably the best way to implement a date picker manually. We will absolutely take your Feedback about wanting a DatePicker for watchOS, tho! In the Feedback, tell us the combinations of dates/use cases you\u2019re most interested in! Thank you! The multiple picker implementation is almost 95% done in WatchKit/UIKit, but I am moving it to SwiftUI to see if it\u2019s \u201ceasier\u201d. The main problem I am encountering is matching the checks needed to make sure days offered are correct with respect to year (leap or not) and 30/31 days for the rest of the months. I hope the new capabilities in watchOS9 and SwiftUI will help me finish! Homework accepted: will file tonight! I'd have thought the watch screen size is just about big enough for the new date picker option on swiftui, would be great to see it there Same here, but I\u2019ve been waiting for something like it for two years\u2026 I\u2019m sure it will happen right after I hit my last bug. And the official implementation will be better (of course)!","title":"Hello! Can anyone suggest an alternative to a DatePicker (still absent from watchOS)? Multiple wheel pickers are a possibility, especially since people would not want to type month/day/year in their Watch. Dictation is another possibility I\u2019m considering; Steppers would have too many numbers for days\u2026 thanks in advance for the input!"},{"location":"wwdc22/watchos-lounge.html#this-looks-to-be-a-great-year-for-watchos-from-the-simplified-app-target-to-the-improvements-brought-through-swiftui-i-already-know-ill-be-building-some-better-apps-this-year-thank-you","text":"That is so awesome to hear. Thanks Josh!","title":"This looks to be a great year for watchOS. From the simplified app target to the improvements brought through SwiftUI, I already know I'll be building some better apps this year. Thank you!"},{"location":"wwdc22/watchos-lounge.html#thank-you-for-the-presentation-is-there-anyway-to-know-what-watchos-versions-my-users-are-on-my-app-supports-watchos6-but-if-no-one-is-using-that-os-i-would-like-to-remove-code-that-is-specific-to-that-older-version-thank-you","text":"I think what we really need is someone to write some analytics packages for watchOS. :wink: It\u2019s possible to query the version number from the watch itself, but iirc there isn\u2019t a way to get that info from the phone. WKInterfaceDevice.systemVersion is how to get the info. For a cross-platform / structured data solution there's also: https://developer.apple.com/documentation/foundation/processinfo/1410906-operatingsystemversion Ok thank you. Yes, that would be nice to have analytics package for watchOS! It would be nice to see the break down on AppStore connect for watchOS like you can for iOS :blush: Definitely file that Feedback!!","title":"Thank you for the presentation! Is there anyway to know what watchOS versions my users are on? My app supports watchOS6, but if no one is using that OS I would like to remove code that is specific to that older version. Thank you!"},{"location":"wwdc22/watchos-lounge.html#whats-the-cool-magnifying-glass-thing-thats-in-your-presentation-when-you-are-telling-us-where-to-go-to-find-more-information-about-swiftui-is-there-a-story-there","text":"That is a hand lens for examining rocks in the field. I got that one when I was at university getting my degree. After being a geologist for a few years, I\u2019ve spent most of my career as a software engineer. But all of my math, physics, chemistry, biology, and communications serve me well.","title":"What's the cool magnifying glass thing that's in your presentation when you are telling us where to go to find more information about SwiftUI? Is there a story there?"},{"location":"wwdc22/watchos-lounge.html#howdy-face_with_cowboy_hat-first-of-all-many-thanks-for-the-marvellous-updates-this-year-and-such-a-great-sessions-my-question-is-regarding-portrait-watch-face-ive-been-playing-around-with-different-images-and-what-pique-my-curiosity-was-the-fact-that-the-same-image-might-be-used-with-depth-effect-on-the-new-ios-lock-screen-while-not-supporting-portrait-on-the-watch-it-is-correct-to-assume-that-the-reason-is-different-screen-dimensions-and-depth-effect-requires-some-padding-around-the-subject-to-work","text":"Starting with watchOS 9, the Portraits face does support photos from your entire photos library (not only those taken in Portrait mode), just like the new lock screen. However, I think you're right in your analysis of screen size/shape, padding around the subject, etc. In order for the depth effect to work, there needs to be the right amount of overlap between the foreground subject and the time and the variables you mentioned that are different between the phone and watch would affect that amount. Gotcha, thanks <@U03HHHK57K5>! Of course!","title":"Howdy :face_with_cowboy_hat: First of all, many thanks for the marvellous updates this year and such a great sessions!  My question is regarding Portrait watch face. I've been playing around with different images and what pique my curiosity was the fact that the same image might be used with depth effect on the new iOS Lock Screen while not supporting Portrait on the watch. It is correct to assume that the reason is different screen dimensions and Depth Effect requires some padding around the subject to work?"},{"location":"wwdc22/watchos-lounge.html#will-the-watch-ui-testing-be-able-to-handle-complications-in-clockkit-as-well-as-widgetkit-soon-its-hard-to-automate-testing-for-the-watch-but-i-need-it-the-most-on-complications","text":"You should be able to test the SwiftUI views that make up your WidgetKit complications and parts of your ClockKit complications as you would with any other SwiftUI view! If there's some specific support you're looking for in regards to having a more widget-like environment, please file a feedback and be sure to include specific details about what we can improve on! :slightly_smiling_face: ok! yeah! i sorely need ui testing for complications (they multiply like tribbles) - will file feedback if necessary. good to know!!! :tada:","title":"will the watch ui testing be able to handle complications in clockkit as well as widgetkit soon? It's hard to automate testing for the watch but i need it the most on complications"},{"location":"wwdc22/watchos-lounge.html#hi-all-when-developing-for-watchos-in-xcode-we-currently-use-two-targets-apple-watch-and-apple-watch-extension-to-be-honest-im-still-pretty-confused-about-why-this-is-needed-and-i-feel-like-both-targets-use-my-assets-some-of-the-time-do-upcoming-releases-simplify-this-workflow-into-a-single-target-and-make-it-more-similar-to-iosmacos-development","text":"hi Jan! When the original Apple Watch was released, it was pretty revolutionary\u2014but not very powerful (at least compared to the current models!). Thus in watchOS 1, the code for third-party WatchKit apps ran on iPhone, as an extension to the companion iOS app, and drove the UI on the Apple Watch. The code had to be in two targets, because they went to different devices! And the WatchKit Extension target truly was an extension. This changed in watchOS 2, when the contents of both targets moved to Apple Watch. We are so excited to simplify the developer experience this year by merging the targets in Xcode, and by offering a new Watch App target that makes your options clearer up front.","title":"Hi all! When developing for watchOS in XCode, we currently use two targets: Apple Watch and Apple Watch Extension. To be honest, I'm still pretty confused about why this is needed, and I feel like both targets use my assets some of the time. Do upcoming releases simplify this workflow into a single target and make it more similar to iOS/macOS development?"},{"location":"wwdc22/watchos-lounge.html#there-are-problems-with-syncing-data-application-context-messages-between-the-watch-and-ios-simulator-httpsdeveloperapplecomforumsthread682915httpsdeveloperapplecomforumsthread682915-are-these-problems-fixed-with-xcode-14","text":"We gave some guidance about this on Tuesday: https://wwdc22.slack.com/archives/C03GSLANZJT/p1654637598881109 Debugging tip that may help @Simon, based on a few years now of working on a very complex iMessage app extension. Not quite as hard as WatchKit comms but Messages simulator debugging is pretty fragile too. Thanks to a friend pushing me, I made the bulk of my complex logic able to compile cleanly on Mac and run unit tests there. (I have a document model that rivals Keynote in complexity, so encode/decode & DOM manipulation is significant - 500 tests now). That saves me massive time & so can focus on just UI testing on the devices.","title":"There are problems with syncing data (application context, messages, ...) between the Watch and iOS simulator (https://developer.apple.com/forums/thread/682915).|https://developer.apple.com/forums/thread/682915). Are these problems fixed with Xcode 14?"},{"location":"wwdc22/watchos-lounge.html#we-use-a-wkextendedruntimesession-smartalarm-mode-when-the-user-dismisses-the-alarm-we-want-to-send-a-request-to-our-server-this-happens-in-the-func-extendedruntimesession_-wkextendedruntimesession-didinvalidatewith-_-wkextendedruntimesessioninvalidationreason-error-error-delegate-function-do-we-need-to-use-a-network-background-session-for-that-the-normal-async-urlsession-data-task-does-not-finish","text":"Background URLSessions are almost always the right strategy on watchOS, and in this specific case as well. :slightly_smiling_face:","title":"We use a WKExtendedRuntimeSession (SmartAlarm Mode). When the user dismisses the alarm, we want to send a request to our server. This happens in the func extendedRuntimeSession(_: WKExtendedRuntimeSession, didInvalidateWith _: WKExtendedRuntimeSessionInvalidationReason, error: Error?) delegate function. Do we need to use a network background session for that? The normal async URLsession data task does not finish?"},{"location":"wwdc22/watchos-lounge.html#can-swiftuiswift-be-use-to-create-a-watchos-custom-keyboard-if-so-can-you-point-me-to-the-resourcesdocs-that-covers-this-topic-thx","text":"Hi Nolan! This isn\u2019t supported on watchOS yet, but we\u2019ll happily take a Feedback enhancement request! I just sent a Feedback enhancement request; can I expect this functionality by next week? :laughing:","title":"Can SwiftUI/Swift be use to create a WatchOS custom keyboard? If so, can you point me to the resources/doc(s) that covers this topic?  Thx"},{"location":"wwdc22/watchos-lounge.html#if-we-go-with-the-new-app-target-for-an-independent-watch-app-which-doesnt-contain-an-iphone-app-will-we-be-able-to-convert-it-to-a-iphone-app-with-an-independent-watch-app","text":"This can be done! 1. Add the iOS App Target 2. On the General settings for the iOS App Target, add the Watch App to the Frameworks, Libraries, and Embedded Content list. 3. On the Build Settings, search for Bundle Identifier. Set the WatchKit Companion App Bundle Identifier to the Bundle Identifier of the Watch App. 4. Delete the iOS Stub target that was originally created when you created the Watch Only app. 5. Delete the iOS Stub from the Watch App scheme Build.","title":"If we go with the new app target for an independent Watch app which doesn't contain an iPhone app, will we be able to convert it to a iPhone app with an independent watch app?"},{"location":"wwdc22/watchos-lounge.html#are-there-any-changes-to-a-complications-daily-reload-budget-when-using-widgetkit-instead-of-clockkit-in-particular-the-widgetkit-docs-say-that-reloads-arent-counted-when-the-containing-app-has-an-active-audio-or-navigation-session-does-this-also-apply-on-watchos","text":"Yes, audio and navigation sessions also give you unmetered refreshes on watchOS! The timeline reload budget is roughly the same, but the Background App Refresh and runtime budgets are different; the widget extension is metered separately from your main app with WidgetKit <@U03HHJ4LRU2> Thanks! Would the watchOS complication get unmetered refreshes if the companion iOS application had an active audio or navigation session? Or only if the watchOS app had an active session? Only on the same device, there's no sharing between them.","title":"Are there any changes to a complication's daily reload budget when using WidgetKit instead of ClockKit? In particular, the WidgetKit docs say that reloads aren't counted when the containing app has an active audio or navigation session. Does this also apply on watchOS?"},{"location":"wwdc22/watchos-lounge.html#in-our-application-weve-got-a-screen-with-recently-played-content-which-pulled-from-repositoriesapi-client-in-ios-counterpart-app-but-when-the-ios-app-is-not-running-then-watch-application-will-never-load-that-screen-the-data-exchange-works-through-watchconnectivity-and-in-documentation-it-says-sending-a-message-from-a-watchos-app-wakes-up-its-paired-ios-app-if-its-reachable-but-if-we-kill-ios-application-the-messages-will-be-never-delivered-as-far-as-we-see-the-mentioned-screen-above-doesnt-loads-is-there-any-other-way-to-wake-it-up-maybe-you-have-any-advice-how-to-debug-it","text":"I\u2019d suggest using the WatchConnectivity\u2019s Application Context to communicate the information from iOS to Watch. The Application Context is intended to be there when the app launches and the values can be updated if something changes. For more information about the different parts of WatchConnectivity and when you might use each one (and other data communication technologies that might be useful), check out this session: https://developer.apple.com/videos/play/wwdc2021/10003/ .","title":"In our application we've got a screen with recently played content which pulled from repositories/api client in iOS counterpart app. But when the iOS app is not running then Watch application will never load that screen. The data exchange works through WatchConnectivity and in documentation it says \"Sending a message from a watchOS app wakes up its paired iOS app if it's reachable.\" But if we \"kill\" iOS application the messages will be never delivered (as far as we see the mentioned screen above doesn't loads). Is there any other way to wake it up? Maybe you have any advice how to debug it?"},{"location":"wwdc22/watchos-lounge.html#i-apologize-if-this-is-an-overly-basic-question-when-the-watch-series-6-came-out-i-was-very-excited-about-the-oxygen-sensor-outside-of-spot-checks-i-was-interested-in-righting-and-app-to-gather-oxygen-data-overnight-along-with-other-parameters-to-provide-an-additional-dataset-for-people-with-obstructive-sleep-apnea-to-share-with-their-doctors-is-their-a-way-to-modify-the-polling-rate-on-the-oxygen-sensor-to-pull-more-frequently-if-so-whats-the-limit-and-whats-the-best-way-to-approach-this-please-excuse-my-ignorance-im-new-to-apple-watch-and-a-novice-at-software-developmentthis-is-more-of-a-passion-project-for-me-not-for-a-jobincome-and-im-intending-to-have-this-app-be-free-and-as-a-conversation-starter-for-people-and-their-physicians-who-are-either-diagnosed-or-think-they-may-have-sleep-apnea-and-thus-their-doctor-can-conducted-additional-questioners-and-sleep-studies-to-make-an-actual-diagnoses-or-reassessment","text":"Hi, Ahmed. Unfortunately there's currently no way to do this. But I'd suggest you file a feedback requesting the ability to do this. <@U03JYB15E2G> I would also benefit from this feature. If you file a feedback, please send it to me. Thank you will do!","title":"I apologize if this is an overly basic question. When the watch series 6 came out I was very excited about the oxygen sensor. Outside of spot checks I was interested in righting and app to gather oxygen data overnight along with other parameters to provide an additional dataset for people with obstructive sleep apnea to share with their doctors. Is their a way to modify the polling rate on the oxygen sensor to pull more frequently (if so what\u2019s the limit) and what\u2019s the best way to approach this? \u00a0Please excuse my ignorance I\u2019m new to Apple Watch and a novice at software development(this is more of a passion project for me not for a job/income and I\u2019m intending to have this app be free and as a conversation starter for people and their physicians who are either diagnosed or think they may have sleep apnea and thus their doctor can conducted additional questioners and sleep studies to make an actual diagnoses or reassessment)"},{"location":"wwdc22/watchos-lounge.html#hi-all-swiftui-on-watchos-now-appears-to-have-a-keyboard-with-quicktype-can-we-provide-values-for-quicktype-to-suggest-specific-strings-for-example-lets-say-the-user-is-searching-in-a-long-list-of-fruits-by-passing-the-fruits-data-to-quicktype-we-could-tell-the-system-to-suggest-specific-fruits-whose-name-property-matches-the-user-typed-string-so-that-typing-a-does-not-suggest-and-but-apple-right-now-we-can-pass-suggestions-to-searchable-but-the-suggestions-are-not-shown-while-the-user-is-typing-instead-they-appear-as-an-overlay-to-list-once-the-user-finished-typing-and-returns-to-the-view-also-searchable-does-not-bring-up-the-keyboard-right-away-unlike-textfieldlink","text":"Hi again <@U03JMMN8659>! Just to reiterate from yesterday when we talked in the swiftui-lounge, if you can file a feedback request with your use case that would be great. Some of us were also in the swiftui-lounge for watch specific questions Ah, hello <@U03HHHXDL03>! I will file feedback, I just want to exhaust all that is already available before requesting new features :blush: as you see this is a followup to yesterday's thread. Since then, I looked at .searchable as you suggested. Hence my question above. .searchable is nice, but it does not suggest results to the user on watchOS while the user is typing We would love to have a Feedback about that! Alright, sent as FB10135103 :slightly_smiling_face: great thanks! thank you <@U03HHHXDL03>!","title":"Hi all! SwiftUI on watchOS now appears to have a keyboard with QuickType. Can we provide values for QuickType to suggest specific strings? For example, let's say the user is searching in a long list of fruits. By passing the fruits data to QuickType, we could tell the system to suggest specific fruits whose name property matches the user-typed string, so that typing \"a\" does not suggest \"and\" but \"Apple\".  Right now we can pass suggestions to .searchable, but the suggestions are not shown while the user is typing. Instead, they appear as an overlay to List once the user finished typing and returns to the view. Also, .searchable does not bring up the keyboard right away, unlike TextFieldLink."},{"location":"wwdc22/watchos-lounge.html#i-am-currently-working-on-an-ios-app-that-stores-data-on-device-in-core-data-what-way-would-be-best-to-add-an-optional-watch-app-that-could-be-used-along-side-it-prefer-not-to-use-cloud-storage-if-possible-similar-to-how-workout-functions-but-the-workout-could-be-started-on-either-iphone-or-watch-thanks-for-keeping-the-questions-open-for-devs-working-gmt-time","text":"Hi Steven! I\u2019m curious about why you want to avoid using cloud storage. If you used Core Data with CloudKit, then you\u2019d get the data synchronization there. This session might be helpful in looking at various strategies: https://developer.apple.com/videos/play/wwdc2021/10003/ . Sometimes you might need to combine strategies to get the right experience. Hi Anne, thanks for replying. I am hoping as far as possible to allow the user complete control over their data (for privacy), So I was working on the assumption that the most private is on device, then iCloud. So in my mind I had the idea of allowing the user to select either to store data only on their device, or on iCloud. I assumed that with Watch connectivity the data would stay on the Phone, with the Watch having access to it. Thanks for the link to the video, I will going and watch it now. Thanks again. So many of us are a lot more conscious now about how our data is being stored and shared, so it\u2019s great that you\u2019re considering this so strongly. :heart: It\u2019s a balancing act. You want to protect everyone\u2019s privacy and also ensure that they don\u2019t lose data should something happen to their phone. Options are always good. I hope the video helps, and definitely read about privacy and security with iCloud. Once you know all the options, you can make the best choice to provide the level of privacy you want to deliver. That video was great, just at the right level too. I see what you mean about maybe needing to think about combining strategies, and also the trade off between privacy and usability.","title":"I am currently working on an iOS app that stores data on device in core data. What way would be best to add an optional Watch app, that could be used along side it. (Prefer not to use cloud storage if possible). Similar to how workout functions, but the workout could be started on either iPhone, or Watch. Thanks for keeping the questions open for devs working GMT time :)"},{"location":"wwdc22/watchos-lounge.html#i-want-to-expose-a-textfield-to-users-on-my-watch-app-but-only-allow-numeric-entry-can-i-show-just-a-number-pad-keyboard-for-input","text":"We do not support numeric keyboards at this time. Could you file a feedback with your use case? Hello! I posted a feedback about this same thing yesterday (FB ), and had been referred to 3 months ago in the Developer Forum. The developer that typed that comment said he had filed a report about the lack of number keyboard in watchOS 8 in August 2021 under the number FB9568384 (searched but couldn\u2019t find it as active). Thanks for filing! Duplicates are VOTES. :slightly_smiling_face:","title":"I want to expose a TextField to users on my Watch app but only allow numeric entry. Can I show just a number pad keyboard for input?"},{"location":"wwdc22/watchos-lounge.html#hi-i-enjoy-the-exciting-wwdc-i-have-one-question-about-watchkit-can-you-tell-that-you-are-wearing-a-apple-watch-i-can-know-the-charging-status-with-wkinterfacedevicebatterystate-how-can-the-app-tell-if-a-user-wears-an-apple-watch-on-their-wrist-thank-you","text":"Hi Yoichi! It would be helpful to understand the use case better before responding. If threads are locked by the time you see this, can you please file Feedback and describe what you\u2019re trying to do? watchOS does support LocalAuthentication , so if your case is about, say, requiring that the watch be unclocked or that the user enter their passcode before showing data, you should be good.","title":"Hi, I enjoy the exciting WWDC. I have one question about WatchKit. Can you tell that you are wearing a Apple Watch? I can know the charging status with WKInterfaceDeviceBatteryState. How can the app tell if a user wears an Apple Watch on their wrist? Thank you!"},{"location":"wwdc22/watchos-lounge.html#ive-a-game-design-which-relies-heavily-on-the-digital-crown-conceptually-am-i-right-in-thinking-that-wkcrownsequencer-will-allow-me-to-directly-capture-the-rotations-so-i-can-map-them-into-manipulating-the-state-of-the-game-rather-than-it-only-being-able-to-change-focused-pickers-its-been-a-fair-while-since-i-dived-into-wk-gave-up-as-early-versions-were-too-constraining","text":"You're correct that you can use the digital crown to drive much more than pickers. But we would recommend looking at the https://developer.apple.com/documentation/swiftui/form/digitalcrownrotation(_:)|digitalCrownRotation in SwiftUI instead.","title":"I've a game design which relies heavily on the digital crown (conceptually). Am I right in thinking that WKCrownSequencer will allow me to directly capture the rotations so I can map them into manipulating the state of the game, rather than it only being able to change focused pickers?  (It's been a fair while since I dived into WK &amp; gave up as early versions were too constraining)"},{"location":"wwdc22/watchos-lounge.html#the-new-sharelink-to-open-a-share-sheet-on-watchos-finally-is-great-is-there-a-way-to-make-my-app-a-sharing-destination-on-the-watch-like-an-equivalent-to-share-and-action-extensions-on-ios","text":"Hi Christopher, I'm so glad you like the new ShareLink API! There is not a way to make your app a sharing destination in watchOS 9.0, but please file Feedback with some details about your use case!","title":"The new ShareLink to open a share sheet on watchOS (finally!) is great. Is there a way to make my app a sharing destination on the Watch, like an equivalent to Share and Action Extensions on iOS?"},{"location":"wwdc22/watchos-lounge.html#question-about-subscriptions-on-apple-watch-i-would-like-to-have-a-single-subscription-to-unlock-content-in-my-app-across-apple-devices-according-to-this-httpsdeveloperapplecomsupportuniversal-purchasehttpsdeveloperapplecomsupportuniversal-purchase-support-doc-to-offer-your-app-on-iphone-and-apple-watch-create-an-ios-app-with-a-watchos-counterpart-in-xcode-and-upload-the-apps-to-app-store-connect-from-the-same-xcode-project-watch-only-apps-cannot-be-part-of-a-universal-purchase-but-during-the-wwdc22-talk-about-multiplatform-apps-both-my-ios-and-macos-app-products-use-the-same-bundle-identifier-by-default-which-is-awesome-because-that-means-when-i-publish-them-to-the-app-store-they-will-be-made-available-for-universal-purchase-so-folks-who-buy-my-ios-app-will-also-get-my-mac-app-automatically-so-question-can-we-have-a-single-subscription-for-content-in-an-ios-app-and-an-independent-watchos-app-face_with_monocle","text":"Hi Jan. Yes, your watch app is considered part of your iOS app for App Store purposes, but it can still be independent. Check the Runs Independently of iOS Companion App box in the Xcode project editor to make your watchOS app independent. Thank you <@U03H96N55U5> :pray: so it does not matter (for universal subscription) if the watch app targets have their own bundle identifiers that differ from the iOS app? If I recall correctly, the watchOS app should have the same root identifier as the iOS app. E.g., com.fancyapps.mymealtracker (iOS) and com.fancyapps.mymealtracker.watchkitapp (watchOS) yep, it does","title":"Question about subscriptions on Apple Watch. I would like to have a single subscription to unlock content in my app across Apple devices.  According to this (https://developer.apple.com/support/universal-purchase/)|https://developer.apple.com/support/universal-purchase/) support doc: \"To offer your app on iPhone and Apple Watch, create an iOS app with a watchOS counterpart in Xcode and upload the apps to App Store Connect from the same Xcode project. Watch-only apps cannot be part of a universal purchase. But during the WWDC22 talk about multiplatform apps: \"Both my iOS and macOS app products use the same bundle identifier by default, which is awesome, because that means when I publish them to the App Store, they will be made available for Universal Purchase. So folks who buy my iOS app will also get my Mac app automatically.\" So, question: can we have a single subscription for content in an iOS app, and an independent watchOS app? :face_with_monocle:"},{"location":"wwdc22/watchos-lounge.html#multi-line-text-support-in-textfield-is-very-exciting-but-doesnt-seem-to-work-in-watchos-9-beta-1-will-it-be-supported-and-if-so-will-the-keyboard-editor-support-new-lines-currently-all-newlines-are-removed-from-any-existing-text-collapsing-it-to-a-single-line-and-theres-no-way-to-type-a-return","text":"<@U03HVF46TEJ> Could you please file a feedback request with your use case? and the details about new lines here as well? Sure, will do. Thanks!","title":"Multi-line text support in TextField is very exciting, but doesn\u2019t seem to work in watchOS 9 beta 1. Will it be supported? And if so, will the keyboard editor support new lines? Currently all newlines are removed from any existing text (collapsing it to a single line) and there\u2019s no way to type a return."},{"location":"wwdc22/watchos-lounge.html#checking-an-analogy-here-ive-been-working-for-years-on-an-imessage-app-extension-which-also-has-fragile-debugging-on-simulator-something-that-helps-hugely-is-i-built-a-unit-test-suite-that-can-run-on-mac-so-all-my-complex-data-handling-logic-gets-heavily-exercised-in-fast-builds-there-leaving-the-on-device-testing-to-being-ui-only-is-this-a-useful-strategy-for-watchos-kinda-a-comment-as-question-that-might-help-simon-others-with-the-sync-issues-and-am-asking-for-my-own-benefit-as-i-restart-a-watch-project","text":"This is certainly a reasonable analogy (and not even where something is fragile). The more you make your model testable and test that directly, the more efficient you can make your testing. You can also potentially keep your quality high (assuming your tests are well-designed, of course). :slightly_smiling_face: UI tests will take longer to develop and run. Having good model tests allows you to focus your UI tests on testing the actual UI. I have a complex document model for encoding games that does things like particle emitters in 1/20th the Apple formats. Lots of bit fields. You bet my tests are exhaustive 500 of them, about 25% of code is test code when I'm working on the core model! TDD Purists will disagree with some of my torture tests though but I can refactor with impunity :smile:","title":"Checking an analogy here. I've been working for years on an iMessage app extension which also has fragile debugging on simulator. Something that helps hugely is I built a unit test suite that can run on Mac so all my complex data handling logic gets heavily exercised in fast builds there, leaving the on-device testing to being UI-only. Is this a useful strategy for WatchOS?  Kinda a comment-as-question that might help @Simon &amp; others with the sync issues, and am asking for my own benefit as I restart a Watch project."},{"location":"wwdc22/watchos-lounge.html#when-using-watchkit-and-the-extensiondelegate-we-could-access-the-root-interface-controller-to-change-the-snapshot-controller-for-instance-is-there-a-way-to-do-this-using-swiftui-lifecycle","text":"Hi <@U03JPJ277SQ>! New in watchOS 9 is the https://developer.apple.com/documentation/swiftui/backgroundtask/|BackgroundTask API, which has a snapshot task type. When you get the task, you can replace the root view of your app when a snapshot is requested from the system. In watchOS 8 and previous, you can use the https://developer.apple.com/documentation/swiftui/wkextensiondelegateadaptor|WKExtensionDelegateAdaptor to implement handle(_ backgroundTasks: Set&lt;WKRefreshBackgroundTask&gt;) One more note: If you\u2019ve updated your app to the newest recommended settings from Xcode, you\u2019ll no longer have a watchOS extension and you can use https://developer.apple.com/documentation/swiftui/wkapplicationdelegateadaptor|WKApplicationDelegateAdaptor instead of WKExtensionDelegateAdaptor. Oh I didn\u2019t know that. Thanks.","title":"When using WatchKit and the ExtensionDelegate, we could access the root interface controller to change the snapshot controller for instance. Is there a way to do this using SwiftUI lifecycle?"},{"location":"wwdc22/watchos-lounge.html#can-you-use-spritekit-on-watchos-with-swiftui-is-spritekit-limited-on-watches-i-cant-find-any-listed-most-stuff-goes-back-to-watchos2-or-3","text":"You can embed SpriteKit into your SwiftUI views using https://developer.apple.com/documentation/spritekit/spriteview|SpriteView . Thanks, partly answering my own question - https://developer.apple.com/documentation/spritekit/sknode/controlling_user_interaction_on_nodes neatly side-steps mentioning WatchOS > SKNode subclasses UIResponder in iOS and tvOS, and NSResponder in macOS, allowing nodes to respond to user interaction events such as touches and mouse movements are there any other significant missing bits of SpriteKit on Watch?","title":"Can you use SpriteKit on WatchOS with SwiftUI? Is SpriteKit limited on Watches? I can't find any listed - most stuff goes back to WatchOS2 or 3."},{"location":"wwdc22/watchos-lounge.html#thanks-for-extending-the-ask-questions-workflow-left-town-right-after-wwdc-are-there-any-conceptual-limitations-to-building-an-app-with-only-1-input-method-one-finger-and-40mm-45mm-screens-with-apple-watches-getting-more-and-more-independent-from-iphones-how-far-should-we-take-apps-gaming-social-media-etc-also-watchos-supports-scenekit-but-not-metal-at-least-not-in-the-xcode-template-is-there-a-reason-for-this-conceptually-or-just-technically","text":"Hi Ethan! Lots of developers are doing pretty amazing things on Apple Watch\u2014I\u2019ve played some games that were so immersive that I had to rest my arm on a table to keep my shoulder from getting tired. :slightly_smiling_face: Speaking of your arm getting tired tho, that is a thing, and he primary use case for watchOS apps is quick interactions and glanceable experiences. As to one-finger interactions, Assistive Touch actually makes it possible to interact with an Apple Watch without having any fingers at all! Awesome! Makes the platform feel full of potential discovery. I haven't developed much for watchOS but with SwiftUI getting better I'm excited In terms of support for Metal, please do file feedback! Assistive touch might be one of my favorite Apple Watch features. Absolutely incredible! Will do! I really want Metal to come to the watch because the Ray traced Unity Lights showed how capable the watch can be That watch face really is incredible!","title":"Thanks for extending the ask questions workflow (left town right after WWDC \ud83e\udee0).  Are there any conceptual limitations to building an app with only 1 input method (one finger) and 40mm-45mm screens? With Apple Watches getting more and more independent from iPhones, how far should we take apps (gaming, social media, etc.)?  Also, watchOS supports SceneKit but not Metal (at least not in the Xcode template), is there a reason for this conceptually or just technically?"},{"location":"wwdc22/watchos-lounge.html#i-already-ask-a-similar-question-but-can-i-use-the-nsubiquitouskeyvaluestore-to-sync-data-between-different-complications-widgets-on-multiple-devices-example-i-tell-siri-on-my-phone-that-i-ate-a-donut-i-store-then-an-integer-in-nsubiquitouskeyvaluestore-with-the-amount-of-donuts-i-ate-on-my-watch-i-have-a-complication-that-should-show-the-amount-of-donuts-would-nsubiquitouskeyvaluestore-a-good-approach-to-sync-data-between-siri-intents-and-watch-complications","text":"No. KVS is for syncing configuration/preferences, not data. From what you\u2019re describing, you don\u2019t need data transfer at all. Your app can store the fact that you ate the donut (your intent could be invoked by something like, \u201chey Siri, tell MyDietTracker that I ate a donut,\u201d and MyDietTracker records that you ate the donut. The complication can be updated based on that data when the app is running, or using background app refresh.","title":"I already ask a similar question, but can I use the NSUbiquitousKeyValueStore to sync data between different Complications / Widgets on multiple devices? Example: I tell Siri on my phone that I ate a Donut. I store then an Integer in NSUbiquitousKeyValueStore with the Amount of Donuts I ate. On my Watch I have a Complication that should show the amount of Donuts. Would NSUbiquitousKeyValueStore a good approach to sync data between Siri Intents and Watch Complications?"},{"location":"wwdc22/watchos-lounge.html#we-want-to-use-fall-detection-on-the-watch-to-be-informed-when-a-fall-occurs-will-the-system-continue-to-handle-fall-events-natively-and-call-the-rescure-services-when-we-register-as-a-fall-detection-delegate-or-are-we-from-then-on-alone-responsible-for-handling-the-fall-event","text":"Hi, Simon. I have verified that if you use CMFallDetectionManager to observe fall events this does not affect the default system behavior. Your app will be briefly woken up in response to fall events so that it can do a small amount of work, like sending a notification, or something similar. You can also request recent fall event samples from HealthKit. Keep in mind that you will need an entitlement to use CMFallDetectionManager . https://developer.apple.com/documentation/coremotion/cmfalldetectionmanager Here's the documentation for the HealthKit fall detection samples: https://developer.apple.com/documentation/healthkit/hkquantitytypeidentifier/1615592-numberoftimesfallen For that, you need to authorize HealthKit access to that sample type.","title":"We want to use fall detection on the Watch to be informed when a fall occurs. Will the system continue to handle fall events natively and call the rescure services when we register as a fall detection delegate? Or are we from then on alone responsible for handling the fall event?"},{"location":"wwdc22/watchos-lounge.html#testing-swiftui-on-watchos-is-there-documentation-or-a-video-you-would-recommend-for-getting-started-with-this-thank-you","text":"The best way to test your SwiftUI app on watchOS is to make your model layer testable and write thorough tests for that with XCTest. This will allow you to concentrate you UI testing on Watch to the UI itself. You can build a test suite for your model layer and run it to catch any regressions. You can find documentation for XCTest here: https://developer.apple.com/documentation/xctest , and there are a lot of great WWDC session videos about designing and writing tests. Cool thank you. I have my model in a swift package and wrote tests for it with XCTest. Just need to figure out UI tests now ! :blush:","title":"Testing SwiftUI on watchOS - is there documentation or a video you would recommend for getting started with this? Thank you!"},{"location":"wwdc22/watchos-lounge.html#is-there-a-component-or-a-way-to-animate-marquee-style-horizontal-scrolling-for-text-like-in-the-now-playing-ui-back-in-swiftui-v1-and-watchos-6-i-couldnt-figure-it-out-has-something-been-added-since","text":"There is nothing specifically built for this exact use-case. But I've built similar things using Canvas along with a TimelineView and an TimelineAnimationSchedule Ok, thanks for the info. I\u2019ll have a look at those newer APIs to see if I can implement it myself. Or you could use SpriteKit as in https://www.hackingwithswift.com/forums/swift/opening-credits-crawl-like-star-wars/287 (at the end)","title":"Is there a component, or a way to animate marquee style (horizontal) scrolling for text, like in the Now Playing UI? Back in SwiftUI v1 and watchOS 6 I couldn\u2019t figure it out, has something been added since?"},{"location":"wwdc22/watchos-lounge.html#dear-watchos-team-it-is-so-easy-to-download-a-watchos-app-through-the-apple-watch-app-on-the-iphone-as-a-companion-app-just-great-but-when-our-team-tried-to-download-an-app-direct-through-the-apple-watch-appstore-the-experience-was-quite-different-searching-for-the-app-switching-to-passcode-mode-trying-to-enter-the-appleid-not-what-we-want-to-customers-to-endure-any-other-way-to-download-an-independent-apple-watch-app","text":"Watch-only (and independent Watch) apps can be found on the iOS App Store, and installation can be triggered from there, yes. I honestly install my Watch apps from the App Store on Apple Watch, tho. At this point I have over 90 third-party watchOS apps on my watch; most of them were installed automatically because I have the companion iOS app, but I often search for apps directly on my watch because it\u2019s pretty fast to download from there\u2014and sometimes I only want the watchOS app. For example, I only ever use Insight Timer on my watch, so I install it from the App Store on Apple Watch. :slightly_smiling_face: Thanks for the insight!:slightly_smiling_face::de:","title":"Dear WatchOS team: It is so easy to download a WatchOS app through the Apple-watch app on the iPhone as a companion app. Just great. But when our team tried to download an App direct through the Apple Watch Appstore the experience was quite different. Searching for the App, switching to Passcode mode, trying to enter the AppleID - not what we want to customers to endure. Any other way to download an independent Apple Watch App?"},{"location":"wwdc22/watchos-lounge.html#in-watchos-7-the-corebluetoooth-framework-worked-great-in-the-background-allowing-for-a-constantly-updated-app-interface-on-the-watch-especially-if-this-application-communicates-via-ble-with-a-peripheral-that-has-more-than-20-characteristics-there-are-problems-with-the-ble-connection-and-then-the-application-has-to-connect-to-the-periphery-every-time-and-read-the-initial-values-of-the-characteristics-this-process-could-take-up-to-10-seconds-for-my-app-starting-with-watchos-8-the-application-on-the-watch-going-into-the-background-stops-communicating-with-the-periphery-how-will-the-corebluetooth-framework-change-in-the-future-to-properly-design-my-application-on-the-watch","text":"Hi Sergey! Starting in watchOS 9.0, bluetooth peripherals that your app connects to will stay connected even when the application is suspended. In addition, there are some nifty new features like Timely Alerts which will wake your application in the background if a characteristic marked as Notify on Change is changed! I recommend watching the \"Get Timely Alerts from Bluetooth Devices on watchOS\" video, new this year to learn about all the details of the new capabilities! https://developer.apple.com/videos/play/wwdc2022/10135/ :+1:","title":"In watchos 7, the CoreBluetoooth framework worked great in the background, allowing for a constantly updated app interface on the watch. Especially if this application communicates via BLE with a peripheral that has more than 20 characteristics. There are problems with the BLE connection, and then the application has to connect to the periphery every time and read the initial values of the characteristics. This process could take up to 10 seconds for my app. Starting with watchos 8, the application on the watch, going into the background, stops communicating with the periphery. How will the CoreBluetooth framework change in the future to properly design my application on the watch?"},{"location":"wwdc22/watchos-lounge.html#whats-the-best-way-to-sending-fast-messages-with-less-enery-using-with-watchconnectivity-does-sending-as-data-instead-of-dictionary-performs-better-do-you-recommend-sending-a-big-payload-as-smaller-parts-or-at-once-also-is-there-any-payload-size-limit","text":"Hi, Emre. There are a couple of parts to this question, but I think the most important part is whether it's better to do a large amount of work spread out over a longer period of time or to do it all at once. There are a couple of great WWDC talks that address this. One that really helped me to get into the right mindset about this was this one: https://developer.apple.com/videos/play/wwdc2015/707/ Doing a task on a mobile device requires spinning up whatever hardware is needed to do the task. That can be a CPU, or wireless radios. All of those things require power to run, and turning them on and off frequently is a very quick way to burn down the user's battery. So the best thing to do is to do as much work as you can in as short a time as possible and then let the device go back to sleep. There's also a great talk from last year about watch connectivity: https://developer.apple.com/wwdc21/10003 That one was done by <@U03HL004760>! Thank you! :pray:","title":"What's the best way to sending fast messages with less enery using with WatchConnectivity? Does sending as Data instead of Dictionary performs better? Do you recommend sending a big payload as smaller parts or at once? Also is there any payload size limit?"},{"location":"wwdc22/watchos-lounge.html#with-the-new-sleep-stage-is-it-possible-to-monitor-and-reading-from-realtime-in-extending-runtime-of-what-stage-the-user-is-in-or-all-these-data-are-still-calculated-and-write-to-healthkit-when-the-user-wake-up","text":"It is not possible to monitor sleep stage in realtime. I would suggest you file a Feedback. Really would support sleep stages in real time!","title":"With the new sleep stage? is it possible to monitor and reading from realtime in extending runtime of what stage the user is in? Or all these data are still calculated and write to healthkit when the user wake up"},{"location":"wwdc22/watchos-lounge.html#is-there-a-way-to-send-a-message-from-the-iphone-application-to-wake-up-the-associated-applewatch-application-note-the-applewatch-application-is-already-started-i-just-need-to-wake-it-up","text":"Hi Frederic! What do you mean that the application is already started, and you just need to wake it up? Can you elaborate on your use case? Yes. The application is a workout application with a workout in progress. I just would like the iPhone application to send it a message in order to update some data that are only available on the iPhone so that when the user raise his/her hand, he/she can see the updated data immediately. OK, so it\u2019s not really about \u201cwaking\u201d the watch or the app (it\u2019s already running); you\u2019re looking for data to be transferred immediately. Yes, but when the Watch application is in the inactive state (watch screen is off). OK then no, there is no way for you to wake the screen programmatically. You can still send data over, and the user will see it when they raise their wrist (if that\u2019s how you\u2019ve designed the UI, of course). I\u2019m assuming you\u2019ve already seen <@U03HL004760>\u2019s great talk https://developer.apple.com/videos/play/wwdc2021/10003/ :slightly_smiling_face: Thanks! I did miss this WWDC21 video and from the code examples it seems that i will be able to implement my use case. thanks again!","title":"Is there a way to send a message from the iPhone application to wake up the associated AppleWatch application ? Note: the AppleWatch application is already started, I just need to wake it up."},{"location":"wwdc22/watchos-lounge.html#how-can-i-make-sure-that-my-coredata-cloudkitcontainer-objects-sync-over-correctly-from-watchos-to-ios-when-does-the-watchos-app-start-the-sync-or-is-coredata-with-icloud-maybe-not-the-best-way-to-sync-objects-its-one-of-the-most-recurring-complaint-from-my-users-at-the-moment-sorry-if-its-not-detailed-enough-but-im-a-bit-lost-a-modern-example-of-coredata-support-between-watchos-and-ios-would-be-amazing-smile","text":"Hi Hidde! So nice to meet you in person on Monday. :blush: CoreData sync via iCloud is pretty much automatic, but keep in mind that when syncing happens is based on system and network conditions. Usually data is available pretty quickly, but sometimes it can take longer. It\u2019s helpful to design your experience with the expectation that data might not arrive immediately. I can relate to the question. A lot of my support questions were related to \u2018stale\u2019 data (which would correct itself in just a few seconds upon launch of the Watch app). One thing that helped is communicating proper expectations to my users. Overall, CoreData+CloudKit was super convenient for both myself and my users. I just had to temper their expectations: Yes, your data is safe. You just need to be patient if you\u2019re doing a fresh launch of the app. Yep! And it is possible to design experiences that don\u2019t leave users waiting and wondering. :slightly_smiling_face: It\u2019s just a little different than designing iOS experiences, which more developers have experience with. :joy: I will probably relate to this soon, since I want to expand my app to watchOS. <@U03HMDG985D> could this be solved with a loading indicator (e.g. progress view) while the data is being loaded? Can we even detect that data after a fresh launch is still being loaded? If you\u2019re doing the CloudKit yourself, I think you could. If you\u2019re using NSPersistentCloudKitContainer , there is no way to know when the syncing begins (though you might be able to tell when it ends via the associated events notifications). In my case, I\u2019m using the persistent cloud kit container, so I can\u2019t provide that UI to my users. (I tried; it wasn\u2019t consistent.) Which is where informing/instructing my user was more valuable. Thanks, that's very insightful. I am using NSPersistentCloudKitContainer. So I guess I will go with a simple disclaimer. Can we even detect that the user already has some data in CloudKit, so that the disclaimer will not be shown to users who are totally new to the app? Wouldn\u2019t that map directly to the presence of data in your persistent container? I suppose. But if I just set up a disclaimer to show once container receives data, the disclaimer will also appear if a new user creates some data what is the data that y\u2019all are waiting for? fwiw I was thinking along the lines of relying on data that\u2019s already on the watch (because it\u2019s generated by the watch, or stored there) whenever possible, and augmenting with data from other sources when it becomes available. this isn\u2019t always possible, of course; if you have a reminders/task or grocery list app across platforms, the user is going to expect those to sync, and in that case spinners and other syncing UI might be approrpriate. And even if your app has no local data on first launch and you show your \u2018new user\u2019 UI, once your local Core Data store syncs up, you could update your UI. So the disclaimer would go away. And maybe the disclaimer can include wording that, if they have used the app on another device with the same Apple ID, that they can expect to see data soon.","title":"How can I make sure that my CoreData (CloudKitContainer) objects sync over correctly from watchOS to iOS?    When does the watchOS app start the sync?  Or is CoreData with iCloud maybe not the best way to sync Objects?   It\u2019s one of the most recurring complaint from my users at the moment. Sorry if it\u2019s not detailed enough but I\u2019m a bit lost, A modern example of CoreData support between watchOS and iOS would be amazing :smile:"},{"location":"wwdc22/watchos-lounge.html#hi-wave-were-using-hkliveworkoutbuilder-for-tracking-indoor-swimming-but-the-workoutbuilderdidcollectevent-for-the-distance-datatype-is-coming-in-delayed-and-resulting-in-an-inaccurate-data-usually-2-or-more-laps-of-distance-50m-are-missing-could-you-maybe-help-me-with-this-issue","text":"Hi, <@U03J7AWBLDT> and I'm sorry you're running into trouble with your swimming app. This sounds very much like something the Health team would like to look at. Can you please file feedback about this? Thanks!","title":"Hi! :wave: We\u2019re using HKLiveWorkoutBuilder for tracking indoor swimming. But the workoutBuilderDidCollectEvent for the distance datatype is coming in delayed and resulting in an inaccurate data.  Usually 2 or more laps of distance (~50m) are missing. Could you maybe help me with this issue?"},{"location":"wwdc22/watchos-lounge.html#how-can-we-hook-into-assistive-touch-sorry-if-covered-in-videos-due-16hr-time-diff-being-busy-elsewhere-havent-seen-them-yet-ive-been-in-love-with-the-feature-since-a-staffer-at-local-apple-store-was-wizzing-through-but-i-cant-find-an-api-just-uikit-stuff","text":"You can! The \u201c https://developer.apple.com/videos/play/wwdc2022/10052/|What\u2019s New in SwiftUI \u201d video shows how you can use the accessibilityQuickAction API to configure what will happen when the user makes an assistive touch gesture. Also, here's the documentation link for this. https://developer.apple.com/documentation/swiftui/view/accessibilityquickaction(style:content:) It's a new API available for watchOS only in watchOS9. The user does need to have accessibility quick actions enabled via Settings -> Accessibility -> Quick Actions -> On. It's also limited to detecting the double pinch gesture only. I'd also found last years's video https://developer.apple.com/videos/play/wwdc2021/10223/ but no matching docs, was looking at the code. AssistiveTouch works with the same accessibility infrastructure as VoiceOver. Following best practices for https://developer.apple.com/documentation/swiftui/view-accessibility|Accessibility in SwiftUI will make your app work well with AssistiveTouch.","title":"How can we hook into assistive touch? (Sorry if covered in videos, due 16hr time diff &amp; being busy elsewhere haven't seen them yet).  I've been in love with the feature since a staffer at local Apple Store was wizzing through but I can't find an API, just UIKit stuff."},{"location":"wwdc22/watchos-lounge.html#is-it-possible-to-know-that-the-watch-app-will-exceed-the-75mb-limit-before-it-has-been-submitted-and-processed-in-app-store-connect","text":"I don\u2019t have a great recommendation for how to check this in advance. But in general, are you having issues with this when submitting your app? I know we all want to be good Watch app citizens and some devices are quite limited on storage. If you\u2019re using a lot of storage, people might get frustrated and uninstall your app. Have you considered ways to reduce the size of your app and still deliver a great experience? I know this is a constant challenge.","title":"Is it possible to know that the Watch App will exceed the 75mb limit before it has been submitted and processed in App Store connect?"},{"location":"wwdc22/watchos-lounge.html#if-you-make-the-decision-to-make-a-watch-only-app-does-that-mean-you-have-to-distribute-it-separately-or-can-it-still-be-sold-with-an-iphone-app-functionally-they-could-be-used-separately-but-buying-the-iphone-app-would-provide-the-watch-app-too-is-that-the-definition-of-companion-app","text":"Hi Dan! You should only make a watch-only app if you don\u2019t have an iOS app with the same or related functionality already. Functionally a watch app that can run independently of its iOS companion app operates as if it is watch only if its companion is not installed, as in my Insight Timer use case from an earlier thread (I only install Insight Timer on the watch, and never install the companion iOS app). that\u2019s a lot of words, I realize. :stuck_out_tongue: If you have an iOS app, and your watchOS app offers the same or related functionality, they should be considered \u201cthe same app\u201d and submitted together. In iMessage apps, another restriction is lack of access to IAP for an extension-only app. So you're strongly encouraged there to have a \"real\" parent app. hi Andrew, that\u2019s unrelated Watch apps are not extensions of iOS apps. are you saying Watch-only apps have store IAP access? and Watch apps do support IAP! yes! yay! Thanks for clarifying the architecture - I started with WatchOS when they were much more tightly coupled. yeah! I have a little bit on the history in a previous thread; let me find it. Thanks! I think watch-only would be rare. https://wwdc22.slack.com/archives/C03GSLANZJT/p1654725594769139?thread_ts=1654725447.910529&cid=C03GSLANZJT Watch-only is less common, certainly, but there are still several hundred watch-only apps in the App Store. :slightly_smiling_face: Ah yes - I do have one called Geneva Moon that shows you moon phases and can locate the moon with the compass! Very fun Yeah! Watch-only apps are also a great way to learn SwiftUI, especially for students. You can create small and useful applications without having to worry about also building an iOS app. How does the connection to Xcode work? I don\u2019t think I\u2019ve tried since Xcode 12 - but it seemed like getting a real device to connect was difficult. It still has to go through a paired iPhone, right? yes, plug in the iPhone via lightning cable (don\u2019t try to do WiFi debugging on the phone and then also debug the watch app!) you don\u2019t want to do more hops than you need to. :slightly_smiling_face: Matthew had some good advice here: https://wwdc22.slack.com/archives/C03GSLANZJT/p1654634872820659?thread_ts=1654634863.147179&cid=C03GSLANZJT I\u2019m looking forward to Xcode 14 connecting to my iPhone better - I had to do my share of rebooting to get my iPhone connected with Xcode 13. And the AppleTV is easier to deploy to through TestFlight than directly across my house :dizzy_face: Thanks so much for those tips\u2026 now that I\u2019m thinking about it - I was using a S2 watch so that wasn\u2019t probably very quick either. I\u2019ll have to try with Xcode 14 and my S7! :rocket: Yes! That should help. Series 7 supports 5Ghz WiFi as well. :crossed_fingers: you should have a better time. omg yes please use a Series 6 or later if you have one! It got to be a challenge to see how long my S2 could last - it was something like 5 years! I still have a Series 2 that I use to test various scenarios, but I definitely don\u2019t debug on it. :slightly_smiling_face:","title":"if you make the decision to make a Watch only app - does that mean you have to distribute it separately or can it still be sold with an iPhone app. Functionally they could be used separately but buying the iPhone app would provide the watch app too? Is that the definition of \"companion app\"?"},{"location":"wwdc22/watchos-lounge.html#back-when-developing-against-watchos-6-watchdog-killed-my-app-after-the-binary-grew-too-big-removing-some-dependencies-solved-the-issue-but-i-want-to-ask-is-this-size-limit-documented-somewhere-and-has-it-been-raised-in-the-last-few-releases","text":"<@U03JFUBCW85> Could you elaborate on what issue you see? What are the error messages / symptoms when the problem occurs? or... rather, what were they when you were hitting the issue in watchOS 6. Spoiler alert: It\u2019s not the binary size that\u2019s the problem here. :slightly_smiling_face: So this was happening during the watchOS 6 beta period. Haven\u2019t tested out the limit since. As I remember it, Watchdog killed the app, but maybe I\u2019m misremembering and it was happening in App Store Connect? Like was just mentioned here: https://wwdc22.slack.com/archives/C03GSLANZJT/p1654795299375599 Not sure anymore I\u2019m afraid. So Sebastian is asking about the size limit when submitting to the App Store (because that\u2019s where the 75MB limit applies). I see, that\u2019s probably what I was hitting as well then. I\u2019m a bit hazy on the details, as it\u2019s been so long. give it another whirl! :slightly_smiling_face: I will, for sure. Thanks for the info. These lounges are really great!","title":"Back when developing against watchOS 6, Watchdog killed my app after the binary grew too big. Removing some dependencies solved the issue.  But I want to ask:  is this size limit documented somewhere, and has it been raised in the last few releases?"},{"location":"wwdc22/watchos-lounge.html#whats-the-best-way-to-wake-up-ios-app-from-watch-app-to-keep-it-running-my-ios-app-is-connecting-to-a-server-by-socket-and-keep-sending-real-time-data-to-the-watch-app-is-sending-short-messages-to-iphone-with-an-interval-good-way-thank-you","text":"Hi, Emre. Sending a message via WatchConnectivity from your Watch App will wake the iOS app. However, you can't due this forever, as the messaging will be limited eventually. Can you tell us a bit more about what you're trying to do here? Sorry to say we have to wrap up now, so we\u2019ll be turning threading off.","title":"What's the best way to wake up iOS app from Watch app to keep it running? My iOS app is connecting to a server by socket and keep sending real time data to the Watch app. Is sending short messages to iPhone with an interval good way? Thank you"}]}