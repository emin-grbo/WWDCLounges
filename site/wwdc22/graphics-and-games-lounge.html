
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.15">
    
    
      
        <title>graphics and games - WWDC Lounges</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.c382b1dc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.cc9b2e1e.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/app.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="cyan">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#graphics-and-games-lounge-qas" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="WWDC Lounges" class="md-header__button md-logo" aria-label="WWDC Lounges" data-md-component="logo">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            WWDC Lounges
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              graphics and games
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="cyan"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="WWDC Lounges" class="md-nav__button md-logo" aria-label="WWDC Lounges" data-md-component="logo">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    WWDC Lounges
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Welcome
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          WWDC 21
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="WWDC 21" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          WWDC 21
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wwdc21/SwiftUI.html" class="md-nav__link">
        SwiftUI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wwdc21/a11y.html" class="md-nav__link">
        A11y
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wwdc21/DevTools.html" class="md-nav__link">
        DevTools
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          WWDC 22
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="WWDC 22" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          WWDC 22
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="swiftui-lounge.html" class="md-nav__link">
        swiftui
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="localization-and-internationalization.html" class="md-nav__link">
        L10n & i18n
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="watchos-lounge.html" class="md-nav__link">
        watchos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="uiframeworks-lounge.html" class="md-nav__link">
        uiframeworks
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          graphics and games
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="graphics-and-games-lounge.html" class="md-nav__link md-nav__link--active">
        graphics and games
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#by-feetiki" class="md-nav__link">
    by FeeTiki
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#there-any-main-tradeoffs-between-tessellation-and-mesh-shading-for-vertex-creation" class="md-nav__link">
    There any main tradeoffs between tessellation and mesh shading for vertex creation?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#not-much-of-a-question-because-im-just-starting-to-move-towards-rendering-but-id-like-to-say-thanks-to-everyone-involved-with-this-the-documentation-tooling-and-apis-seems-better-than-ever-and-the-metalfx-upscaling-seems-a-game-changer-i-can-see-some-foveated-rendering-hacks-being-used-with-this-api-and-using-different-lod-with-the-fast-resource-cheers-lads-awesome-work" class="md-nav__link">
    Not much of a question because I'm just starting to move towards rendering but I'd like to say thanks to everyone involved with this. The documentation, tooling and APIs seems better than ever and the MetalFX Upscaling seems a game changer. I can see some foveated rendering hacks being used with this API and using different LOD with the fast resource. Cheers lads, awesome work.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-the-package-size-considerations-when-shipping-with-offline-compiled-metal-shaders-do-these-need-to-be-updated-over-time-to-keep-up-with-any-driver-or-software-updates-for-a-particular-piece-of-hardware" class="md-nav__link">
    What are the package size considerations when shipping with offline compiled metal shaders?  Do these need to be updated over time to keep up with any driver or software updates for a particular piece of hardware?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-start-creating-anything-in-metal-3-can-you-please-show-me-code-for-a-hello-world-type-metal-3-maybe-a-box-with-a-shaded-gradient-of-color-inside" class="md-nav__link">
    How do I start creating anything in Metal 3? Can you please show me code for a "Hello, World!" type Metal 3; maybe a box with a shaded gradient of color inside.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hi-are-metal-3-features-exclusive-to-apple-silicon-or-do-they-also-work-on-other-processors-eg-a-series-on-ios-or-the-gpus-in-intel-based-macs" class="md-nav__link">
    Hi, are Metal 3 features exclusive to Apple Silicon or do they also work on other processors (e.g A-series on iOS, or the GPUs in Intel-based Macs)?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-best-way-to-get-started-with-metal-as-a-beginner-in-the-graphics-field" class="md-nav__link">
    What's the best way to get started with Metal as a beginner in the graphics field?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-learning-graphics-coming-from-swift-and-am-really-interested-in-ray-tracing-what-should-i-learn-first-to-learn-how-to-make-a-hybrid-renderer-in-metal" class="md-nav__link">
    I’m learning graphics (coming from Swift) and am really interested in Ray tracing. What should I learn first to learn how to make a hybrid renderer in Metal?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-memory-and-texture-format-considerations-when-using-metalfx-im-guessing-the-historical-frames-need-to-be-stored-in-some-way-though-the-trade-off-would-be-smaller-intermediate-render-targets" class="md-nav__link">
    Are there any memory and texture format considerations when using MetalFX?  I'm guessing the historical frames need to be stored in some way, though the trade off would be smaller intermediate render targets.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#since-the-shaders-are-compiled-at-project-build-time-and-compilation-speed-is-no-longer-as-critical-is-that-extra-headroom-used-to-allow-the-compiler-to-try-to-optimize-the-shaders-further-or-is-does-it-produce-identical-machine-code-as-when-building-it-at-runtime" class="md-nav__link">
    Since the shaders are compiled at project build time and compilation speed is no longer as critical, is that extra headroom used to allow the compiler to try to optimize the shaders further? Or is does it produce identical machine code as when building it at runtime?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hi-great-wwdc-session-question-do-you-have-a-hello-world-example-for-making-shaders-using-apple-technologies-thank-you" class="md-nav__link">
    Hi! Great wwdc session, question: do you have a hello world example for making shaders using Apple technologies? Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-you-explain-how-to-use-triple-buffering-for-complicated-scene-with-2d-overlayui-and-3d-what-kind-of-data-i-should-store-in-buffers-in-vulkan-examples-they-use-framebuffer-but-metal-dont-have-any-similar-entity" class="md-nav__link">
    Can you explain how to use triple buffering for complicated scene with 2d (overlay/ui) and 3d? What kind of data I should store in buffers? In Vulkan examples they use framebuffer, but metal dont have any similar entity.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#didnt-see-it-earlier-new-to-slack-not-sure-if-this-is-the-right-place-to-ask-but-i-am-still-looking-through-the-plugins-but-would-it-be-able-to-use-bluetooth-or-related-tech-to-share-say-an-item-with-another-player-that-also-has-the-gameapp-like-if-there-is-a-unity-plugin-from-apple-that-does-this" class="md-nav__link">
    Didn't see it earlier, new to Slack:  "Not sure if this is the right place to ask, but I am still looking through the plugins. But would it be able to use Bluetooth or related tech to share say an item with another player that also has the game/app. Like if there is a Unity plugin from apple that does this. "
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-earliest-ios-you-can-use-this-with" class="md-nav__link">
    What's the earliest iOS you can use this with?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-were-making-feature-requests-i-really-like-the-ability-to-email-bug-reports-from-my-game-with-mfmailcomposeviewcontroller" class="md-nav__link">
    If we're making feature requests: I really like the ability to email bug reports from my game with MFMailComposeViewController!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-question-on-the-accessability-plugin-for-unity-is-it-only-available-for-apple-devices-or-can-it-also-be-used-for-windows-xbox-etc-reason-for-asking-is-that-it-would-be-great-to-see-a-cross-platform-plugin-to-increase-the-reach-of-accessability-making-it-easier-to-prioritize-for-stakeholders" class="md-nav__link">
    A question on the Accessability plugin for Unity. Is it only available for Apple devices or can it also be used for Windows, Xbox etc. Reason for asking is that it would be great to see a cross platform plugin to increase the reach of accessability (making it easier to prioritize for stakeholders)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-recommended-framework-for-connecting-to-another-player-stably-for-a-multiplayer-match-not-the-old-turn-based-games-but-a-modern-approach-to-having-a-5-10-minute-connection-for-pvp-would-arkit-realitykit-be-better-today-than-multipeer-connectivity-which-can-be-flaky-with-frequent-disconnects" class="md-nav__link">
    What is the recommended framework for connecting to another player stably for a multiplayer match? Not the old turn-based games, but a modern approach to having a 5-10 minute connection for PvP. (Would ARKit, RealityKit be better today than multipeer connectivity, which can be flaky with frequent disconnects?)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hi-it-looks-like-the-documentation-for-the-unity-plugins-has-not-been-submitted-to-the-github-repo-all-the-documentation-links-are-broken" class="md-nav__link">
    Hi! It looks like the documentation for the unity plugins has not been submitted to the GitHub repo -- all the documentation links are broken.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-had-a-quick-browse-of-the-accessibility-unity-plugin-and-noticed-that-its-only-supporting-ios-and-tvos-is-macos-planned-as-well-there-are-also-some-performance-concerns-with-the-code-that-id-love-to-see-resolved-will-github-prs-be-considered" class="md-nav__link">
    I've had a quick browse of the Accessibility Unity plugin and noticed that it's only supporting iOS and tvOS. Is macOS planned as well? There are also some performance concerns with the code that I'd love to see resolved, will GitHub PR's be considered?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-look-into-unity-plugins-and-have-a-relative-question-why-you-using-swift-instead-of-objc-in-applecore-module-why-swift-with-all-unmanaged-things-better-than-objc-but-thanks-for-that-bridging-i-grab-information-how-to-use-_cdecl" class="md-nav__link">
    I look into unity plugins and have a relative question: Why you using swift instead of objc in AppleCore module? Why Swift with all Unmanaged things better, than ObjC?   But thanks for that bridging! I grab information how to use _cdecl :)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-wheel-support-only-on-mac-the-api-seems-to-be-available-on-all-platforms-except-apple-watch-but-ive-only-seen-it-mentioned-with-regard-to-mac-am-i-misreading-something" class="md-nav__link">
    Is wheel support only on Mac? The API seems to be available on all platforms except Apple Watch, but I’ve only seen it mentioned with regard to Mac. Am I misreading something?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#could-mesh-shaders-be-used-for-procedural-terrain-generation-with-continuous-lod-currently-i-use-a-tessellation-compute-shader-which-also-does-some-basic-culling-followed-by-a-vertexfragment-shader" class="md-nav__link">
    Could mesh shaders be used for procedural terrain generation with continuous LOD? Currently I use a tessellation compute shader (which also does some basic culling) followed by a vertex/fragment shader.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-should-we-approach-issues-like-shader-debugger-crashes-and-gpu-faults-we-dont-know-where-to-look-for-hints" class="md-nav__link">
    How should we approach issues like Shader Debugger crashes and GPU faults? We don't know where to look for hints
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-we-somehow-cancel-work-committed-to-gpu-for-example-we-commited-some-neural-net-processing-example-of-huge-work-and-we-received-memory-warning-from-the-system-can-we-somehow-stop-execution" class="md-nav__link">
    Can we somehow cancel work committed to GPU? For example we commitеed some neural net processing (example of huge work) and we received memory warning from the system, can we somehow stop execution?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-best-way-to-denoise-the-ray-traced-frame-in-metal-is-mpssvgfdenoiser-recommended-for-this-purpose" class="md-nav__link">
    What is the best way to denoise the Ray traced frame in metal? Is MPSSVGFDenoiser recommended for this purpose?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#does-mfxspatialscalingeffect-support-pq-encoded-hdr-color-specifically-pixel-format-mtlpixelformatbgr10a2unorm-and-colorspace-kcgcolorspaceitur_2100_pq-if-so-which-colorprocessingmode-should-be-used" class="md-nav__link">
    Does MFXSpatialScalingEffect support PQ encoded HDR color? Specifically, pixel format MTLPixelFormatBGR10A2Unorm and colorspace kCGColorSpaceITUR_2100_PQ? If so, which colorProcessingMode should be used?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-demos-using-the-mtlparallelrendercommandencoder-is-there-a-possibility-of-supporting-a-parallel-compute-encoder-in-the-future" class="md-nav__link">
    Are there any demos using the MTLParallelRenderCommandEncoder? Is there a possibility of supporting a parallel compute encoder in the future?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-limit-to-the-scale-factor-outputwidth-inputwidth-that-mfxspatialscalingeffect-can-apply" class="md-nav__link">
    Is there a limit to the scale factor (outputWidth / inputWidth) that MFXSpatialScalingEffect can apply?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-most-performant-way-to-copy-a-calayer-into-a-mtltexture-on-ios" class="md-nav__link">
    What's the most performant way to copy a CALayer into a MTLTexture on iOS?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-new-edr-support-in-metal-for-ios-looks-really-exciting-but-on-what-current-devices-can-we-test-this-on" class="md-nav__link">
    The new EDR support in Metal for iOS looks really exciting, but on what current devices can we test this on?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-out-of-curiosity-what-are-the-potential-use-cases-of-quadgroup-functions-ive-only-seen-one-video-mentioning-them-discovering-advances-in-metal-for-a15-bionic-where-theyre-used-to-reduce-the-number-of-texture-reads-per-thread-are-there-any-other-use-common-use-cases" class="md-nav__link">
    More out of curiosity, what are the potential use cases of quadgroup functions? I've only seen one video mentioning them ("Discovering advances in Metal for A15 Bionic") where they're used to reduce the number of texture reads per thread. Are there any other use "common" use cases?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-interested-in-the-accelerated-ray-tracing-features-in-metal-3-but-the-specific-session-that-would-contain-more-details-is-tomorrow-are-these-changes-specifically-designed-for-realtime-raytracing-in-a-rasterized-renderer-or-are-they-general-enough-to-be-used-on-their-own-to-ray-trace-entire-scenes-i-see-that-there-are-general-purpose-ray-tracing-features-in-older-versions-of-metal-im-specifically-curious-how-these-intersect-with-the-metal-3-ray-tracing-features" class="md-nav__link">
    I'm interested in the accelerated ray tracing features in Metal 3, but the specific session that would contain more details is tomorrow. Are these changes specifically designed for realtime raytracing in a rasterized renderer, or are they general enough to be used on their own to ray trace entire scenes? I see that there are general purpose ray tracing features in older versions of Metal, I'm specifically curious how these intersect with the Metal 3 ray tracing features.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#as-far-as-i-can-tell-ios-devices-still-run-opengles-apps-just-fine-is-there-a-hard-deadline-when-this-will-not-be-the-case-any-more" class="md-nav__link">
    As far as I can tell iOS devices still run OpenGLES apps just fine. Is there a hard deadline when this will not be the case any more?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-recommended-formats-or-workflows-for-dealing-with-3d-textures-in-metal" class="md-nav__link">
    Are there any recommended formats or workflows for dealing with 3D textures in Metal?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-all-of-the-metal-3-features-available-on-all-gpus-do-vega-gpus-and-uhd-630-get-mesh-shaders-with-metal-3" class="md-nav__link">
    Are all of the Metal 3 features available on all GPUs?  Do Vega GPUs and UHD 630 get mesh shaders with Metal 3?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-question-what-happened-to-the-metal-language-version-build-setting-seemed-it-disappeared-one-year" class="md-nav__link">
    Quick question: What happened to the Metal Language Version build setting? Seemed it disappeared one year...
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-there-i-have-been-working-with-a-highly-programmatic-scenekit-nodes-and-geometries-i-know-i-know-old-hat-and-i-have-been-suggested-by-some-friendly-helpful-developers-at-the-wwdc-apple-park-event-hello-again-rintaro-that-a-pathway-to-help-some-of-my-performance-problems-would-be-to-rewrite-some-of-my-core-components-with-arkit-primitives-or-perhaps-metal-unfortunately-i-dont-really-know-where-i-would-begin-that-conversion-or-even-if-doing-the-work-would-net-the-performance-benefits-im-seeking-the-context-of-this-question-is-my-arvr-visualization-project-which-uses-10s-of-thousands-of-small-geometries-and-nodes-shared-as-much-possible-to-render-individual-text-glyphs-like-a-sheet-of-paper-httpgithubcomtikimcfeelookatthatgithubcomtikimcfeelookatthat-a-good-solid-first-step-would-be-what-would-be-a-suggested-migration-pattern-from-scnnodescnplaneuiimage-to-similar-metalarkit-patterns" class="md-nav__link">
    Hey there! I have been working with a highly programmatic SceneKit nodes and geometries (I know, I know, old hat), and I have been suggested by some friendly helpful developers at the WWDC Apple Park event (Hello again, Rintaro!) that a pathway to help some of my performance problems would be to 'rewrite' some of my core components with ARKit primitives, or perhaps Metal. Unfortunately, I don't really know where I would begin that conversion, or even if doing the work would net the performance benefits I'm seeking. The context of this question is my AR/VR visualization project which uses 10's of thousands of small geometries and nodes (shared as much possible) to render individual text glyphs like a sheet of paper (http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat). A good, solid first step would be: What would be a suggested migration pattern from SCNNode/SCNPlane/UIImage to similar Metal/ARKit patterns?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#with-metalfx-are-there-any-concerns-or-issues-when-dealing-with-transparencyvolumetrics-when-doing-temporal-upscaling-would-it-sometimes-be-better-to-use-spatial-upscaling-and-msaa-in-some-cases-with-a-lot-of-transparency" class="md-nav__link">
    With MetalFX are there any concerns or issues when dealing with transparency/volumetrics when doing temporal upscaling?  Would it sometimes be better to use Spatial upscaling and MSAA in some cases with a lot of transparency?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-screencapturekit-be-used-by-two-apps-at-once-each-capturing-all-or-parts-of-the-screen" class="md-nav__link">
    Can ScreenCaptureKit be used by two apps at once, each capturing all or parts of the screen?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-system-level-permission-required-even-if-an-app-only-wants-to-capture-itself-ie-only-its-own-windows-and-not-anything-external" class="md-nav__link">
    Is system-level permission required even if an app only wants to capture /itself/ (i.e. only its own window(s)), and not anything external?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#our-video-conferencing-app-offers-screen-sharing-functionality-but-we-want-the-screen-share-video-to-exclude-certain-windows-eg-av-controls-and-floating-participant-videos-this-is-easily-solvable-with-content-filtering-in-screencapturekit-but-what-is-the-recommended-way-to-get-this-behavior-with-older-versions-of-macos-that-do-not-support-screencapturekit" class="md-nav__link">
    Our video conferencing app offers screen sharing functionality, but we want the screen share video to exclude certain windows (e.g., A/V controls and floating participant videos). This is easily solvable with content filtering in ScreenCaptureKit, but what is the recommended way to get this behavior with older versions of macOS that do not support ScreenCaptureKit?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-you-want-to-display-the-interactions-with-menu-at-the-top-of-macos-does-that-constrain-you-to-just-capturing-a-display-vs-application-or-window-in-order-to-show-that" class="md-nav__link">
    If you want to display the interactions with menu at the top of macOS, does that constrain you to just capturing a display (vs. application or window) in order to show that?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-make-media-playback-systems-but-havent-really-yet-played-with-producing-media-could-you-give-me-an-idea-of-what-the-next-steps-would-be-in-terms-of-macos-frameworksapi-usage-to-go-from-screencapturekits-sample-buffers-to-say-a-playback-ready-h264hevcaac-hls-output" class="md-nav__link">
    I make media /playback/ systems, but haven't really yet played with /producing/ media– could you give me an idea of what the next steps would be (in terms of macOS frameworks/API usage), to go from ScreenCaptureKit's sample buffers, to, say, a 'playback-ready' H264/HEVC+AAC HLS output?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-capturing-windows-only-is-the-windows-drop-shadow-captured-or-only-the-actual-frame-of-the-windows" class="md-nav__link">
    When capturing windows only, is the window's drop shadow captured, or only the actual frame of the windows?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#would-be-great-to-be-able-to-make-screen-captures-either-screenshots-or-screen-recordings-in-hdr" class="md-nav__link">
    Would be great to be able to make screen captures (either screenshots or screen recordings) in HDR!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#one-thing-that-wasnt-totally-clear-to-me-from-the-videos-obviously-audio-capture-is-per-application-since-audio-is-not-associated-with-a-window-if-i-exclude-only-some-windows-of-an-application-from-capture-does-that-mean-i-dont-get-any-audio" class="md-nav__link">
    One thing that wasn't totally clear to me from the videos: Obviously, audio-capture is per-application, since audio is not associated with a window. If I exclude only some windows of an application from capture, does that mean I don't get any audio?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-im-capturing-all-windows-from-a-single-application-will-i-also-get-xpc-hosted-dialogs-im-thinking-of-opensave-panels-and-the-like-which-were-not-easily-captured-with-the-cgwindowlist-api-since-they-are-associated-with-a-different-pid-than-the-application-windows" class="md-nav__link">
    If I'm capturing all windows from a single application, will I also get XPC-hosted dialogs? I'm thinking of Open/Save panels, and the like, which were not easily captured with the CGWindowList API, since they are associated with a different PID than the application windows.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#just-to-reiterate-my-pledge-please-please-push-mip-bias-as-a-part-of-the-sampler-for-next-metal-releases-i-think-even-gl-had-it" class="md-nav__link">
    Just to reiterate my pledge - please please push mip bias as a part of the sampler for next metal releases. I think even GL had it :)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-use-a-constant-jitter-offset-for-a-set-of-pixels-instead-of-a-random-one-that-would-probably-make-the-result-more-smoother" class="md-nav__link">
    Why use a constant jitter offset for a set of pixels instead of a random one that would probably make the result more smoother?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wonderful-presentation-kevin-thank-you-very-much-the-technology-works-beautifully-with-slow-moving-scenes-could-you-comment-on-the-impact-of-using-temporal-information-on-quick-moving-scenes-or-quick-changing-content-should-there-be-some-adaptive-setting-with-respect-to-the-amount-of-previous-pixels-used-based-on-the-richnessrate-of-change-of-the-scenes-content" class="md-nav__link">
    Wonderful presentation @Kevin! Thank you very much! The technology works beautifully with slow moving scenes. Could you comment on the impact of using temporal information on quick moving scenes or quick changing content? Should there be some adaptive setting with respect to the amount of previous pixels used based on the richness/rate of change of the scene's content?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#machine-learning-has-shown-great-results-with-respect-to-upscaling-both-in-terms-of-quality-as-well-as-performance-do-you-see-implementing-such-a-functionality-in-metalfx-over-the-near-future" class="md-nav__link">
    Machine learning has shown great results with respect to upscaling both in terms of quality as well as performance. Do you see implementing such a functionality in MetalFX over the near future?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-couldnt-help-noticing-watching-the-no-mans-sky-clip-that-there-was-quite-a-bit-of-smearing-and-things-looked-fairly-low-quality-on-the-big-camera-move-are-these-kind-of-camera-moves-a-big-concern-with-this-technique-i-understand-taa-can-have-issues-with-smearing-and-ghosting" class="md-nav__link">
    I couldn't help noticing watching the No Man's Sky clip that there was quite a bit of smearing and things looked fairly low quality on the big camera move.  Are these kind of camera moves a big concern with this technique.  I understand TAA can have issues with smearing and ghosting.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-lab-is-best-for-asking-scenekit-questions-this-year" class="md-nav__link">
    What lab is best for asking SceneKit questions this year?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-best-way-to-render-biplanar-edr-cvpixelbuffer-from-avfoundation-in-metal-is-better-to-convert-pixelbuffer-into-mtltexture-via-compute-or-sample-from-2-mtltextures-created-from-each-plane" class="md-nav__link">
    What is the best way to render biplanar EDR cvpixelbuffer from avfoundation in Metal? Is better to convert pixelbuffer into mtltexture via compute, or sample from 2 mtltextures created from each plane?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-best-way-to-build-a-full-screen-game-experience-on-the-mac-should-we-still-be-using-cgdisplaycapture-or-the-appkit-full-screen-window-support-both-seem-to-have-pros-and-cons-for-example-appkit-allows-the-menu-bar-to-pop-and-for-easy-access-but-cgdisplaycapture-can-use-the-area-around-the-notch" class="md-nav__link">
    What's the best way to build a full screen game experience on the Mac? Should we still be using CGDisplayCapture? Or the AppKit full screen window support? Both seem to have pros and cons. For example: AppKit allows the menu bar to pop and for easy access. But CGDisplayCapture can use the area around the notch.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hello-everybody-is-there-any-sample-or-instructions-how-to-make-offscreen-render-using-metal-for-example-i-have-a-scene-and-i-want-to-save-it-as-png-file-how-can-i-do-it-can-i-control-resolution-of-saved-file" class="md-nav__link">
    Hello everybody! Is there any sample or instructions how to make offscreen render using Metal? For example: I have a scene and I want to save it as png file. How can I do it? Can I control resolution of saved file?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-you-leverage-the-screen-capture-api-to-cast-data-that-is-being-rendered-off-screen-eg-two-cameras-in-a-scene-one-camera-view-is-your-ipad-screen-another-camera-view-would-be-streamed-to-apple-tv-each-camera-are-viewing-different-location-but-in-the-same-scene" class="md-nav__link">
    Can you leverage the screen capture API to cast data that is being rendered off screen? eg two cameras in a scene, one camera view is your ipad screen, another camera view would be streamed to apple TV. each camera are viewing different location but in the same scene.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-sorry-i-wasnt-able-to-watch-the-video-so-just-wanted-to-confirm-with-screencapturekit-were-able-to-capture-the-mixed-system-sound-or-filter-in-or-out-the-applications-that-we-want-or-dont-want-to-include-right-for-example-capture-sounds-from-my-own-application-zoom-call-already-mixed-for-us-basically-replacing-things-like-blackhole-multi-outputdevice-etc" class="md-nav__link">
    I'm sorry, I wasn't able to watch the video, so just wanted to confirm: with ScreenCaptureKit we're able to capture the mixed system sound, or filter in or out the applications that we want or don't want to include, right? For example capture sounds from my own application + Zoom call already mixed for us, basically replacing things like Blackhole + Multi-Output/Device etc?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#carenderer-was-not-previously-available-on-ios-only-macos-but-xcode-14-and-docs-on-the-web-now-show-it-as-available-since-ios-20-httpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minorhttpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minor-is-this-a-mistake-or-is-it-now-available-to-older-devices-if-built-with-the-ios-16-sdk" class="md-nav__link">
    CARenderer was not previously available on iOS (only macOS). But Xcode 14 and docs on the web now show it as available since iOS 2.0!  https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor|https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor  Is this a mistake, or is it now available to older devices if built with the iOS 16 SDK?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-fastest-way-to-get-something-on-screen-with-metal-is-there-way-to-draw-directly-to-screen-instead-of-metal-layer-contents-and-then-wait-for-frame-composing" class="md-nav__link">
    what is the fastest way to get something on screen with Metal? is there way to draw directly to screen instead of metal layer contents and then wait for frame composing?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#might-anyone-be-able-to-speak-to-whether-it-makes-sense-to-combine-indirect-command-buffers-httpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjchttpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjc-with-bindless-rendering-and-dynamic-data-specifically-i-am-trying-to-avoid-a-lot-of-draw-calls-with-the-soc-architecture-it-seems-passing-data-is-not-really-expensive-so-i-figure-per-frame-i-could-just-specify-some-parameters-and-generate-draw-calls-on-the-gpu-as-in-the-example-broad-question-i-realize-but-id-appreciate-some-thoughts-on-how-to-reduce-draw-calls-for-rapidly-updated-andor-deleted-vertex-data-i-also-checked-mesh-shaders-but-i-think-those-are-overkill-for-my-use-case" class="md-nav__link">
    Might anyone be able to speak to whether it makes sense to combine indirect command buffers (https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc|https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc ) with bindless rendering and dynamic data? Specifically, I am trying to avoid a lot of draw calls. With the SOC architecture, it seems passing data is not really expensive, so I figure per-frame I could just specify some parameters and generate draw calls on the GPU, as in the example.  -broad question, I realize, but I'd appreciate some thoughts on how to reduce draw calls for rapidly-updated and/or deleted vertex data.  I also checked mesh shaders, but I think those are overkill for my use case.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-will-the-activity-feed-be-available" class="md-nav__link">
    When will the activity feed be available?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hello-wave-playing-around-with-the-new-screencapturekit-im-experiencing-problems-with-tcc-revoking-access-to-screen-recording-while-the-apps-running-domain-comapplescreencapturekitscstreamerrordomain-code-3801-message-the-user-declined-tccs-for-application-window-display-capture-confusingly-when-my-app-launched-it-did-have-the-proper-access-anyone-around-knowing-under-which-special-circumstances-the-system-may-suddenly-revoke-access-to-screen-recording-for-an-already-running-app" class="md-nav__link">
    Hello! :wave: Playing around with the new ScreenCaptureKit I'm experiencing problems with TCC, revoking access to Screen Recording while the app's running  (domain: com.apple.ScreenCaptureKit.SCStreamErrorDomain, code -3801 message: "The user declined TCCs for application, window, display capture"). Confusingly, when my app launched it did have the proper access.  Anyone around knowing under which special circumstances the system may suddenly revoke access to Screen Recording for an already running app?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-all-question-is-related-to-scenekit-uiview-swiftui-view-layers-there-has-been-some-historical-functionality-allowing-the-layers-of-views-not-presented-in-a-typical-hierarchy-to-be-presented-as-the-material-contents-of-geometries-like-scnplanes-one-example-is-this-repo-here-httpsgithubcomsarangborudeswiftuiarviewhttpsgithubcomsarangborudeswiftuiarview-unfortunately-the-technique-here-doesnt-seem-to-work-anymore-a-couple-of-years-later-im-wondering-what-might-be-possible-to-bring-this-back-to-usage-as-i-have-a-ton-of-use-cases-id-love-to-explore-with-this-thank-you-so-much-as-always" class="md-nav__link">
    Hey all! Question is related to SceneKit + UIView / SwiftUI view layers. There has been some historical functionality allowing the layers of views not presented in a typical hierarchy to be presented as the material contents of geometries like SCNPlanes. One example is this repo here: https://github.com/sarangborude/SwiftUIARView|https://github.com/sarangborude/SwiftUIARView . Unfortunately, the technique here doesn't seem to work anymore a couple of years later. I'm wondering what might be possible to bring this back to usage, as I have a TON of use cases I'd love to explore with this! Thank you so much as always!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#for-a-relatively-simple-game-low-poly-graphics-simple-textures-no-ai-which-framework-would-you-suggest-i-use-to-develop-in-swift-all-logic-would-be-custom-so-no-need-for-scripts-library-or-similar-features-scenekit-seems-like-an-obvious-choice-as-it-has-the-functionality-i-need-but-im-not-sure-how-actively-it-is-still-being-maintained-and-supported" class="md-nav__link">
    For a relatively simple game (low poly graphics, simple textures, no AI), which framework would you suggest I use to develop in Swift? All logic would be custom, so no need for scripts library or similar features. SceneKit seems like an obvious choice as it has the functionality I need, but I'm not sure how actively it is still being maintained and supported?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-send-a-raw-mtltexture-or-a-specific-view-to-screencapturekit-or-some-lower-level-api-related-to-it-for-example-lets-say-i-have-a-real-time-graphics-application-written-using-metal-and-i-want-to-stream-content-containing-only-audience-relevant-elements-in-my-scene-rendered-to-one-texture-i-broadcast-that-texture-in-another-renderpass-i-composite-some-diagnostic-info-only-relevant-to-me-locally-that-i-dont-want-to-share-with-others-how-might-this-be-achieved-its-a-use-case-i-keep-running-into-asymmetric-views-and-perspectives-going-even-further-maybe-i-am-developing-a-game-and-i-want-to-render-a-completely-different-perspective-in-my-scene-from-a-different-camera-i-send-that-texture-to-the-stream-but-maybe-i-render-from-a-different-perspective-locally" class="md-nav__link">
    Is there a way to send a raw MTLTexture or a specific view to ScreenCaptureKit, or some lower-level API related to it? For example, let's say I have a real-time graphics application written using Metal, and I want to stream content containing only audience-relevant elements in my scene, rendered to one texture.  I broadcast that texture. In another renderpass, I composite some diagnostic info only relevant to me locally that I don't want to share with others.  How might this be achieved? It's a use case I keep running into (asymmetric views and perspectives.)  Going even further, maybe I am developing a game and I want to render a completely different perspective in my scene from a different camera. I send that texture to the stream, but maybe I render from a different perspective locally.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#would-screencapturekit-be-appropriate-for-a-remote-desktopscreen-sharing-application" class="md-nav__link">
    Would ScreenCaptureKit be appropriate for a remote desktop/screen sharing application?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-have-a-classic-dumb-question-is-screencapturekit-the-macos-equivalent-of-replaykit-i-want-people-to-be-able-to-create-reaction-videos-of-them-playing-touchgrams-which-is-running-on-spritekit" class="md-nav__link">
    I have a classic "dumb question" - is ScreenCaptureKit the macOS equivalent of ReplayKit? I want people to be able to create reaction videos of them playing touchgrams, which is running on SpriteKit.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-want-to-record-the-whole-screen-but-i-keep-getting-the-microphone-active-orange-dot-in-the-recording-how-can-i-make-a-recording-of-a-screen-using-screencapturekit-without-the-orange-dot" class="md-nav__link">
    I want to record the whole screen but I keep getting the microphone active orange dot in the recording. How can I make a recording of a screen using ScreenCaptureKit without the orange dot?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-screencapturekit-capture-an-off-screen-window-as-in-if-i-wanted-to-capture-some-visuals-that-normally-show-in-a-window-1920x1080-in-my-app-but-theres-no-need-to-show-the-user-because-they-just-care-about-the-resulting-captured-output-if-so-does-that-seemingly-make-sense-as-a-use-of-sck-or-are-there-more-direct-methods-for-app-internal-view-capture-youd-suggest-instead" class="md-nav__link">
    Can ScreenCaptureKit capture an 'off-screen' window?– as in, if I wanted to capture some visuals that normally show in a window (1920x1080) in my app, but there's no need to show the user (because they just care about the /resulting/ captured output)  &amp; if so: does that seemingly /make sense/ as a use of SCK, or are there more 'direct' methods for 'app-internal' view capture you'd suggest instead?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-i-use-sccontentfilterinitdesktopindependentwindow-in-screencapturekit-the-popup-buttons-contextual-menus-sheets-that-appear-on-the-window-are-not-captured-is-there-any-way-to-capture-them-in-screencapturekit-previously-i-implemented-using-cgwindowlistcopywindowinfooptionincludingwindow-optiononscreenabovewindow-windowid-and-cgimageinitwindowlistfromarrayscreenbounds-windowarray-imageoption-api-but-i-would-like-to-know-how-it-is-possible-in-screencapturekit" class="md-nav__link">
    When I use SCContentFilter.init(desktopIndependentWindow:) in ScreenCaptureKit, the popup buttons, contextual menus, sheets that appear on the window are not captured. Is there any way to capture them in ScreenCaptureKit?  Previously, I implemented using CGWindowListCopyWindowInfo([.optionIncludingWindow, .optionOnScreenAboveWindow], windowID) and CGImage.init?(windowListFromArrayScreenBounds:, windowArray:, imageOption:) API, but I would like to know how it is possible in ScreenCaptureKit.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#id-like-to-use-reality-composer-in-the-new-xcode-to-generate-simple-usdz-shapes-then-use-the-new-reality-converter-to-re-texture-these-shapes-because-reality-composer-cannot-add-custom-textures-in-reality-composer-you-have-to-enable-usdz-export-in-preferences-but-when-i-export-a-simple-box-to-usdz-reality-converter-cannot-open-it-conversion-failed-1-error-which-is-unexpectedunknown-is-there-a-secret-way-to-make-this-work-thanks-for-leaving-questions-open-im-in-australia-i-realise-this-not-quite-the-right-forum-for-this-but-the-right-forum-hasnt-left-questions-open-and-its-open-around-2-3am-my-time" class="md-nav__link">
    I'd like to use Reality Composer (in the new Xcode) to generate simple USDZ shapes, then use the new Reality Converter to re-texture these shapes (because Reality Composer cannot add custom textures). In Reality Composer, you have to enable USDZ export in Preferences, but when I export a simple box to USDZ, Reality Converter cannot open it (Conversion failed: 1 error, which is unexpected/unknown). Is there a secret way to make this work?  (Thanks for leaving questions open — I'm in Australia. I realise this not quite the right forum for this, but the right forum hasn't left questions open, and it's open around 2-3am my time.)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-hlsl-texture2d-and-texture2d_array-are-treated-almost-the-same-ie-texture2d-can-be-used-instead-of-array-one-however-in-metal-the-type-is-much-more-strict-does-it-mean-that-the-only-way-in-metal-is-to-create-respected-texture-view-to-convert-types-the-texture-view-looks-to-be-performance-hit" class="md-nav__link">
    In HLSL Texture2d and Texture2d_array are treated almost the same i.e. texture2d can be used instead of array one. However in metal the type is much more strict. Does it mean that the only way in Metal is to create respected texture view to convert types? The texture view looks to be performance hit :(
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-just-moved-from-my-old-and-hot-intel-imac-to-an-m1-max-macbook-pro-which-im-loving-one-snag-our-games-still-run-on-opengl-es-but-the-ios-simulator-on-apple-silicon-seems-to-crash-when-executing-opengl-es-commands-is-it-game-over-do-i-finally-need-to-move-to-metal" class="md-nav__link">
    I've just moved from my old and hot Intel iMac, to an M1 Max MacBook Pro, which I'm loving! One snag - our games still run on OpenGL ES, but the iOS Simulator on Apple Silicon seems to crash when executing OpenGL ES commands. Is it Game Over? Do I finally need to move to Metal?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-thanks-for-all-your-hard-work-and-cool-new-apis-i-watched-meet-distributed-actors-in-swift-and-im-curious-if-theres-currently-a-way-to-use-distributed-actors-in-a-peer-to-peer-gkmatch" class="md-nav__link">
    First, thanks for all your hard work and cool new APIs!  I watched "Meet distributed actors in Swift", and I'm curious if there's currently a way to use distributed actors in a peer-to-peer GKMatch?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-there-heres-a-basic-one-with-some-context-im-a-mobile-developer-working-for-about-a-decade-in-the-space-and-i-have-very-little-experience-with-high-fidelity-graphics-and-their-libraries-i-am-highly-interested-in-learning-c-especially-to-interact-with-the-new-metal-cpp-tools-my-question-is-would-the-metal-cpp-examples-shared-here-in-this-lounge-be-a-good-starting-point-into-learning-enough-c-to-be-productive-with-metal-or-is-there-a-better-starting-point-that-may-be-a-more-gradual-or-helpful-introduction-to-the-tools-i-dont-mind-hitting-the-ground-running-so-to-speak-just-curious-to-see-what-paths-are-most-suggested-for-this-thank-you" class="md-nav__link">
    Hey there! Here's a basic one, with some context: I'm a mobile developer working for about a decade in the space, and I have very little experience with high-fidelity graphics and their libraries. I am highly interested in learning C++, especially to interact with the new metal-cpp tools. My question is, would the metal-cpp examples shared here in this lounge be a good starting point into learning enough C++ to be productive with Metal? Or, is there a better starting point that may be a more gradual or helpful introduction to the tools? I don't mind hitting the ground running so to speak, just curious to see what paths are most suggested for this! Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#question-re-core-media-io-extension-how-the-host-app-can-communicate-with-the-extension-can-hostextension-exchange-iosurface-for-example" class="md-nav__link">
    Question re Core Media IO Extension. How the host app can communicate with the extension? Can host/extension exchange IOSurface for example?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#just-want-to-say-huuuge-thank-you-to-all-xcode-gpu-devteam-you-guys-rock-i-was-testing-first-metal-gpu-capture-and-since-that-time-its-miraculous-improvements-still-pixel-history-please-make-it-happen-and-if-this-is-possible-can-apple-team-work-at-least-a-bit-with-renderdoc-author-so-we-can-replay-dx1112-traces-on-mac-its-a-very-big-taskvery-complex-but-its-super-big-pain-to-need-a-windows-with-renderdoc-to-check-stuff-if-only-renderdoc-can-run-on-mac-of-course-id-be-happy-if-it-can-replay-metal-as-well-but-its-a-different-story" class="md-nav__link">
    Just want to say HUUUGE thank you to all XCode GPU dev.team. You guys rock. I was testing first metal gpu capture and since that time it's miraculous improvements.  Still ;)  Pixel history - please make it happen And if this is possible can Apple team work at least a bit with RenderDoc author so we can replay DX11/12 traces on Mac? It's a very big task/very complex but it's super big pain to need a Windows with RenderDoc to check stuff. If only RenderDoc can run on Mac. Of course I'd be happy if it can replay Metal as well, but it's a different story!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-any-documentation-for-the-new-metal-pipeline-script-json-format-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session" class="md-nav__link">
    Is there any documentation for the new Metal Pipeline Script JSON format?   Mentioned in the "Target and optimize GPU binaries with Metal 3" session.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-opt-out-of-qos-to-avoid-priority-decay-on-the-render-thread-if-we-are-writing-our-application-in-swift" class="md-nav__link">
    Is there a way to opt out of QoS to avoid priority decay on the render thread if we are writing our application in Swift?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rendering-the-mandelbrot-set-is-an-embarrassingly-parallel-smile-task-so-it-seems-perfect-for-the-gpu-i-wrote-a-metal-fragment-shader-to-do-this-but-you-cant-zoom-into-the-set-too-far-because-metal-only-supports-float-not-double-i-am-a-novice-gpu-programmer-so-bear-with-me-but-is-there-any-way-to-do-increased-precision-math-on-the-gpu-with-metal-i-understand-that-gpus-are-generally-much-slower-with-double-precision-if-they-even-support-it-but-it-seems-like-it-would-be-useful-for-scientific-computing-at-least-thank-you" class="md-nav__link">
    Rendering the Mandelbrot set is an "embarrassingly parallel" :smile: task, so it seems perfect for the GPU. I wrote a Metal fragment shader to do this, but you can't zoom into the set too far because Metal only supports float, not double. I am a novice GPU programmer, so bear with me, but is there any way to do increased precision math on the GPU with Metal? I understand that GPUs are generally much slower with double-precision, if they even support it, but it seems like it would be useful for scientific computing at least. Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-reference-templated-compute-functions-from-a-compute-pipeline-descriptor-such-that-the-templated-type-inherits-the-bound-textures-pixel-format" class="md-nav__link">
    Is there a way to reference templated compute functions from a compute pipeline descriptor such that the templated type inherits the bound texture's pixel format?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-generating-the-new-json-pipelines-script-suppose-to-work-on-macos-13-beta-and-xcode-14-beta-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session-trying-the-sample-terminal-commands-in-the-session-on-a-harvested-metallib-super-simply-draw-a-point-with-a-vertexfragment-shader-errored-out-with-metal-source-error-unsupported-binary-format" class="md-nav__link">
    Is generating the new JSON Pipelines Script suppose to work on MacOS 13 Beta and XCode 14 Beta? Mentioned in the "Target and optimize GPU binaries with Metal 3" session.  Trying the sample terminal commands in the session, on a harvested metallib (super simply draw a point, with a vertex/fragment shader), errored out with metal-source: error: unsupported binary format.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-possibility-to-render-3d-objects-with-scenekit-on-the-new-map-eg-a-car-driving-down-the-road" class="md-nav__link">
    Is there a possibility to render 3D objects with SceneKit on the new Map? e.g. a Car driving down the road...
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-new-objectmesh-shader-pipelines-are-pure-genius-when-will-we-get-a-chance-to-see-an-updated-metal-shading-language-spec-so-we-can-dig-into-the-details" class="md-nav__link">
    The new object/mesh shader pipelines are pure genius. When will we get a chance to see an updated Metal Shading Language Spec so we can dig into the details?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-the-new-object-and-mesh-shader-stage-it-looks-like-culling-techniques-can-be-refined-greatly-but-i-didnt-quite-grasp-the-meshlet-concept-discussed-on-the-presentation-is-there-some-external-theory-on-this-you-could-point-us-to-for-more-on-the-concept-or-is-this-something-that-we-will-be-able-to-do-in-the-new-pipeline-stage-ie-carve-up-a-mesh-into-smaller-chunks-on-the-fly" class="md-nav__link">
    In the new object and mesh shader stage, it looks like culling techniques can be refined greatly. But I didn’t quite grasp the meshlet concept discussed on the presentation. Is there some external theory on this you could point us to for more on the concept? Or is this something that we will be able to do in the new pipeline stage? (Ie carve up a mesh into smaller chunks on the fly)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-apple-choose-usdz-instead-of-gltf" class="md-nav__link">
    Why apple choose USDZ instead of GLTF?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-sample-code-which-uses-a-mtlsharedevent-im-looking-to-test-one-of-my-compute-pipelines-which-uses-a-shared-event-against-a-sample-pipeline-which-uses-the-shared-event-correctly-as-im-running-into-unexpected-behavior" class="md-nav__link">
    Is there sample code which uses a MTLSharedEvent? I'm looking to test one of my compute pipelines which uses a shared event against a sample pipeline which uses the shared event correctly as I'm running into unexpected behavior
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-i-render-a-scene-to-a-mtltexture-with-large-resolution-can-i-split-scene-to-a-small-pieces-and-async-render-them-or-i-need-to-render-a-whole-scene-al-once" class="md-nav__link">
    If I render a scene to a MTLTexture with large resolution can I split scene to a small pieces and async render them or I need to render a whole scene al once?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-would-you-break-down-a-scene-into-acceleration-structures-im-working-on-a-small-raytracing-side-project-and-im-a-bit-stuck-determining-how-a-scene-should-be-broken-down-im-using-model-io-to-load-some-assets-and-then-im-planning-to-represent-the-scene-with-an-instance-acceleration-structure-with-each-mdlmesh-mapping-to-a-primitive-acceleration-structure-does-this-sound-like-an-appropriate-breakdown" class="md-nav__link">
    How would you break down a scene into acceleration structures? I'm working on a small raytracing side project and I'm a bit stuck determining how a scene should be broken down. I'm using Model I/O to load some assets and then I'm planning to represent the scene with an instance acceleration structure, with each MDLMesh mapping to a primitive acceleration structure. Does this sound like an appropriate breakdown?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hello-are-there-any-plans-of-supporting-page-faulting-in-metal-for-the-future-the-purpose-is-to-have-an-11-cpugpu-va-mapping-with-even-file-mmap-being-accessible-from-the-gpu-side-to-be-able-to-layer-more-high-level-programming-models-on-top" class="md-nav__link">
    Hello, are there any plans of supporting page faulting in Metal for the future?   The purpose is to have an 1:1 CPU:GPU VA mapping, with even file mmap() being accessible from the GPU side, to be able to layer more high-level programming models on top.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#will-gpu-shader-debugging-for-mesh-shaders-be-ready-for-the-xcode-14-release-i-tried-debugging-the-sample-code-adjusting-the-level-of-detail-using-metal-mesh-shaders-httpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shadershttpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shaders-and-2-observations-1-there-didnt-seem-to-be-a-way-to-select-object-or-mesh-shader-to-debug-2-attempting-to-debug-the-fragment-shader-hung-xcode" class="md-nav__link">
    Will GPU Shader debugging for Mesh Shaders be ready for the Xcode 14 release?  I tried debugging the sample code "Adjusting the level of detail using Metal mesh shaders" (https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)|https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders) and 2 observations: 1. There didn't seem to be a way to select object or mesh shader to debug. 2. Attempting to debug the fragment shader, hung XCode.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-really-love-all-the-innovative-metal-3-features-i-may-be-a-little-early-asking-but-i-was-curious-about-whats-happening-with-mtlargumentencoders-are-they-being-deprecated-across-the-board-in-metal-3-such-that-we-can-take-advantage-of-the-streamlined-api-for-most-of-our-metal-2-code-it-also-looks-like-some-but-not-all-of-the-methods-are-deprecated-for-example-the-versions-that-can-set-multiple-buffers-at-once-in-a-range-is-that-correct" class="md-nav__link">
    Hey - Really love all the innovative Metal 3 features! I may be a little early asking, but I was curious about what’s happening with MTLArgumentEncoders. Are they being deprecated across the board in Metal 3 such that we can take advantage of the streamlined API for most of our Metal 2 code? It also looks like some, but not all, of the methods are deprecated. For example, the versions that can set multiple buffers at once in a range. Is that correct?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-cases-wed-need-to-interact-with-mtlgpuhandle-and-mtlresourceid-properties-i-see-them-added-in-a-bunch-of-places-but-unsure-where-or-by-whom-theyd-be-consumed" class="md-nav__link">
    Are there any cases we'd need to interact with MTLGPUHandle and MTLResourceID properties? I see them added in a bunch of places, but unsure where or by whom they'd be consumed.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-have-the-equivalent-of-work_group_barrier-clk_global_mem_fence-memory_scope_device-in-metal-all-threads-on-a-device-barriers-not-threadgroup-local-also-a-part-of-the-vulkan-memory-model" class="md-nav__link">
    Is there a way to have the equivalent of work_group_barrier (CLK_GLOBAL_MEM_FENCE, memory_scope_device) in Metal?   (all threads on a device barriers, not threadgroup local, also a part of the Vulkan memory model)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-never-used-mtlrenderpipelinereflection-before-what-sort-of-use-cases-is-it-meant-for-could-it-be-used-to-replace-shared-sourceshader-constants-defining-binding-indices" class="md-nav__link">
    I've never used MTLRenderPipelineReflection before. What sort of use cases is it meant for? Could it be used to replace shared source/shader constants defining binding indices?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heya-quick-shared-post-from-the-study-hall-section-thank-you-much-i-think-i-have-a-lot-to-learn-with-metal-now-ill-likely-start-with-basic-triangle-patterns-and-moving-those-around-with-a-compute-kernel-which-sounds-so-darn-fun-and-then-start-upgrading-to-the-mesh-shader-feature-now-that-ive-said-that-i-wonder-if-its-a-better-idea-to-maybe-start-with-mesh-shaders-ie-starting-from-the-new-tool-instead-of-building-up-to-it-the-context-being-im-brand-new-to-metal-and-am-looking-to-do-some-fairly-simple-direct-interactions-with-geometry-positions-and-eventually-texture-manipulations-for-things-like-highlighting-areas" class="md-nav__link">
    Heya! Quick shared post from the study-hall section, thank you much!  I think I have a lot to learn with Metal now. I’ll likely start with basic triangle patterns and moving those around with a compute kernel (which sounds so darn fun), and then start ‘upgrading’ to the mesh shader feature.   Now that I’ve said that, I wonder if it’s a better idea to maybe start with mesh shaders? I.e., starting from the new-tool instead of building up to it? The context being I'm brand new to Metal, and am looking to do some fairly simple direct interactions with geometry positions, and eventually texture manipulations for things like highlighting areas.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-trying-to-track-down-a-rare-metal-crash-in-our-app-were-seeing-this-in-two-completely-independent-subsystems-when-deallocating-metal-buffers-invariably-vertex-or-index-buffers" class="md-nav__link">
    I'm trying to track down a rare Metal crash in our app. We're seeing this in two completely independent subsystems when deallocating Metal buffers, invariably vertex or index buffers.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#have-there-been-any-changes-to-resource-limits-with-metal-3-specifically-the-maximum-number-of-buffers-inside-an-argument-buffer-thank-you" class="md-nav__link">
    Have there been any changes to resource limits with Metal 3? Specifically the maximum number of buffers inside an argument buffer. Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-any-list-where-i-can-find-which-devices-support-metal-3" class="md-nav__link">
    Is there any list where I can find which devices support Metal 3?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-noticed-that-in-compute-shader-access-to-a-2d-texture-is-faster-than-access-to-a-buffer-due-to-a-lot-more-of-a-cache-missed-how-does-caching-strategy-work-for-textures-vs-buffers-in-compute-can-we-specify-one-as-ive-pretty-good-idea-what-parts-of-the-buffer-will-be-accessed-from-which-thread" class="md-nav__link">
    I've noticed that in compute shader access to a 2d texture is faster, than access to a buffer. (Due to a lot more of a cache missed). How does caching strategy work for textures vs buffers in compute? Can we specify one (as i've pretty good idea what parts of the buffer will be accessed from which thread)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#does-screencapturekit-support-pulling-audio-from-different-processes-than-the-video-there-are-some-use-cases-where-audio-comes-from-one-process-but-the-window-and-graphics-come-from-another-process-this-is-common-in-situations-like-crossover-and-wine-this-feature-if-not-supported-would-be-very-useful" class="md-nav__link">
    Does ScreenCaptureKit support pulling audio from different processes than the video? There are some use cases where audio comes from one process but the window and graphics come from another process, this is common in situations like CrossOver and Wine. This feature if not supported would be very useful.
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="object-capture-and-room-plan-lounge.html" class="md-nav__link">
        object capture and room plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="photos-camera-lounge.html" class="md-nav__link">
        photos camera
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="accessibility-lounge.html" class="md-nav__link">
        accessibility
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="arkit-realitykit-usdz-lounge.html" class="md-nav__link">
        arkit realitykit
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="audio-and-video-lounge.html" class="md-nav__link">
        audio and video
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="design-lounge.html" class="md-nav__link">
        design
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="app-clips-lounge.html" class="md-nav__link">
        app clips
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="app-store-lounge.html" class="md-nav__link">
        app store
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="cloudkit-lounge.html" class="md-nav__link">
        cloudkit
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="coredata-lounge.html" class="md-nav__link">
        coredata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="device-management-lounge.html" class="md-nav__link">
        device management
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="education-lounge.html" class="md-nav__link">
        education
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="health-and-fitness-lounge.html" class="md-nav__link">
        health and fitness
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning-lounge.html" class="md-nav__link">
        machine learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="maps-lounge.html" class="md-nav__link">
        maps
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="privacy-and-security-lounge.html" class="md-nav__link">
        privacy and security
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="safari-lounge.html" class="md-nav__link">
        safari
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="sirikit-and-shortcuts-lounge.html" class="md-nav__link">
        sirikit and shortcuts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="widgetkit-lounge.html" class="md-nav__link">
        widgetkit
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#by-feetiki" class="md-nav__link">
    by FeeTiki
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#there-any-main-tradeoffs-between-tessellation-and-mesh-shading-for-vertex-creation" class="md-nav__link">
    There any main tradeoffs between tessellation and mesh shading for vertex creation?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#not-much-of-a-question-because-im-just-starting-to-move-towards-rendering-but-id-like-to-say-thanks-to-everyone-involved-with-this-the-documentation-tooling-and-apis-seems-better-than-ever-and-the-metalfx-upscaling-seems-a-game-changer-i-can-see-some-foveated-rendering-hacks-being-used-with-this-api-and-using-different-lod-with-the-fast-resource-cheers-lads-awesome-work" class="md-nav__link">
    Not much of a question because I'm just starting to move towards rendering but I'd like to say thanks to everyone involved with this. The documentation, tooling and APIs seems better than ever and the MetalFX Upscaling seems a game changer. I can see some foveated rendering hacks being used with this API and using different LOD with the fast resource. Cheers lads, awesome work.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-the-package-size-considerations-when-shipping-with-offline-compiled-metal-shaders-do-these-need-to-be-updated-over-time-to-keep-up-with-any-driver-or-software-updates-for-a-particular-piece-of-hardware" class="md-nav__link">
    What are the package size considerations when shipping with offline compiled metal shaders?  Do these need to be updated over time to keep up with any driver or software updates for a particular piece of hardware?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-start-creating-anything-in-metal-3-can-you-please-show-me-code-for-a-hello-world-type-metal-3-maybe-a-box-with-a-shaded-gradient-of-color-inside" class="md-nav__link">
    How do I start creating anything in Metal 3? Can you please show me code for a "Hello, World!" type Metal 3; maybe a box with a shaded gradient of color inside.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hi-are-metal-3-features-exclusive-to-apple-silicon-or-do-they-also-work-on-other-processors-eg-a-series-on-ios-or-the-gpus-in-intel-based-macs" class="md-nav__link">
    Hi, are Metal 3 features exclusive to Apple Silicon or do they also work on other processors (e.g A-series on iOS, or the GPUs in Intel-based Macs)?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-best-way-to-get-started-with-metal-as-a-beginner-in-the-graphics-field" class="md-nav__link">
    What's the best way to get started with Metal as a beginner in the graphics field?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-learning-graphics-coming-from-swift-and-am-really-interested-in-ray-tracing-what-should-i-learn-first-to-learn-how-to-make-a-hybrid-renderer-in-metal" class="md-nav__link">
    I’m learning graphics (coming from Swift) and am really interested in Ray tracing. What should I learn first to learn how to make a hybrid renderer in Metal?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-memory-and-texture-format-considerations-when-using-metalfx-im-guessing-the-historical-frames-need-to-be-stored-in-some-way-though-the-trade-off-would-be-smaller-intermediate-render-targets" class="md-nav__link">
    Are there any memory and texture format considerations when using MetalFX?  I'm guessing the historical frames need to be stored in some way, though the trade off would be smaller intermediate render targets.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#since-the-shaders-are-compiled-at-project-build-time-and-compilation-speed-is-no-longer-as-critical-is-that-extra-headroom-used-to-allow-the-compiler-to-try-to-optimize-the-shaders-further-or-is-does-it-produce-identical-machine-code-as-when-building-it-at-runtime" class="md-nav__link">
    Since the shaders are compiled at project build time and compilation speed is no longer as critical, is that extra headroom used to allow the compiler to try to optimize the shaders further? Or is does it produce identical machine code as when building it at runtime?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hi-great-wwdc-session-question-do-you-have-a-hello-world-example-for-making-shaders-using-apple-technologies-thank-you" class="md-nav__link">
    Hi! Great wwdc session, question: do you have a hello world example for making shaders using Apple technologies? Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-you-explain-how-to-use-triple-buffering-for-complicated-scene-with-2d-overlayui-and-3d-what-kind-of-data-i-should-store-in-buffers-in-vulkan-examples-they-use-framebuffer-but-metal-dont-have-any-similar-entity" class="md-nav__link">
    Can you explain how to use triple buffering for complicated scene with 2d (overlay/ui) and 3d? What kind of data I should store in buffers? In Vulkan examples they use framebuffer, but metal dont have any similar entity.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#didnt-see-it-earlier-new-to-slack-not-sure-if-this-is-the-right-place-to-ask-but-i-am-still-looking-through-the-plugins-but-would-it-be-able-to-use-bluetooth-or-related-tech-to-share-say-an-item-with-another-player-that-also-has-the-gameapp-like-if-there-is-a-unity-plugin-from-apple-that-does-this" class="md-nav__link">
    Didn't see it earlier, new to Slack:  "Not sure if this is the right place to ask, but I am still looking through the plugins. But would it be able to use Bluetooth or related tech to share say an item with another player that also has the game/app. Like if there is a Unity plugin from apple that does this. "
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-earliest-ios-you-can-use-this-with" class="md-nav__link">
    What's the earliest iOS you can use this with?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-were-making-feature-requests-i-really-like-the-ability-to-email-bug-reports-from-my-game-with-mfmailcomposeviewcontroller" class="md-nav__link">
    If we're making feature requests: I really like the ability to email bug reports from my game with MFMailComposeViewController!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-question-on-the-accessability-plugin-for-unity-is-it-only-available-for-apple-devices-or-can-it-also-be-used-for-windows-xbox-etc-reason-for-asking-is-that-it-would-be-great-to-see-a-cross-platform-plugin-to-increase-the-reach-of-accessability-making-it-easier-to-prioritize-for-stakeholders" class="md-nav__link">
    A question on the Accessability plugin for Unity. Is it only available for Apple devices or can it also be used for Windows, Xbox etc. Reason for asking is that it would be great to see a cross platform plugin to increase the reach of accessability (making it easier to prioritize for stakeholders)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-recommended-framework-for-connecting-to-another-player-stably-for-a-multiplayer-match-not-the-old-turn-based-games-but-a-modern-approach-to-having-a-5-10-minute-connection-for-pvp-would-arkit-realitykit-be-better-today-than-multipeer-connectivity-which-can-be-flaky-with-frequent-disconnects" class="md-nav__link">
    What is the recommended framework for connecting to another player stably for a multiplayer match? Not the old turn-based games, but a modern approach to having a 5-10 minute connection for PvP. (Would ARKit, RealityKit be better today than multipeer connectivity, which can be flaky with frequent disconnects?)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hi-it-looks-like-the-documentation-for-the-unity-plugins-has-not-been-submitted-to-the-github-repo-all-the-documentation-links-are-broken" class="md-nav__link">
    Hi! It looks like the documentation for the unity plugins has not been submitted to the GitHub repo -- all the documentation links are broken.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-had-a-quick-browse-of-the-accessibility-unity-plugin-and-noticed-that-its-only-supporting-ios-and-tvos-is-macos-planned-as-well-there-are-also-some-performance-concerns-with-the-code-that-id-love-to-see-resolved-will-github-prs-be-considered" class="md-nav__link">
    I've had a quick browse of the Accessibility Unity plugin and noticed that it's only supporting iOS and tvOS. Is macOS planned as well? There are also some performance concerns with the code that I'd love to see resolved, will GitHub PR's be considered?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-look-into-unity-plugins-and-have-a-relative-question-why-you-using-swift-instead-of-objc-in-applecore-module-why-swift-with-all-unmanaged-things-better-than-objc-but-thanks-for-that-bridging-i-grab-information-how-to-use-_cdecl" class="md-nav__link">
    I look into unity plugins and have a relative question: Why you using swift instead of objc in AppleCore module? Why Swift with all Unmanaged things better, than ObjC?   But thanks for that bridging! I grab information how to use _cdecl :)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-wheel-support-only-on-mac-the-api-seems-to-be-available-on-all-platforms-except-apple-watch-but-ive-only-seen-it-mentioned-with-regard-to-mac-am-i-misreading-something" class="md-nav__link">
    Is wheel support only on Mac? The API seems to be available on all platforms except Apple Watch, but I’ve only seen it mentioned with regard to Mac. Am I misreading something?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#could-mesh-shaders-be-used-for-procedural-terrain-generation-with-continuous-lod-currently-i-use-a-tessellation-compute-shader-which-also-does-some-basic-culling-followed-by-a-vertexfragment-shader" class="md-nav__link">
    Could mesh shaders be used for procedural terrain generation with continuous LOD? Currently I use a tessellation compute shader (which also does some basic culling) followed by a vertex/fragment shader.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-should-we-approach-issues-like-shader-debugger-crashes-and-gpu-faults-we-dont-know-where-to-look-for-hints" class="md-nav__link">
    How should we approach issues like Shader Debugger crashes and GPU faults? We don't know where to look for hints
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-we-somehow-cancel-work-committed-to-gpu-for-example-we-commited-some-neural-net-processing-example-of-huge-work-and-we-received-memory-warning-from-the-system-can-we-somehow-stop-execution" class="md-nav__link">
    Can we somehow cancel work committed to GPU? For example we commitеed some neural net processing (example of huge work) and we received memory warning from the system, can we somehow stop execution?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-best-way-to-denoise-the-ray-traced-frame-in-metal-is-mpssvgfdenoiser-recommended-for-this-purpose" class="md-nav__link">
    What is the best way to denoise the Ray traced frame in metal? Is MPSSVGFDenoiser recommended for this purpose?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#does-mfxspatialscalingeffect-support-pq-encoded-hdr-color-specifically-pixel-format-mtlpixelformatbgr10a2unorm-and-colorspace-kcgcolorspaceitur_2100_pq-if-so-which-colorprocessingmode-should-be-used" class="md-nav__link">
    Does MFXSpatialScalingEffect support PQ encoded HDR color? Specifically, pixel format MTLPixelFormatBGR10A2Unorm and colorspace kCGColorSpaceITUR_2100_PQ? If so, which colorProcessingMode should be used?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-demos-using-the-mtlparallelrendercommandencoder-is-there-a-possibility-of-supporting-a-parallel-compute-encoder-in-the-future" class="md-nav__link">
    Are there any demos using the MTLParallelRenderCommandEncoder? Is there a possibility of supporting a parallel compute encoder in the future?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-limit-to-the-scale-factor-outputwidth-inputwidth-that-mfxspatialscalingeffect-can-apply" class="md-nav__link">
    Is there a limit to the scale factor (outputWidth / inputWidth) that MFXSpatialScalingEffect can apply?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-most-performant-way-to-copy-a-calayer-into-a-mtltexture-on-ios" class="md-nav__link">
    What's the most performant way to copy a CALayer into a MTLTexture on iOS?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-new-edr-support-in-metal-for-ios-looks-really-exciting-but-on-what-current-devices-can-we-test-this-on" class="md-nav__link">
    The new EDR support in Metal for iOS looks really exciting, but on what current devices can we test this on?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-out-of-curiosity-what-are-the-potential-use-cases-of-quadgroup-functions-ive-only-seen-one-video-mentioning-them-discovering-advances-in-metal-for-a15-bionic-where-theyre-used-to-reduce-the-number-of-texture-reads-per-thread-are-there-any-other-use-common-use-cases" class="md-nav__link">
    More out of curiosity, what are the potential use cases of quadgroup functions? I've only seen one video mentioning them ("Discovering advances in Metal for A15 Bionic") where they're used to reduce the number of texture reads per thread. Are there any other use "common" use cases?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-interested-in-the-accelerated-ray-tracing-features-in-metal-3-but-the-specific-session-that-would-contain-more-details-is-tomorrow-are-these-changes-specifically-designed-for-realtime-raytracing-in-a-rasterized-renderer-or-are-they-general-enough-to-be-used-on-their-own-to-ray-trace-entire-scenes-i-see-that-there-are-general-purpose-ray-tracing-features-in-older-versions-of-metal-im-specifically-curious-how-these-intersect-with-the-metal-3-ray-tracing-features" class="md-nav__link">
    I'm interested in the accelerated ray tracing features in Metal 3, but the specific session that would contain more details is tomorrow. Are these changes specifically designed for realtime raytracing in a rasterized renderer, or are they general enough to be used on their own to ray trace entire scenes? I see that there are general purpose ray tracing features in older versions of Metal, I'm specifically curious how these intersect with the Metal 3 ray tracing features.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#as-far-as-i-can-tell-ios-devices-still-run-opengles-apps-just-fine-is-there-a-hard-deadline-when-this-will-not-be-the-case-any-more" class="md-nav__link">
    As far as I can tell iOS devices still run OpenGLES apps just fine. Is there a hard deadline when this will not be the case any more?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-recommended-formats-or-workflows-for-dealing-with-3d-textures-in-metal" class="md-nav__link">
    Are there any recommended formats or workflows for dealing with 3D textures in Metal?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-all-of-the-metal-3-features-available-on-all-gpus-do-vega-gpus-and-uhd-630-get-mesh-shaders-with-metal-3" class="md-nav__link">
    Are all of the Metal 3 features available on all GPUs?  Do Vega GPUs and UHD 630 get mesh shaders with Metal 3?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-question-what-happened-to-the-metal-language-version-build-setting-seemed-it-disappeared-one-year" class="md-nav__link">
    Quick question: What happened to the Metal Language Version build setting? Seemed it disappeared one year...
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-there-i-have-been-working-with-a-highly-programmatic-scenekit-nodes-and-geometries-i-know-i-know-old-hat-and-i-have-been-suggested-by-some-friendly-helpful-developers-at-the-wwdc-apple-park-event-hello-again-rintaro-that-a-pathway-to-help-some-of-my-performance-problems-would-be-to-rewrite-some-of-my-core-components-with-arkit-primitives-or-perhaps-metal-unfortunately-i-dont-really-know-where-i-would-begin-that-conversion-or-even-if-doing-the-work-would-net-the-performance-benefits-im-seeking-the-context-of-this-question-is-my-arvr-visualization-project-which-uses-10s-of-thousands-of-small-geometries-and-nodes-shared-as-much-possible-to-render-individual-text-glyphs-like-a-sheet-of-paper-httpgithubcomtikimcfeelookatthatgithubcomtikimcfeelookatthat-a-good-solid-first-step-would-be-what-would-be-a-suggested-migration-pattern-from-scnnodescnplaneuiimage-to-similar-metalarkit-patterns" class="md-nav__link">
    Hey there! I have been working with a highly programmatic SceneKit nodes and geometries (I know, I know, old hat), and I have been suggested by some friendly helpful developers at the WWDC Apple Park event (Hello again, Rintaro!) that a pathway to help some of my performance problems would be to 'rewrite' some of my core components with ARKit primitives, or perhaps Metal. Unfortunately, I don't really know where I would begin that conversion, or even if doing the work would net the performance benefits I'm seeking. The context of this question is my AR/VR visualization project which uses 10's of thousands of small geometries and nodes (shared as much possible) to render individual text glyphs like a sheet of paper (http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat). A good, solid first step would be: What would be a suggested migration pattern from SCNNode/SCNPlane/UIImage to similar Metal/ARKit patterns?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#with-metalfx-are-there-any-concerns-or-issues-when-dealing-with-transparencyvolumetrics-when-doing-temporal-upscaling-would-it-sometimes-be-better-to-use-spatial-upscaling-and-msaa-in-some-cases-with-a-lot-of-transparency" class="md-nav__link">
    With MetalFX are there any concerns or issues when dealing with transparency/volumetrics when doing temporal upscaling?  Would it sometimes be better to use Spatial upscaling and MSAA in some cases with a lot of transparency?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-screencapturekit-be-used-by-two-apps-at-once-each-capturing-all-or-parts-of-the-screen" class="md-nav__link">
    Can ScreenCaptureKit be used by two apps at once, each capturing all or parts of the screen?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-system-level-permission-required-even-if-an-app-only-wants-to-capture-itself-ie-only-its-own-windows-and-not-anything-external" class="md-nav__link">
    Is system-level permission required even if an app only wants to capture /itself/ (i.e. only its own window(s)), and not anything external?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#our-video-conferencing-app-offers-screen-sharing-functionality-but-we-want-the-screen-share-video-to-exclude-certain-windows-eg-av-controls-and-floating-participant-videos-this-is-easily-solvable-with-content-filtering-in-screencapturekit-but-what-is-the-recommended-way-to-get-this-behavior-with-older-versions-of-macos-that-do-not-support-screencapturekit" class="md-nav__link">
    Our video conferencing app offers screen sharing functionality, but we want the screen share video to exclude certain windows (e.g., A/V controls and floating participant videos). This is easily solvable with content filtering in ScreenCaptureKit, but what is the recommended way to get this behavior with older versions of macOS that do not support ScreenCaptureKit?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-you-want-to-display-the-interactions-with-menu-at-the-top-of-macos-does-that-constrain-you-to-just-capturing-a-display-vs-application-or-window-in-order-to-show-that" class="md-nav__link">
    If you want to display the interactions with menu at the top of macOS, does that constrain you to just capturing a display (vs. application or window) in order to show that?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-make-media-playback-systems-but-havent-really-yet-played-with-producing-media-could-you-give-me-an-idea-of-what-the-next-steps-would-be-in-terms-of-macos-frameworksapi-usage-to-go-from-screencapturekits-sample-buffers-to-say-a-playback-ready-h264hevcaac-hls-output" class="md-nav__link">
    I make media /playback/ systems, but haven't really yet played with /producing/ media– could you give me an idea of what the next steps would be (in terms of macOS frameworks/API usage), to go from ScreenCaptureKit's sample buffers, to, say, a 'playback-ready' H264/HEVC+AAC HLS output?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-capturing-windows-only-is-the-windows-drop-shadow-captured-or-only-the-actual-frame-of-the-windows" class="md-nav__link">
    When capturing windows only, is the window's drop shadow captured, or only the actual frame of the windows?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#would-be-great-to-be-able-to-make-screen-captures-either-screenshots-or-screen-recordings-in-hdr" class="md-nav__link">
    Would be great to be able to make screen captures (either screenshots or screen recordings) in HDR!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#one-thing-that-wasnt-totally-clear-to-me-from-the-videos-obviously-audio-capture-is-per-application-since-audio-is-not-associated-with-a-window-if-i-exclude-only-some-windows-of-an-application-from-capture-does-that-mean-i-dont-get-any-audio" class="md-nav__link">
    One thing that wasn't totally clear to me from the videos: Obviously, audio-capture is per-application, since audio is not associated with a window. If I exclude only some windows of an application from capture, does that mean I don't get any audio?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-im-capturing-all-windows-from-a-single-application-will-i-also-get-xpc-hosted-dialogs-im-thinking-of-opensave-panels-and-the-like-which-were-not-easily-captured-with-the-cgwindowlist-api-since-they-are-associated-with-a-different-pid-than-the-application-windows" class="md-nav__link">
    If I'm capturing all windows from a single application, will I also get XPC-hosted dialogs? I'm thinking of Open/Save panels, and the like, which were not easily captured with the CGWindowList API, since they are associated with a different PID than the application windows.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#just-to-reiterate-my-pledge-please-please-push-mip-bias-as-a-part-of-the-sampler-for-next-metal-releases-i-think-even-gl-had-it" class="md-nav__link">
    Just to reiterate my pledge - please please push mip bias as a part of the sampler for next metal releases. I think even GL had it :)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-use-a-constant-jitter-offset-for-a-set-of-pixels-instead-of-a-random-one-that-would-probably-make-the-result-more-smoother" class="md-nav__link">
    Why use a constant jitter offset for a set of pixels instead of a random one that would probably make the result more smoother?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wonderful-presentation-kevin-thank-you-very-much-the-technology-works-beautifully-with-slow-moving-scenes-could-you-comment-on-the-impact-of-using-temporal-information-on-quick-moving-scenes-or-quick-changing-content-should-there-be-some-adaptive-setting-with-respect-to-the-amount-of-previous-pixels-used-based-on-the-richnessrate-of-change-of-the-scenes-content" class="md-nav__link">
    Wonderful presentation @Kevin! Thank you very much! The technology works beautifully with slow moving scenes. Could you comment on the impact of using temporal information on quick moving scenes or quick changing content? Should there be some adaptive setting with respect to the amount of previous pixels used based on the richness/rate of change of the scene's content?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#machine-learning-has-shown-great-results-with-respect-to-upscaling-both-in-terms-of-quality-as-well-as-performance-do-you-see-implementing-such-a-functionality-in-metalfx-over-the-near-future" class="md-nav__link">
    Machine learning has shown great results with respect to upscaling both in terms of quality as well as performance. Do you see implementing such a functionality in MetalFX over the near future?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-couldnt-help-noticing-watching-the-no-mans-sky-clip-that-there-was-quite-a-bit-of-smearing-and-things-looked-fairly-low-quality-on-the-big-camera-move-are-these-kind-of-camera-moves-a-big-concern-with-this-technique-i-understand-taa-can-have-issues-with-smearing-and-ghosting" class="md-nav__link">
    I couldn't help noticing watching the No Man's Sky clip that there was quite a bit of smearing and things looked fairly low quality on the big camera move.  Are these kind of camera moves a big concern with this technique.  I understand TAA can have issues with smearing and ghosting.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-lab-is-best-for-asking-scenekit-questions-this-year" class="md-nav__link">
    What lab is best for asking SceneKit questions this year?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-best-way-to-render-biplanar-edr-cvpixelbuffer-from-avfoundation-in-metal-is-better-to-convert-pixelbuffer-into-mtltexture-via-compute-or-sample-from-2-mtltextures-created-from-each-plane" class="md-nav__link">
    What is the best way to render biplanar EDR cvpixelbuffer from avfoundation in Metal? Is better to convert pixelbuffer into mtltexture via compute, or sample from 2 mtltextures created from each plane?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-the-best-way-to-build-a-full-screen-game-experience-on-the-mac-should-we-still-be-using-cgdisplaycapture-or-the-appkit-full-screen-window-support-both-seem-to-have-pros-and-cons-for-example-appkit-allows-the-menu-bar-to-pop-and-for-easy-access-but-cgdisplaycapture-can-use-the-area-around-the-notch" class="md-nav__link">
    What's the best way to build a full screen game experience on the Mac? Should we still be using CGDisplayCapture? Or the AppKit full screen window support? Both seem to have pros and cons. For example: AppKit allows the menu bar to pop and for easy access. But CGDisplayCapture can use the area around the notch.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hello-everybody-is-there-any-sample-or-instructions-how-to-make-offscreen-render-using-metal-for-example-i-have-a-scene-and-i-want-to-save-it-as-png-file-how-can-i-do-it-can-i-control-resolution-of-saved-file" class="md-nav__link">
    Hello everybody! Is there any sample or instructions how to make offscreen render using Metal? For example: I have a scene and I want to save it as png file. How can I do it? Can I control resolution of saved file?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-you-leverage-the-screen-capture-api-to-cast-data-that-is-being-rendered-off-screen-eg-two-cameras-in-a-scene-one-camera-view-is-your-ipad-screen-another-camera-view-would-be-streamed-to-apple-tv-each-camera-are-viewing-different-location-but-in-the-same-scene" class="md-nav__link">
    Can you leverage the screen capture API to cast data that is being rendered off screen? eg two cameras in a scene, one camera view is your ipad screen, another camera view would be streamed to apple TV. each camera are viewing different location but in the same scene.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-sorry-i-wasnt-able-to-watch-the-video-so-just-wanted-to-confirm-with-screencapturekit-were-able-to-capture-the-mixed-system-sound-or-filter-in-or-out-the-applications-that-we-want-or-dont-want-to-include-right-for-example-capture-sounds-from-my-own-application-zoom-call-already-mixed-for-us-basically-replacing-things-like-blackhole-multi-outputdevice-etc" class="md-nav__link">
    I'm sorry, I wasn't able to watch the video, so just wanted to confirm: with ScreenCaptureKit we're able to capture the mixed system sound, or filter in or out the applications that we want or don't want to include, right? For example capture sounds from my own application + Zoom call already mixed for us, basically replacing things like Blackhole + Multi-Output/Device etc?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#carenderer-was-not-previously-available-on-ios-only-macos-but-xcode-14-and-docs-on-the-web-now-show-it-as-available-since-ios-20-httpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minorhttpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minor-is-this-a-mistake-or-is-it-now-available-to-older-devices-if-built-with-the-ios-16-sdk" class="md-nav__link">
    CARenderer was not previously available on iOS (only macOS). But Xcode 14 and docs on the web now show it as available since iOS 2.0!  https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor|https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor  Is this a mistake, or is it now available to older devices if built with the iOS 16 SDK?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-the-fastest-way-to-get-something-on-screen-with-metal-is-there-way-to-draw-directly-to-screen-instead-of-metal-layer-contents-and-then-wait-for-frame-composing" class="md-nav__link">
    what is the fastest way to get something on screen with Metal? is there way to draw directly to screen instead of metal layer contents and then wait for frame composing?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#might-anyone-be-able-to-speak-to-whether-it-makes-sense-to-combine-indirect-command-buffers-httpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjchttpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjc-with-bindless-rendering-and-dynamic-data-specifically-i-am-trying-to-avoid-a-lot-of-draw-calls-with-the-soc-architecture-it-seems-passing-data-is-not-really-expensive-so-i-figure-per-frame-i-could-just-specify-some-parameters-and-generate-draw-calls-on-the-gpu-as-in-the-example-broad-question-i-realize-but-id-appreciate-some-thoughts-on-how-to-reduce-draw-calls-for-rapidly-updated-andor-deleted-vertex-data-i-also-checked-mesh-shaders-but-i-think-those-are-overkill-for-my-use-case" class="md-nav__link">
    Might anyone be able to speak to whether it makes sense to combine indirect command buffers (https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc|https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc ) with bindless rendering and dynamic data? Specifically, I am trying to avoid a lot of draw calls. With the SOC architecture, it seems passing data is not really expensive, so I figure per-frame I could just specify some parameters and generate draw calls on the GPU, as in the example.  -broad question, I realize, but I'd appreciate some thoughts on how to reduce draw calls for rapidly-updated and/or deleted vertex data.  I also checked mesh shaders, but I think those are overkill for my use case.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-will-the-activity-feed-be-available" class="md-nav__link">
    When will the activity feed be available?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hello-wave-playing-around-with-the-new-screencapturekit-im-experiencing-problems-with-tcc-revoking-access-to-screen-recording-while-the-apps-running-domain-comapplescreencapturekitscstreamerrordomain-code-3801-message-the-user-declined-tccs-for-application-window-display-capture-confusingly-when-my-app-launched-it-did-have-the-proper-access-anyone-around-knowing-under-which-special-circumstances-the-system-may-suddenly-revoke-access-to-screen-recording-for-an-already-running-app" class="md-nav__link">
    Hello! :wave: Playing around with the new ScreenCaptureKit I'm experiencing problems with TCC, revoking access to Screen Recording while the app's running  (domain: com.apple.ScreenCaptureKit.SCStreamErrorDomain, code -3801 message: "The user declined TCCs for application, window, display capture"). Confusingly, when my app launched it did have the proper access.  Anyone around knowing under which special circumstances the system may suddenly revoke access to Screen Recording for an already running app?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-all-question-is-related-to-scenekit-uiview-swiftui-view-layers-there-has-been-some-historical-functionality-allowing-the-layers-of-views-not-presented-in-a-typical-hierarchy-to-be-presented-as-the-material-contents-of-geometries-like-scnplanes-one-example-is-this-repo-here-httpsgithubcomsarangborudeswiftuiarviewhttpsgithubcomsarangborudeswiftuiarview-unfortunately-the-technique-here-doesnt-seem-to-work-anymore-a-couple-of-years-later-im-wondering-what-might-be-possible-to-bring-this-back-to-usage-as-i-have-a-ton-of-use-cases-id-love-to-explore-with-this-thank-you-so-much-as-always" class="md-nav__link">
    Hey all! Question is related to SceneKit + UIView / SwiftUI view layers. There has been some historical functionality allowing the layers of views not presented in a typical hierarchy to be presented as the material contents of geometries like SCNPlanes. One example is this repo here: https://github.com/sarangborude/SwiftUIARView|https://github.com/sarangborude/SwiftUIARView . Unfortunately, the technique here doesn't seem to work anymore a couple of years later. I'm wondering what might be possible to bring this back to usage, as I have a TON of use cases I'd love to explore with this! Thank you so much as always!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#for-a-relatively-simple-game-low-poly-graphics-simple-textures-no-ai-which-framework-would-you-suggest-i-use-to-develop-in-swift-all-logic-would-be-custom-so-no-need-for-scripts-library-or-similar-features-scenekit-seems-like-an-obvious-choice-as-it-has-the-functionality-i-need-but-im-not-sure-how-actively-it-is-still-being-maintained-and-supported" class="md-nav__link">
    For a relatively simple game (low poly graphics, simple textures, no AI), which framework would you suggest I use to develop in Swift? All logic would be custom, so no need for scripts library or similar features. SceneKit seems like an obvious choice as it has the functionality I need, but I'm not sure how actively it is still being maintained and supported?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-send-a-raw-mtltexture-or-a-specific-view-to-screencapturekit-or-some-lower-level-api-related-to-it-for-example-lets-say-i-have-a-real-time-graphics-application-written-using-metal-and-i-want-to-stream-content-containing-only-audience-relevant-elements-in-my-scene-rendered-to-one-texture-i-broadcast-that-texture-in-another-renderpass-i-composite-some-diagnostic-info-only-relevant-to-me-locally-that-i-dont-want-to-share-with-others-how-might-this-be-achieved-its-a-use-case-i-keep-running-into-asymmetric-views-and-perspectives-going-even-further-maybe-i-am-developing-a-game-and-i-want-to-render-a-completely-different-perspective-in-my-scene-from-a-different-camera-i-send-that-texture-to-the-stream-but-maybe-i-render-from-a-different-perspective-locally" class="md-nav__link">
    Is there a way to send a raw MTLTexture or a specific view to ScreenCaptureKit, or some lower-level API related to it? For example, let's say I have a real-time graphics application written using Metal, and I want to stream content containing only audience-relevant elements in my scene, rendered to one texture.  I broadcast that texture. In another renderpass, I composite some diagnostic info only relevant to me locally that I don't want to share with others.  How might this be achieved? It's a use case I keep running into (asymmetric views and perspectives.)  Going even further, maybe I am developing a game and I want to render a completely different perspective in my scene from a different camera. I send that texture to the stream, but maybe I render from a different perspective locally.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#would-screencapturekit-be-appropriate-for-a-remote-desktopscreen-sharing-application" class="md-nav__link">
    Would ScreenCaptureKit be appropriate for a remote desktop/screen sharing application?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-have-a-classic-dumb-question-is-screencapturekit-the-macos-equivalent-of-replaykit-i-want-people-to-be-able-to-create-reaction-videos-of-them-playing-touchgrams-which-is-running-on-spritekit" class="md-nav__link">
    I have a classic "dumb question" - is ScreenCaptureKit the macOS equivalent of ReplayKit? I want people to be able to create reaction videos of them playing touchgrams, which is running on SpriteKit.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-want-to-record-the-whole-screen-but-i-keep-getting-the-microphone-active-orange-dot-in-the-recording-how-can-i-make-a-recording-of-a-screen-using-screencapturekit-without-the-orange-dot" class="md-nav__link">
    I want to record the whole screen but I keep getting the microphone active orange dot in the recording. How can I make a recording of a screen using ScreenCaptureKit without the orange dot?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#can-screencapturekit-capture-an-off-screen-window-as-in-if-i-wanted-to-capture-some-visuals-that-normally-show-in-a-window-1920x1080-in-my-app-but-theres-no-need-to-show-the-user-because-they-just-care-about-the-resulting-captured-output-if-so-does-that-seemingly-make-sense-as-a-use-of-sck-or-are-there-more-direct-methods-for-app-internal-view-capture-youd-suggest-instead" class="md-nav__link">
    Can ScreenCaptureKit capture an 'off-screen' window?– as in, if I wanted to capture some visuals that normally show in a window (1920x1080) in my app, but there's no need to show the user (because they just care about the /resulting/ captured output)  &amp; if so: does that seemingly /make sense/ as a use of SCK, or are there more 'direct' methods for 'app-internal' view capture you'd suggest instead?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-i-use-sccontentfilterinitdesktopindependentwindow-in-screencapturekit-the-popup-buttons-contextual-menus-sheets-that-appear-on-the-window-are-not-captured-is-there-any-way-to-capture-them-in-screencapturekit-previously-i-implemented-using-cgwindowlistcopywindowinfooptionincludingwindow-optiononscreenabovewindow-windowid-and-cgimageinitwindowlistfromarrayscreenbounds-windowarray-imageoption-api-but-i-would-like-to-know-how-it-is-possible-in-screencapturekit" class="md-nav__link">
    When I use SCContentFilter.init(desktopIndependentWindow:) in ScreenCaptureKit, the popup buttons, contextual menus, sheets that appear on the window are not captured. Is there any way to capture them in ScreenCaptureKit?  Previously, I implemented using CGWindowListCopyWindowInfo([.optionIncludingWindow, .optionOnScreenAboveWindow], windowID) and CGImage.init?(windowListFromArrayScreenBounds:, windowArray:, imageOption:) API, but I would like to know how it is possible in ScreenCaptureKit.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#id-like-to-use-reality-composer-in-the-new-xcode-to-generate-simple-usdz-shapes-then-use-the-new-reality-converter-to-re-texture-these-shapes-because-reality-composer-cannot-add-custom-textures-in-reality-composer-you-have-to-enable-usdz-export-in-preferences-but-when-i-export-a-simple-box-to-usdz-reality-converter-cannot-open-it-conversion-failed-1-error-which-is-unexpectedunknown-is-there-a-secret-way-to-make-this-work-thanks-for-leaving-questions-open-im-in-australia-i-realise-this-not-quite-the-right-forum-for-this-but-the-right-forum-hasnt-left-questions-open-and-its-open-around-2-3am-my-time" class="md-nav__link">
    I'd like to use Reality Composer (in the new Xcode) to generate simple USDZ shapes, then use the new Reality Converter to re-texture these shapes (because Reality Composer cannot add custom textures). In Reality Composer, you have to enable USDZ export in Preferences, but when I export a simple box to USDZ, Reality Converter cannot open it (Conversion failed: 1 error, which is unexpected/unknown). Is there a secret way to make this work?  (Thanks for leaving questions open — I'm in Australia. I realise this not quite the right forum for this, but the right forum hasn't left questions open, and it's open around 2-3am my time.)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-hlsl-texture2d-and-texture2d_array-are-treated-almost-the-same-ie-texture2d-can-be-used-instead-of-array-one-however-in-metal-the-type-is-much-more-strict-does-it-mean-that-the-only-way-in-metal-is-to-create-respected-texture-view-to-convert-types-the-texture-view-looks-to-be-performance-hit" class="md-nav__link">
    In HLSL Texture2d and Texture2d_array are treated almost the same i.e. texture2d can be used instead of array one. However in metal the type is much more strict. Does it mean that the only way in Metal is to create respected texture view to convert types? The texture view looks to be performance hit :(
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-just-moved-from-my-old-and-hot-intel-imac-to-an-m1-max-macbook-pro-which-im-loving-one-snag-our-games-still-run-on-opengl-es-but-the-ios-simulator-on-apple-silicon-seems-to-crash-when-executing-opengl-es-commands-is-it-game-over-do-i-finally-need-to-move-to-metal" class="md-nav__link">
    I've just moved from my old and hot Intel iMac, to an M1 Max MacBook Pro, which I'm loving! One snag - our games still run on OpenGL ES, but the iOS Simulator on Apple Silicon seems to crash when executing OpenGL ES commands. Is it Game Over? Do I finally need to move to Metal?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-thanks-for-all-your-hard-work-and-cool-new-apis-i-watched-meet-distributed-actors-in-swift-and-im-curious-if-theres-currently-a-way-to-use-distributed-actors-in-a-peer-to-peer-gkmatch" class="md-nav__link">
    First, thanks for all your hard work and cool new APIs!  I watched "Meet distributed actors in Swift", and I'm curious if there's currently a way to use distributed actors in a peer-to-peer GKMatch?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-there-heres-a-basic-one-with-some-context-im-a-mobile-developer-working-for-about-a-decade-in-the-space-and-i-have-very-little-experience-with-high-fidelity-graphics-and-their-libraries-i-am-highly-interested-in-learning-c-especially-to-interact-with-the-new-metal-cpp-tools-my-question-is-would-the-metal-cpp-examples-shared-here-in-this-lounge-be-a-good-starting-point-into-learning-enough-c-to-be-productive-with-metal-or-is-there-a-better-starting-point-that-may-be-a-more-gradual-or-helpful-introduction-to-the-tools-i-dont-mind-hitting-the-ground-running-so-to-speak-just-curious-to-see-what-paths-are-most-suggested-for-this-thank-you" class="md-nav__link">
    Hey there! Here's a basic one, with some context: I'm a mobile developer working for about a decade in the space, and I have very little experience with high-fidelity graphics and their libraries. I am highly interested in learning C++, especially to interact with the new metal-cpp tools. My question is, would the metal-cpp examples shared here in this lounge be a good starting point into learning enough C++ to be productive with Metal? Or, is there a better starting point that may be a more gradual or helpful introduction to the tools? I don't mind hitting the ground running so to speak, just curious to see what paths are most suggested for this! Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#question-re-core-media-io-extension-how-the-host-app-can-communicate-with-the-extension-can-hostextension-exchange-iosurface-for-example" class="md-nav__link">
    Question re Core Media IO Extension. How the host app can communicate with the extension? Can host/extension exchange IOSurface for example?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#just-want-to-say-huuuge-thank-you-to-all-xcode-gpu-devteam-you-guys-rock-i-was-testing-first-metal-gpu-capture-and-since-that-time-its-miraculous-improvements-still-pixel-history-please-make-it-happen-and-if-this-is-possible-can-apple-team-work-at-least-a-bit-with-renderdoc-author-so-we-can-replay-dx1112-traces-on-mac-its-a-very-big-taskvery-complex-but-its-super-big-pain-to-need-a-windows-with-renderdoc-to-check-stuff-if-only-renderdoc-can-run-on-mac-of-course-id-be-happy-if-it-can-replay-metal-as-well-but-its-a-different-story" class="md-nav__link">
    Just want to say HUUUGE thank you to all XCode GPU dev.team. You guys rock. I was testing first metal gpu capture and since that time it's miraculous improvements.  Still ;)  Pixel history - please make it happen And if this is possible can Apple team work at least a bit with RenderDoc author so we can replay DX11/12 traces on Mac? It's a very big task/very complex but it's super big pain to need a Windows with RenderDoc to check stuff. If only RenderDoc can run on Mac. Of course I'd be happy if it can replay Metal as well, but it's a different story!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-any-documentation-for-the-new-metal-pipeline-script-json-format-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session" class="md-nav__link">
    Is there any documentation for the new Metal Pipeline Script JSON format?   Mentioned in the "Target and optimize GPU binaries with Metal 3" session.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-opt-out-of-qos-to-avoid-priority-decay-on-the-render-thread-if-we-are-writing-our-application-in-swift" class="md-nav__link">
    Is there a way to opt out of QoS to avoid priority decay on the render thread if we are writing our application in Swift?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rendering-the-mandelbrot-set-is-an-embarrassingly-parallel-smile-task-so-it-seems-perfect-for-the-gpu-i-wrote-a-metal-fragment-shader-to-do-this-but-you-cant-zoom-into-the-set-too-far-because-metal-only-supports-float-not-double-i-am-a-novice-gpu-programmer-so-bear-with-me-but-is-there-any-way-to-do-increased-precision-math-on-the-gpu-with-metal-i-understand-that-gpus-are-generally-much-slower-with-double-precision-if-they-even-support-it-but-it-seems-like-it-would-be-useful-for-scientific-computing-at-least-thank-you" class="md-nav__link">
    Rendering the Mandelbrot set is an "embarrassingly parallel" :smile: task, so it seems perfect for the GPU. I wrote a Metal fragment shader to do this, but you can't zoom into the set too far because Metal only supports float, not double. I am a novice GPU programmer, so bear with me, but is there any way to do increased precision math on the GPU with Metal? I understand that GPUs are generally much slower with double-precision, if they even support it, but it seems like it would be useful for scientific computing at least. Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-reference-templated-compute-functions-from-a-compute-pipeline-descriptor-such-that-the-templated-type-inherits-the-bound-textures-pixel-format" class="md-nav__link">
    Is there a way to reference templated compute functions from a compute pipeline descriptor such that the templated type inherits the bound texture's pixel format?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-generating-the-new-json-pipelines-script-suppose-to-work-on-macos-13-beta-and-xcode-14-beta-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session-trying-the-sample-terminal-commands-in-the-session-on-a-harvested-metallib-super-simply-draw-a-point-with-a-vertexfragment-shader-errored-out-with-metal-source-error-unsupported-binary-format" class="md-nav__link">
    Is generating the new JSON Pipelines Script suppose to work on MacOS 13 Beta and XCode 14 Beta? Mentioned in the "Target and optimize GPU binaries with Metal 3" session.  Trying the sample terminal commands in the session, on a harvested metallib (super simply draw a point, with a vertex/fragment shader), errored out with metal-source: error: unsupported binary format.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-possibility-to-render-3d-objects-with-scenekit-on-the-new-map-eg-a-car-driving-down-the-road" class="md-nav__link">
    Is there a possibility to render 3D objects with SceneKit on the new Map? e.g. a Car driving down the road...
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-new-objectmesh-shader-pipelines-are-pure-genius-when-will-we-get-a-chance-to-see-an-updated-metal-shading-language-spec-so-we-can-dig-into-the-details" class="md-nav__link">
    The new object/mesh shader pipelines are pure genius. When will we get a chance to see an updated Metal Shading Language Spec so we can dig into the details?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-the-new-object-and-mesh-shader-stage-it-looks-like-culling-techniques-can-be-refined-greatly-but-i-didnt-quite-grasp-the-meshlet-concept-discussed-on-the-presentation-is-there-some-external-theory-on-this-you-could-point-us-to-for-more-on-the-concept-or-is-this-something-that-we-will-be-able-to-do-in-the-new-pipeline-stage-ie-carve-up-a-mesh-into-smaller-chunks-on-the-fly" class="md-nav__link">
    In the new object and mesh shader stage, it looks like culling techniques can be refined greatly. But I didn’t quite grasp the meshlet concept discussed on the presentation. Is there some external theory on this you could point us to for more on the concept? Or is this something that we will be able to do in the new pipeline stage? (Ie carve up a mesh into smaller chunks on the fly)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-apple-choose-usdz-instead-of-gltf" class="md-nav__link">
    Why apple choose USDZ instead of GLTF?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-sample-code-which-uses-a-mtlsharedevent-im-looking-to-test-one-of-my-compute-pipelines-which-uses-a-shared-event-against-a-sample-pipeline-which-uses-the-shared-event-correctly-as-im-running-into-unexpected-behavior" class="md-nav__link">
    Is there sample code which uses a MTLSharedEvent? I'm looking to test one of my compute pipelines which uses a shared event against a sample pipeline which uses the shared event correctly as I'm running into unexpected behavior
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#if-i-render-a-scene-to-a-mtltexture-with-large-resolution-can-i-split-scene-to-a-small-pieces-and-async-render-them-or-i-need-to-render-a-whole-scene-al-once" class="md-nav__link">
    If I render a scene to a MTLTexture with large resolution can I split scene to a small pieces and async render them or I need to render a whole scene al once?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-would-you-break-down-a-scene-into-acceleration-structures-im-working-on-a-small-raytracing-side-project-and-im-a-bit-stuck-determining-how-a-scene-should-be-broken-down-im-using-model-io-to-load-some-assets-and-then-im-planning-to-represent-the-scene-with-an-instance-acceleration-structure-with-each-mdlmesh-mapping-to-a-primitive-acceleration-structure-does-this-sound-like-an-appropriate-breakdown" class="md-nav__link">
    How would you break down a scene into acceleration structures? I'm working on a small raytracing side project and I'm a bit stuck determining how a scene should be broken down. I'm using Model I/O to load some assets and then I'm planning to represent the scene with an instance acceleration structure, with each MDLMesh mapping to a primitive acceleration structure. Does this sound like an appropriate breakdown?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hello-are-there-any-plans-of-supporting-page-faulting-in-metal-for-the-future-the-purpose-is-to-have-an-11-cpugpu-va-mapping-with-even-file-mmap-being-accessible-from-the-gpu-side-to-be-able-to-layer-more-high-level-programming-models-on-top" class="md-nav__link">
    Hello, are there any plans of supporting page faulting in Metal for the future?   The purpose is to have an 1:1 CPU:GPU VA mapping, with even file mmap() being accessible from the GPU side, to be able to layer more high-level programming models on top.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#will-gpu-shader-debugging-for-mesh-shaders-be-ready-for-the-xcode-14-release-i-tried-debugging-the-sample-code-adjusting-the-level-of-detail-using-metal-mesh-shaders-httpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shadershttpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shaders-and-2-observations-1-there-didnt-seem-to-be-a-way-to-select-object-or-mesh-shader-to-debug-2-attempting-to-debug-the-fragment-shader-hung-xcode" class="md-nav__link">
    Will GPU Shader debugging for Mesh Shaders be ready for the Xcode 14 release?  I tried debugging the sample code "Adjusting the level of detail using Metal mesh shaders" (https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)|https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders) and 2 observations: 1. There didn't seem to be a way to select object or mesh shader to debug. 2. Attempting to debug the fragment shader, hung XCode.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hey-really-love-all-the-innovative-metal-3-features-i-may-be-a-little-early-asking-but-i-was-curious-about-whats-happening-with-mtlargumentencoders-are-they-being-deprecated-across-the-board-in-metal-3-such-that-we-can-take-advantage-of-the-streamlined-api-for-most-of-our-metal-2-code-it-also-looks-like-some-but-not-all-of-the-methods-are-deprecated-for-example-the-versions-that-can-set-multiple-buffers-at-once-in-a-range-is-that-correct" class="md-nav__link">
    Hey - Really love all the innovative Metal 3 features! I may be a little early asking, but I was curious about what’s happening with MTLArgumentEncoders. Are they being deprecated across the board in Metal 3 such that we can take advantage of the streamlined API for most of our Metal 2 code? It also looks like some, but not all, of the methods are deprecated. For example, the versions that can set multiple buffers at once in a range. Is that correct?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#are-there-any-cases-wed-need-to-interact-with-mtlgpuhandle-and-mtlresourceid-properties-i-see-them-added-in-a-bunch-of-places-but-unsure-where-or-by-whom-theyd-be-consumed" class="md-nav__link">
    Are there any cases we'd need to interact with MTLGPUHandle and MTLResourceID properties? I see them added in a bunch of places, but unsure where or by whom they'd be consumed.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-a-way-to-have-the-equivalent-of-work_group_barrier-clk_global_mem_fence-memory_scope_device-in-metal-all-threads-on-a-device-barriers-not-threadgroup-local-also-a-part-of-the-vulkan-memory-model" class="md-nav__link">
    Is there a way to have the equivalent of work_group_barrier (CLK_GLOBAL_MEM_FENCE, memory_scope_device) in Metal?   (all threads on a device barriers, not threadgroup local, also a part of the Vulkan memory model)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-never-used-mtlrenderpipelinereflection-before-what-sort-of-use-cases-is-it-meant-for-could-it-be-used-to-replace-shared-sourceshader-constants-defining-binding-indices" class="md-nav__link">
    I've never used MTLRenderPipelineReflection before. What sort of use cases is it meant for? Could it be used to replace shared source/shader constants defining binding indices?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heya-quick-shared-post-from-the-study-hall-section-thank-you-much-i-think-i-have-a-lot-to-learn-with-metal-now-ill-likely-start-with-basic-triangle-patterns-and-moving-those-around-with-a-compute-kernel-which-sounds-so-darn-fun-and-then-start-upgrading-to-the-mesh-shader-feature-now-that-ive-said-that-i-wonder-if-its-a-better-idea-to-maybe-start-with-mesh-shaders-ie-starting-from-the-new-tool-instead-of-building-up-to-it-the-context-being-im-brand-new-to-metal-and-am-looking-to-do-some-fairly-simple-direct-interactions-with-geometry-positions-and-eventually-texture-manipulations-for-things-like-highlighting-areas" class="md-nav__link">
    Heya! Quick shared post from the study-hall section, thank you much!  I think I have a lot to learn with Metal now. I’ll likely start with basic triangle patterns and moving those around with a compute kernel (which sounds so darn fun), and then start ‘upgrading’ to the mesh shader feature.   Now that I’ve said that, I wonder if it’s a better idea to maybe start with mesh shaders? I.e., starting from the new-tool instead of building up to it? The context being I'm brand new to Metal, and am looking to do some fairly simple direct interactions with geometry positions, and eventually texture manipulations for things like highlighting areas.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#im-trying-to-track-down-a-rare-metal-crash-in-our-app-were-seeing-this-in-two-completely-independent-subsystems-when-deallocating-metal-buffers-invariably-vertex-or-index-buffers" class="md-nav__link">
    I'm trying to track down a rare Metal crash in our app. We're seeing this in two completely independent subsystems when deallocating Metal buffers, invariably vertex or index buffers.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#have-there-been-any-changes-to-resource-limits-with-metal-3-specifically-the-maximum-number-of-buffers-inside-an-argument-buffer-thank-you" class="md-nav__link">
    Have there been any changes to resource limits with Metal 3? Specifically the maximum number of buffers inside an argument buffer. Thank you!
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-there-any-list-where-i-can-find-which-devices-support-metal-3" class="md-nav__link">
    Is there any list where I can find which devices support Metal 3?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ive-noticed-that-in-compute-shader-access-to-a-2d-texture-is-faster-than-access-to-a-buffer-due-to-a-lot-more-of-a-cache-missed-how-does-caching-strategy-work-for-textures-vs-buffers-in-compute-can-we-specify-one-as-ive-pretty-good-idea-what-parts-of-the-buffer-will-be-accessed-from-which-thread" class="md-nav__link">
    I've noticed that in compute shader access to a 2d texture is faster, than access to a buffer. (Due to a lot more of a cache missed). How does caching strategy work for textures vs buffers in compute? Can we specify one (as i've pretty good idea what parts of the buffer will be accessed from which thread)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#does-screencapturekit-support-pulling-audio-from-different-processes-than-the-video-there-are-some-use-cases-where-audio-comes-from-one-process-but-the-window-and-graphics-come-from-another-process-this-is-common-in-situations-like-crossover-and-wine-this-feature-if-not-supported-would-be-very-useful" class="md-nav__link">
    Does ScreenCaptureKit support pulling audio from different processes than the video? There are some use cases where audio comes from one process but the window and graphics come from another process, this is common in situations like CrossOver and Wine. This feature if not supported would be very useful.
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="graphics-and-games-lounge-qas">graphics-and-games-lounge QAs</h1>
<h4 id="by-feetiki">by <a href="https://twitter.com/FeeTiki">FeeTiki</a></h4>
<hr />
<hr />
<blockquote>
<h4 id="there-any-main-tradeoffs-between-tessellation-and-mesh-shading-for-vertex-creation">There any main tradeoffs between tessellation and mesh shading for vertex creation?</h4>
</blockquote>
<p>Mesh Shading has a more flexible workflow and allows for dynamic scaling of your workload through the object shader stage. In terms of performance, Mesh Shading allows a more pipelined flow directly from the Mesh Shader stage to the rasterizer, so you have full mesh visibility during vertex creation. On the other hand - I would suggest using tessellation if you are doing actual tessellation of an existing geometry pipeline. 
For users who are doing procedural work that doesn't directly map to tessellation per-se, I would definitely suggest Mesh Shaders. 
The new <a href="https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders?language=objc|sample code">https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders?language=objc|sample code</a> we published shows how mesh shaders work and you can get an idea of the difference. 
The sample code generates Bezier patches and uses mesh shaders to control the level of detail. While the patches are generated on the CPU, you will see that it would not be a huge leap to do that in the object stage or mesh stage. </p>
<hr />
<blockquote>
<h4 id="not-much-of-a-question-because-im-just-starting-to-move-towards-rendering-but-id-like-to-say-thanks-to-everyone-involved-with-this-the-documentation-tooling-and-apis-seems-better-than-ever-and-the-metalfx-upscaling-seems-a-game-changer-i-can-see-some-foveated-rendering-hacks-being-used-with-this-api-and-using-different-lod-with-the-fast-resource-cheers-lads-awesome-work">Not much of a question because I'm just starting to move towards rendering but I'd like to say thanks to everyone involved with this. The documentation, tooling and APIs seems better than ever and the MetalFX Upscaling seems a game changer. I can see some foveated rendering hacks being used with this API and using different LOD with the fast resource. Cheers lads, awesome work.</h4>
</blockquote>
<p>Thanks for the feedback. We are curious about your idea "I can see some foveated rendering hacks being used with this API". Can you elaborate more on it? 
Using the same concept that VR googles are starting now but on a flat screen. Render the main point of interest in 4k and moving away from it, create lower zones (almost like a gradient) where the borders can be rendered at like 1080p with high anti alias combining with lower LOD. Again, starting so not even sure if it’s possible. 
For foveated rendering there’s also a variable rasterization rate API in Metal to help with that
<a href="https://developer.apple.com/documentation/metal/render_passes/rendering_at_different_rasterization_rates">https://developer.apple.com/documentation/metal/render_passes/rendering_at_different_rasterization_rates</a> 
Thanks &lt;@U03HHP77WUB&gt;, there’s not even the need for hacks then. Cheers. 
&lt;@U03J76T9WG6&gt; Just a quick note: MetalFX Upscaling is designed with fixed rasterization rate in mind. You can find more details in the MetalFX session tomorrow. Cheers. 
I’ll keep an eye on it. Cheers &lt;@U03HW8Y0RFB&gt;. </p>
<hr />
<blockquote>
<h4 id="what-are-the-package-size-considerations-when-shipping-with-offline-compiled-metal-shaders-do-these-need-to-be-updated-over-time-to-keep-up-with-any-driver-or-software-updates-for-a-particular-piece-of-hardware">What are the package size considerations when shipping with offline compiled metal shaders?  Do these need to be updated over time to keep up with any driver or software updates for a particular piece of hardware?</h4>
</blockquote>
<p>Hi &lt;@U03J9S1L38W&gt;,
1. Regarding package size considerations, optimizing for your particular deployment platform and particular set of supported devices is what make most sense.
2. Please refer to the video “Target and optimize GPU binaries with Metal 3”  available tomorrow for information on this. </p>
<hr />
<blockquote>
<h4 id="how-do-i-start-creating-anything-in-metal-3-can-you-please-show-me-code-for-a-hello-world-type-metal-3-maybe-a-box-with-a-shaded-gradient-of-color-inside">How do I start creating anything in Metal 3? Can you please show me code for a "Hello, World!" type Metal 3; maybe a box with a shaded gradient of color inside.</h4>
</blockquote>
<p>Yup! We have a list of examples with source code at <a href="https://developer.apple.com/metal/sample-code/">https://developer.apple.com/metal/sample-code/</a> 
You can also create a new project in Xcode, select the <code>Game</code> template and pick <code>Game Technology: Metal</code>, which will create a new project that renders a cube in Metal 
Excellent! The sample code link will do well. Thanks 
And just created the Metal cube project in Xcode; this is a huge boost to getting me started using Metal 3! 
Awesome! :tada: If you have any questions throughout the week please let us know :smile: 
I also recommend checking out <a href="https://developer.apple.com/documentation/metal/debugging_tools">https://developer.apple.com/documentation/metal/debugging_tools</a> to help you debug any issues you encounter </p>
<hr />
<blockquote>
<h4 id="hi-are-metal-3-features-exclusive-to-apple-silicon-or-do-they-also-work-on-other-processors-eg-a-series-on-ios-or-the-gpus-in-intel-based-macs">Hi, are Metal 3 features exclusive to Apple Silicon or do they also work on other processors (e.g A-series on iOS, or the GPUs in Intel-based Macs)?</h4>
</blockquote>
<p>hello! thanks for the question, Alessandro
That information is in the video at approx 12m 45s 
Thank you very much! </p>
<hr />
<blockquote>
<h4 id="whats-the-best-way-to-get-started-with-metal-as-a-beginner-in-the-graphics-field">What's the best way to get started with Metal as a beginner in the graphics field?</h4>
</blockquote>
<p>Hello, we have a bunch of great samples for getting started with Metal going from the very basics all the way to more complicated rendering.
<a href="https://developer.apple.com/metal/sample-code/">https://developer.apple.com/metal/sample-code/</a></p>
<p>The samples under "Metal Fundamentals" are a great place to get started. 
Thank you very much! Will look into it. </p>
<hr />
<blockquote>
<h4 id="im-learning-graphics-coming-from-swift-and-am-really-interested-in-ray-tracing-what-should-i-learn-first-to-learn-how-to-make-a-hybrid-renderer-in-metal">I’m learning graphics (coming from Swift) and am really interested in Ray tracing. What should I learn first to learn how to make a hybrid renderer in Metal?</h4>
</blockquote>
<p>Hi Ethan! In addition to this year's Raytracing content, we also had a session fully dedicated to the topic of Hybrid Rendering in last year's WWDC. Hopefully you find it a good introduction to the topic - <a href="https://developer.apple.com/videos/play/wwdc2021/10150/">https://developer.apple.com/videos/play/wwdc2021/10150/</a>. In particular, the session shows how to do your "primaries" using rasterization, and then how to use that as the basis for RT Shadows, Ambient Occlusion, and Mirror-like Reflections. 
This sample is also great for beginners <a href="https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal">https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal</a> (it's not in Swift though :sweat_smile: sorry!) 
Thanks! Metal makes Ray tracing really exciting since you can now pair it with MetalFX. Going to be interesting! </p>
<hr />
<blockquote>
<h4 id="are-there-any-memory-and-texture-format-considerations-when-using-metalfx-im-guessing-the-historical-frames-need-to-be-stored-in-some-way-though-the-trade-off-would-be-smaller-intermediate-render-targets">Are there any memory and texture format considerations when using MetalFX?  I'm guessing the historical frames need to be stored in some way, though the trade off would be smaller intermediate render targets.</h4>
</blockquote>
<p>For more details on MetalFX Upscaling, please watch the session tomorrow. Thanks. 
Thanks!  Will be sure to check it out.  Also will be interesting if this could be used for some intermediate render targets like an AO pass. 
If you have any further questions, join us in the digital lounge for the session 1PM-2PM PST. We'd be happy to answer any further questions. </p>
<hr />
<blockquote>
<h4 id="since-the-shaders-are-compiled-at-project-build-time-and-compilation-speed-is-no-longer-as-critical-is-that-extra-headroom-used-to-allow-the-compiler-to-try-to-optimize-the-shaders-further-or-is-does-it-produce-identical-machine-code-as-when-building-it-at-runtime">Since the shaders are compiled at project build time and compilation speed is no longer as critical, is that extra headroom used to allow the compiler to try to optimize the shaders further? Or is does it produce identical machine code as when building it at runtime?</h4>
</blockquote>
<p>Independent from offline compilation, we did introduce a new optimization level -Os (size) as an option, but there is not additional optimization that occurs during runtime. The produced code will be identical. 
Understood, thanks! 
Also, be sure to check out “Target and optimize GPU binaries with Metal 3” available tomorrow for more information! </p>
<hr />
<blockquote>
<h4 id="hi-great-wwdc-session-question-do-you-have-a-hello-world-example-for-making-shaders-using-apple-technologies-thank-you">Hi! Great wwdc session, question: do you have a hello world example for making shaders using Apple technologies? Thank you!</h4>
</blockquote>
<p>Please see the article "<a href="https://developer.apple.com/documentation/metal/developing_and_debugging_metal_shaders?language=objc|Developing and Debugging Metal Shaders">https://developer.apple.com/documentation/metal/developing_and_debugging_metal_shaders?language=objc|Developing and Debugging Metal Shaders</a>" that will introduce you to the Xcode's excellent shader debugger. Then you can try out the Game Template if you want to get a Metal app running. And you can check out our <a href="https://developer.apple.com/documentation/metal/metal_sample_code_library|sample code library">https://developer.apple.com/documentation/metal/metal_sample_code_library|sample code library</a> that have many kinds of Metal samples (and shaders) you can learn from. </p>
<hr />
<blockquote>
<h4 id="can-you-explain-how-to-use-triple-buffering-for-complicated-scene-with-2d-overlayui-and-3d-what-kind-of-data-i-should-store-in-buffers-in-vulkan-examples-they-use-framebuffer-but-metal-dont-have-any-similar-entity">Can you explain how to use triple buffering for complicated scene with 2d (overlay/ui) and 3d? What kind of data I should store in buffers? In Vulkan examples they use framebuffer, but metal dont have any similar entity.</h4>
</blockquote>
<p>Could you give a bit more context? In general all interfacing with the compositing is done through the Drawable. (usually through a MTKView in an app). Are you referring to manually triple-buffering GPU resources for CPU synchronization? 
Yeah, indeed. I wanna fill some data on CPU side while frame rendering (inflight frames) and pass filled buffer to command queue. In apple examples I see how to use inflight frames to fill camera data to buffers, but camera data isn't complicated.</p>
<p>Im newbie in Metal and trying to create my own render engine.<br />
One way to do this is to have a set of data for each frame in flight. You can then use a mutex (or other thread-safe structure) to keep track of what sets are in flight by:
• marking them when you use them in a command bufferand 
• unmarking them in the completion handler of the command buffer. 
If you run out of free sets, you can simply wait on the mutex and halt rendering on CPU until GPU has caught up. 
Ok, I will try it! Thanks a lot.<br />
check <code>synchronize</code> <a href="https://developer.apple.com/documentation/metal/mtlblitcommandencoder/1400775-synchronize|link">https://developer.apple.com/documentation/metal/mtlblitcommandencoder/1400775-synchronize|link</a> and the various ways of signalling and reading data between buffers on GPU and CPU <a href="https://developer.apple.com/documentation/metal/mtlbuffer|link">https://developer.apple.com/documentation/metal/mtlbuffer|link</a> 
Also, if you wanted a example of this pattern check out the game template in XCode by creating a new project in Xcode, selecting the <code>Game</code> template and picking <code>Game Technology: Metal</code> . In that template check how <code>_inFlightSemaphore</code> is used to multi buffer the uniforms buffer. 
From the Dynamic Terrain Sample code:</p>
<pre><code class="language-objective-c">// We allow up to three command buffers to be in flight on GPU before we wait
    static const NSUInteger kMaxBuffersInFlight = 3;

    _device             = device;
    _commandQueue       = [_device newCommandQueue];
    _startTime          = [NSDate date];
    _inFlightSemaphore  = dispatch_semaphore_create (kMaxBuffersInFlight);
    _frameAllocator     = new AAPLAllocator (device, 1024 * 1024 * 16, kMaxBuffersInFlight);
    _uniforms_gpu       = _frameAllocator-&amp;gt;allocBuffer &amp;lt;AAPLUniforms&amp;gt; (1);
</code></pre>
<p>You can see we explicitly limit our in-flight sets to 3 in this example. (<a href="https://developer.apple.com/documentation/metal/buffers/rendering_terrain_dynamically_with_argument_buffers|link to sample code">https://developer.apple.com/documentation/metal/buffers/rendering_terrain_dynamically_with_argument_buffers|link to sample code</a> ) 
The main idea is that you shouldn’t modify data with the CPU if the GPU might be reading it, so using that semaphore with a completion handler (per the examples above) allows you to know when at least one of your 3 copies of any data are able to be modified.</p>
<p>If you are not using the CPU to modify textures/buffers on the fly then there isn’t a need for this semaphore. -[CAMetalLayer nextDrawable] will automatically provide up to 3 drawables and will limit an app from getting too far ahead (if you render faster than the screen refresh). 
Thanks! I will watch linked examples asap.  </p>
<hr />
<blockquote>
<h4 id="didnt-see-it-earlier-new-to-slack-not-sure-if-this-is-the-right-place-to-ask-but-i-am-still-looking-through-the-plugins-but-would-it-be-able-to-use-bluetooth-or-related-tech-to-share-say-an-item-with-another-player-that-also-has-the-gameapp-like-if-there-is-a-unity-plugin-from-apple-that-does-this">Didn't see it earlier, new to Slack:  "Not sure if this is the right place to ask, but I am still looking through the plugins. But would it be able to use Bluetooth or related tech to share say an item with another player that also has the game/app. Like if there is a Unity plugin from apple that does this. "</h4>
</blockquote>
<p>Unfortunately we don't currently offer a plug-in that exposes this functionality, but this is certainly an interesting idea. 
Can you tell me more about what you're trying to do. Are you looking for something that behaves like the share sheet? 
It's more of a thought experiment at this point, but I kind of want to create a fantasy/rpg game and want to incentivize working together in real life to defeat certain bosses or challenges. My goal is to not rely on having any servers if possible. I think having bluetooth access might be a way for people to create a party, like in a typical 4 player rpg game like final fantasy. For example, if you are a healer. You can only heal or buff other people most of the time. In this case to beat certain challenges you would need to meet up with other people to be able to progress. </p>
<hr />
<blockquote>
<h4 id="whats-the-earliest-ios-you-can-use-this-with">What's the earliest iOS you can use this with?</h4>
</blockquote>
<p>The plug-ins are compatible as far back as iOS 13 as long as the underlying framework is also supported. 
For example, the PHASE framework was new in iOS 15, so is not supported prior to that version. 
Thanks. (My current game supports iPad 2 still, iOS 9… But for new projects, that’s probably fine.) 
Keep in mind the plug-ins are open-source, so feel free to make modifications if needed to help with your specific backwards compatibility goals. </p>
<hr />
<blockquote>
<h4 id="if-were-making-feature-requests-i-really-like-the-ability-to-email-bug-reports-from-my-game-with-mfmailcomposeviewcontroller">If we're making feature requests: I really like the ability to email bug reports from my game with <code>MFMailComposeViewController</code>!</h4>
</blockquote>
<p>Thanks for the feedback. We're definitely tracking feature requests, so this input is super valuable. 
Though that might be really tricky with Unity — I use it in UIKit-based games. 
Yeah, we'll have to investigate how we might surface the functionality. It's certainly interesting - but there may be a more Unity-esque approach to solving your issue as well. I believe Unity has some integrated bug report metrics that you can leverage as a Unity developer. 
We did it in Kobold2D (Cocos2D) years ago, too. Email is nice for indies because you don’t need to worry about a back end. </p>
<hr />
<blockquote>
<h4 id="a-question-on-the-accessability-plugin-for-unity-is-it-only-available-for-apple-devices-or-can-it-also-be-used-for-windows-xbox-etc-reason-for-asking-is-that-it-would-be-great-to-see-a-cross-platform-plugin-to-increase-the-reach-of-accessability-making-it-easier-to-prioritize-for-stakeholders">A question on the Accessability plugin for Unity. Is it only available for Apple devices or can it also be used for Windows, Xbox etc. Reason for asking is that it would be great to see a cross platform plugin to increase the reach of accessability (making it easier to prioritize for stakeholders)</h4>
</blockquote>
<p>Hi Denny, thanks for the perspective. Currently this plug-in is only available for Apple platforms as it ties into foundational Apple accessibility technologies. 
Ok! I did miss to say it in the question--it's really great to see the accessibility plugin :-) </p>
<hr />
<blockquote>
<h4 id="what-is-the-recommended-framework-for-connecting-to-another-player-stably-for-a-multiplayer-match-not-the-old-turn-based-games-but-a-modern-approach-to-having-a-5-10-minute-connection-for-pvp-would-arkit-realitykit-be-better-today-than-multipeer-connectivity-which-can-be-flaky-with-frequent-disconnects">What is the recommended framework for connecting to another player stably for a multiplayer match? Not the old turn-based games, but a modern approach to having a 5-10 minute connection for PvP. (Would ARKit, RealityKit be better today than multipeer connectivity, which can be flaky with frequent disconnects?)</h4>
</blockquote>
<p>Hi - GameKit currently provides real time multiplayer support and is currently used in a number of games. Coincidentally, we've just shared this functionality as part of the Unity plug-ins made available today. 
Besides that, GameKit provides a few way for our users to play with each other:
• Auto matching.
• Inviting Game Center friends via <a href="http://Messages.app|Messages.app">http://Messages.app|Messages.app</a> or push notifications.
• Or play with nearby friends.
You will be able to see some games in Apple Arcade utilizing the multiplayer functionality provided by gamekit. </p>
<hr />
<blockquote>
<h4 id="hi-it-looks-like-the-documentation-for-the-unity-plugins-has-not-been-submitted-to-the-github-repo-all-the-documentation-links-are-broken">Hi! It looks like the documentation for the unity plugins has not been submitted to the GitHub repo -- all the documentation links are broken.</h4>
</blockquote>
<p>Thanks for the report! We’ll look into this. 
Hi, thanks for pointing out! Documentation links should now work again. </p>
<hr />
<blockquote>
<h4 id="ive-had-a-quick-browse-of-the-accessibility-unity-plugin-and-noticed-that-its-only-supporting-ios-and-tvos-is-macos-planned-as-well-there-are-also-some-performance-concerns-with-the-code-that-id-love-to-see-resolved-will-github-prs-be-considered">I've had a quick browse of the Accessibility Unity plugin and noticed that it's only supporting iOS and tvOS. Is macOS planned as well? There are also some performance concerns with the code that I'd love to see resolved, will GitHub PR's be considered?</h4>
</blockquote>
<p>We don't currently support macOS, but you can always file a bug through feedback assistant for us to consider these requests. 
Hi Alex, this is very appreciated. Unfortunately we don't currently accept external PRs - but we do really take your feedback to heart, so keep the feedback coming and we'll do our best to address in the mean time. 
Great to hear. Just based on a cursory scan through the codebase, the most serious performance issues are allocating in the mono heap with calls like GetComponentsInChildren&lt;Renderer&gt;() (there are allocation-free versions of these available), and serializing rects and points to native code via strings. 
Fantastic. Thanks for already helping to make these plug-ins better. 
Would love to get some more detail on how the element ordering is working too. It looks like the plugin determines an element ordering based on screen-space positioning which seems quite complex; however my project already knows the logical layout and grouping of every UI element; is there a mechanism for using this actual logical ordering? 
That is a great feedback! Currently no, but we will add this to our feedback list to allow honoring element ordering provided by developers:thumbsup: 
I haven’t looked at the Accessibility plugin yet but it’s the one I’m most excited about. My games are playable via VoiceOver, and I definitely override <code>-accessibilityElements</code> to do things like give a consistent navigation order when the visuals are mirrored. </p>
<hr />
<blockquote>
<h4 id="i-look-into-unity-plugins-and-have-a-relative-question-why-you-using-swift-instead-of-objc-in-applecore-module-why-swift-with-all-unmanaged-things-better-than-objc-but-thanks-for-that-bridging-i-grab-information-how-to-use-_cdecl">I look into unity plugins and have a relative question: Why you using swift instead of objc in AppleCore module? Why Swift with all Unmanaged things better, than ObjC?   But thanks for that bridging! I grab information how to use _cdecl :)</h4>
</blockquote>
<p>Thanks for asking Vladislav! We used Swift because it is the best language for Apple platforms. Glad you learned something new too! 
Yey! That is very great answer for me!  Thanks Brett </p>
<hr />
<blockquote>
<h4 id="is-wheel-support-only-on-mac-the-api-seems-to-be-available-on-all-platforms-except-apple-watch-but-ive-only-seen-it-mentioned-with-regard-to-mac-am-i-misreading-something">Is wheel support only on Mac? The API seems to be available on all platforms except Apple Watch, but I’ve only seen it mentioned with regard to Mac. Am I misreading something?</h4>
</blockquote>
<p>Yes, racing wheels are only supported on macOS at this time. If you’re interested in supporting racing wheels in your game on other platforms, please file feedback to let us know! </p>
<hr />
<blockquote>
<h4 id="could-mesh-shaders-be-used-for-procedural-terrain-generation-with-continuous-lod-currently-i-use-a-tessellation-compute-shader-which-also-does-some-basic-culling-followed-by-a-vertexfragment-shader">Could mesh shaders be used for procedural terrain generation with continuous LOD? Currently I use a tessellation compute shader (which also does some basic culling) followed by a vertex/fragment shader.</h4>
</blockquote>
<p>You can definitely use Mesh Shaders to do this! - The great thing about Mesh Shaders is that you are not limited to the fixed tessellation layout; you can kick off an Object Shader to calculate what tiles to populate (and which ones to leave out for custom tiles), and then dynamically schedule Mesh Shader grids to create the individual tile geometry. :) 
This way, you could effectively generate and cull your entire terrain on the GPU 
Is there any specific issues you are worried/thinking about? 
Fantastic! Are there any more details/docs on mesh shaders? I’m having trouble finding much about it. 
One of the problems I have at the moment is culling. I pass in some quads to the tessellator, then do frustum transforms to try to figure out whether the quad would be onscreen or not (using -1 for the tessellation factor if I want to cull it). Then my vertex shader displaces vertices with the procedural function and does the transforms again, and it all feels a bit clunky. 
I know there’s a session tomorrow on mesh shaders, but I’m having trouble understanding the basic behavior of them atm. 
There will be a full session on Mesh Shaders tomorrow! And you can always drop back into the next Q&amp;A on friday if you have any follow-up.</p>
<p>My suggestion for doing these kinds of things, is to assume each "tile" has a AABB of the min/max displacement within the segment (pre-calc if needed), then use the Object Shader (which is a full featured compute shader) to generate work items for tiles of various sizes etc. Then run a Mesh Shader Grid with a TG per tile to generate. You can simply cull your AABB in the OS and you should never have any overhead aside from the AABB check :slightly_smiling_face: 
You can effectively have the OS TG fill the payload buffer with an array of TileDesc structs (or something like that), then have each MS TG pick up a TileDesc to expand into a mesh and send it straight to the rasterizer 
Tomorrow's Mesh Shader session will be here: <a href="https://developer.apple.com/videos/play/wwdc2022/10162/">https://developer.apple.com/videos/play/wwdc2022/10162/</a>
Documentation will be updated in the near future, and I believe a sample project will be available along with the session. 
OK, this sounds good! Is there any integration with the existing tessellation pipeline, or would I need to somehow implement something like it in the OS or MS? 
I believe you have to do a manual tessellation in your MS. If I can make a suggestion, a doubling straight forward quad tessellation scheme while you fade in height dat through manual mip levels might be a very good and quick way of getting a simple-but-smooth solution. The nice thing about OS/MS is that you can nicely debug both the tile scheduling and the mesh generation separately, as they are both basically compute kernels you can run individually. 
Hmm I see. It definitely sounds like something to try out. I love the built-in tessellator because it distinguishes between internal and edge tessellation which lets me avoid cracks in the terrain. I guess I would just need to do a bit more work in my MS to make that work. 
I wonder if the current fixed tessellator could be exposed as a function we could call from our MS… 
I think the main issue with the classic tessellation scheme is that it tends to generate noisy silhouettes when tessellation smoothly increases. (you get this wave-effect as the vertices move across the displacement samples).
There should be a way to integrate an external "classic tessellation function" into the MS kernel to generate a list of 2D triples for a given tessellation. I wouldn't be surprised if there are examples of this online. 
Yes the swimming vertices are an issue. Although it’s an issue in my case anyway as I’m using a single square mesh to render an entire spherical terrain from ground to space by “wrapping” it over the sphere depending on distance and what is potentially visible. My solution is to buy better GPUs and increase the vertex count to minimise the sample aliasing.. :smile: 
Thanks for your help Jaap, looking forward to the session tomorrow and I’ll probably be back for more Q&amp;A following it! 
If you wanna have more of an in-depth discussion, we still have a few Lab openings to sign up!
<a href="https://developer.apple.com/wwdc22/labs/">https://developer.apple.com/wwdc22/labs/</a>
These are in-depth 1:1 discussions :) </p>
<hr />
<blockquote>
<h4 id="how-should-we-approach-issues-like-shader-debugger-crashes-and-gpu-faults-we-dont-know-where-to-look-for-hints">How should we approach issues like Shader Debugger crashes and GPU faults? We don't know where to look for hints</h4>
</blockquote>
<p>The best thing to do for debugger crashes is to create a feedback assistant with steps to reproduce.</p>
<p>As for GPU faults, I would encourage you to try enabling shader validation for your application. 
We already have all validation turned on (no results), and we use FA all the time, but what do we do about these "game over" issues in the meantime? 
I would encourage you to try the new betas with your use case, we improved a lot of areas there 
If you are still having troubles, maybe you could create new FBA tickets, attach a gputrace and post the numbers here? 
One example of GPU Fault FB9968899 
Is the GPU fault happening at application runtime or while you are working with the gputrace? 
The fault is a separate issue, but I mention both because there are no diagnostics we can understand 
But this is a common occurrence for us, something fails catastrophically with no hints and we don't know how to proceed 
Does this happen on the latest OSs? 
I haven't tried the fault on Ventura because it can lock things up if not quickly resolved, may result in a loginwindow watchdog timeout (force reboot) 
Are you using raytracing or ICBs? 
No raytracing, I assume ICB is indirect command buffer, also no 
Which GPU architecture is this Steven? 
Same app worked fine on Intel (AMD GPU) fails badly on M1 for no apparent reason 
But I'm really asking about methodology, because we want to keep making forward progress without DTS all the time 
The crash log on M1 appears to indicate GPU firmware detected lockup, meaning that from the software perspective the GPU is hung for some time hence it is rebooted. Do you by chance implement a concurrent producer/consumer pattern via atomics on M1? 
It uses atomics, but I wouldn't call it producer/consumer, it's just an atomic allocator to a buffer 
We found entirely by chance that increasing the number of allocators from "1" fixed it 
But have no idea why or how, etc 
Are you ever in a situation where you rely on forward progress guarantees to access the allocator? 
Not sure what that means from a GPU standpoint 
E.g.:  would threads spin until memory is available or do you just assume there’s always enough memory? 
My mention of "forward progress" above just means development progress, not getting stuck on crashes 
there is no spinning, just atomics 
though we check the allocated value hasn't gone off the end of the array, we discard in that case 
there is nothing "consuming" 
Oh coincidentally you also used the term “forward progress”, although I meant the property of a program to make progress no matter what 
There is no concurrent producer/consumer 
I see, then it’s likely not the problem I thought, thanks for clarifying 
Have you tried running without shader validation enabled, just the API validation? 
Also, one of the ways GPU Faults happen when there’s basically unpaged memory reads/writes, basically out of bounds reads/writes or access to a memory that’s not resident - meaning the underlying memory of a resource that wasn’t made resident using <code>useResource</code> or <code>useHeap</code> 
I don't think that explains how increasing the allocators fixed it 
1 = crash, 4 = ok 
I realize there may be more parallelism on TBDR, but some kind of hint would help us out 
List of FB for reference FB10020742, FB10020198, FB10014465, FB9968922, FB9968899 
Had a quick look at the shader code in the Metal library contained in the app you attached to FB9968899 and I noticed a function called <code>transparency_populate</code> that loops on a linked list. Does this function get called in the workload that hangs? 
That's in the second "resolve" pass, which is called but shouldn't be directly affected by the number of allocators or their values 
<code>transparency_add</code> , <code>transparency_add_counter</code>, <code>transparency_write</code>, etc are the ones affected 
If you remove entirely the second “resolve” pass, do you still see a GPU fault though? 
I could try that, but it seems unlikely 
if you're looking for sample code, check the attachment to FB9968922 
The reason I’m asking is that that <code>transparency_populate</code> relies on the data structure to be well formed to complete, otherwise it might spin forever 
That's not an issue we see 
It's an A-Buffer implementation, a 2D buffer of head pointers (inited to -1) then a linked list 
Yes, I can see the A-buffer structure for sure. At first glance the code looks sane to me, so I guess we need to repro locally to understand what’s going on 
FYI, just triggered in Ventura, will be attaching the reports 
Looks like some diagnostics are missing from the messages? 
<em><code>2022-06-08 13:18:09.498390-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang)</code></em>
<em><code>2022-06-08 13:18:09.498450-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang)</code></em>
<em><code>2022-06-08 13:18:09.498713-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim)</code></em>
<em><code>2022-06-08 13:18:09.498734-0400 CADApp[8724:128585] Execution of the command buffer was aborted due to an error during execution. Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim)</code></em>
<em><code>2022-06-08 13:18:09.498867-0400 CADApp[8724:128585] GPU Soft Fault count: 1</code></em>
<em><code>2022-06-08 13:18:09.559102-0400 CADApp[8724:128580] [Metal Diagnostics] __message__: MTLCommandBuffer "Main Loop" execution failed: The commands associated with an encoder caused an error __:::__ __delegate_identifier__: GPUToolsDiagnostics</code></em>
<em><code>2022-06-08 13:18:09.559156-0400 CADApp[8724:128580] [Metal Diagnostics] __message__: MTLCommandBuffer "Callout 3" execution failed: The commands associated with the encoder were affected by an error, which may or may not have been caused by the commands themselves, and failed to execute in full __:::__ __delegate_identifier__: GPUToolsDiagnostics</code></em> 
Thanks for the update. What message do you think is missing though? 
Seems like <em><code>__:::__ __delegate_identifier__:</code></em> is missing the identifier? 
I see what you mean, could you please add this message to the original FB? 
also, I managed to reproduce the lock up locally, so it’s easy enough for somebody to bottom that out 
I updated FB9968899 at 1:22PM EDT 
<em><code>GPU Soft Fault count: 1</code> kinda identifies that theres something either accessing out of bounds or at 0</em> 
&lt;@U03HLFDFS20&gt; I guess I would expect some kind of diagnostic for that rather than an unknown issue. Also wouldn't that affect AMD equally? 
There’s a possibility that amd reads out of bounds, but doesn’t page fault 
What I’m seeing locally is definitely a hang, not an out-of-bounds though 
I guess the current answer is going to be that we would need to investigate these feedback assistant reports 
That's fine, though my original question was a little different 
And to answer your initial question, the fastest way to reach us is through developer forums 
ok 
I will start using that more 
Unfortunately there might not be a single solution to progress when you are faced with these issues 
Thanks for the feedback 
Definitely thanks for the feedback. Somebody will be investigating the GPU hang soon hopefully </p>
<hr />
<blockquote>
<h4 id="can-we-somehow-cancel-work-committed-to-gpu-for-example-we-commited-some-neural-net-processing-example-of-huge-work-and-we-received-memory-warning-from-the-system-can-we-somehow-stop-execution">Can we somehow cancel work committed to GPU? For example we commitеed some neural net processing (example of huge work) and we received memory warning from the system, can we somehow stop execution?</h4>
</blockquote>
<p>Hi Pavel, unfortunately no facility exists to cancel committed GPU work. You can use Indirect Command Buffers and NOP out the commands before execution though, could that be a work around?</p>
<p>(Also we'd appreciate it if you can file a feedback request with this scenario for us to evaluate.) </p>
<hr />
<blockquote>
<h4 id="what-is-the-best-way-to-denoise-the-ray-traced-frame-in-metal-is-mpssvgfdenoiser-recommended-for-this-purpose">What is the best way to denoise the Ray traced frame in metal? Is MPSSVGFDenoiser recommended for this purpose?</h4>
</blockquote>
<p>MPSSVGFDenoiser will work for denoising the full application frame. Typically you want a denoiser targeted at each ray traced effect, focusing on the visibility signal for shadows and ambient occlusion, and a separate denoiser for reflections aware of how to reproject reflected content. 
thanks for the response 
:thumbsup:  Denoising is a rapidly moving field and there's several open source options that you could look at adding to your application! 
Those are new topics for me, so sorry if it is a silly question, but what is the latest best way to organize global illumination passes with metal ray tracing then? In my inexperienced understanding it was like: accumulate the light from ~3 bounces and apply denoising to the whole result 
Not a silly question at all! Accumulating light from multiple bounces will result in an approximation to global illumination and will likely be noisy depending on how you define the rays for each bounce.  Typically for later bounces you could accept something more diffuse for your global illumination representation - there's more diffuse representations such as <a href="https://jcgt.org/published/0008/02/01/|Dynamic Diffuse Global Illumination">https://jcgt.org/published/0008/02/01/|Dynamic Diffuse Global Illumination</a> </p>
<hr />
<blockquote>
<h4 id="does-mfxspatialscalingeffect-support-pq-encoded-hdr-color-specifically-pixel-format-mtlpixelformatbgr10a2unorm-and-colorspace-kcgcolorspaceitur_2100_pq-if-so-which-colorprocessingmode-should-be-used">Does <code>MFXSpatialScalingEffect</code> support PQ encoded HDR color? Specifically, pixel format <code>MTLPixelFormatBGR10A2Unorm</code> and colorspace <code>kCGColorSpaceITUR_2100_PQ</code>? If so, which <code>colorProcessingMode</code> should be used?</h4>
</blockquote>
<p>HDR colorProcessingMode should be used in that case. </p>
<hr />
<blockquote>
<h4 id="are-there-any-demos-using-the-mtlparallelrendercommandencoder-is-there-a-possibility-of-supporting-a-parallel-compute-encoder-in-the-future">Are there any demos using the <code>MTLParallelRenderCommandEncoder</code>? Is there a possibility of supporting a parallel compute encoder in the future?</h4>
</blockquote>
<p>Hi, thanks for your question. Currently, we do not have a MTLParallelRenderCommandEncoder sample. When you said a parallel compute encoder, can you tell us more about your use case? 
We do support a compute encoder that executes dispatches in parallel on GPU but not encoding compute dispatches in parallel on CPU. 
I suppose I don't have a use case in mind :sweat_smile: but since Metal compute seems to becoming more popular (GPU-driven pipelines, ICBs, new machine learning stuff) I was just curious if a parallel encoder would be available. I suppose though it might not actually be necessary if compute encoding is not a bottleneck 
Typically what we've seen of GPU compute is that encoding is quite light weight while the GPU work is heavy. Encoding is just feeding pointers to GPU shader cores of where all the data lives and then GPU go on a massive parallel data processing/computing spree. :smile: </p>
<hr />
<blockquote>
<h4 id="is-there-a-limit-to-the-scale-factor-outputwidth-inputwidth-that-mfxspatialscalingeffect-can-apply">Is there a limit to the scale factor (outputWidth / inputWidth) that MFXSpatialScalingEffect can apply?</h4>
</blockquote>
<p>We recommend using a scale factor that's between &gt; 1x &amp;&amp; &lt;= 2x. 
Just an FYI, we have a digital lounge session dedicated to MetalFX from 1PM-2PM PST today. Cheers. 
Alright, thanks. I'll try to check in later. :slightly_smiling_face: </p>
<hr />
<blockquote>
<h4 id="whats-the-most-performant-way-to-copy-a-calayer-into-a-mtltexture-on-ios">What's the most performant way to copy a CALayer into a MTLTexture on iOS?</h4>
</blockquote>
<p>Can you elaborate on your use case? What kind of layer are you trying to copy, and does your app provide the original content of this layer? 
We’re trying to render Lottie animations into a Metal texture interactively in real-time. 
Our goal is for a user to be able to drag a Lottie animation around a canvas as it plays and have it composited with other Metal content. 
The Lottie library we’re using renders to a CALayer and we need to copy it into a MTLTexture. 
Do you know what kind of layer they’re rendering into? It matters significantly whether the library is also using a CAMetalLayer or if it is using the traditional CALayer drawing APIs. 
I think it’s not a CAMetalLayer, which is the problem. I think it’s using CAShapeLayer, CATextLayer, etc. (which are much simpler than implementing text rendering for instance). But it seems to be tricky to then get the result from the CALayer into Metal. On macOS CARenderer seems to permit this but not in iOS<br />
Most generally, the only way to capture the composited appearance of a layer is via CARenderer. You will have difficulty syncing the framerate of this renderer with the display. 
CARenderer is available on iOS, can you elaborate on your difficulty with it? 
Hmm.. I could have sworn that CARenderer::rendererWithMTLTexture was not available on iOS.. but the docs says it has been there since iOS 11. 
Well, I think that’s the way to do it, right? 
One difference I can think of between macOS and iOS is that iOS makes much heavier use of implicit CATransactions. If you try to render the layer tree while the main queue’s implicit transaction is still open, you might not capture the latest drawing.</p>
<p>On iOS, to guarantee correct drawing, use UIGraphicsImageRenderer in conjunction with <code>-[UIView drawViewHierarchyInRect:afterScreenUpdates:YES]</code>. You can then use the resulting UIImage to populate an MTLTexture. This will not be as fast as using CARenderer directly but it is more likely to be resilient to the complexities of how iOS apps use Core Animation. 
But do try CARenderer first. :slightly_smiling_face: 
Thanks, I’ll take another look! 
Good luck! If neither approach works for you, please file Feedback so the teams involved can understand more about your use case. </p>
<hr />
<blockquote>
<h4 id="the-new-edr-support-in-metal-for-ios-looks-really-exciting-but-on-what-current-devices-can-we-test-this-on">The new EDR support in Metal for iOS looks really exciting, but on what current devices can we test this on?</h4>
</blockquote>
<p>Devices with an XDR display currently support EDR as long as the device brightness isn't set too high (among other factors) which would clamp those EDR values. Please see <a href="https://developer.apple.com/documentation/metal/hdr_content/determining_support_for_edr_values?language=objc">https://developer.apple.com/documentation/metal/hdr_content/determining_support_for_edr_values?language=objc</a> for more details. On macOS, you can read the <code>maximumPotentialExtendedDynamicRangeColorComponentValue</code> property on an <code>NSScreen</code> object for that display. And on iOS, you can use <code>UIScreen.potentialEDRHeadroom</code>. 
Is the iPhone 13 Pro included in this? 
I haven't tested it, but I think you should get a large EDR headroom on iPhone 13 pro. 
We also recommend to poll the value of <code>UIScreen.currentEDRHeadroom</code> (for iOS) or <code>NSScreen.maximumExtendedDynamicRangeColorComponentValue</code> (for macOS) to accommodate for possible display brightness changes that might affect available headroom. Those can be caused for ex. by the user interaction - changing the screen brightness or by the system if auto-brightness setting is enabled or as a result of thermal event when device has to throttle. </p>
<hr />
<blockquote>
<h4 id="more-out-of-curiosity-what-are-the-potential-use-cases-of-quadgroup-functions-ive-only-seen-one-video-mentioning-them-discovering-advances-in-metal-for-a15-bionic-where-theyre-used-to-reduce-the-number-of-texture-reads-per-thread-are-there-any-other-use-common-use-cases">More out of curiosity, what are the potential use cases of quadgroup functions? I've only seen one video mentioning them ("Discovering advances in Metal for A15 Bionic") where they're used to reduce the number of texture reads per thread. Are there any other use "common" use cases?</h4>
</blockquote>
<p>It can be used for any workload where you want to spread orthogonal work across a group; for instance, you can have each thread in a quad cull a part of a light list, then use quadgroup functions to peek at each others' lists and sum lights. 
The gist of quadgroup and simdgroup functions are to perform data exchange in a more efficient manner. Traditionally, threadgroup barrier and threadgroup memory is required to perform data exchanges, but its a lot more heavier than finer granularity exchanges like quadgroup or simdgroup functions. 
When would you decide to use a quadgroup over a simdgroup function? 
When all you need is to exchange among 4 threads in a row. 
The finer granularity you use, the more efficient it is. 
If you are in compute, I would probably always use simdgroup, just to get the best bang for your buck. I believe quadgroups are interesting for fragment workloads; I'm not sure what the availability and details are by heart. (rasterization, helper threads etc) </p>
<hr />
<blockquote>
<h4 id="im-interested-in-the-accelerated-ray-tracing-features-in-metal-3-but-the-specific-session-that-would-contain-more-details-is-tomorrow-are-these-changes-specifically-designed-for-realtime-raytracing-in-a-rasterized-renderer-or-are-they-general-enough-to-be-used-on-their-own-to-ray-trace-entire-scenes-i-see-that-there-are-general-purpose-ray-tracing-features-in-older-versions-of-metal-im-specifically-curious-how-these-intersect-with-the-metal-3-ray-tracing-features">I'm interested in the accelerated ray tracing features in Metal 3, but the specific session that would contain more details is tomorrow. Are these changes specifically designed for realtime raytracing in a rasterized renderer, or are they general enough to be used on their own to ray trace entire scenes? I see that there are general purpose ray tracing features in older versions of Metal, I'm specifically curious how these intersect with the Metal 3 ray tracing features.</h4>
</blockquote>
<p>Metal RT is intended for use in both pure ray traced renderers and hybrid renderers that ray trace from rasterized content. This year's features are applicable to both with a focus on improving performance with some quality of life and ease of use features too! We have a Q&amp;A session on Friday if you’d like to ask additional questions following the release of the video. Some of our samples, such as <a href="https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal|Accelerating Ray Tracing Using Metal">https://developer.apple.com/documentation/metal/metal_sample_code_library/accelerating_ray_tracing_using_metal|Accelerating Ray Tracing Using Metal</a> and <a href="https://developer.apple.com/documentation/metal/metal_sample_code_library/control_the_ray_tracing_process_using_intersection_queries|Control the Ray Tracing Process Using Intersection Queries">https://developer.apple.com/documentation/metal/metal_sample_code_library/control_the_ray_tracing_process_using_intersection_queries|Control the Ray Tracing Process Using Intersection Queries</a> do pure ray tracing, and <a href="https://developer.apple.com/documentation/metal/metal_sample_code_library/rendering_reflections_in_real_time_using_ray_tracing|Rendering reflections in real time using ray tracing">https://developer.apple.com/documentation/metal/metal_sample_code_library/rendering_reflections_in_real_time_using_ray_tracing|Rendering reflections in real time using ray tracing</a> uses a more hybrid model. 
If anyone would like to have more of an in-depth/follow-up discussion, we still have a few Lab slots available!
<a href="https://developer.apple.com/wwdc22/labs/">https://developer.apple.com/wwdc22/labs/</a> </p>
<hr />
<blockquote>
<h4 id="as-far-as-i-can-tell-ios-devices-still-run-opengles-apps-just-fine-is-there-a-hard-deadline-when-this-will-not-be-the-case-any-more">As far as I can tell iOS devices still run OpenGLES apps just fine. Is there a hard deadline when this will not be the case any more?</h4>
</blockquote>
<p>We can't comment on future plans, but we recommend porting existing OpenGL-based applications to Metal to take advantage of the latest features available on our devices. </p>
<hr />
<blockquote>
<h4 id="are-there-any-recommended-formats-or-workflows-for-dealing-with-3d-textures-in-metal">Are there any recommended formats or workflows for dealing with 3D textures in Metal?</h4>
</blockquote>
<p>Can you clarify more about “workflows”? Are you trying to integrate with an existing asset pipeline? 
Is there a specific thing you are trying to do? I believe you can manually fill the texture one slice at a time if needed. If you are not interpolating along the 3rd axis, you might want to use a 2D Texture Array instead. 
Just be careful with blit API, Simple APIs don’t work on 3D textures 
you need full swing blit copy with one full slice calculated not whole texture! 
I believe <code>MTLTextureLoader</code> supports KTX files, including 3D texture formats. 
Thanks for the pointers.  I'm just starting to explore ideas.  I had exported a PNG with vertical slices from a voxel editor. 
Thanks for the pointer on KTX!  I'll look into that 
Importing one slice at a time works as well! If you are doing uncompressed data (voxel art etc), that would work. If you are going more "natural" 3D textures, beware that you probably want a compressed format as it makes a big difference in memory size and performance. 
Right.  Are there any gotchas using sparse textures for 3D textures? 
I don't believe so - it should effectively work the same. </p>
<hr />
<blockquote>
<h4 id="are-all-of-the-metal-3-features-available-on-all-gpus-do-vega-gpus-and-uhd-630-get-mesh-shaders-with-metal-3">Are all of the Metal 3 features available on all GPUs?  Do Vega GPUs and UHD 630 get mesh shaders with Metal 3?</h4>
</blockquote>
<p>• Metal 3 hardware support can be found in <code>Discover Metal 3</code> <a href="https://developer.apple.com/videos/play/wwdc2022/10066/">https://developer.apple.com/videos/play/wwdc2022/10066/</a>
• Specific features (Mesh Shaders, Metal FX) may have a different hardware support. For example, Mesh Shaders should be supported on MTLGPUFamilyApple7 and MTLGPUFamilyMac2. Please refer to their associated session for more information. ie, for Mesh Shaders : <a href="https://developer.apple.com/videos/play/wwdc2022/10162/">https://developer.apple.com/videos/play/wwdc2022/10162/</a> </p>
<hr />
<blockquote>
<h4 id="quick-question-what-happened-to-the-metal-language-version-build-setting-seemed-it-disappeared-one-year">Quick question: What happened to the Metal Language Version build setting? Seemed it disappeared one year...</h4>
</blockquote>
<p>Hi Steven, this looks a bug, could you please file a feedback request for us to track it? In the meanwhile you can work around this by passing <code>-std=ios-metalX.X</code> to your .metal file compilation flags 
Same for macOS? 
~I think for macOS it is <code>-std=osx-metalX.X</code>~ 
Looks like it may be back in Xcode 14, would it be fixed for 13.4? 
For macOS, it is <code>-std=macos-metalX.Y</code>  until MSL 3.0 where we have unified language and it is <code>-std=metal3.0</code> 
thank you </p>
<hr />
<blockquote>
<h4 id="hey-there-i-have-been-working-with-a-highly-programmatic-scenekit-nodes-and-geometries-i-know-i-know-old-hat-and-i-have-been-suggested-by-some-friendly-helpful-developers-at-the-wwdc-apple-park-event-hello-again-rintaro-that-a-pathway-to-help-some-of-my-performance-problems-would-be-to-rewrite-some-of-my-core-components-with-arkit-primitives-or-perhaps-metal-unfortunately-i-dont-really-know-where-i-would-begin-that-conversion-or-even-if-doing-the-work-would-net-the-performance-benefits-im-seeking-the-context-of-this-question-is-my-arvr-visualization-project-which-uses-10s-of-thousands-of-small-geometries-and-nodes-shared-as-much-possible-to-render-individual-text-glyphs-like-a-sheet-of-paper-httpgithubcomtikimcfeelookatthatgithubcomtikimcfeelookatthat-a-good-solid-first-step-would-be-what-would-be-a-suggested-migration-pattern-from-scnnodescnplaneuiimage-to-similar-metalarkit-patterns">Hey there! I have been working with a highly programmatic SceneKit nodes and geometries (I know, I know, old hat), and I have been suggested by some friendly helpful developers at the WWDC Apple Park event (Hello again, Rintaro!) that a pathway to help some of my performance problems would be to 'rewrite' some of my core components with ARKit primitives, or perhaps Metal. Unfortunately, I don't really know where I would begin that conversion, or even if doing the work would net the performance benefits I'm seeking. The context of this question is my AR/VR visualization project which uses 10's of thousands of small geometries and nodes (shared as much possible) to render individual text glyphs like a sheet of paper (<a href="http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat">http://github.com/tikimcfee/LookAtThat|github.com/tikimcfee/LookAtThat</a>). A good, solid first step would be: What would be a suggested migration pattern from SCNNode/SCNPlane/UIImage to similar Metal/ARKit patterns?</h4>
</blockquote>
<p>Are you using individual nodes for each letter/glyph? If so, I would recommend "baking" the glyphs into a vertex/index buffer at run-time and then inserting the entire mesh as a single node. 
So for instance, you would read in the text, then generate the mesh for a single page, and add that page to your scene as a single mesh (that you can then modify as if it is a static mesh).</p>
<p>Alternatively, you can simply generate a large image with all the text on it using our Text APIs and simply apply it as a texture/Image. 
But the latter would take up more memory as you are effectively creating a full-sized texture each time. 
Jaap, thanks for your time! That’s exactly the case - at the moment, I render each glyph as a node, and then <code>flatten()</code> them into a single geometry. 
I have a large texture method too, and it does work, but indeed has the memory issue you’re mentioning.</p>
<p>The <code>flatten()</code> has the advantage that is actually seems to create that mesh from the nodes, but I think it does it in a non-optimized-for-my-use-case-way.</p>
<p>If I were to generate the mesh, is there something that I could use as a sample to convert a node’s general presentation / backing geometry to a mesh? I apologize if that’s a vague question. Essentially, I’m looking to see if there’s some intermediary step I can take that would allow me to migrate from the patterns I have now, even if the translation would be slow, just to build out some observations to make better hypotheses from.</p>
<p>That would be something like, “a function that takes a parent node and generates a mesh” or something along that. 
Also, at the moment I ‘hot swap’ between the glyphs and the <code>flattened</code> nodes to get that boost in FPS. I need to keep the nodes around, though, because they’re actionable individually - updating positions, backing contents, that kinda thing. It seems like I’d be able to do the same thing with a mesh by dynamically updating the mesh itself for things like positioning, and then figuring out how to map texture coordinates to those dynamically. I was even looking at ‘textureCoordinates’ as a potential option, but that exists on <code>geometry</code>, and all of my nodes with the same glyph share a geometry, again to keep total counts down. 
Hi &lt;@U03JRPTDF6U&gt;, sorry for the late reply, as we closed down the Q&amp;A, but if you want to discuss this more, please drop by the Q&amp;A on Friday, or sign up for a Developer Lab, which is a 1:1 in-depth chat with one of our developers!</p>
<p>Just to quickly reply to your question, I do agree that you probably want to do this manually to remove a lot of the overhead. I'm not sure how familiar you are with mesh representations, but you should probably be able to do this by effectively writing your own "flatten" function. Without knowing where most of the performance is lost, I would suggest reading the nodes data manually and then composing the vertices for the mesh yourself. This would require you to manually create vertex and index data for your mesh and then upload and bind these to your Mesh.</p>
<p>I would suggest putting your question up in the Labs and indicate this is a SceneKit question - I think those folks will be better equipped to point out the best path :) </p>
<hr />
<blockquote>
<h4 id="with-metalfx-are-there-any-concerns-or-issues-when-dealing-with-transparencyvolumetrics-when-doing-temporal-upscaling-would-it-sometimes-be-better-to-use-spatial-upscaling-and-msaa-in-some-cases-with-a-lot-of-transparency">With MetalFX are there any concerns or issues when dealing with transparency/volumetrics when doing temporal upscaling?  Would it sometimes be better to use Spatial upscaling and MSAA in some cases with a lot of transparency?</h4>
</blockquote>
<p>Temporal AA requires full motion and depth information to produce the best result. Transparent objects usually do not provide that, so TAA will have insufficient input data to process everything correctly. Similar logic applies to reflections. 
The choice will depend on your particular setup of the rendering pipeline. You could apply temporal upscaling to lower resolution color buffer with opaque objects only and then render transparent objects and volumetrics on top (in output resolution). 
That makes sense, thanks.  Would the upscalers also work for targets other than the final tone mapped image, ie an AO pass? 
Yes, you can do that. Just allocate separate scaling effect instances for those. </p>
<hr />
<blockquote>
<h4 id="can-screencapturekit-be-used-by-two-apps-at-once-each-capturing-all-or-parts-of-the-screen">Can ScreenCaptureKit be used by two apps at once, each capturing all or parts of the screen?</h4>
</blockquote>
<p>Hi Matt! Fantastic question! ScreenCaptureKit allows for any number of active streams from any number of clients capturing any combinations of filters! 
Great thanks. </p>
<hr />
<blockquote>
<h4 id="is-system-level-permission-required-even-if-an-app-only-wants-to-capture-itself-ie-only-its-own-windows-and-not-anything-external">Is system-level permission required even if an app only wants to capture /itself/ (i.e. only its own window(s)), and not anything external?</h4>
</blockquote>
<p>Hi Michael! Yes, system level permission (one time) is required for the initial time any applications wants to use ScreenCaptureKit. Even if the application is capturing itself, it'll need to get the SCShareableContent list in order to create a filter, which requires system level permission on the initial use. 
Gotcha, gracias! </p>
<hr />
<blockquote>
<h4 id="our-video-conferencing-app-offers-screen-sharing-functionality-but-we-want-the-screen-share-video-to-exclude-certain-windows-eg-av-controls-and-floating-participant-videos-this-is-easily-solvable-with-content-filtering-in-screencapturekit-but-what-is-the-recommended-way-to-get-this-behavior-with-older-versions-of-macos-that-do-not-support-screencapturekit">Our video conferencing app offers screen sharing functionality, but we want the screen share video to exclude certain windows (e.g., A/V controls and floating participant videos). This is easily solvable with content filtering in ScreenCaptureKit, but what is the recommended way to get this behavior with older versions of macOS that do not support ScreenCaptureKit?</h4>
</blockquote>
<p>You can use the screen capture portion of ScreenCaptureKit as of 12.3, and send it to other users using earlier versions of macOS. In order to get the maximum screen capture performance with ScreenCaptureKit, you'll need 12.3 or later. To achieve similar behavior in previous versions of macOS, you can use the <code>CGWindow</code> API to capture video, but not audio. 
Specifically, CGWindowListCreate() and CGWindowListCreateImage(). They’re <em>much</em> slower than the other APIs, but available back to earlier MacOS versions. </p>
<hr />
<blockquote>
<h4 id="if-you-want-to-display-the-interactions-with-menu-at-the-top-of-macos-does-that-constrain-you-to-just-capturing-a-display-vs-application-or-window-in-order-to-show-that">If you want to display the interactions with menu at the top of macOS, does that constrain you to just capturing a display (vs. application or window) in order to show that?</h4>
</blockquote>
<p>You can use <code>init(display:including:exceptingWindows:)</code>, but this will capture the surface the size of the desktop. The app will be positioned as it is in the desktop, and the menu bar will be at the top of the capture. 
Ah, brilliant! Thank you! </p>
<hr />
<blockquote>
<h4 id="i-make-media-playback-systems-but-havent-really-yet-played-with-producing-media-could-you-give-me-an-idea-of-what-the-next-steps-would-be-in-terms-of-macos-frameworksapi-usage-to-go-from-screencapturekits-sample-buffers-to-say-a-playback-ready-h264hevcaac-hls-output">I make media /playback/ systems, but haven't really yet played with /producing/ media– could you give me an idea of what the next steps would be (in terms of macOS frameworks/API usage), to go from ScreenCaptureKit's sample buffers, to, say, a 'playback-ready' H264/HEVC+AAC HLS output?</h4>
</blockquote>
<p>Hi Michael!</p>
<p>ScreenCaptureKit gives samples back as CMSampleBuffers. So if you wanted to make it playback ready, I would suggest you use AVAssetWriter to create a movie. This should allow you create a movie with the output you wish 
For HLS, you're going to use VideoToolBox to fragmented mp4 files that you can then upload to the server and update your HLS playlist 
And if you need a CBR-based stream, Ventura added hardware-encoding-based H.264 and HEVC CBR support, so you can offload the work from the CPU. 
&lt;@U03H3GXFRSB&gt; This is incredibly helpful! You, in two paragraphs, have just demystified something for me that has, honestly, just been a bit of an “I can’t do that…” topic so far :ok_hand::skin-tone-3: 
&lt;@U03HF6TL5L5&gt; Ah, as in; pre-Ventura H.264/HEVC  hardware-encoding was VBR-only, or? 
Correct! 
I think it's technically ABR (instead of VBR), but I'm no encoding expert, so grain of salt. </p>
<hr />
<blockquote>
<h4 id="when-capturing-windows-only-is-the-windows-drop-shadow-captured-or-only-the-actual-frame-of-the-windows">When capturing windows only, is the window's drop shadow captured, or only the actual frame of the windows?</h4>
</blockquote>
<p>Using <code>init(desktopIndependentWindow:)</code>, you will get back an <code>IOSurface</code> that is the size of the window exactly. No drop shadow included. 
Oh well. Thanks 
This is definitely a lesser-used feature in the existing CGWindowList API, but it would be great to see it in ScreenCaptureKit, as well. 
Please submit a feedback request for this! Sounds like a good idea. </p>
<hr />
<blockquote>
<h4 id="would-be-great-to-be-able-to-make-screen-captures-either-screenshots-or-screen-recordings-in-hdr">Would be great to be able to make screen captures (either screenshots or screen recordings) in HDR!</h4>
</blockquote>
<p>Thanks Chris! Please file a feedback in the feedback assistant for new features for ScreenCaptureKit. We're super excited about it, and we hope it can meet all of your needs. 
Will do; thanks! </p>
<hr />
<blockquote>
<h4 id="one-thing-that-wasnt-totally-clear-to-me-from-the-videos-obviously-audio-capture-is-per-application-since-audio-is-not-associated-with-a-window-if-i-exclude-only-some-windows-of-an-application-from-capture-does-that-mean-i-dont-get-any-audio">One thing that wasn't totally clear to me from the videos: Obviously, audio-capture is per-application, since audio is not associated with a window. If I exclude only <em>some</em> windows of an application from capture, does that mean I don't get any audio?</h4>
</blockquote>
<p>If you’re using display capture, and exclude any window of an app, then you will get system audio minus the whole app’s audio. We also will cover this and other scenarios in our advanced talk “Take ScreenCaptureKit to the Next Level” :slightly_smiling_face: 
Thanks. I think this is fine for my typical use-case, but might be problematic for some others 
Another suggestion is to have a separate stream with the audio you want.
If you have a particular use case or scenario you think we could address better, please file a feedback request! </p>
<hr />
<blockquote>
<h4 id="if-im-capturing-all-windows-from-a-single-application-will-i-also-get-xpc-hosted-dialogs-im-thinking-of-opensave-panels-and-the-like-which-were-not-easily-captured-with-the-cgwindowlist-api-since-they-are-associated-with-a-different-pid-than-the-application-windows">If I'm capturing all windows from a single application, will I also get XPC-hosted dialogs? I'm thinking of Open/Save panels, and the like, which were not easily captured with the CGWindowList API, since they are associated with a different PID than the application windows.</h4>
</blockquote>
<p>It should capture things like the Open/Save dialog. If it doesn't, make sure to file a feedback request! 
I’ll check, and let y’all know. </p>
<hr />
<blockquote>
<h4 id="just-to-reiterate-my-pledge-please-please-push-mip-bias-as-a-part-of-the-sampler-for-next-metal-releases-i-think-even-gl-had-it">Just to reiterate my pledge - please please push mip bias as a part of the sampler for next metal releases. I think even GL had it :)</h4>
</blockquote>
<p>Please submit your request via Feedback Assistant. 
Sure! </p>
<hr />
<blockquote>
<h4 id="why-use-a-constant-jitter-offset-for-a-set-of-pixels-instead-of-a-random-one-that-would-probably-make-the-result-more-smoother">Why use a constant jitter offset for a set of pixels instead of a random one that would probably make the result more smoother?</h4>
</blockquote>
<p>Using a fixed set of low-discrepancy jitter offsets ensures that the space of potential jitters is covered relatively evenly for any given window in time. 
Thank you &lt;@U03HJ4DLF7D&gt; for your answer! Oh I see... You get a good representation of jitter space, even under this coupling between neighbouring offsets? Interesting... </p>
<hr />
<blockquote>
<h4 id="wonderful-presentation-kevin-thank-you-very-much-the-technology-works-beautifully-with-slow-moving-scenes-could-you-comment-on-the-impact-of-using-temporal-information-on-quick-moving-scenes-or-quick-changing-content-should-there-be-some-adaptive-setting-with-respect-to-the-amount-of-previous-pixels-used-based-on-the-richnessrate-of-change-of-the-scenes-content">Wonderful presentation @Kevin! Thank you very much! The technology works beautifully with slow moving scenes. Could you comment on the impact of using temporal information on quick moving scenes or quick changing content? Should there be some adaptive setting with respect to the amount of previous pixels used based on the richness/rate of change of the scene's content?</h4>
</blockquote>
<p>Thanks. If the scene is truly moving super quick, where there's very little temporal information that can be reused, the result will simply not be as high resolution and anti-aliased. We do expect this to work as well as the amount of information that's present and for things to be more refined as motion slows. 
Thank you &lt;@U03HJ4HULBC&gt; for your answer. Yes, I guess this is expected! Fast moving content is always a challenge with these technologies... Once again, thank you for a wonderful, easy to watch and beautifully prepared presentation. (Really nice and illustrative clips! :slightly_smiling_face: ) </p>
<hr />
<blockquote>
<h4 id="machine-learning-has-shown-great-results-with-respect-to-upscaling-both-in-terms-of-quality-as-well-as-performance-do-you-see-implementing-such-a-functionality-in-metalfx-over-the-near-future">Machine learning has shown great results with respect to upscaling both in terms of quality as well as performance. Do you see implementing such a functionality in MetalFX over the near future?</h4>
</blockquote>
<p>We cannot comment on particular implementation details of MetalFX framework. 
Thank you &lt;@U03H3GQDUP9&gt; for your answer. :slightly_smiling_face: I wouldn't say using ML for upscaling is an implementation detail, but rather a radically different approach to classical upscaling methods. Of course, it has its own implementation details, should one opts to actually build it (that naturally can't be shared). However, my question was about the approach itself and wether it is under consideration (and, of course, not any specifics on its implementation). 
Note that the answer is yeah, machine learning is used. you can see the models at <code>/System/Library/Frameworks/MetalFX.framework/Versions/A/Resources/brnet_v31_quant</code> . Hopefully that clarifies things a bit. 
Hi &lt;@U03J07WJLRK&gt;! Thank you very much for the reference. I will check it out! :slightly_smiling_face: </p>
<hr />
<blockquote>
<h4 id="i-couldnt-help-noticing-watching-the-no-mans-sky-clip-that-there-was-quite-a-bit-of-smearing-and-things-looked-fairly-low-quality-on-the-big-camera-move-are-these-kind-of-camera-moves-a-big-concern-with-this-technique-i-understand-taa-can-have-issues-with-smearing-and-ghosting">I couldn't help noticing watching the No Man's Sky clip that there was quite a bit of smearing and things looked fairly low quality on the big camera move.  Are these kind of camera moves a big concern with this technique.  I understand TAA can have issues with smearing and ghosting.</h4>
</blockquote>
<p>Depending on the network connection you have, the video quality may be worse and lower in quality. Can you point to us on which time-stamp in the video you saw the smearing effect? 
I think I had downloaded the presentation to my iPhone and watched.  The timestamp was around 23:35, though looking at it more closely it could very well be getting destroyed by video compression. 
Are you maybe referring to 21:35 timestamp? The session video is 22:11 in length 
Yes, so sorry </p>
<hr />
<blockquote>
<h4 id="what-lab-is-best-for-asking-scenekit-questions-this-year">What lab is best for asking SceneKit questions this year?</h4>
</blockquote>
<p>I think Games technologies Q&amp;A would be the best fit. Please look in the schedule to see when is the next session. 
Thank you! </p>
<hr />
<blockquote>
<h4 id="what-is-the-best-way-to-render-biplanar-edr-cvpixelbuffer-from-avfoundation-in-metal-is-better-to-convert-pixelbuffer-into-mtltexture-via-compute-or-sample-from-2-mtltextures-created-from-each-plane">What is the best way to render biplanar EDR cvpixelbuffer from avfoundation in Metal? Is better to convert pixelbuffer into mtltexture via compute, or sample from 2 mtltextures created from each plane?</h4>
</blockquote>
<p>To render EDR CVPixelBuffer contents using Metal you’d need following:
• Setup CAMetalLayer to accept EDR values using <code>CAMetalLayer.wantsExtendedDynamicRangeContent</code> property set to <code>YES</code>
• Create MTLTexture instances that shares corresponding IOSurface from CVPixelBuffer
• Use them for rendering</p>
<p>Thanks for the explanation about EDR:)</p>
<p>And what about performance difference between using two planes in a shader directly, and converting it to rgb beforehand? 
I can’t predict that to be honest :slightly_smiling_face: It might completely depend on the context and use case of your particular scenario. 
Ok, thanks!) I’ve thought there was a well known best practice:) </p>
<hr />
<blockquote>
<h4 id="whats-the-best-way-to-build-a-full-screen-game-experience-on-the-mac-should-we-still-be-using-cgdisplaycapture-or-the-appkit-full-screen-window-support-both-seem-to-have-pros-and-cons-for-example-appkit-allows-the-menu-bar-to-pop-and-for-easy-access-but-cgdisplaycapture-can-use-the-area-around-the-notch">What's the best way to build a full screen game experience on the Mac? Should we still be using CGDisplayCapture? Or the AppKit full screen window support? Both seem to have pros and cons. For example: AppKit allows the menu bar to pop and for easy access. But CGDisplayCapture can use the area around the notch.</h4>
</blockquote>
<p>Using the AppKit APIs will give your users the experience they expect, especially if they have customized Spaces. Use presentation options to control things like the menu bar behavior: <a href="https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions|https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions">https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions|https://developer.apple.com/documentation/appkit/nsapplication/presentationoptions</a> 
Thanks! I'm having a few issues with presentation options but that seems like a lab I should book for later. :smiley: 
Yes, please do!</p>
<p>Regarding the display shape, look into safe area insets: <a href="https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets|https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets">https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets|https://developer.apple.com/documentation/appkit/nsscreen/3882821-safeareainsets</a> 
I just confirmed with the responsible engineering team that as long as nothing else is drawing atop your full screen NSWindow, you will be on the fastest-performance drawing path through the compositor. 
Thanks. Is there a way to display in the insets using NSWindow's toggleFullScreen? We don't need to draw in the insets, but wanted to try it as a possible full screen style. 
You can always draw within the full size of your window. The safe area insets are informative about places where your drawing might be clipped. 
Thanks. It's possible I've configured my layout to be constrained into the safe area. I'll check it out! </p>
<hr />
<blockquote>
<h4 id="hello-everybody-is-there-any-sample-or-instructions-how-to-make-offscreen-render-using-metal-for-example-i-have-a-scene-and-i-want-to-save-it-as-png-file-how-can-i-do-it-can-i-control-resolution-of-saved-file">Hello everybody! Is there any sample or instructions how to make offscreen render using Metal? For example: I have a scene and I want to save it as png file. How can I do it? Can I control resolution of saved file?</h4>
</blockquote>
<p>To offscreen render a scene and save it as a PNG file, you may first render the scene into a MTLTexture with the desired resolution. The texture will need to be a &lt;https://developer.apple.com/documentation/metal/mtltextureusage/1515701-rendertarget
|render target&gt;.</p>
<p>Then there are a few different ways to get a CGImage from the MTLTexture: A quick way will be to &lt;https://developer.apple.com/documentation/coreimage/ciimage/1437890-init
|initialize a CIImage&gt; with the MTLTexture and then &lt;https://developer.apple.com/documentation/coreimage/cicontext/1437784-createcgimage
|use a CIContext to create the CGImage&gt;, which you can use to write to disk. 
Oh, thank you so much for the answer!!! </p>
<hr />
<blockquote>
<h4 id="can-you-leverage-the-screen-capture-api-to-cast-data-that-is-being-rendered-off-screen-eg-two-cameras-in-a-scene-one-camera-view-is-your-ipad-screen-another-camera-view-would-be-streamed-to-apple-tv-each-camera-are-viewing-different-location-but-in-the-same-scene">Can you leverage the screen capture API to cast data that is being rendered off screen? eg two cameras in a scene, one camera view is your ipad screen, another camera view would be streamed to apple TV. each camera are viewing different location but in the same scene.</h4>
</blockquote>
<p>Hi Simon, Can you elaborate a bit on what you mean by cameras and scenes? Do you mean a scene inside an app/game? Do you mean physical cameras capturing multiple feeds at once and aggregating them somewhere? 
Scene as in a level in a game or asset in a 3D engine (MTKView), Cameras would be digital viewports/view frustums 
Think Blender scene, with digital cameras 
SCK capture comes from the display pipeline, so content thats captured must have gone through that to be eligible. Does that make sense? 
a yeah it does, so that would not work then. Will have to read up some of the docs. Thanks for the info :slightly_smiling_face: 
Yeah it sounds like you were trying to ‘save’ doing some computations, but that won’t work for this scenario :smiley: 
cut corners where you can :smile: 
it makes sense if you were trying to show say, ‘behind me’ or maybe a companion VR viewer for the non immersed person, etc, so we feel ya :smiley:</p>
<p>Its an interesting use case, feel free to log a Feedback requesting an answer for the high level problem so we can take a look ! </p>
<hr />
<blockquote>
<h4 id="im-sorry-i-wasnt-able-to-watch-the-video-so-just-wanted-to-confirm-with-screencapturekit-were-able-to-capture-the-mixed-system-sound-or-filter-in-or-out-the-applications-that-we-want-or-dont-want-to-include-right-for-example-capture-sounds-from-my-own-application-zoom-call-already-mixed-for-us-basically-replacing-things-like-blackhole-multi-outputdevice-etc">I'm sorry, I wasn't able to watch the video, so just wanted to confirm: with ScreenCaptureKit we're able to capture the mixed system sound, or filter in or out the applications that we want or don't want to include, right? For example capture sounds from my own application + Zoom call already mixed for us, basically replacing things like Blackhole + Multi-Output/Device etc?</h4>
</blockquote>
<p>Yes, absolutely! In this case, you might want to use <code>init(display:excludingApplications:exceptingWindows:)</code>, where you can capture all audio, excluding the applications that you don't want in the audio. </p>
<hr />
<blockquote>
<h4 id="carenderer-was-not-previously-available-on-ios-only-macos-but-xcode-14-and-docs-on-the-web-now-show-it-as-available-since-ios-20-httpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minorhttpsdeveloperapplecomdocumentationquartzcorecarendererchangeslatest_minor-is-this-a-mistake-or-is-it-now-available-to-older-devices-if-built-with-the-ios-16-sdk">CARenderer was not previously available on iOS (only macOS). But Xcode 14 and docs on the web now show it as available since iOS 2.0!  <a href="https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor|https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor">https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor|https://developer.apple.com/documentation/quartzcore/carenderer?changes=latest_minor</a>  Is this a mistake, or is it now available to older devices if built with the iOS 16 SDK?</h4>
</blockquote>
<p>It’s not a mistake, and the availability annotations are correct! CARenderer is available on all devices. You can back-deploy your code built with the iOS 16 SDK to devices running older OSes. 
That’s great news! (But no wonder I was so confused yesterday..!) Thanks 
Does the <em>Overview</em> need a bit of editing then?
&gt; For real-time output you should use an instance of <code>NSView</code> to host the layer-tree.</p>
<p>Good catch! Please file Feedback about the documentation :slightly_smiling_face: </p>
<hr />
<blockquote>
<h4 id="what-is-the-fastest-way-to-get-something-on-screen-with-metal-is-there-way-to-draw-directly-to-screen-instead-of-metal-layer-contents-and-then-wait-for-frame-composing">what is the fastest way to get something on screen with Metal? is there way to draw directly to screen instead of metal layer contents and then wait for frame composing?</h4>
</blockquote>
<p>The fastest way to get your drawing on-screen is to create a full-screen CAMetalLayer. You can also optionally set <code>displaySyncEnabled</code> to false on your layer to render faster than the display’s refresh rate at the cost of potential artifacts. 
Be sure to take advantage of a ProMotion display if the user has it, too! You can set a minimum and maximum CADisplayLink callback rate using a CAFrameRateRange, for instance. </p>
<hr />
<blockquote>
<h4 id="might-anyone-be-able-to-speak-to-whether-it-makes-sense-to-combine-indirect-command-buffers-httpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjchttpsdeveloperapplecomdocumentationmetalindirect_command_encodingencoding_indirect_command_buffers_on_the_gpulanguageobjc-with-bindless-rendering-and-dynamic-data-specifically-i-am-trying-to-avoid-a-lot-of-draw-calls-with-the-soc-architecture-it-seems-passing-data-is-not-really-expensive-so-i-figure-per-frame-i-could-just-specify-some-parameters-and-generate-draw-calls-on-the-gpu-as-in-the-example-broad-question-i-realize-but-id-appreciate-some-thoughts-on-how-to-reduce-draw-calls-for-rapidly-updated-andor-deleted-vertex-data-i-also-checked-mesh-shaders-but-i-think-those-are-overkill-for-my-use-case">Might anyone be able to speak to whether it makes sense to combine indirect command buffers (<a href="https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc|https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc">https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc|https://developer.apple.com/documentation/metal/indirect_command_encoding/encoding_indirect_command_buffers_on_the_gpu?language=objc</a> ) with bindless rendering <em>and</em> dynamic data? Specifically, I am trying to avoid a lot of draw calls. With the SOC architecture, it seems passing data is not really expensive, so I figure per-frame I could just specify some parameters and generate draw calls on the GPU, as in the example.  -broad question, I realize, but I'd appreciate some thoughts on how to reduce draw calls for rapidly-updated and/or deleted vertex data.  I also checked mesh shaders, but I think those are overkill for my use case.</h4>
</blockquote>
<p>Hi Karl! Metal 3 adds support for starting raytracing work from ICBs. ICBs give you more flexibility at rendering time, but this flexibility does come with a cost that will vary according to your specific use case. For example ICBs will give you the edge when you are CPU-bound. As with all things related to performance, we recommend you profile your game (or app) :slightly_smiling_face: 
Hi! Thanks for the reply, and I agree I should always profile. To clarify, I’m not doing raytracing, so perhaps this is pre-optimization, but while WWDC is happening, I thought I’d ask a follow-up question:</p>
<p>Since you mention ICB-use cost, what would you say the common tradeoffs are with ICBs?
Also, do you imagine them working well with bindless rendering?
For example, let’s many textures and my entire scene in a buffer within an argument buffer.
Would it then make sense to use an ICB to issue draw calls on the scene stored that way?</p>
<p>However, I suspect that ICBs and bindless are orthogonal.</p>
<p>Thanks again for your time! </p>
<hr />
<blockquote>
<h4 id="when-will-the-activity-feed-be-available">When will the activity feed be available?</h4>
</blockquote>
<p>The activity feed is available today in iOS/iPadOS/tvOS 16 beta and macOS Ventura beta. :video_game::star2: </p>
<hr />
<blockquote>
<h4 id="hello-wave-playing-around-with-the-new-screencapturekit-im-experiencing-problems-with-tcc-revoking-access-to-screen-recording-while-the-apps-running-domain-comapplescreencapturekitscstreamerrordomain-code-3801-message-the-user-declined-tccs-for-application-window-display-capture-confusingly-when-my-app-launched-it-did-have-the-proper-access-anyone-around-knowing-under-which-special-circumstances-the-system-may-suddenly-revoke-access-to-screen-recording-for-an-already-running-app">Hello! :wave: Playing around with the new ScreenCaptureKit I'm experiencing problems with TCC, revoking access to Screen Recording while the app's running  (domain: com.apple.ScreenCaptureKit.SCStreamErrorDomain, code -3801 message: "The user declined TCCs for application, window, display capture"). Confusingly, when my app launched it did have the proper access.  Anyone around knowing under which special circumstances the system may suddenly revoke access to Screen Recording for an already running app?</h4>
</blockquote>
<p>There shouldn’t be any reason why an app has their Screen Recording tcc revoked - besides the user denying the app permission. Could you please file a feedback request, with full steps to reproduce the issue? :slightly_smiling_face: 
That was my impression, too. Thanks for clarifying, &lt;@U03HF6TK2J1&gt;! Unfortunately, there doesn't appear to be a clear pattern to this. Usually, it just takes some time of the app sitting around to get the revocation suddenly, maybe an hour or two. Does it have anything to do with the process being a Service Management Login Item and LSUIElement=1 background process? 
It's hard for us to say for sure. But in lieu of a reproducible case, a sysdiagnose shortly after the time of the issue should help us figure out what might have caused it! 
I see, well then I'll file the bug report, thank you. Amazing new API by the way, I'm sure lots of developers are going to benefit greatly from it. :slightly_smiling_face: 
Glad you like the new framework! </p>
<hr />
<blockquote>
<h4 id="hey-all-question-is-related-to-scenekit-uiview-swiftui-view-layers-there-has-been-some-historical-functionality-allowing-the-layers-of-views-not-presented-in-a-typical-hierarchy-to-be-presented-as-the-material-contents-of-geometries-like-scnplanes-one-example-is-this-repo-here-httpsgithubcomsarangborudeswiftuiarviewhttpsgithubcomsarangborudeswiftuiarview-unfortunately-the-technique-here-doesnt-seem-to-work-anymore-a-couple-of-years-later-im-wondering-what-might-be-possible-to-bring-this-back-to-usage-as-i-have-a-ton-of-use-cases-id-love-to-explore-with-this-thank-you-so-much-as-always">Hey all! Question is related to SceneKit + UIView / SwiftUI view layers. There has been some historical functionality allowing the layers of views <em>not</em> presented in a typical hierarchy to be presented as the material contents of geometries like SCNPlanes. One example is this repo here: <a href="https://github.com/sarangborude/SwiftUIARView|https://github.com/sarangborude/SwiftUIARView">https://github.com/sarangborude/SwiftUIARView|https://github.com/sarangborude/SwiftUIARView</a> . Unfortunately, the technique here doesn't seem to work anymore a couple of years later. I'm wondering what might be possible to bring this back to usage, as I have a TON of use cases I'd love to explore with this! Thank you so much as always!</h4>
</blockquote>
<p>Hi &lt;@U03JRPTDF6U&gt; I'm not seeing why SceneKit + SwiftUI shouldn't be working in this situation still. If you have a moment and can submit to <a href="https://feedbackassistant.apple.com">https://feedbackassistant.apple.com</a> and reference that GitHub project, it would be great for us to get this to the right folks inside Apple. If you do this soon and can post the FB number back here, that would help me make sure it routes. 
&lt;@U03H3GPSE6B&gt; Heya, and thanks for the good news, haha! I’ll file it right now and add some more information about what I’m trying to do. Also, this repo is a great resource as a sample, but I’m curious if there might be some tighter documentation about <em>how</em> this is working? For example, there’s some fun magic happening in the hosting of the controller and when it gets added, and I think at least part of my issue is I’m not mimicking the pattern correctly. 
Also, here’s that Feedback!
<code>FB10141722</code> 
thank you! 
Of course. And hey, in the mean time, might you know of any docs that might be related to this? I’m thinking of giving this another go soon, hehe. Happy to sit back and relax though to hear from the Feedback side :smiley: </p>
<hr />
<blockquote>
<h4 id="for-a-relatively-simple-game-low-poly-graphics-simple-textures-no-ai-which-framework-would-you-suggest-i-use-to-develop-in-swift-all-logic-would-be-custom-so-no-need-for-scripts-library-or-similar-features-scenekit-seems-like-an-obvious-choice-as-it-has-the-functionality-i-need-but-im-not-sure-how-actively-it-is-still-being-maintained-and-supported">For a relatively simple game (low poly graphics, simple textures, no AI), which framework would you suggest I use to develop in Swift? All logic would be custom, so no need for scripts library or similar features. SceneKit seems like an obvious choice as it has the functionality I need, but I'm not sure how actively it is still being maintained and supported?</h4>
</blockquote>
<p>Hi &lt;@U03HMCD9UQ7&gt; If SceneKit is a good fit for your needs, it's a great choice! It is still being maintained and supported. If in the future you outgrow its functionality, you will have learned a lot of key parts of dealing with transforms, models, and geometry which you can use if you want to drop down to Metal and ModelIO, or move up to a higher-level framework or tool. 
On the topic of maintained and supported, should we assume SpriteKit is also still supported and maintained? 
Main reason I am asking is because I have submitted quite a few SpriteKit-related issues through Feedback Assistant, and I have not received any feedback at all on any of them. 
I <em>dearly</em> hope so as SpriteKit is core to my app. I add touches and logic and page layout on top but it's all SK, all the way down. 
Hoping for an answer to this, I will tag &lt;@U03H3GPSE6B&gt; . Nat, can you please check the previous 3 replies here, regarding SpriteKit? Thanks. 
<a href="https://github.com/AndyDentFree/SpriteKittenly">https://github.com/AndyDentFree/SpriteKittenly</a> is where I post my SpriteKit explorations of tech, following my philosophy of having lots of little test apps, as useful regression checks. 
Sorry for the slow reply - yes SpriteKit is still maintained and available!<br />
Thanks Nat. This is good news. I hope for maybe a little bit more maintaining in the future, maybe some of the reported bugs in the Feedback Assistant can be looked at. Thanks again! 
Hey &lt;@U03JZ2H3NNR&gt; do you also publish your bugs in somewhere open to the rest of us non-Apple employees to see? I ran into a doozy with SpriteKit that was (strictly speaking) my bug but was encouraged to lodge a Feedback when I discussed with an engineer during the Tech Talks last year, as <em>that is such weird behaviour we should probably try to fix it anyway</em>. (:blush: at realising it's been a few months &amp; still haven't lodged that).
You can read the entire saga <a href="https://medium.com/touchgram/oops-hitting-a-5yo-apple-bug-17d2703519f4">https://medium.com/touchgram/oops-hitting-a-5yo-apple-bug-17d2703519f4</a> 
Given my massive dependency on SpriteKit I'd like to know about any and all lurking gremlins :pray: 
&lt;@U03JELM0ZNV&gt; Yes, I usually publish them on both Dev Forums and StackOverflow. You can see some of them here: <a href="https://developer.apple.com/forums/profile/calin">https://developer.apple.com/forums/profile/calin</a></p>
<p>I have more but to be honest I kinda gave up reporting them because, like I said, I am not getting any feedback, so it seems like a waste of my time, as it does take some time to file a properly documented bug report.
Instead I am simply trying to work around them.</p>
<p>I was thinking to maybe try on Twitter too. My twitter is @upurupu if you want to keep in contact. 
I'll definitely read your Medium post. 
For example, one bug I was hoping will get fixed (maybe it was, haven't tried it yet), is that <code>isPaused</code>, when passed to a <code>SpriteView</code> , doesn't seem to affect the state. See <a href="https://stackoverflow.com/questions/69610165/spriteview-doesnt-pause-scene-on-state-change/69610906#69610906">https://stackoverflow.com/questions/69610165/spriteview-doesnt-pause-scene-on-state-change/69610906#69610906</a> 
Great, will stay in touch. I 
am very active on Twitter as @andydentperth 
I have yet to get into SwiftUI as my app is iOS12+ so all UIKit but am just on the cusp. Thanks to feedback this WWDC will be trying to host SpriteKit for <em>some</em> screens. Just got a demo going with SwiftUI working inside an iMessage app extension at <a href="https://github.com/AndyDentFree/im-plausibilities/tree/master/imUrlDataAppSUI">https://github.com/AndyDentFree/im-plausibilities/tree/master/imUrlDataAppSUI</a> </p>
<hr />
<blockquote>
<h4 id="is-there-a-way-to-send-a-raw-mtltexture-or-a-specific-view-to-screencapturekit-or-some-lower-level-api-related-to-it-for-example-lets-say-i-have-a-real-time-graphics-application-written-using-metal-and-i-want-to-stream-content-containing-only-audience-relevant-elements-in-my-scene-rendered-to-one-texture-i-broadcast-that-texture-in-another-renderpass-i-composite-some-diagnostic-info-only-relevant-to-me-locally-that-i-dont-want-to-share-with-others-how-might-this-be-achieved-its-a-use-case-i-keep-running-into-asymmetric-views-and-perspectives-going-even-further-maybe-i-am-developing-a-game-and-i-want-to-render-a-completely-different-perspective-in-my-scene-from-a-different-camera-i-send-that-texture-to-the-stream-but-maybe-i-render-from-a-different-perspective-locally">Is there a way to send a raw MTLTexture or a specific view to ScreenCaptureKit, or some lower-level API related to it? For example, let's say I have a real-time graphics application written using Metal, and I want to stream content containing only audience-relevant elements in my scene, rendered to one texture.  I broadcast that texture. In another renderpass, I composite some diagnostic info only relevant to me locally that I don't want to share with others.  How might this be achieved? It's a use case I keep running into (asymmetric views and perspectives.)  Going even further, maybe I am developing a game and I want to render a completely different perspective in my scene from a different camera. I send that texture to the stream, but maybe I render from a different perspective locally.</h4>
</blockquote>
<p>Hi Karl,</p>
<p>SCK is dependent on the display pipeline so the content needs to be rendered before it can be captured. 
Ah, I see, so it sounds like I’d need to roll my own solution for something along the lines of what I described. 
We discussed this a little bit yesterday too, in <a href="https://wwdc22.slack.com/archives/C03H77PER5G/p1654728707927069">https://wwdc22.slack.com/archives/C03H77PER5G/p1654728707927069</a> 
You should log a <a href="https://developer.apple.com/bug-reporting/|feedback">https://developer.apple.com/bug-reporting/|feedback</a> detailing your use case so we can consider it for future improvements! 
Will do! 
&lt;@U03HWP445CH&gt; Thanks for the link, I was to late yesterday to ask where I should submit the use case 
&lt;@U03HWP445CH&gt; &lt;@U03HZ4EJJ05&gt; I wrote this hopefully compelling argument: FB10143711 (ScreenCaptureKit Suggestion: Selective streaming of textures for Asymmetric Views) 
Thanks! We've got it </p>
<hr />
<blockquote>
<h4 id="would-screencapturekit-be-appropriate-for-a-remote-desktopscreen-sharing-application">Would ScreenCaptureKit be appropriate for a remote desktop/screen sharing application?</h4>
</blockquote>
<p>Hi &lt;@U03HZ4HF31T&gt;, yes it would be appropriate! Did you have some specific scenarios you were concerned it might not accommodate? 
Just looking at something that would provide better performance than vnc (not difficult!) and add audio for remote working on a mac. Obviously keyboard/mouse input would need to be handled seperately. 
Screen sharing is not quite the same use case as bi directional content flow. Handling where the mouse is, capturing input and passing it back would require a not-insignificant amount of work 
To split your question, it would be appropriate for screen sharing but more challenging for VNC where you want to interface with the source from the destination 
Since audio was mentioned too, I'm assuming it's still a quite tough challenge to be able to capture the system's audio, right? May require writing an own low-level driver for it? 
No, you can get system or app audio! 
ScreenCaptureKit can include system audio as well I believe. 
Ahh beat me to it! 
wait what...? :flushed: I must've missed that part of the API 
You can include or exclude apps and capture all system audio.</p>
<p><a href="https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955572-excludescurrentprocessaudio">https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955572-excludescurrentprocessaudio</a> 
The session also covers these use cases with specific examples 
<a href="https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955571-capturesaudio">https://developer.apple.com/documentation/screencapturekit/scstreamconfiguration/3955571-capturesaudio</a> 
Given the amount of work required, I may provide feedback and hope/pray that the built in macos remote desktop will add audio! 
&lt;@U03K0FHV4M8&gt; Its much requested and long awaited, we are thrilled its surprising and delighting you :smiley: 
amazing! it doesn't allow capturing only the audio from one app though, does it? 
It does I believe. 
Yes, it allows you to specify just a single apps audio as well. &lt;@U03HF6TL5L5&gt; covers this use case specifically near the end of the session 
If you wanted to say, exclude a games audio and include your friends chat audio from another app, SCK would support that use case 
If you think about a capture session as being "audio, video, or both" you can setup a capture for a process and just drop the frames, making it an "audio only" capture. 
I'm blown away by this. We've all been waiting soooo long for this and now it's finally here. :pray::heart: 
If you don't set <code>capturesAudio</code> you have a video only capture and if you set it, you get both! 
really cool! thanks for all the explanations :stuck_out_tongue: 
The framework looks great. Congratulations on your hard work! 
And thanks for your help. 
Then its just how you want to combine "including" or "excluding" the calls like</p>
<p><a href="https://developer.apple.com/documentation/screencapturekit/sccontentfilter/3919807-init">https://developer.apple.com/documentation/screencapturekit/sccontentfilter/3919807-init</a> 
that lets you narrow in on what you want depending on your use case 
Are the docs going to be updated soon or why am I missing these audio APIs you're talking about there somehow? 
There is a sample project and videos that should help. 
When you go to <a href="https://developer.apple.com/documentation/screencapturekit">https://developer.apple.com/documentation/screencapturekit</a> and at the bottom search for "Audio", do you not see those results? 
The <a href="https://developer.apple.com/documentation/screencapturekit/capturing_screen_content_in_macos|sample app">https://developer.apple.com/documentation/screencapturekit/capturing_screen_content_in_macos|sample app</a> also includes Audio 
Well, I can see the functions about whether to capture audio, exclude the current process and the audio stream output type but not how I would exclude/include certain apps 
ah wait, those are not part of the ScreenCaptureKit API? 
Audio capture follows similar rules to video capture, however audio capture is at an application level. So your selection will have to be application-based. 
&lt;@U03K0FHV4M8&gt; You'd use the SCContentFilters for that</p>
<p><code>init(display:excludingWindows:)</code>
or
<code>init(display:excludingApplications:exceptingWindows:)</code> 
ahh, I think I got it now :grin::see_no_evil: 
The team notes "Excluding a window will exclude all of that apps audio" as a FYI 
that was the missing info, thanks! 
Yeah. I think Meng talks about this a bit in the "Take ScreenCaptureKit to the next level" WWDC talk, if you want to double check your understanding. 
Will definitely do </p>
<hr />
<blockquote>
<h4 id="i-have-a-classic-dumb-question-is-screencapturekit-the-macos-equivalent-of-replaykit-i-want-people-to-be-able-to-create-reaction-videos-of-them-playing-touchgrams-which-is-running-on-spritekit">I have a classic "dumb question" - is ScreenCaptureKit the macOS equivalent of ReplayKit? I want people to be able to create <em>reaction videos</em> of them playing touchgrams, which is running on SpriteKit.</h4>
</blockquote>
<p>Hi Andrew! No question is "dumb"! 
So ReplayKit is also on macOS! 
but there are a lot of differences between ReplayKit and ScreenCaptureKit. ReplayKit has several features that aren't in ScreenCaptureKit, including broadcast extensions, Clips, and built in movie recording for in app use case 
Where ScreenCaptureKit differs is in its ability to capture far more content. ScreencaptureKit allows more than just in-application capture. It was designed to give you performant capture with custom filters for applications outside of your own 
So if you wanted someone to be able to capture video of their reactions (maybe using an iPhone clipped to the top of their laptop :wink: ) as well as playing a game experience, which would you use? 
ReplayKit has built in support for camera! so if you wanted to record just the singular application and their camera, you can do that with replaykit 
you could also do the same with screen capture kit as well! 
using AVCaptureDevice to get camera frames, you can get a custom filter of the content you want (more than 1 application) and then get the frames from the camera and using metal you can put them together and render that as well :smiley: 
Though you'd need something else for the camera capture part if you were using SCKit. Right, Johnny? 
correct &lt;@U03HF6TL5L5&gt;! you'd have to run an AVCaptureDevice (camera) to get frames from there 
All the user experience would be happening inside SpriteKit with audio playing via AVAudioPlayer
(audio is in response to user actions, gestures, tapping parts of message, whatever has been specified by the original message author - it's user-generated content so utterly random)</p>
<p>Is that likely to that break recording? 
not at all! ScreenCaptureKit and ReplayKit allows you to capture the applications audio, even if its played via AVAudioPlayer 
you'll get the application audio 
(I should just stop before the <em>outstanding issues</em> count on the tracker of <em>every bright idea ever</em> passes 1K)
but
is this robust with <em>multiple</em> players?
Someone can have one or more background looping sounds start on entering a page and then have more short ones in reaction to touches - feels like I'm going to be pushing boundaries.</p>
<p>I have a suspicion that making it easy for people to share videos of their play experience could be a huge accelerator, so this tech niggles at me :star-struck: 
is this local multiplayer? 
or the multiplayer instance played remotely? 
"Multiple players" is multiple sound players, sorry for the ambiguity. <em>Not</em> <em>game players</em> 
ahh got it, okay, and are these sound players coming from the same process? 
and by same process, I mean the same application process to which you are attempting to capture the video ? 
ReplayKit will only capture your applications audio, so if your application has several avplayers instances but its all in the process, ReplayKit will capture that audio for you 
Yes - Touchgram runs on top of SpriteKit with a document model where the message sender composes the experience, which can be utterly driven by what they want to do - think of it as a game authoring toolkit - meets Keynote - inside a message.</p>
<p>The playback experience might be the person sending the message or, more likely, the receiver.</p>
<p>Currently (deep breath), playback is only within iMessage as an app extension, which may complicate the ReplayKit scenario.
However, it's relatively trivial to also have playback in the accompanying <em>parent app</em> and I'll be adding that soon. 
So <em>inside one process</em> is kinda true but as an app extension may have nuances 
ahhh okay, so if its coming from an app extension, then it will not get captured, as extensions are run as a separate process 
App extensions should have their own process, and that won’t be captured 
it sounds like your application is built for iOS? 
It's an iMessage app extension but all the playback stuff plays on macOS as well. Although it's touch-driven, it's single-touches most of the time so is usable via mouse 
I'm planning to move to SwiftUI so started considering a Mac version :innocent: 
ahh okay! so ScreenCaptureKit is only for macOS 
it looks like the best fit for your application is in fact ReplayKit, you'll the audio from different processes / bundleID will not be captured 
Thanks for the clarification, and inspiration 
you are most welcome!! 
&lt;@U03H3GXFRSB&gt; Hey Johnny, it's Tobias from the one-on-one session today about AVAssetWriter producing corrupted files (the Teams related app). :wave: Thank you so much again for sharing your expertise with me and answering all of my questions. 
&lt;@U03H3GXFRSB&gt; I think I've figured out what the issue was. Turns out that I was attempting to append a sample buffer with the same CMTime twice due to time rounding reasons in the asset writer. Changing the timescale from 600 to something larger so far seems to have resolved the frame dropping entirely but as you've suggested I'll keep the check about whether the time of the last frame was before or equal to the current one in there just to be sure. In case you still see this, do my explanations make any sense to you? Either way, it was awesome talking to you. :nerd_face: 
&lt;@U03K0FHV4M8&gt; yay! Im sooo happy you were able to figure it out after our talk!! </p>
<hr />
<blockquote>
<h4 id="i-want-to-record-the-whole-screen-but-i-keep-getting-the-microphone-active-orange-dot-in-the-recording-how-can-i-make-a-recording-of-a-screen-using-screencapturekit-without-the-orange-dot">I want to record the whole screen but I keep getting the microphone active orange dot in the recording. How can I make a recording of a screen using ScreenCaptureKit without the orange dot?</h4>
</blockquote>
<p>We are aware of feedback around the orange dot. ScreenCaptureKit adheres to the same system privacy and security requirements of other capture methods. 
OK thank you! </p>
<hr />
<blockquote>
<h4 id="can-screencapturekit-capture-an-off-screen-window-as-in-if-i-wanted-to-capture-some-visuals-that-normally-show-in-a-window-1920x1080-in-my-app-but-theres-no-need-to-show-the-user-because-they-just-care-about-the-resulting-captured-output-if-so-does-that-seemingly-make-sense-as-a-use-of-sck-or-are-there-more-direct-methods-for-app-internal-view-capture-youd-suggest-instead">Can ScreenCaptureKit capture an 'off-screen' window?– as in, if I wanted to capture some visuals that normally show in a window (1920x1080) in my app, but there's no need to show the user (because they just care about the /resulting/ captured output)  &amp; if so: does that seemingly /make sense/ as a use of SCK, or are there more 'direct' methods for 'app-internal' view capture you'd suggest instead?</h4>
</blockquote>
<p>Hi! If you mean offscreen (but rendered), yes SCKit can capture it. 
If you mean something more like non rendered content like we discussed <a href="https://wwdc22.slack.com/archives/C03H77PER5G/p1654817389344279|here">https://wwdc22.slack.com/archives/C03H77PER5G/p1654817389344279|here</a>, then no 
If you drag a window partially offscreen, SCK can get the "off screen" content in its capture. 
Is that what you were after? 
If you want what the other thread discussed, be sure to log a feedback requesting it! 
Yep, it was! Thanks</p>
<p>(I just remembered the one-time permission requirement, which would probably confuse the user for my use-case, as they wouldn’t expect my app to be ‘recording the screen’ as such, so I’ll do some Googling on existing ‘app-owned’ window/view capture options, if any) 
Your own app should have access to its own content without TCC permissions </p>
<hr />
<blockquote>
<h4 id="when-i-use-sccontentfilterinitdesktopindependentwindow-in-screencapturekit-the-popup-buttons-contextual-menus-sheets-that-appear-on-the-window-are-not-captured-is-there-any-way-to-capture-them-in-screencapturekit-previously-i-implemented-using-cgwindowlistcopywindowinfooptionincludingwindow-optiononscreenabovewindow-windowid-and-cgimageinitwindowlistfromarrayscreenbounds-windowarray-imageoption-api-but-i-would-like-to-know-how-it-is-possible-in-screencapturekit">When I use <code>SCContentFilter.init(desktopIndependentWindow:)</code> in ScreenCaptureKit, the popup buttons, contextual menus, sheets that appear on the window are not captured. Is there any way to capture them in ScreenCaptureKit?  Previously, I implemented using <code>CGWindowListCopyWindowInfo([.optionIncludingWindow, .optionOnScreenAboveWindow], windowID)</code> and <code>CGImage.init?(windowListFromArrayScreenBounds:, windowArray:, imageOption:)</code> API, but I would like to know how it is possible in ScreenCaptureKit.</h4>
</blockquote>
<p>Additional pop-over items like these will not be shown with <code>init(desktopIndependentWindow:)</code>. If you would like to capture these, you should use an application-based capture. However, please submit a feedback request for this enhancement if you would like to see these inside of <code>init(desktopIndependentWindow:)</code>. How would you prefer the capture handle things like context menus that exceed the bounds of the window? </p>
<hr />
<blockquote>
<h4 id="id-like-to-use-reality-composer-in-the-new-xcode-to-generate-simple-usdz-shapes-then-use-the-new-reality-converter-to-re-texture-these-shapes-because-reality-composer-cannot-add-custom-textures-in-reality-composer-you-have-to-enable-usdz-export-in-preferences-but-when-i-export-a-simple-box-to-usdz-reality-converter-cannot-open-it-conversion-failed-1-error-which-is-unexpectedunknown-is-there-a-secret-way-to-make-this-work-thanks-for-leaving-questions-open-im-in-australia-i-realise-this-not-quite-the-right-forum-for-this-but-the-right-forum-hasnt-left-questions-open-and-its-open-around-2-3am-my-time">I'd like to use Reality Composer (in the new Xcode) to generate simple USDZ shapes, then use the new Reality Converter to re-texture these shapes (because Reality Composer cannot add custom textures). In Reality Composer, you have to enable USDZ export in Preferences, but when I export a simple box to USDZ, Reality Converter cannot open it (Conversion failed: 1 error, which is unexpected/unknown). Is there a secret way to make this work?  (Thanks for leaving questions open — I'm in Australia. I realise this not quite the right forum for this, but the right forum hasn't left questions open, and it's open around 2-3am my time.)</h4>
</blockquote>
<p>Hi, I tried to reproduce the error as you described. This was the order of steps:
• Create a new project in Reality Composer using the horizontal anchor.
• Turn on "Enable USDZ export" in the settings. 
• Export the project to USDZ format
• Open a Finder window and double-click the exported USDZ file.
If this does not work, would you file a Feedback Assistant? </p>
<hr />
<blockquote>
<h4 id="in-hlsl-texture2d-and-texture2d_array-are-treated-almost-the-same-ie-texture2d-can-be-used-instead-of-array-one-however-in-metal-the-type-is-much-more-strict-does-it-mean-that-the-only-way-in-metal-is-to-create-respected-texture-view-to-convert-types-the-texture-view-looks-to-be-performance-hit">In HLSL Texture2d and Texture2d_array are treated almost the same i.e. texture2d can be used instead of array one. However in metal the type is much more strict. Does it mean that the only way in Metal is to create respected texture view to convert types? The texture view looks to be performance hit :(</h4>
</blockquote>
<p>Texture views allow you to use a single texture backing with more than one format (i.e. RGBA8 and RG16). When a texture is marked with the “view” option this can result in lossless compression being disabled. Can you explain in a little more detail what performance hit you are seeing? 
I actually don’t need to change format 
let’s say my shader always accepts only texture2d&lt;float&gt; 
then to push their texture2darray texture I will need to ‘slice’ it in slices 
even if I need always a zero slice 
it works btw though validator complains 
and by ‘slice’ I mean I need to make X slices of texture2d type 
and then pass that slice 
the perf hit is the necessity to slice the texture 
and disable of tex compression 
Ah, so you wish to use a single slice of a texture2d array – is it the requirement in MSL of putting Texture2D or Texture2DArray that you are bumping into? 
yes 
HLSL allows you to push texture2darray into texture2d in shader 
<a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/resource-binding-in-hlsl">https://docs.microsoft.com/en-us/windows/win32/direct3d12/resource-binding-in-hlsl</a> 
the problem is that engine we’re porting uses mixed texture2d and texture2d_array types all around 
and in shader there is just texture2d 
and of course validator coughs 
may be there is some hidden trick to push tex2d_array slice as tex2d :wink: 
Hold tight so we can get a proper answer! 
(or ask more questions :slightly_smiling_face:) 
sure thing 
the alternative is to make an array of slices for each tex2d_array 
I’d really want to avoid this but.. 
array of slices obtained by textureViewing.. 
Do you happen to have the print out of the validation layer for your use case so I can make sure I’m looking at the same path? 
Basically it says wow wow wow you have MTTextureType2D_Array and in shader we have texture2d 
Sorry indeed I don’t have exact line 
Ha no that was perfect.</p>
<p>So I did confirm, because there is no reinterpretation of the pixels with taking a single slice of an array, <code>MTLTextureUsagePixelFormatView</code> should <em>not</em> be required! 
oh nice! 
hope validator won’t bark 
so you think going ‘array of slices’ is the only way :wink: 
the thing is - without validator it works fine! 
i.e. shader texture2d and tex2d_array as input 
it shows first slice 
I was thinking about the idea of having the shader always take an array, too, and just passing in a single entry array, but I don’t know how your shaders might get duplicated or used. 
and vice versa too! 
So you only need to use array entry 0 ever? 
sometimes nope 
by 90% of time yes 
Got it. To be clear, you still need to make a Texture View with slice N, but you don’t have to set the <code>MTLTextureUsagePixelFormatView</code> bit, 
all righty!! 
hope views are cheap 
it’s interesting whether I can do some tricks with new arg buffers 
to offset the GPU address of texture 
Should be good performance without any interpretation. And as implied above, if there is pixel interpretation then you lose things like lossless compression (which is what the PixelFormatView option is for). 
You can always find out which textures don't have the lossless compression in your GPU capture, by right clicking on the table header and choosing Texture-&gt;Lossless Compression when you're in the Memory view. 
Normally it shows Label, Insights, Type, Allocated Size, Storage Mode, Purgeable State, CPU Access, and Time Since Last Bound 
&gt; to offset the GPU address of texture
If I understand correctly the intent, offsetting the GPU address of a texture won’t work in most cases and it’s generally not recommended. Creating views is the way to go 
Yeah :slightly_smiling_face: I understand it’s just fun to have GPU address finally 
&gt; hope views are cheap
Yes, creating a view is extremely cheap: there’s no memory allocation / mapping, no trip to the kernel, etc. 
Like in GNM 
Having GPU pointer writing to arbitrary addresses, having Mac borked completely.. Fun! 
Still waiting btw for major tech breakthrough gents on your side 
2022 is not a year for Metal to be able to kill mac in 1 second :wink: 
and right now I can do it easily 
Windows manages to survive 
In theory we have mechanisms to recover from a GPU crash pretty quickly. Also, we have mechanisms to handle some undefined behaviors more gently so we don’t even need to crash at all 
I’m specifically talking about Apple-Designed SoCs though 
yeah I understand 
still I code Mac games all days and I use eGPU btw 
just because it allows me to survive GPU external crashes 
but indeed hard crashes are so much better analysed on Apple Socs 
and yeah I think eGPU with Apple Soc is a good idea :wink: </p>
<hr />
<blockquote>
<h4 id="ive-just-moved-from-my-old-and-hot-intel-imac-to-an-m1-max-macbook-pro-which-im-loving-one-snag-our-games-still-run-on-opengl-es-but-the-ios-simulator-on-apple-silicon-seems-to-crash-when-executing-opengl-es-commands-is-it-game-over-do-i-finally-need-to-move-to-metal">I've just moved from my old and hot Intel iMac, to an M1 Max MacBook Pro, which I'm loving! One snag - our games still run on OpenGL ES, but the iOS Simulator on Apple Silicon seems to crash when executing OpenGL ES commands. Is it Game Over? Do I finally need to move to Metal?</h4>
</blockquote>
<p>Hi David! This is not expected. Please submit a report using the Feedback Assistant app and we’ll take a look. Thank you! </p>
<hr />
<blockquote>
<h4 id="first-thanks-for-all-your-hard-work-and-cool-new-apis-i-watched-meet-distributed-actors-in-swift-and-im-curious-if-theres-currently-a-way-to-use-distributed-actors-in-a-peer-to-peer-gkmatch">First, thanks for all your hard work and cool new APIs!  I watched "Meet distributed actors in Swift", and I'm curious if there's currently a way to use distributed actors in a peer-to-peer GKMatch?</h4>
</blockquote>
<p>Hi &lt;@U03HMD2BP55&gt;! I believe using distributed actors with GKMatch will have to wait until Swift actors support custom executors. But I encourage you to ask the friendly folks in the Swift lounge for a more authoritative answer! </p>
<hr />
<blockquote>
<h4 id="hey-there-heres-a-basic-one-with-some-context-im-a-mobile-developer-working-for-about-a-decade-in-the-space-and-i-have-very-little-experience-with-high-fidelity-graphics-and-their-libraries-i-am-highly-interested-in-learning-c-especially-to-interact-with-the-new-metal-cpp-tools-my-question-is-would-the-metal-cpp-examples-shared-here-in-this-lounge-be-a-good-starting-point-into-learning-enough-c-to-be-productive-with-metal-or-is-there-a-better-starting-point-that-may-be-a-more-gradual-or-helpful-introduction-to-the-tools-i-dont-mind-hitting-the-ground-running-so-to-speak-just-curious-to-see-what-paths-are-most-suggested-for-this-thank-you">Hey there! Here's a basic one, with some context: I'm a mobile developer working for about a decade in the space, and I have very little experience with high-fidelity graphics and their libraries. I am highly interested in learning C++, especially to interact with the new metal-cpp tools. My question is, would the metal-cpp examples shared here in this lounge be a good starting point into learning enough C++ to be productive with Metal? Or, is there a better starting point that may be a more gradual or helpful introduction to the tools? I don't mind hitting the ground running so to speak, just curious to see what paths are most suggested for this! Thank you!</h4>
</blockquote>
<p>Hi - let's chat a bit. I'm curious to know what languages you already have familiarity with - do you have experience with Objective-C++? 
Cheers Jared!</p>
<p>I unfortunately <em>don’t</em> have experience with Obj-C++, but I do with straight Obj-C from the pre-Swift years, heh. I also have a glancing familiarity with -C-like languages, but that’s mostly syntactic pattern recognition over function (“oh, that thing is accessing a pointer, that’s updating a field..“) 
Ok, interesting. Well, as I'm sure you've seen there's a ton of resource for learning C++. If you're used to Obj-C, I think you'll understand the basic concepts pretty quickly.</p>
<p>The reason I ask is because Metal is natively implemented as an Objective-C API. What's nice about Objective-C++ is that inter-op with C++ is super easy. Basically, you just rename your <code>.m</code> files to <code>.mm</code> files and you're using Objective-C++. You can now include C++ headers and start playing immediately.</p>
<p>Now, if you really want to use Metal-cpp I don't want to discourage you at all. It's great to see developers interested. I think it's worth noting, though, that if you want to go the 'pure' C++ route, you'll likely need to end up wrapping some of our other platform libraries which are exposed as Obj-C or Swift. 
Ahhhh… I’m seeing the boundary lines a little more clearly now. This makes a lot of sense. It seems like the learning path would be C++ -&gt; [metal-cpp + interop wrapping], which isn’t too bad at all.</p>
<p>I have a really, really limited scope that I’d like to try out first. Like, really limited: just rendering a plain ol’ plane with a bunch of backed textures that act like a text font, and using the geometry bounds and coordinates to make the final mesh of an object animated.</p>
<p>Backing up a bit, with more context, I currently have a working prototype in straight SceneKit land where my flow is essentially render a bunch of sibling nodes, flatten the hierarchy into a single mesh, and then keep the old nodes around when I want to do things like animate highlights and movement.</p>
<p>My hope is that by picking up even some core metal-cpp, I’ll be able to reach into the rendering fundamentals to get that basic use case down, with the bonus being more power under the hood for more complicated use cases down the road :wink: 
You don't necessarily need to adopt metal-cpp. You could continue interacting with Metal via its native Objective-C interface, but use C++ to integrate with other libraries from the same Objective-C++ source file. 
Here’s a quick shot: the one of the left is in ‘flatten’ mode, with a single node that was flattened from a few hundred. The one on the right is in ‘glyph’ mode, where each node is rendered separately. 
&lt;@U03J3UW6LSD&gt; That’s a really good point - I <em>could</em> use the Obj-C interface! Since that’s the core, it almost seems like it makes <em>sense</em> to start with that first since the cpp lib is a tight wrapper around it. 
I think that would also make learning Metal a little easier. We do have some metal-cpp examples, but there are years of ObjC Metal resources out there. Maybe start with ObjC Metal, then move to Metal-cpp if you want that 'all one language' feeling. 
I think I’ll do just that. I really appreciate the words of wisdom here - I was all ready to start down a quite intensive path, haha! 
If I may while I have an eye or two from ya, and just off the top of your head, no pressure: what sorts of primitives do you think I should be looking into to replicate this kind of behavior like the one above?</p>
<p>In terms of triangles, meshes, texture coordinates, animating mesh coordinates, that kinda thing. 
I’m biased because I’ve spent years working with Apple’s text rendering engine, but: I recommend using an actual text engine like Core Text or TextKit 2 to render to a texture. Lots of simplifying assumptions that hold true in English don't hold true across all languages.</p>
<p>Another really interesting option is <a href="http://sluglibrary.com|Slug">http://sluglibrary.com|Slug</a>, which does correct Unicode rendering entirely on the GPU. But I don’t know if there’s a trial option. 
I would recommend you to take a look at the LearnMetalCPP samples(<a href="https://developer.apple.com/metal/LearnMetalCPP.zip">https://developer.apple.com/metal/LearnMetalCPP.zip</a>). It’s written in metal-cpp but it provides a series of incremental graphics samples which can help you learn graphics and Metal from scratch, e.g draw a triangle, cube, texture etc. 
&gt;  Core Text or TextKit 2 to render to a texture
&lt;@U03J3UW6LSD&gt; This 1_000x over! I would absolutely love to do something like this.. at the moment, I quite literally just create a <code>CATextLayer</code> with a single string character and render it out into a bitmap :sweat_smile: I’ll start taking a look at those two kits to see what I can break, haha. I’d need to render the text, and then understand the placement of those glyphs as they’re rendered to make sure I can move them individually in space.</p>
<p>&lt;@U03HJ4DKQRY&gt; You got it - this was the original reason I had the thought to try it, in that it seemed like it had that gradual build up of domain knowledge, through the lens of a different language. This might be a weekend project, haha. </p>
<hr />
<blockquote>
<h4 id="question-re-core-media-io-extension-how-the-host-app-can-communicate-with-the-extension-can-hostextension-exchange-iosurface-for-example">Question re Core Media IO Extension. How the host app can communicate with the extension? Can host/extension exchange IOSurface for example?</h4>
</blockquote>
<p>What kind of information are you hoping to communicate? There might be a better lounge for this question. 
If you're trying to send video frames, the supported way to do that is <code>CMIOExtensionStream</code>. </p>
<hr />
<blockquote>
<h4 id="just-want-to-say-huuuge-thank-you-to-all-xcode-gpu-devteam-you-guys-rock-i-was-testing-first-metal-gpu-capture-and-since-that-time-its-miraculous-improvements-still-pixel-history-please-make-it-happen-and-if-this-is-possible-can-apple-team-work-at-least-a-bit-with-renderdoc-author-so-we-can-replay-dx1112-traces-on-mac-its-a-very-big-taskvery-complex-but-its-super-big-pain-to-need-a-windows-with-renderdoc-to-check-stuff-if-only-renderdoc-can-run-on-mac-of-course-id-be-happy-if-it-can-replay-metal-as-well-but-its-a-different-story">Just want to say HUUUGE thank you to all XCode GPU dev.team. You guys rock. I was testing first metal gpu capture and since that time it's miraculous improvements.  Still ;)  Pixel history - please make it happen And if this is possible can Apple team work at least a bit with RenderDoc author so we can replay DX11/12 traces on Mac? It's a very big task/very complex but it's super big pain to need a Windows with RenderDoc to check stuff. If only RenderDoc can run on Mac. Of course I'd be happy if it can replay Metal as well, but it's a different story!</h4>
</blockquote>
<p>Hi - thanks for feedback. We'll definitely chat with the tools team about your ideas. 
I'm porting a game that happens to work when in a Windows virtual machine, so on my Mac Pro I keep Direct3D tooling/tracing in a VM running side by side with the Metal debugger. Really missing that workflow on Apple Silicon, but I also know there are no easy answers there. :slightly_frowning_face: Considering just Boot Camping my Mac Pro and keeping it as a remote Windows debug box when the time comes. </p>
<hr />
<blockquote>
<h4 id="is-there-any-documentation-for-the-new-metal-pipeline-script-json-format-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session">Is there any documentation for the new Metal Pipeline Script JSON format?   Mentioned in the "Target and optimize GPU binaries with Metal 3" session.</h4>
</blockquote>
<p>Official documentation is coming soon! In the meantime here is a quick overview of creating a pre-compiled MTLBinaryArchive:</p>
<p>First create a MTLBinaryArchive using the existing API. After that is done you can extract the Metal Pipeline JSON with:
<code>$ metal-source -flatbuffers=json theBinaryArchive.metallib -o /tmp/descriptors.mtlp-json</code></p>
<p>Then you can use the mtlp-json file with <code>metal</code> to generate a MTLBinaryArchive offline that will work with all GPUs:
<code>$ metal shaders.metal -N descriptors.mtlp-json -o archive.metallib</code> 
Here is a link to the talk for others who may be interested in more: <a href="https://developer.apple.com/videos/play/wwdc2022/10102/">https://developer.apple.com/videos/play/wwdc2022/10102/</a> 
I tried to extract the Metal Pipeline JSON from a harvested archive, but got <code>metal-source: error: unsupported binary format</code> 
I harvested the archive using (swift)...</p>
<pre><code class="language-swift">let lib = device.makeDefaultLibrary()!
let desc = MTLRenderPipelineDescriptor()
desc.vertexFunction = lib.makeFunction(name: &quot;vert_main&quot;)
desc.fragmentFunction = lib.makeFunction(name: &quot;frag_main&quot;)
desc.colorAttachments[0]?.pixelFormat = .bgra8Unorm

let archdesc = MTLBinaryArchiveDescriptor()

let archive = try device.makeBinaryArchive(descriptor: archdesc)
try archive.addRenderPipelineFunctions(descriptor: desc)
try archive.serialize(to: NSURL.fileURL(withPath: &quot;/Users/pwong/Downloads/x-game.metallib&quot;))
</code></pre>
<p>Not sure if this is suppose to work yet on MacOS 13 beta/XCode 14 beta... 
also, I see that the workflow doesn't work at all without having a Metal pipeline JSON (applegpu-nt: note: [AGX] Plugin interface not implemented: AIRNTEmitExecutableImage error) </p>
<hr />
<blockquote>
<h4 id="is-there-a-way-to-opt-out-of-qos-to-avoid-priority-decay-on-the-render-thread-if-we-are-writing-our-application-in-swift">Is there a way to opt out of QoS to avoid priority decay on the render thread if we are writing our application in Swift?</h4>
</blockquote>
<p>The techniques we recommend in our tech talk on <a href="https://developer.apple.com/videos/play/tech-talks/110147/|tuning CPU job scheduling for Apple silicon games">https://developer.apple.com/videos/play/tech-talks/110147/|tuning CPU job scheduling for Apple silicon games</a> are also applicable to games written in Swift. The pthread APIs might not be the nicest to work with from Swift, but it should be possible to use them. </p>
<hr />
<blockquote>
<h4 id="rendering-the-mandelbrot-set-is-an-embarrassingly-parallel-smile-task-so-it-seems-perfect-for-the-gpu-i-wrote-a-metal-fragment-shader-to-do-this-but-you-cant-zoom-into-the-set-too-far-because-metal-only-supports-float-not-double-i-am-a-novice-gpu-programmer-so-bear-with-me-but-is-there-any-way-to-do-increased-precision-math-on-the-gpu-with-metal-i-understand-that-gpus-are-generally-much-slower-with-double-precision-if-they-even-support-it-but-it-seems-like-it-would-be-useful-for-scientific-computing-at-least-thank-you">Rendering the Mandelbrot set is an "embarrassingly parallel" :smile: task, so it seems perfect for the GPU. I wrote a Metal fragment shader to do this, but you can't zoom into the set too far because Metal only supports <code>float</code>, not <code>double</code>. I am a novice GPU programmer, so bear with me, but is there any way to do increased precision math on the GPU with Metal? I understand that GPUs are generally much slower with double-precision, if they even support it, but it seems like it would be useful for scientific computing at least. Thank you!</h4>
</blockquote>
<p>Hi, thanks for your question. Since Metal doesn't support the <code>double</code> data type, you would have to use (or make) an arbitrary precision floating point library. Another option is to use 64-bit integers using fixed-point approaches. We would also encourage you to send a report with your use case using Feedback Assistant so the team can consider it as a future enhancement. 
Thanks for those suggestions, I will explore them! 
Also, I want to add to my answer that you could also experiment with the "fast-math" option that is normally turned on by default. If you disable it, you may get a little better precision in your results when calling sin, cos, sqrt, etc. It's not the same thing as a 64-bit floating point type of course, but handy if you're debugging a precision issue. 
Thanks &lt;@U03J7T89SQG&gt; , I didn't know about that option! </p>
<hr />
<blockquote>
<h4 id="is-there-a-way-to-reference-templated-compute-functions-from-a-compute-pipeline-descriptor-such-that-the-templated-type-inherits-the-bound-textures-pixel-format">Is there a way to reference templated compute functions from a compute pipeline descriptor such that the templated type inherits the bound texture's pixel format?</h4>
</blockquote>
<p>Hi, this sounds like an interesting question. For example sake (using non-real code), I think you are trying to ask if you can do something like</p>
<pre><code>template &amp;lt;typename T&amp;gt;
void computeShader(...) { ... }
</code></pre>
<p>in your shader code and then reference this function using something like</p>
<pre><code>id &amp;lt;MTLFunction&amp;gt; function = [defaultLibrary newFunctionWithName:@&quot;computeShader&amp;lt;MyType&amp;gt;&quot;];
</code></pre>
<p>Is that what you are asking? </p>
<p>You can use the <code>host_name</code> attribute and then define your template specializations. In the <a href="https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf|Metal Shading Language Specification">https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf|Metal Shading Language Specification</a>, section 5.1.10 and 5.1.11 show how you can change the name that Metal will use to reference the function name. 
For example, you could try something like:</p>
<pre><code>template &amp;lt;typename MyType&amp;gt;
kernel void computeShader(device MyType* output, constant MyType &amp;amp;argument)
{
  *output = argument;
}

template [[host_name(&quot;computeShader_int&quot;)]]
kernel void computeShader&amp;lt;int&amp;gt;(device int* output, constant int &amp;amp;argument);

template [[host_name(&quot;computeShader_float&quot;)]]
kernel void computeShader&amp;lt;float&amp;gt;(device float* output, constant float &amp;amp;argument);
</code></pre>
<p>And then reference the functions in your app with:</p>
<pre><code>id &amp;lt;MTLFunction&amp;gt; function1 = [defaultLibrary newFunctionWithName:@&quot;computeShader_int&quot;];
id &amp;lt;MTLFunction&amp;gt; function2 = [defaultLibrary newFunctionWithName:@&quot;computeShader_float&quot;];
</code></pre>
<p>&lt;@U03J7T89SQG&gt; Yes, your example illustrates what I was looking for. So sounds like I'd still need to create a compute pipeline state per pixel format? 
For context, I was hoping to build my compute pipeline state once up front and allow a consumer of an MPS-like framework provide textures with various pixel formats. 
Yes, you would need to create a pipeline state for each one. 
Something like:</p>
<pre><code>template &amp;lt;typename T&amp;gt;
void computeShader(texture2d&amp;lt;T&amp;gt; myTexture [[texture(0)]]) { ... }
</code></pre>
<p>where T is inferred from <code>texture(0)</code> 
And the reason for that is so the compiler can create an optimized version for each permutation you have. 
Yeah, makes sense. Would be cool if a compute pipeline state could be build with a set of template parameter permutations and then it selects the correct one based on bindings 
In fact, the term "shader permutations" refers to this specialization whether you're using templates, or function constants. 
If the pipeline is built too late though, it could trigger the compiler and that can result in performance problems. So, if you had a choice to compile your shaders before your app begins, versus on-demand, it's preferable to do it before. 
I see, so without template specialization in the shader source, the pipeline would have to compile versions for each of the permutations set on the pipeline state 
For the use case I outlined above, would you recommend lazily constructing the necessary pipeline state objects or building all possible versions once up front? 
If you know that you will need to use all possible ones, then you should do it once up front. Consider a real-time graphics application with many kinds of materials. If you were to compile the pipeline state objects when they were needed, then you may get stutters while you wait for them to compile. 
Great, thanks for your responses, very informative! </p>
<hr />
<blockquote>
<h4 id="is-generating-the-new-json-pipelines-script-suppose-to-work-on-macos-13-beta-and-xcode-14-beta-mentioned-in-the-target-and-optimize-gpu-binaries-with-metal-3-session-trying-the-sample-terminal-commands-in-the-session-on-a-harvested-metallib-super-simply-draw-a-point-with-a-vertexfragment-shader-errored-out-with-metal-source-error-unsupported-binary-format">Is generating the new JSON Pipelines Script suppose to work on MacOS 13 Beta and XCode 14 Beta? Mentioned in the "Target and optimize GPU binaries with Metal 3" session.  Trying the sample terminal commands in the session, on a harvested metallib (super simply draw a point, with a vertex/fragment shader), errored out with <code>metal-source: error: unsupported binary format</code>.</h4>
</blockquote>
<p>Thanks for the feedback! The team is looking into this issue. </p>
<hr />
<blockquote>
<h4 id="is-there-a-possibility-to-render-3d-objects-with-scenekit-on-the-new-map-eg-a-car-driving-down-the-road">Is there a possibility to render 3D objects with SceneKit on the new Map? e.g. a Car driving down the road...</h4>
</blockquote>
<p>Unfortunately this is not possible to do, but please file Feedback on this request. It seems like a great idea with lots of possibilities! </p>
<hr />
<blockquote>
<h4 id="the-new-objectmesh-shader-pipelines-are-pure-genius-when-will-we-get-a-chance-to-see-an-updated-metal-shading-language-spec-so-we-can-dig-into-the-details">The new object/mesh shader pipelines are pure genius. When will we get a chance to see an updated Metal Shading Language Spec so we can dig into the details?</h4>
</blockquote>
<p>The spec was updated when we announced Metal 3! You can find it in the usual place <a href="https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf">https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf</a> 
Yay!!! 
Do you have any interesting plans &lt;@U03JK18HNR2&gt;? We had some interesting discussions about terrain rendering earlier this week (you should be able to find it if you scroll all the way back up to the Wednesday session :stuck_out_tongue: ) 
I will definitely check that out! I don't have any specific ideas just yet. Still digesting everything, but I was properly blown away by the potential. This + MetalFX is an absolute game changer.  </p>
<hr />
<blockquote>
<h4 id="in-the-new-object-and-mesh-shader-stage-it-looks-like-culling-techniques-can-be-refined-greatly-but-i-didnt-quite-grasp-the-meshlet-concept-discussed-on-the-presentation-is-there-some-external-theory-on-this-you-could-point-us-to-for-more-on-the-concept-or-is-this-something-that-we-will-be-able-to-do-in-the-new-pipeline-stage-ie-carve-up-a-mesh-into-smaller-chunks-on-the-fly">In the new object and mesh shader stage, it looks like culling techniques can be refined greatly. But I didn’t quite grasp the meshlet concept discussed on the presentation. Is there some external theory on this you could point us to for more on the concept? Or is this something that we will be able to do in the new pipeline stage? (Ie carve up a mesh into smaller chunks on the fly)</h4>
</blockquote>
<p>Good question! The term refers to the general idea of splitting up meshes into smaller pieces in order to do efficient culling. This only really becomes a useful practice when culling is done on GPU (otherwise the CPU overhead would be prohibitive due to increased complexity).</p>
<p>We don't supply an out-of-the-box solution for carving up mesh resources, and we leave that up to the developer. Personally I had pretty good (and quick) results using K-means clustering, but there are existing mesh tools out there that can help you prepare your meshlets. :) 
<code>meshlet culling</code> is a good search term to find out more from online resources :slightly_smiling_face: 
&lt;@U03JK18HNR2&gt; <code>meshoptimizer</code> has a set of functions to build meshlets: <a href="https://github.com/zeux/meshoptimizer#mesh-shading">https://github.com/zeux/meshoptimizer#mesh-shading</a> 
:raised_hands:<br />
One key observation is that meshlets have fixed upper bound on number of vertices and number of primitives, so they can be efficiently mapped to threadgroups of the same size(s); so chopping up the (potentially much larger) input meshes to fit into (multiple) meshlets has to be done up front (ideally at build-time of the assets) 
The library author also recorded a series of live streams on the topic, including GPU meshlet and view frustum culling: <a href="https://www.youtube.com/watch?v=KckRq7Rm3Mw">https://www.youtube.com/watch?v=KckRq7Rm3Mw</a> 
Ha I was about to mention SG2015 :smile: Lots of interesting things, including using meshlets and GPU driven rendering for efficient shadows etc. 
One of my favorite talks! Working through implementations at the moment, which is why I'm very happy mesh shaders arrived! 
As for the second question, whether you can carve up meshes inside the pipeline, I don't see a reason why not. Mesh shader pipeline is a good fit for techniques that can be expressed as one or more mesh shader threadgroups (meaning, each such threadgroup maps 1:1 to a fixed size output mesh you declare in the shader), and you have the object shader stage to dynamically decide how many of those output meshes you will need.</p>
<p>There is no particular input formats or intermediate data formats that the pipeline is tied to, so if your input is meshes in device memory, that could work. 
&lt;@U03HJ3X8V43&gt; &lt;@U03JLQ9J0LB&gt; &lt;@U03HJ54DBT4&gt;  great resources! Thank you :) </p>
<hr />
<blockquote>
<h4 id="why-apple-choose-usdz-instead-of-gltf">Why apple choose USDZ instead of GLTF?</h4>
</blockquote>
<p>While USDz and GLTF are both excellent content delivery formats, USDz has the advantage in that it is a direct implementation of the USD format. A USDz file is a zip archive which contains a USD or USDc file along with all of its referenced resources such as textures, animations and shaders. 
This makes it very convenient for using with DCC packages that natively support USD, since there is no conversion or transcoding required. 
I think glTF is a great format, but it is not nearly as flexible as usd formats. USD is also widely supported on all sorts of Apple frameworks so you can save a lot of dev time using that as a spec for your file format. Model I/O, RealityKit etc. In my experience, the biggest pain point for USD lies in the tools the artists want to use. A lot of Blender enthusiasts for example are still lacking a fully featured USD export with skinned animation. The best you can do without a custom exporter is send the file to glTF and then use RealityConverter to get it to USD. But that isn't ideal for all use cases.<br />
Also another great thing about usd is the ability to convert to usda to get something human readable. That's saved me a lot of time pinpointing weird issues with geometry.<br />
Thanks guys! </p>
<hr />
<blockquote>
<h4 id="is-there-sample-code-which-uses-a-mtlsharedevent-im-looking-to-test-one-of-my-compute-pipelines-which-uses-a-shared-event-against-a-sample-pipeline-which-uses-the-shared-event-correctly-as-im-running-into-unexpected-behavior">Is there sample code which uses a <code>MTLSharedEvent</code>? I'm looking to test one of my compute pipelines which uses a shared event against a sample pipeline which uses the shared event correctly as I'm running into unexpected behavior</h4>
</blockquote>
<p>We have sample code here:
<a href="https://developer.apple.com/documentation/metal/memory_heaps/implementing_a_multistage_image_filter_using_heaps_and_events?language=objc">https://developer.apple.com/documentation/metal/memory_heaps/implementing_a_multistage_image_filter_using_heaps_and_events?language=objc</a></p>
<p>We also mention shared event specifically here:
<a href="https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_between_a_gpu_and_the_cpu?language=objc">https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_between_a_gpu_and_the_cpu?language=objc</a>
<a href="https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_across_multiple_devices_or_processes?language=objc">https://developer.apple.com/documentation/metal/resource_synchronization/synchronizing_events_across_multiple_devices_or_processes?language=objc</a> </p>
<hr />
<blockquote>
<h4 id="if-i-render-a-scene-to-a-mtltexture-with-large-resolution-can-i-split-scene-to-a-small-pieces-and-async-render-them-or-i-need-to-render-a-whole-scene-al-once">If I render a scene to a MTLTexture with large resolution can I split scene to a small pieces and async render them or I need to render a whole scene al once?</h4>
</blockquote>
<p>Very interesting question :slightly_smiling_face: There are a lot of interesting things you can do to do this somewhat efficiently 
The most basic way would just be to render the scene in multiple parts, each to a different render target (or viewport within a target). This would probably require you to first properly sort geometry to different draws 
One other way to do this, would be to use Vertex Amplification and Layered Rendering to emit vertices to multiple render targets at once (for all the vertices that are overlapping multiple viewports) 
Unless your render target is bigger than the allowed size though, you should just render everything at once where possible :slightly_smiling_face: The more you render at once, the more you gain efficiency. 
I'm curious, what is the reason you would need such a huge resolution? :slightly_smiling_face: (I believe our maximum texture size is 16x16k) 
The 16k is perfect to my case :grinning:. Last year I face the problem with render to a texture. I don’t remember correct resolution, but if I set the resolution to 4k the test app is crashing. Other resolution (smaller or bigger) work perfect. 
Is there any way to get current max texture resolution directly in the app in runtime? 
&lt;@U03HJ54DBT4&gt; I work on sort of Standalone Render app and I need to figure out limits that I should set for export resolution, that the user cannot try to render image with bigger resolution. 
I don't believe so, but you can look at this table:
<a href="https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf">https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf</a>
It appears that anything newer than Apple2 will support 16K (iPhone7+) 
&lt;@U03HJ54DBT4&gt; Thank you so much! </p>
<hr />
<blockquote>
<h4 id="how-would-you-break-down-a-scene-into-acceleration-structures-im-working-on-a-small-raytracing-side-project-and-im-a-bit-stuck-determining-how-a-scene-should-be-broken-down-im-using-model-io-to-load-some-assets-and-then-im-planning-to-represent-the-scene-with-an-instance-acceleration-structure-with-each-mdlmesh-mapping-to-a-primitive-acceleration-structure-does-this-sound-like-an-appropriate-breakdown">How would you break down a scene into acceleration structures? I'm working on a small raytracing side project and I'm a bit stuck determining how a scene should be broken down. I'm using Model I/O to load some assets and then I'm planning to represent the scene with an instance acceleration structure, with each <code>MDLMesh</code> mapping to a primitive acceleration structure. Does this sound like an appropriate breakdown?</h4>
</blockquote>
<p>Thanks for the question. Yes, this does sound like a proper mapping. You could fill out a <code>MTLAccelerationStructureTriangleGeometryDescriptor</code> for each submesh in an <code>MDLMesh</code> and then put those into <code>geometryDescriptors</code> in a <code>MTLPrimitiveAccelerationStructureDescriptor</code> 
Yeah that's pretty close to what I was planning actually 
then you would use the transformations for each mesh and put together a transformation buffer for <code>MTLInstanceAccelerationStructureDescriptor</code> </p>
<hr />
<blockquote>
<h4 id="hello-are-there-any-plans-of-supporting-page-faulting-in-metal-for-the-future-the-purpose-is-to-have-an-11-cpugpu-va-mapping-with-even-file-mmap-being-accessible-from-the-gpu-side-to-be-able-to-layer-more-high-level-programming-models-on-top">Hello, are there any plans of supporting page faulting in Metal for the future?   The purpose is to have an 1:1 CPU:GPU VA mapping, with even file mmap() being accessible from the GPU side, to be able to layer more high-level programming models on top.</h4>
</blockquote>
<p>Hi Mohamed, we do not comment on the future of the API, but I would like to direct your attention to two features that may be sufficiently flexible for your use-case, Metal Fast Resource Loading to load data from files and Metal Sparse Textures to manage partial residency with page map and unmap support. We are also interested in learning more about your use-case, so please feel free to expand on it :slightly_smiling_face: Thanks 
The use case is porting HPC code from other platforms which rely on heterogenous memory management more extensively today, including the regular host heaps being accessible from the GPU. (eg. C++ standard parallelism on GPUs as deployed by some vendors) 
Wow that is indeed an interesting use-case, and to make it work with Metal 3 you would need some sort of shim to hide the memory management details. Please feel free to file a feedback request for us to track this. 
There's support for file mmap being made accessible to the GPU using <a href="https://developer.apple.com/documentation/metal/mtldevice/1433382-newbufferwithbytesnocopy?language=objc">https://developer.apple.com/documentation/metal/mtldevice/1433382-newbufferwithbytesnocopy?language=objc</a>; however it causes the VM region to be paged in entirely (which may not be what you want to achieve here, depending on the size of the file) 
&gt;  depending on the size of the file
yeah pinned memory is useful - but is not what I'm after in this scenario :slightly_smiling_face: 
FB10133608 </p>
<hr />
<blockquote>
<h4 id="will-gpu-shader-debugging-for-mesh-shaders-be-ready-for-the-xcode-14-release-i-tried-debugging-the-sample-code-adjusting-the-level-of-detail-using-metal-mesh-shaders-httpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shadershttpsdeveloperapplecomdocumentationmetalmetal_sample_code_libraryadjusting_the_level_of_detail_using_metal_mesh_shaders-and-2-observations-1-there-didnt-seem-to-be-a-way-to-select-object-or-mesh-shader-to-debug-2-attempting-to-debug-the-fragment-shader-hung-xcode">Will GPU Shader debugging for Mesh Shaders be ready for the Xcode 14 release?  I tried debugging the sample code "Adjusting the level of detail using Metal mesh shaders" (<a href="https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)|https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)">https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)|https://developer.apple.com/documentation/metal/metal_sample_code_library/adjusting_the_level_of_detail_using_metal_mesh_shaders)</a> and 2 observations: 1. There didn't seem to be a way to select object or mesh shader to debug. 2. Attempting to debug the fragment shader, hung XCode.</h4>
</blockquote>
<p>Many new and exciting features are always being planned to enable a better development experience for Metal. In the meantime if you encounter any issues we would highly encourage you to use the Feedback Assistant. 
Additionally, feel free to provide the case number here if you have filed feedback already. Thank you! </p>
<hr />
<blockquote>
<h4 id="hey-really-love-all-the-innovative-metal-3-features-i-may-be-a-little-early-asking-but-i-was-curious-about-whats-happening-with-mtlargumentencoders-are-they-being-deprecated-across-the-board-in-metal-3-such-that-we-can-take-advantage-of-the-streamlined-api-for-most-of-our-metal-2-code-it-also-looks-like-some-but-not-all-of-the-methods-are-deprecated-for-example-the-versions-that-can-set-multiple-buffers-at-once-in-a-range-is-that-correct">Hey - Really love all the innovative Metal 3 features! I may be a little early asking, but I was curious about what’s happening with MTLArgumentEncoders. Are they being deprecated across the board in Metal 3 such that we can take advantage of the streamlined API for most of our Metal 2 code? It also looks like some, but not all, of the methods are deprecated. For example, the versions that can set multiple buffers at once in a range. Is that correct?</h4>
</blockquote>
<p>Hi Louis, thanks and glad you are enjoying Metal 3!</p>
<p>MTLArgumentEncoders are indeed deprecated, however if you are targeting Tier 1 devices on Metal 2 you will still have to use them and disable the deprecation warnings, alternatively you could target only Tier 2 devices by performing a Metal 3 feature check.</p>
<p>We'd also really appreciate it if you could file feedback for this use-case. </p>
<hr />
<blockquote>
<h4 id="are-there-any-cases-wed-need-to-interact-with-mtlgpuhandle-and-mtlresourceid-properties-i-see-them-added-in-a-bunch-of-places-but-unsure-where-or-by-whom-theyd-be-consumed">Are there any cases we'd need to interact with MTLGPUHandle and MTLResourceID properties? I see them added in a bunch of places, but unsure where or by whom they'd be consumed.</h4>
</blockquote>
<p><code>MTLGPUHandle</code> is deprecated and we use <code>MTLResourceID</code> to access the underlying resource index of a resource. It's similar to the <code>gpuAddress</code> of a buffer.</p>
<p>We use them in the bindless workflow for argument buffers. Instead of using the argument encoder's <code>set[ResourceType]</code>  functionality, we can simply cast the contents of the argument buffer to the struct type, and map the resources directly.</p>
<p>Please refer to <a href="https://developer.apple.com/wwdc22/10101">https://developer.apple.com/wwdc22/10101</a> for more usage examples. </p>
<hr />
<blockquote>
<h4 id="is-there-a-way-to-have-the-equivalent-of-work_group_barrier-clk_global_mem_fence-memory_scope_device-in-metal-all-threads-on-a-device-barriers-not-threadgroup-local-also-a-part-of-the-vulkan-memory-model">Is there a way to have the equivalent of work_group_barrier (CLK_GLOBAL_MEM_FENCE, memory_scope_device) in Metal?   (all threads on a device barriers, <em>not</em> threadgroup local, also a part of the Vulkan memory model)</h4>
</blockquote>
<p>Hello,
To barrier all threads within a threadgroup (equivalent to workgroup) properly ordering memory operations to device memory use <code>threadgroup_barrier(mem_device)</code> . If you want to barrier against all threads spawned, the proper way to achieve it in Metal is to split your operation into 2 sequentially dependent compute dispatches.
Thanks. 
ok... that'll pose some intriguing work for me on the compiler side to make that work out :slightly_smiling_face: 
I'd use <code>computeCommandEncoderWithDispatchType</code>  with the sequential dispatch type in that scenario right? 
are you talking about CLK_GLOBAL_MEM_FENCE or a scenario where one implements an actual across-grid synchronization primitive? 
there's indeed a cross-grid synchronisation primitive involved, and memory_scope_device is used to guarantee that 
My interpretation of the Vulkan spec for <code>work_group_barrier</code> is that the actual synchronization happens at workgroup boundary — i.e.: only between threads in the same workgroup (or threadgroup in Metal parlance). The memory fence behavior is kinda orthogonal and, in the case of <code>CLK_GLOBAL_MEM_FENCE</code> you are making sure that before any thread in a workgroup/threadgroup can proceed all in-flight device memory operations are visibile to the thread itself. That does not mean that a given thread will wait until all the other threads in the grid will reach the barrier to continue though 
See the SPV_KHR_vulkan_memory_model extension - more specifically the VulkanMemoryModelDeviceScopeKHR capability 
compute is my primary target instead of Vulkan though :slightly_smiling_face: 
Sorry, not really a Vulkan expert. In Metal unfortunately we do not currently support a shader-controller mechanism to barrier across all threads in a grid. In order to do that you would beed to split your operation into 2 sequential compute dispatches as mentioned by Kelvin. 
is it a known/already considered issue? do you mind if I file a radar about it? 
I’m not aware of this particular feature request, but that does not mean it has not been discussed so far. If Vulkan offers you this functionality and you think it would be useful to have it in Metal as well, you can definitely file a radar for it, it will be screened by the GPU SW team. Thanks! 
FB10166471 
Thanks! </p>
<hr />
<blockquote>
<h4 id="ive-never-used-mtlrenderpipelinereflection-before-what-sort-of-use-cases-is-it-meant-for-could-it-be-used-to-replace-shared-sourceshader-constants-defining-binding-indices">I've never used MTLRenderPipelineReflection before. What sort of use cases is it meant for? Could it be used to replace shared source/shader constants defining binding indices?</h4>
</blockquote>
<p>MTLRenderPipelineReflection enables your application to reflect on shader bindings and allows you to perform data-driven tasks. For an example, you have a game engine and can pull out binding information to determine what resource needs bound where. 
This is super cool. I've historical just had a bunch of macros defining the binding indices. Will explore this approach. Thanks! 
It's also very handy if you have a lot of shaders you are constantly editing. By using reflection, your engine can intelligently bind the right data to the right slot. Less worry, less bugs! 
Sounds so nice! </p>
<hr />
<blockquote>
<h4 id="heya-quick-shared-post-from-the-study-hall-section-thank-you-much-i-think-i-have-a-lot-to-learn-with-metal-now-ill-likely-start-with-basic-triangle-patterns-and-moving-those-around-with-a-compute-kernel-which-sounds-so-darn-fun-and-then-start-upgrading-to-the-mesh-shader-feature-now-that-ive-said-that-i-wonder-if-its-a-better-idea-to-maybe-start-with-mesh-shaders-ie-starting-from-the-new-tool-instead-of-building-up-to-it-the-context-being-im-brand-new-to-metal-and-am-looking-to-do-some-fairly-simple-direct-interactions-with-geometry-positions-and-eventually-texture-manipulations-for-things-like-highlighting-areas">Heya! Quick shared post from the study-hall section, thank you much!  I think I have a lot to learn with Metal now. I’ll likely start with basic triangle patterns and moving those around with a compute kernel (which sounds so darn fun), and then start ‘upgrading’ to the mesh shader feature.   Now that I’ve said that, I wonder if it’s a better idea to maybe start with mesh shaders? I.e., starting from the new-tool instead of building up to it? The context being I'm brand new to Metal, and am looking to do some fairly simple direct interactions with geometry positions, and eventually texture manipulations for things like highlighting areas.</h4>
</blockquote>
<p>Both would work! While mesh shaders would be more efficient in most cases, if you are just starting out with compute I would suggest starting with a basic Compute-&gt;Vertex-&gt;Fragment pipeline, where your compute shader simply writes out the vertex buffer (and optionally index buffer). You can read back the written vertices and inspect the data. I feel that would be a great way to slowly build up to mesh shaders :slightly_smiling_face: 
If you have handle on it, you can further optimize your work with mesh shaders (this requires a bit more planning on the side of thread group sizes, mesh sizes etc.) 
This is excellent, thank you so much for the info. My original concern of going down an inconvenient path is now assuaged, haha. It seems like I’ll get a lot of bang-for-my-buck by writing up my kernel, getting to know the API, and then “discovering” where the new tools fit in to how I’ve tried to build things. At that point, even if I <em>have</em> built something that doesn’t quite fit the pattern, I have that core understanding built up to diagnose and refactor as needed. 
It’ll be quite interesting to start transitioning everything from basic SceneKit nodes and geometries to custom Metal ones… I feel like I may finally have the right path to get to the right tools for performance. 
If you happen to have any kinda favorite notes, pages, articles, or docs on some potential equivalences or built in support for SceneKit -&gt; Metal or vice versa, I would really appreciate that as well! 
Apologies I somehow didn't see this pop up! I'm not super familiar with SceneKit, but we do have interop between them. I would definitely suggest just playing around with Metal to get the hang of it. We had a similar question on Wednesday, where someone wanted to optimize their SceneKit scene by manually constructing complex meshes from scratch instead of relying on huge node graphs. I'll try to find it for you 
Haha, no worries at all - quite literally thousands of fellow Apple-heads looking to chat, I’m just happy to be a part! :smiley:</p>
<p>That. Is. <em>AWESOME!</em> It sounds right on track with that I’m talking about. I think this is going to be a weekend of Metal experimentation.. the more I ask about it from my current place, the more I’m itching to see what I can do with it. 
Be sure to drop by the forums for help/questions :slightly_smiling_face: 
Totally. In the past, I’ve only posted there when I’m in “dire straights” so to speak, haha. No reason I can’t ask simpler questions along the way too :smile: 
&lt;@U03HJ54DBT4&gt; Heya, one more question if you don’t mind :sweat_smile:</p>
<p>Sorry if this was asked <code>1_000_000</code> times already, but is there a significant difference in using <code>Swift</code> for interacting with Metal as opposed to <code>Objective-C</code>? I see lots of write ups that use Swift to interact with it, including shader definitions. 
I would suggest using Swift if possible, since that interfaces much nices with stuff like SwiftUI and other technologies. :slightly_smiling_face: 
Wooooooooo! No square brackets for ME this weekend! :confetti_ball: :man_dancing: :dancer: :dancers: :confetti_ball: </p>
<hr />
<blockquote>
<h4 id="im-trying-to-track-down-a-rare-metal-crash-in-our-app-were-seeing-this-in-two-completely-independent-subsystems-when-deallocating-metal-buffers-invariably-vertex-or-index-buffers">I'm trying to track down a rare Metal crash in our app. We're seeing this in two completely independent subsystems when deallocating Metal buffers, invariably vertex or index buffers.</h4>
</blockquote>
<p>Hello David, could you please provide some further details about the crash? Is it a CPU crash pointing at the Metal library or is it a GPU crash? Thanks 
Posted in the study hall, but the common part of the stack trace is:</p>
<pre><code>Thread 0 Crashed:
0   libobjc.A.dylib                      0x000000019c8b7470 objc_release + 16
1   IOGPU                                0x00000001cf8059d8 -[IOGPUMetalResource dealloc] + 204
2   IOGPU                                0x00000001cf80672c -[IOGPUMetalBuffer dealloc] + 288
3   AGXMetalA10                          0x00000001df28ab28 0x1df26b000 + 129832
</code></pre>
<p>It's rare, so I don't think it's an over-release on our part. </p>
<p>Thanks! Do you have the whole stack trace by chance? </p>
<p>Also, did you try to turn on zombie object detection in Xcode or Instruments so far? </p>
<p>These reports are coming from the production app and we haven't been able to reproduce it. </p>
<p>Sanitizing stack traces, standby </p>
<p>It might still be useful to follow the instructions in this page: <a href="https://developer.apple.com/documentation/xcode/investigating-crashes-for-zombie-objects">https://developer.apple.com/documentation/xcode/investigating-crashes-for-zombie-objects</a>, specifically the linked page <a href="https://help.apple.com/instruments/mac/current/#/dev612e6956">https://help.apple.com/instruments/mac/current/#/dev612e6956</a> 
and I mean, on your development environment </p>
<p>Right, like I said we can't repro it, but running with zombies enabled is a good idea. </p>
<p>Based on the signature of your stack track this is quite likely a zombie object. A full stack trace would be useful to understand where in the driver we issue that release </p>
<p>Here ya go, had to anonymize it:</p>
<pre><code>Thread 0 Crashed:
0   libobjc.A.dylib                      0x000000019c8b7470 objc_release + 16
1   IOGPU                                0x00000001cf8059d8 -[IOGPUMetalResource dealloc] + 204
2   IOGPU                                0x00000001cf80672c -[IOGPUMetalBuffer dealloc] + 288
3   AGXMetalA10                          0x00000001df28ab28 0x1df26b000 + 129832
4   MyApp                                0x00000001059f2234 myapp::profile::MetalVertexBuffer::~MetalVertexBuffer() (&lt;http://MetalRenderer.mm:1516|MetalRenderer.mm:1516&gt;)
5   MyApp                                0x00000001059f225c myapp::profile::MetalVertexBuffer::~MetalVertexBuffer() (&lt;http://MetalRenderer.mm:1515|MetalRenderer.mm:1515&gt;)
6   MyApp                                0x00000001059e8afc myapp::profile::Shape::~Shape() (Shape.cpp:36)
7   MyApp                                0x0000000105803444 std::__1::enable_if&amp;lt;(__is_cpp17_forward_iterator&amp;lt;std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;*&amp;gt;::value) &amp;amp;&amp;amp; (is_constructible&amp;lt;std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;, std::__1::iterator_traits&amp;lt;std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;*&amp;gt;::reference&amp;gt;::value), void&amp;gt;::type std::__1::vector&amp;lt;std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;, std::__1::allocator&amp;lt;std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt; &amp;gt; &amp;gt;::assign&amp;lt;std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;*&amp;gt;(std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;*, std::__1::shared_ptr&amp;lt;myapp::mdk::RenderableTile&amp;gt;*) (memory:2506)
8   MyApp                                0x00000001059d9f18 myapp::profile::TerrainLayer::updateNode(myapp::profile::SceneManager*) (TerrainLayer.cpp:46)
9   MyApp                                0x00000001059dd494 myapp::profile::SceneNode::updateNode(myapp::profile::SceneManager*) (SceneNode.cpp:47)
10  MyApp                                0x00000001059dc538 myapp::profile::ModelController::update(double) (ModelController.cpp:55)
11  MyApp                                0x00000001059f6a38 -[TTMProfileModelController(CPP) update:] (&lt;http://TTMProfileModelController.mm:51|TTMProfileModelController.mm:51&gt;)
12  MyApp                                0x0000000105a00648 -[TTMProfileView renderLoop] (&lt;http://TTMProfileView.mm:335|TTMProfileView.mm:335&gt;)
13  QuartzCore                           0x0000000188fe8378 CA::Display::DisplayLink::dispatch_items(unsigned long long, unsigned long long, unsigned long long) + 756
14  QuartzCore                           0x0000000188feeb44 display_timer_callback(__CFMachPort*, void*, long, void*) + 364
15  CoreFoundation                       0x00000001854c869c __CFMachPortPerform + 168
16  CoreFoundation                       0x00000001855084ec __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__ + 52
17  CoreFoundation                       0x000000018550badc __CFRunLoopDoSource1 + 584
18  CoreFoundation                       0x00000001854c90f0 __CFRunLoopRun + 2372
19  CoreFoundation                       0x00000001854dbe1c CFRunLoopRunSpecific + 568
20  GraphicsServices                     0x00000001a576d9a0 GSEventRunModal + 156
21  UIKitCore                            0x0000000187d0fb90 -[UIApplication _run] + 1076
22  UIKitCore                            0x0000000187aa516c UIApplicationMain + 328
23  MyApp                                0x0000000106f36e1c main (main.m:127)
24  ???                                  0x0000000109580250 0x0 + 0
</code></pre>
<p>And another one (different system altogether)</p>
<pre><code>Thread 0 Crashed:
0   libobjc.A.dylib                      0x00000001dc079ef8 objc_msgSend + 56
1   IOGPU                                0x00000002242b87a0 -[IOGPUMetalResource dealloc] + 208
2   IOGPU                                0x00000002242b9558 -[IOGPUMetalBuffer dealloc] + 316
3   AGXMetalA14                          0x0000000236149964 0x235f5c000 + 2021732
4   MyApp                                0x00000001055b21ac std::__1::__any_imp::_SmallHandler&amp;lt;myapp::render::Buffer::Metal&amp;gt;::__handle(std::__1::__any_imp::_Action, std::__1::any const*, std::__1::any*, std::type_info const*, void const*) (any:0)
5   MyApp                                0x000000010559f808 std::__1::any::reset() (any:321)
6   MyApp                                0x000000010559f66c myapp::render::Buffer::~Buffer() (Buffer.cpp:14)
7   MyApp                                0x00000001055a2768 std::__1::unique_ptr&amp;lt;std::__1::__hash_node&amp;lt;std::__1::__hash_value_type&amp;lt;myapp::Name, myapp::render::VertexBuffer&amp;gt;, void*&amp;gt;, std::__1::__hash_node_destructor&amp;lt;std::__1::allocator&amp;lt;std::__1::__hash_node&amp;lt;std::__1::__hash_value_type&amp;lt;myapp::Name, myapp::render::VertexBuffer&amp;gt;, void*&amp;gt; &amp;gt; &amp;gt; &amp;gt;::~unique_ptr() (Buffer.h:188)
8   MyApp                                0x00000001055a1924 myapp::render::Cache&amp;lt;myapp::render::VertexBuffer&amp;gt;::RemoveBuffer(myapp::Name const&amp;amp;) (__hash_table:2498)
9   MyApp                                0x0000000105668870 TTMKTileMetadata::deleteGLBuffers() (TTMKTileMetadata.cpp:178)
10  MyApp                                0x00000001056d8248 -[TTMKMetalView doInContext:] (&lt;http://TTMKMetalView.mm:46|TTMKMetalView.mm:46&gt;)
11  MyApp                                0x000000010569c694 -[TTMKMapView doInBackgroundContextThread:doImmediately:] (&lt;http://TTMKMapView.mm:2958|TTMKMapView.mm:2958&gt;)
12  MyApp                                0x000000010568ba8c -[TTMKMapContainerDelegate doInBackgroundContextThread:doImmediately:] (&lt;http://TTMKMapContainerDelegate.mm:20|TTMKMapContainerDelegate.mm:20&gt;)
13  MyApp                                0x00000001056176c0 TTMKMapContainer::drainTileCache(bool) (TTMKMapContainer.cpp:3393)
14  MyApp                                0x0000000105613ca4 TTMKMapContainer::update(double, double) (TTMKMapContainer.cpp:1442)
15  MyApp                                0x000000010569b6a4 -[TTMKMapView glDrawOversample] (&lt;http://TTMKMapView.mm:2696|TTMKMapView.mm:2696&gt;)
16  MyApp                                0x000000010569b964 -[TTMKMapView drawWithDevice:toDrawable:pass:] (&lt;http://TTMKMapView.mm:2747|TTMKMapView.mm:2747&gt;)
17  MyApp                                0x00000001056d85a0 -[TTMKMetalView drawRect:] (&lt;http://TTMKMetalView.mm:144|TTMKMetalView.mm:144&gt;)
18  MetalKit                             0x00000002009a36d4 -[MTKView draw] + 140
19  MetalKit                             0x00000002009b1fb8 -[MTKViewDisplayLinkTarget draw] + 36
20  QuartzCore                           0x00000001c70d2324 CA::Display::DisplayLink::dispatch_items(unsigned long long, unsigned long long, unsigned long long) + 744
21  QuartzCore                           0x00000001c722d174 CA::Display::DisplayLink::dispatch_deferred_display_links() + 344
22  UIKitCore                            0x00000001c665f254 __setupUpdateSequence_block_invoke + 212
23  UIKitCore                            0x00000001c5fd9084 _UIUpdateSequenceRun + 80
24  UIKitCore                            0x00000001c665ecb0 schedulerStepScheduledMainSection + 140
25  UIKitCore                            0x00000001c665e478 runloopSourceCallback + 88
26  CoreFoundation                       0x00000001c344bf04 __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 24
27  CoreFoundation                       0x00000001c345cc90 __CFRunLoopDoSource0 + 204
28  CoreFoundation                       0x00000001c3396184 __CFRunLoopDoSources0 + 264
29  CoreFoundation                       0x00000001c339bb4c __CFRunLoopRun + 824
30  CoreFoundation                       0x00000001c33af6b8 CFRunLoopRunSpecific + 596
31  GraphicsServices                     0x00000001df449374 GSEventRunModal + 160
32  UIKitCore                            0x00000001c5d14e88 -[UIApplication _run] + 1096
33  UIKitCore                            0x00000001c5a965ec UIApplicationMain + 360
34  MyApp                                0x0000000106d495fc main (main.m:127)
35  ???                                  0x0000000109679ce4 0x0 + 0
</code></pre>
<p>Note: <code>deleteGLBuffers()</code> is part of the legacy code hierarchy. Rendering path is all Metal. 
Thanks, this definitely looks like a zombie object 
It seems like your app is holding a reference to a MTLBuffer object that has been already released 
Yeah, that's what I thought too, but the code paths here manage Metal buffers independently and it seems unlikely totally different implementations would make the same subtle mistake. Static analysis turned up nothing suspicious. 
Do you use command buffers with unretained references or command buffers with retained references? 
Let me look up the creation code 
Also, do you have a common pool of Metal resources that lives across sub-systems? 
Not vertex or index buffers, at least. Creation happens in Obj-C++ code and looks like:</p>
<pre><code>            auto em = std::any_cast &amp;lt;Environment::Metal&amp;gt; ( &amp;amp;_environment-&amp;gt;_metal );
            id &amp;lt;MTLDevice&amp;gt; device = em-&amp;gt;_device;

            auto mtl = std::any_cast &amp;lt;Metal&amp;gt; ( &amp;amp;_metal );
            mtl-&amp;gt;_buffer = [device newBufferWithLength: sizeInBytes options: MTLResourceCPUCacheModeDefaultCache];
</code></pre>
<p>That should be retained, if I'm not mistatken. 
Sorry, what I meant is: how do you create <code>MTLCommandBuffer</code> objects? 
Do you by chance use <code>MTLCommandQueue:commandBufferWithUnretainedReferences</code> or <code>MTLCommandQueue:commandBufferWithDescriptor</code> setting <code>commandBufferWithUnretainedReferences=YES</code>? 
No, I don't think so. Device and queue are created here:</p>
<pre><code>            _metal.emplace &amp;lt;Metal&amp;gt; ();
            auto mtl = std::any_cast &amp;lt;Metal&amp;gt; ( &amp;amp;_metal );

            bool first = ( mtl-&amp;gt;_device == nullptr );
            id &amp;lt;MTLDevice&amp;gt; mtlDevice = *std::any_cast &amp;lt;id &amp;lt;MTLDevice&amp;gt;&amp;gt; ( &amp;amp;device );

            mtl-&amp;gt;_device = mtlDevice;
            mtl-&amp;gt;_queue = [mtlDevice newCommandQueue];

            if ( first )
            {
                initialize();
                initializeMetalRendering();
            }
</code></pre>
<p>Then the command buffer is created here:</p>
<pre><code>    void Environment::start()
        {
            auto mtl = std::any_cast &amp;lt;Metal&amp;gt; ( &amp;amp;_metal );

            mtl-&amp;gt;_buffer = [mtl-&amp;gt;_queue commandBuffer];
</code></pre>
<p>OK, that’s a command buffer with retained resources then 
Based on the info I have I still think your best bet is to use the zombie detector 
Ok, I'll give that a shot, but we haven't been able to repro it ourselves. If we referred to a dealloc'd object we should at least see a crash locally I would think. 
Do you possibly have <code>MetalVertexBuffer</code> objects that do not contain an actual <code>MTLBuffer</code> object? 
Maybe you’re not overreleasing, maybe you just have some smashed pointer 
or uninitialized data structure? 
Not sure, all good things to check 
When we moved from GL to Metal, all the Metal code was put in a single file. I can send that if you'd like to look it over. 
Not sure we have means for you to safely upload your source code here 
let me ask 
You could use feedback assistant 
Not sure where that is 
<a href="https://developer.apple.com/bug-reporting/">https://developer.apple.com/bug-reporting/</a> </p>
<hr />
<blockquote>
<h4 id="have-there-been-any-changes-to-resource-limits-with-metal-3-specifically-the-maximum-number-of-buffers-inside-an-argument-buffer-thank-you">Have there been any changes to resource limits with Metal 3? Specifically the maximum number of buffers inside an argument buffer. Thank you!</h4>
</blockquote>
<p>There is no limit on the number of buffer references stored in an argument buffer. We also made it easier to populate an argument buffer on the CPU by using the buffers' <code>gpuAddress</code> property; so argument encoders are no longer needed.</p>
<p>Please note though, that you still need to actually allocate an argument buffer large enough to hold all the references, and resource creation (of the argument buffer, or all the other buffers) can fail when you run out of memory; your total resident memory set is still limited, but the argument buffer contents are not.</p>
<p>The same applies to texture references stored in an argument buffer (use the <code>MTLResourceID</code> property instead when populating the buffer on the CPU). </p>
<hr />
<blockquote>
<h4 id="is-there-any-list-where-i-can-find-which-devices-support-metal-3">Is there any list where I can find which devices support Metal 3?</h4>
</blockquote>
<p>• Metal 3 hardware support can be found in <code>Discover Metal 3</code><a href="https://developer.apple.com/videos/play/wwdc2022/10066/">https://developer.apple.com/videos/play/wwdc2022/10066/</a>
• Specific features (Mesh Shaders, Metal FX) may have a different hardware support. For example, Mesh Shaders should be supported on MTLGPUFamilyApple7 and MTLGPUFamilyMac2. Please refer to their associated session for more information. ie, for Mesh Shaders : <a href="https://developer.apple.com/videos/play/wwdc2022/10162/">https://developer.apple.com/videos/play/wwdc2022/10162/</a></p>
<p>&lt;@U03HWP3Q65P&gt; Thanks! 
No problem! Looking forward to seeing what y'all make with Metal 3 :raised_hands: </p>
<hr />
<blockquote>
<h4 id="ive-noticed-that-in-compute-shader-access-to-a-2d-texture-is-faster-than-access-to-a-buffer-due-to-a-lot-more-of-a-cache-missed-how-does-caching-strategy-work-for-textures-vs-buffers-in-compute-can-we-specify-one-as-ive-pretty-good-idea-what-parts-of-the-buffer-will-be-accessed-from-which-thread">I've noticed that in compute shader access to a 2d texture is faster, than access to a buffer. (Due to a lot more of a cache missed). How does caching strategy work for textures vs buffers in compute? Can we specify one (as i've pretty good idea what parts of the buffer will be accessed from which thread)</h4>
</blockquote>
<p>Textures are optimized for localized sampling (so sampling things that are more "together" in 2D space). It definitely makes sense to store "2D data" in a texture. :slightly_smiling_face: Alternatively, you can manually encode 2D data in a "1D buffer" using a more optimized order of data using Morton Ordering (Z-ordering). This will make data that is close in 2D also end up close in "1D".</p>
<p>You are totally free to sample textures in compute, and it will have the same performance characteristics as in a fragment function.</p>
<p><a href="https://en.wikipedia.org/wiki/Z-order_curve">https://en.wikipedia.org/wiki/Z-order_curve</a> 
Does textures use the same hardware as buffers? If requesting from a buffer is different than requesting from a texture, there may be a point in “parallelizing” data access 
Since textures and buffers tend to reside in the same memory, that would just end up being a more divergent access pattern. :slightly_smiling_face: 
If you have 2D localized access pattern, I would suggest using textures, unless your data is not compatible with existing texture formats. 
We also discussed matching the layout of your threadgroups with your buffers in <a href="https://developer.apple.com/videos/play/wwdc2022/10159|Scale compute workloads across Apple GPUs">https://developer.apple.com/videos/play/wwdc2022/10159|Scale compute workloads across Apple GPUs</a> 
I have a 1.5d data. I have really wide, but not very tall texture. ATM i’m using 10-15k by 20 (just twenty, without thousand)</p>
<p>And i’m dispatching 10-15k by 1 threads. ATM i’m limited by max texture width on iOS devices, and would like to explore ideas on how to efficiently scale my kernel) 
Ah I see! In that case, it might be interesting to use a buffer, but store your data interleaved (so in "strips" of 20). 
That would still give you pretty good localized access. And as always it is a good idea to take a critical look at the data size per entry. (use half if possible etc) 
that was my original implementation, and it was 10-20% slower than textures:(</p>
<p>Data size is one possibility I haven’t explored yet, as my algorithms performs poorly on halfs, but I haven’t tried computing in f32, and storing in f16 
Hmmm interesting! Are you sure your texture is the same effective data size? Are you using linear interpolation etc? If you are doing a "linear" access across your compute grid, I would expect a simple interleaved scheme to work pretty well 
Hmm, since it was a year ago, I may used not a linear pattern (as my data can be from 1 to 20 tall, I may have packed it tightly) 
I can check, and if there is major difference in performance, I can file a feedback 
Sounds good! Good luck :slightly_smiling_face: 
Just wanted to add: a colleague of mine reminded me that textures often use a separate cache, so you could definitely see some performance difference between the two. Hope it helps :) </p>
<hr />
<blockquote>
<h4 id="does-screencapturekit-support-pulling-audio-from-different-processes-than-the-video-there-are-some-use-cases-where-audio-comes-from-one-process-but-the-window-and-graphics-come-from-another-process-this-is-common-in-situations-like-crossover-and-wine-this-feature-if-not-supported-would-be-very-useful">Does ScreenCaptureKit support pulling audio from different processes than the video? There are some use cases where audio comes from one process but the window and graphics come from another process, this is common in situations like CrossOver and Wine. This feature if not supported would be very useful.</h4>
</blockquote>
<p>ScreenCaptureKit will get audio associated with application, so if the application’s parent process spawns another process to render audio, SCK should also capture that audio. If this doesn’t work in your case, please file a feedback request :slightly_smiling_face:. 
Thank you very much! 
No problem :slightly_smiling_face: </p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="uiframeworks-lounge.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: uiframeworks" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              uiframeworks
            </div>
          </div>
        </a>
      
      
        
        <a href="object-capture-and-room-plan-lounge.html" class="md-footer__link md-footer__link--next" aria-label="Next: object capture and room plan" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              object capture and room plan
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["search.share", "search.highlight"], "search": "../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.a6c66575.min.js"></script>
      
    
  </body>
</html>