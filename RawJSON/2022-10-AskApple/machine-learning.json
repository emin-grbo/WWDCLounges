{
  "name": "machine-learning",
  "messages": [
    {
      "type": "message",
      "user": "U03V30M0C1K",
      "text": "This content can't be displayed.",
      "ts": "1665433610.861529",
      "pinned_to": [
        "C042X5L3V7X"
      ],
      "team": "T03U5MWB2FN",
      "reactions": [
        {
          "name": "+1",
          "count": 3,
          "users": [
            "U046NFNKJDA",
            "U045ZS8FJN9",
            "U0468FNLBEX"
          ]
        },
        {
          "name": "star-struck",
          "count": 4,
          "users": [
            "U04662TK7KR",
            "U045ZD12PK7",
            "U045ZS8FJN9",
            "U0449FGDP9C"
          ]
        },
        {
          "name": "robot_face",
          "count": 2,
          "users": [
            "U04625ZF30T",
            "U045ZS8FJN9"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "header",
          "text": {
            "type": "plain_text",
            "text": "Welcome to Ask Apple",
            "emoji": true
          },
          "block_id": "Yvv"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "We're excited to be hosting you in the Machine Learning channel this week! You can find the full schedule of Q\u0026amp;As for Machine Learning by visiting the \u003chttps://apps.apple.com/us/app/apple-developer/id640199958 | Apple Developer app\u003e and \u003chttps://developer.apple.com/events/ask-apple/questions-and-answers/ | website\u003e."
          },
          "block_id": "7C3/"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "If you haven’t already, please take a moment to familiarize yourself with \u003chttps://developer.apple.com/news/?id=vpbyzfg4 | how Q\u0026amp;As will work\u003e."
          },
          "block_id": "yIxf"
        },
        {
          "type": "header",
          "text": {
            "type": "plain_text",
            "text": "Attendance Policy",
            "emoji": true
          },
          "block_id": "h6/l"
        },
        {
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "We want to make sure these spaces are helpful and welcoming for everyone — developers and Apple employees alike. Please review and follow the \u003chttps://developer.apple.com/events/policy/online-event-attendance-policy/ | attendance policy\u003e."
          },
          "block_id": "RXI"
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U03V30M0C1K\u003e added a workflow to this channel: *Ask Apple - machine-learning*.",
      "ts": "1666018974.182079",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "oLY2",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U03V30M0C1K"
                },
                {
                  "type": "text",
                  "text": " added a workflow to this channel: "
                },
                {
                  "type": "text",
                  "text": "Ask Apple - machine-learning",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "3c835aaf-95fc-44ed-8970-6cf925aced9a",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "*Q\u0026amp;A: Core ML* in 8 minutes! :tada:\nIt's great to have so many of you joining us in the Machine Learning channel today. We'll be live in eight minutes with our first activity—_Q\u0026amp;A Core ML_. There is a broad team of Apple Engineers in the channel today to tackle anything Core ML related you may want to know. Many of you have already been asking some great questions which we'll start answering right away. From there, the topical threads will be active. If you haven't already, start submitting your questions now and we'll be chatting with you live very soon!",
      "ts": "1666025577.888489",
      "team": "T03U5MWB2FN",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "7gfH",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Q\u0026A: Core ML ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "in 8 minutes! "
                },
                {
                  "type": "emoji",
                  "name": "tada",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": "\nIt's great to have so many of you joining us in the Machine Learning channel today. We'll be live in eight minutes with our first activity—"
                },
                {
                  "type": "text",
                  "text": "Q\u0026A Core ML",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": ". There is a broad team of Apple Engineers in the channel today to tackle anything Core ML related you may want to know. Many of you have already been asking some great questions which we'll start answering right away. From there, the topical threads will be active. If you haven't already, start submitting your questions now and we'll be chatting with you live very soon!"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8FJN9\u003e asked\n\u0026gt; Is there a way to share FP16 buffer between ANE and GPU on earlier versions of iOS?",
      "ts": "1666025717.372419",
      "thread_ts": "1666025717.372419",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 2,
      "latest_reply": "1666026499.699629",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "epD",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8FJN9"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a way to share FP16 buffer between ANE and GPU on earlier versions of iOS?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "10aabe43-ffa6-48b0-be5a-a5dfbf1c7e00",
          "type": "message",
          "user": "U044UNQ5GTB",
          "text": "The feature is available on each platform as follows:",
          "ts": "1666026487.134719",
          "thread_ts": "1666025717.372419",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "cVqiX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The feature is available on each platform as follows:"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a1a1f5b6-7b6e-499f-a153-f354b44fd329",
          "type": "message",
          "user": "U044UNQ5GTB",
          "text": "```macos(12.0), ios(16.0), watchos(9.0), tvos(16.0)```",
          "ts": "1666026499.699629",
          "thread_ts": "1666025717.372419",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 1,
              "users": [
                "U0449FGDP9C"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "JY0PT",
              "elements": [
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"macos(12.0), ios(16.0), watchos(9.0), tvos(16.0)\"}],\"border\":0}"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8FJN9\u003e asked\n\u0026gt; GatherND (and some other layers) are not supported by A12 ANE. Is there a definite list of supported/unsupported layers per different SoCs? If not, is comreml-tools aware of these unsupported layers?",
      "ts": "1666026483.377139",
      "thread_ts": "1666026483.377139",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 8,
      "latest_reply": "1666114076.670759",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "7oyZ",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8FJN9"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"GatherND (and some other layers) are not supported by A12 ANE. Is there a definite list of supported\\/unsupported layers per different SoCs? If not, is comreml-tools aware of these unsupported layers?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "2e040b30-a276-4133-9766-e5ff761009cf",
          "type": "message",
          "user": "U044G434P4J",
          "text": "You can learn about neural engine support of specific layers on specific hardware and OS variants using performance reports (\u003chttps://developer.apple.com/machine-learning/core-ml/\u003e). coremltools performs hardware agnostic transforms.",
          "ts": "1666026597.186249",
          "thread_ts": "1666026483.377139",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "78Vz",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You can learn about neural engine support of specific layers on specific hardware and OS variants using performance reports ("
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/machine-learning/core-ml/",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "). coremltools performs hardware agnostic transforms."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8922ad06-527e-4540-8f2a-60708f2b4673",
          "type": "message",
          "user": "U045WJTP0BG",
          "text": "We too would love a list like this. I can't overstate how important it would be for us to have a complete, well-documented, publicly available list of layers running on the ANE / GPU with the CoreML/iOS/macOS versions. A list like this would allow us to design models that fully utilize Apple hardware. Currently, the tools only show you what layers are supported after you have already created/trained the model; at that point, when you have a model already trained, it's far too late, and we often just end up using the onnxruntime on the CPU.",
          "ts": "1666028602.206579",
          "thread_ts": "1666026483.377139",
          "edited": {
            "user": "U045WJTP0BG",
            "ts": "1666028645.000000"
          },
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U045YRUL42J"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "L44",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We too would love a list like this. I can't overstate how important it would be for us to have a complete, well-documented, publicly available list of layers running on the ANE / GPU with the CoreML/iOS/macOS versions. A list like this would allow us to design models that fully utilize Apple hardware. Currently, the tools only show you what layers are supported after you have already created/trained the model; at that point, when you have a model already trained, it's far too late, and we often just end up using the onnxruntime on the CPU."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "97714445-6811-4939-aa77-52207463bd63",
          "type": "message",
          "user": "U044G434P4J",
          "text": "Thank you for the feedback, duly noted.\nI’m just curious about couple of things:\n• in your current workflow, how challenging is it to first exercising the the export and profiling path to CoreML with a model with random weights, prior to actually training it. That should allow you to iterate faster on the right architecture for Apple hardware\n• You also mentioned about using the onnxruntime, I’m curious, is this because you were running into some conversion errors to CoreML, since a converted model will always run on the CPU if not ANE ",
          "ts": "1666029052.308509",
          "thread_ts": "1666026483.377139",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Lj/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you for the feedback, duly noted.\nI’m just curious about couple of things:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"in your current workflow, how challenging is it to first exercising the the export and profiling path to CoreML with a model with random weights, prior to actually training it. That should allow you to iterate faster on the right architecture for Apple hardware\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"You also mentioned about using the onnxruntime, I\\u2019m curious, is this because you were running into some conversion errors to CoreML, since a converted model will always run on the CPU if not ANE \"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d9077826-181a-4946-b082-ee553f04f7d3",
          "type": "message",
          "user": "U045WJTP0BG",
          "text": "TBH, profiling before training is only a half solution. Getting to the point of having a complete model ready to convert is a lot of work would be extremely valuable to us to know ahead of time so we don't use layers/features that CoreML doesn't support. As for your second question, yes, converting to CoreML is absolutely the most painful step in this process. One of our most significant issues is that the stable version of coremltools only ever supports exporting back two operating system versions. Our hardware requirements are rarely that new (currently macOS 10.15, iOS 12). TBH, is apple just supported the open neural network exchange format, a lot of pain supporting would be relieved.",
          "ts": "1666029980.165839",
          "thread_ts": "1666026483.377139",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "+1",
              "count": 3,
              "users": [
                "U045ZS8FJN9",
                "U044G434P4J",
                "U04707WB62C"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "wgL/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "TBH, profiling before training is only a half solution. Getting to the point of having a complete model ready to convert is a lot of work would be extremely valuable to us to know ahead of time so we don't use layers/features that CoreML doesn't support. As for your second question, yes, converting to CoreML is absolutely the most painful step in this process. One of our most significant issues is that the stable version of coremltools only ever supports exporting back two operating system versions. Our hardware requirements are rarely that new (currently macOS 10.15, iOS 12). TBH, is apple just supported the open neural network exchange format, a lot of pain supporting would be relieved."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "339b91af-7026-4cc3-b247-07d5fa3d45c2",
          "type": "message",
          "user": "U044G434P4J",
          "text": "Feedback noted for ahead of time information of layers support. Thanks.\n\n“stable version of coremltools only ever supports exporting back two operating system versions” --\u0026gt; Hmm, I dont’ think so that is the case. As you can see all \u003chttps://github.com/apple/coremltools/blob/da9b8ca98e5a758290f6a5703a995b56c22288c6/coremltools/converters/mil/_deployment_compatibility.py#L16|these versions\u003e are supported. (obviosuly there are always new features every year that are only available on the latest os version)\n\nFor ONNX, we actually had supported it earlier but later realized that a lot of our customers using pytorch models were running into issues exporting to ONNX, which were out of our control. Hence we are now focussing on directly conversion from pytroch and keeping that updated based on the op support in the CoreML runtime.",
          "ts": "1666030393.023669",
          "thread_ts": "1666026483.377139",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "DhZuC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Feedback noted for ahead of time information of layers support. Thanks.\n\n“stable version of coremltools only ever supports exporting back two operating system versions” --\u003e Hmm, I dont’ think so that is the case. As you can see all "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/apple/coremltools/blob/da9b8ca98e5a758290f6a5703a995b56c22288c6/coremltools/converters/mil/_deployment_compatibility.py#L16",
                      "text": "these versions"
                    },
                    {
                      "type": "text",
                      "text": " are supported. (obviosuly there are always new features every year that are only available on the latest os version)\n\nFor ONNX, we actually had supported it earlier but later realized that a lot of our customers using pytorch models were running into issues exporting to ONNX, which were out of our control. Hence we are now focussing on directly conversion from pytroch and keeping that updated based on the op support in the CoreML runtime."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ce2f6c2e-aac6-4e99-9f6c-ce8554473491",
          "type": "message",
          "user": "U045WJTP0BG",
          "text": "Hmm, last time I tried to export to older models of using the latest coremltools, it wouldn't convert to the target I wanted but downgrading coremltools converted to the target I wanted. I will try it again and see what happens; thanks. A page like \u003chttps://github.com/onnx/onnx/blob/main/docs/Operators.md\u003e for CoreML with complete descriptions of what computations are being done, the inputs/outputs/atributes, and what versions layers, where added, would be the most valuable 'feature' of CoreML for us. \u003chttps://apple.github.io/coremltools/mlmodel/index.html\u003e is close but it doesn't include information about when a layer was added and the MIL page is quite lite on details.",
          "ts": "1666031278.807059",
          "thread_ts": "1666026483.377139",
          "attachments": [
            {
              "fallback": "GitHub: onnx/Operators.md at main · onnx/onnx",
              "id": 1,
              "title": "onnx/Operators.md at main · onnx/onnx",
              "title_link": "https://github.com/onnx/onnx/blob/main/docs/Operators.md",
              "text": "Open standard for machine learning interoperability - onnx/Operators.md at main · onnx/onnx",
              "image_url": "https://opengraph.githubassets.com/1998963ececbe72d557987f8ae7ad97fdadc2797380f1c390f4cbbaa79f5d137/onnx/onnx",
              "service_name": "GitHub",
              "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/github.png",
              "from_url": "https://github.com/onnx/onnx/blob/main/docs/Operators.md",
              "original_url": "https://github.com/onnx/onnx/blob/main/docs/Operators.md",
              "blocks": null
            }
          ],
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U04707WB62C"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "9pBY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hmm, last time I tried to export to older models of using the latest coremltools, it wouldn't convert to the target I wanted but downgrading coremltools converted to the target I wanted. I will try it again and see what happens; thanks. A page like "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/onnx/onnx/blob/main/docs/Operators.md",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " for CoreML with complete descriptions of what computations are being done, the inputs/outputs/atributes, and what versions layers, where added, would be the most valuable 'feature' of CoreML for us. "
                    },
                    {
                      "type": "link",
                      "url": "https://apple.github.io/coremltools/mlmodel/index.html",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " is close but it doesn't include information about when a layer was added and the MIL page is quite lite on details."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "701a411c-59f0-4c9d-90de-d510e85ab9ce",
          "type": "message",
          "user": "U044G434P4J",
          "text": "Does this reference help: \u003chttps://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html\u003e?",
          "ts": "1666031529.785009",
          "thread_ts": "1666026483.377139",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "A2TGK",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Does this reference help: "
                    },
                    {
                      "type": "link",
                      "url": "https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "55051bc8-3e55-4fca-97c7-1617dc983d64",
          "type": "message",
          "user": "U045WJTP0BG",
          "text": "That's a god send have I really just been missing this?",
          "ts": "1666114076.670759",
          "thread_ts": "1666026483.377139",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mJ0N",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That's a god send have I really just been missing this?"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045YRUL42J\u003e asked\n\u0026gt; I am currently trying to optimise a model to run on the Neural Engine. Using the new \"Performance\" tab of the .mlmodel/.mlpackage view in XCode (introduced with iOS 16/macOS 13) I can see which operations are placed on the Neural Engine, which ones on the GPU and which ones on the CPU.\n\u0026gt; \n\u0026gt; In my specific case I can vary the size of the last dimension of my input array and I noticed that for small such sizes some operations are indeed placed on the Neural Engine, while for bigger sizes the same operations are placed on the CPU.\n\u0026gt; \n\u0026gt; So I'm guessing that the Neural Engine has a limit on how big the tensors can be that it is able to process.\n\u0026gt; \n\u0026gt; If such a limit exists, could you state some numbers and how this limit manifests? So is it rather \"the maximum size of a dimension that the Neural Engine can process is ...\" or \"the maximum number of elements a tensor may contain to be processed on the ANE is ...\". Or is it bound by something completely different?",
      "ts": "1666026724.347689",
      "thread_ts": "1666026724.347689",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 1,
      "latest_reply": "1666026801.362539",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "uNhUL",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045YRUL42J"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I am currently trying to optimise a model to run on the Neural Engine. Using the new \\\"Performance\\\" tab of the .mlmodel\\/.mlpackage view in XCode (introduced with iOS 16\\/macOS 13) I can see which operations are placed on the Neural Engine, which ones on the GPU and which ones on the CPU.\\n\\nIn my specific case I can vary the size of the last dimension of my input array and I noticed that for small such sizes some operations are indeed placed on the Neural Engine, while for bigger sizes the same operations are placed on the CPU.\\n\\nSo I'm guessing that the Neural Engine has a limit on how big the tensors can be that it is able to process.\\n\\nIf such a limit exists, could you state some numbers and how this limit manifests? So is it rather \\\"the maximum size of a dimension that the Neural Engine can process is ...\\\" or \\\"the maximum number of elements a tensor may contain to be processed on the ANE is ...\\\". Or is it bound by something completely different?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "adb94956-b304-4236-8cc4-87f074dc60a2",
          "type": "message",
          "user": "U0452E161DK",
          "text": "The neural engine supports channel dimension up to 65536, width and height dimension up to 16384, and batch dimension up to 4096.",
          "ts": "1666026801.362539",
          "thread_ts": "1666026724.347689",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U045YRUL42J"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Tbo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The neural engine supports channel dimension up to 65536, width and height dimension up to 16384, and batch dimension up to 4096."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8FJN9\u003e asked\n\u0026gt; Is there a CLI version of CoreML performance benchmarks?",
      "ts": "1666026945.185229",
      "thread_ts": "1666026945.185229",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 5,
      "latest_reply": "1666087578.381059",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "ZJ3",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8FJN9"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a CLI version of CoreML performance benchmarks?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "c3acd8c4-e0c4-488b-9786-a002a9e0d292",
          "type": "message",
          "user": "U0452E161DK",
          "text": "Core ML performance reports are only available via the Xcode interface",
          "ts": "1666026984.227809",
          "thread_ts": "1666026945.185229",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Dsb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Core ML performance reports are only available via the Xcode interface"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "c973e4a4-ef09-44ae-b295-58b57c1e384a",
          "type": "message",
          "user": "U0452E161DK",
          "text": "Do you have a workflow or use case in mind for a CLI ? We’d love to hear about your experience with performance reports and workflows you are most interested in.",
          "ts": "1666027144.601049",
          "thread_ts": "1666026945.185229",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "GQ6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Do you have a workflow or use case in mind for a CLI ? We’d love to hear about your experience with performance reports and workflows you are most interested in."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1a00bedb-b10f-4e03-9491-07aaa967d297",
          "type": "message",
          "user": "U0452E161DK",
          "text": "Feel free to share here or via \u003chttps://feedbackassistant.apple.com\u003e",
          "ts": "1666027195.373329",
          "thread_ts": "1666026945.185229",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Eh0gO",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Feel free to share here or via "
                    },
                    {
                      "type": "link",
                      "url": "https://feedbackassistant.apple.com",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "B09222EA-5A40-478D-8650-332D1D8861F3",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "We’d love to automate the process for optimizing architecture for ANE, and the necessity to open IDE removes the possibility for automation\n\nWe’d love to run 10k tests for different combination of body/head/neck with different dimensions and some layer permutations for the discovery of ‘the fastest’ arch for our task",
          "ts": "1666029419.173529",
          "thread_ts": "1666026945.185229",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "+yHuY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We’d love to automate the process for optimizing architecture for ANE, and the necessity to open IDE removes the possibility for automation"
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "text",
                      "text": "\nWe’d love to run 10k tests for different combination of body/head/neck with different dimensions and some layer permutations for the discovery of ‘the fastest’ arch for our task"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d296441c-ce62-43b7-bd05-af63f5a8cac0",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "A bit of a context: we run a 30fps body tracking model, 15fps segmentation, alongside compute shaders for cloth simulation and rendering. All while trying to keep the device not boiling hot:) So every millisecond counts a lot.",
          "ts": "1666087578.381059",
          "thread_ts": "1666026945.185229",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kvM",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "A bit of a context: we run a 30fps body tracking model, 15fps segmentation, alongside compute shaders for cloth simulation and rendering. All while trying to keep the device not boiling hot:) So every millisecond counts a lot."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8EQ5T\u003e asked\n\u0026gt; In the newest version of Core ML, the 16-bit float grayscale image type was added. It would be great to also get that for RGB! :slightly_smiling_face:",
      "ts": "1666026984.985729",
      "thread_ts": "1666026984.985729",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 2,
      "latest_reply": "1666027255.375159",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "0zTBQ",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8EQ5T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"In the newest version of Core ML, the 16-bit float grayscale image type was added. It would be great to also get that for RGB! \"},{\"type\":\"emoji\",\"name\":\"slightly_smiling_face\",\"unicode\":\"1f642\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "0e50cb4f-4154-4078-8d9c-bd5c9df6e788",
          "type": "message",
          "user": "U044JHA4KJ8",
          "text": "Thanks for the feedback! It would also be helpful if you can file a request for this \u003chttps://feedbackassistant.apple.com\u003e describing your use case for 16-bit per component RGB as input/output with Core ML.",
          "ts": "1666027007.880119",
          "thread_ts": "1666026984.985729",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5qYC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for the feedback! It would also be helpful if you can file a request for this "
                    },
                    {
                      "type": "link",
                      "url": "https://feedbackassistant.apple.com",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": " describing your use case for 16-bit per component RGB as input/output with Core ML."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "5da6485c-9349-4451-96e6-2864ff1a6866",
          "type": "message",
          "user": "U045ZS8EQ5T",
          "text": "Already did: FB10151072\nThanks for the consideration!",
          "ts": "1666027255.375159",
          "thread_ts": "1666026984.985729",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "100",
              "count": 1,
              "users": [
                "U044JHA4KJ8"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ZqMq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Already did: FB10151072\nThanks for the consideration!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8FJN9\u003e asked\n\u0026gt; Is there a way to view per-layer performance data in CoreML profiler?",
      "ts": "1666027210.267169",
      "thread_ts": "1666027210.267169",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 2,
      "latest_reply": "1666029675.073509",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "WFGET",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8FJN9"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is there a way to view per-layer performance data in CoreML profiler?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b6303043-e66f-4bbc-9852-d61df51638d1",
          "type": "message",
          "user": "U0452E161DK",
          "text": "Per-layer compute unit support is available via performance reports.  However, timing information is only available at the model level within the report. For models which use multiple compute units, you can view segment level timing within instruments",
          "ts": "1666027424.316969",
          "thread_ts": "1666027210.267169",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "vDJlS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Per-layer compute unit support is available via performance reports.  However, timing information is only available at the model level within the report. For models which use multiple compute units, you can view segment level timing within instruments"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "AE6E525F-EB0F-4B85-971D-A8BBAFC4B9E4",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "That would be a great feature for better insight into ANE, that would open the way for minimization of inference time on our part. We’d love to run models 60fps for our camera app, and right now we a feeling that without this feature we wouldn't be able to provide enough feedback for our ML engineers to optimize our models",
          "ts": "1666029675.073509",
          "thread_ts": "1666027210.267169",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hxy",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That would be a great feature for better insight into ANE, that would open the way for minimization of inference time on our part. We’d love to run models 60fps for our camera app, and right now we a feeling that without this feature we wouldn't be able to provide enough feedback for our ML engineers to optimize our models"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8FJN9\u003e asked\n\u0026gt; One of my CoreML models got broken on iOS16, when it's running on ANE, there seems to be a corruption in one of the early layers. Can you help with the bug?",
      "ts": "1666027263.246159",
      "thread_ts": "1666027263.246159",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 2,
      "latest_reply": "1666029878.999409",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "zXCjl",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8FJN9"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"One of my CoreML models got broken on iOS16, when it's running on ANE, there seems to be a corruption in one of the early layers. Can you help with the bug?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a1804803-96c2-4627-a65a-5036e2dfc17a",
          "type": "message",
          "user": "U044UNQ5GTB",
          "text": "Thank you for the feedback. Could you please file a bug report with the problem model attached so that we can recreate the problem here?  _(\u003chttps://developer.apple.com/bug-reporting/\u003e)_",
          "ts": "1666027335.087219",
          "thread_ts": "1666027263.246159",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0wW8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you for the feedback. Could you please file a bug report with the problem model attached so that we can recreate the problem here?  "
                    },
                    {
                      "type": "text",
                      "text": "(",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/bug-reporting/",
                      "text": "",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ")",
                      "style": {
                        "italic": true
                      }
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "DBCCFBB9-F7A1-4757-93C8-26E13E982EF3",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "FB10165528",
          "ts": "1666029878.999409",
          "thread_ts": "1666027263.246159",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "8+a1m",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "FB10165528"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZS8EQ5T\u003e asked\n\u0026gt; Do custom layers always prevent the use of the ANE? I have to set `computeUnits` to `.cpuAndGPU`, otherwise I'm getting a `BAD_ACCESS` when loading the model.",
      "ts": "1666027358.169969",
      "thread_ts": "1666027358.169969",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 9,
      "latest_reply": "1666028274.800899",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "gLEpn",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZS8EQ5T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Do custom layers always prevent the use of the ANE? I have to set `computeUnits` to `.cpuAndGPU`, otherwise I'm getting a `BAD_ACCESS` when loading the model.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "934e6047-c6c6-4be9-ae30-8615cda33c1a",
          "type": "message",
          "user": "U044UNX169F",
          "text": "Yes, custom layers and the layers that come after custom layers don't get sent over to ANE.\n\nRegarding the `BAD_ACCESS` , could you please file a bug report \u003chttps://developer.apple.com/bug-reporting/\u003e",
          "ts": "1666027554.431659",
          "thread_ts": "1666027358.169969",
          "attachments": [
            {
              "fallback": "Apple Developer: Bug Reporting - Apple Developer",
              "id": 1,
              "title": "Bug Reporting - Apple Developer",
              "title_link": "https://developer.apple.com/bug-reporting/",
              "text": "Now with Feedback Assistant available on iPhone, iPad, Mac, and the web, it’s easier to submit effective bug reports and request enhancements to APIs and tools.",
              "image_url": "https://developer.apple.com/news/images/og/bug-reporting-og.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/bug-reporting/",
              "original_url": "https://developer.apple.com/bug-reporting/",
              "blocks": null
            }
          ],
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "uMoW7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, custom layers and the layers that come after custom layers don't get sent over to ANE.\n\nRegarding the "
                    },
                    {
                      "type": "text",
                      "text": "BAD_ACCESS",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " , could you please file a bug report "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/bug-reporting/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "f70fa094-21c4-4f18-b6d5-b8ca3a019caa",
          "type": "message",
          "user": "U045ZS8EQ5T",
          "text": "Ok, will do!",
          "ts": "1666027610.762629",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "gratitude-thank-you",
              "count": 1,
              "users": [
                "U044UNX169F"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=pN",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ok, will do!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cd748082-bd45-4ab1-b9da-f43ecc2970e2",
          "type": "message",
          "user": "U045ZS8EQ5T",
          "text": "So the Model _should_ load and the `BAD_ACCESS` is a bug?",
          "ts": "1666027665.051139",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lGZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So the Model "
                    },
                    {
                      "type": "text",
                      "text": "should",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " load and the "
                    },
                    {
                      "type": "text",
                      "text": "BAD_ACCESS",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " is a bug?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "506de28a-563a-47b6-b924-a6063d139bbf",
          "type": "message",
          "user": "U044G434P4J",
          "text": "Yes that would be a bug",
          "ts": "1666027829.322379",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mZk",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes that would be a bug"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "40720584-b4ad-49bf-aa98-8a9a267c500a",
          "type": "message",
          "user": "U044G434P4J",
          "text": "I’m curious, which layer are you using the custom layer functionality for, which is not currently supported?",
          "ts": "1666027848.669759",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "8Mq",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I’m curious, which layer are you using the custom layer functionality for, which is not currently supported?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "33613d79-aa93-4c9b-bcbd-d6120c6d7dec",
          "type": "message",
          "user": "U045ZS8EQ5T",
          "text": "Oh we used that to split a model with instance normalization layers into two: one that could calculate the mean/variance for the layers on a smaller resolution and another one that runs on a higher resolution and uses the mean/variance from the other model as input",
          "ts": "1666027976.659629",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ftggC",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Oh we used that to split a model with instance normalization layers into two: one that could calculate the mean/variance for the layers on a smaller resolution and another one that runs on a higher resolution and uses the mean/variance from the other model as input"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a3266734-5fca-43d7-8743-2c932e7594a3",
          "type": "message",
          "user": "U045ZS8EQ5T",
          "text": "BTW, the crash happens in `Espresso::ANECompilerEngine::transpose_kernel::is_valid_for_engine:`. Maybe that helps",
          "ts": "1666028085.598389",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Gb+GS",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "BTW, the crash happens in "
                    },
                    {
                      "type": "text",
                      "text": "Espresso::ANECompilerEngine::transpose_kernel::is_valid_for_engine:",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": ". Maybe that helps"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "e5541df7-6a60-435f-965a-d8b5ccc1327d",
          "type": "message",
          "user": "U044G434P4J",
          "text": "I see, you may find using composite layers more helpful in this use case, where you can use simple existing layers to build your own custom math: \u003chttps://coremltools.readme.io/docs/composite-operators\u003e\nThat way your model will have better guarantees of being ANE resident.",
          "ts": "1666028189.248549",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iteZ5",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I see, you may find using composite layers more helpful in this use case, where you can use simple existing layers to build your own custom math: "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/composite-operators",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\nThat way your model will have better guarantees of being ANE resident."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "acaff936-c667-4455-9dbf-21698e2faab6",
          "type": "message",
          "user": "U045ZS8EQ5T",
          "text": "Oh, interesting! I will check that out, thanks!",
          "ts": "1666028274.800899",
          "thread_ts": "1666027358.169969",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4re",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Oh, interesting! I will check that out, thanks!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "1feae16d-76bf-47bc-9d11-9f2474f07496",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "*Keep the questions coming!*\n*\u003c!here\u003e* The team is seeing a whole lot of great questions coming in and is working to respond to all of them. Keep them coming!\n\nMeanwhile, here's a question for you: *What features are you currently enabled or hoping to enable using Core ML in your apps? Let's have a discussion about all that is possible.* :thread:",
      "ts": "1666027935.177119",
      "thread_ts": "1666027935.177119",
      "reply_count": 13,
      "latest_reply": "1666031153.262459",
      "team": "T03U5MWB2FN",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "qu/3U",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Keep the questions coming!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\n"
                },
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": " ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "The team is seeing a whole lot of great questions coming in and is working to respond to all of them. Keep them coming!\n\nMeanwhile, here's a question for you: "
                },
                {
                  "type": "text",
                  "text": "What features are you currently enabled or hoping to enable using Core ML in your apps? Let's have a discussion about all that is possible. ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0,
                  "style": {
                    "bold": true
                  }
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "df675f78-6d6e-4511-976f-e92d2acdba68",
          "type": "message",
          "user": "U0465JR5LPN",
          "text": "im hoping to enable more ML into my arkit apps. how about a wall/window detection that's accurate",
          "ts": "1666028328.355989",
          "thread_ts": "1666027935.177119",
          "edited": {
            "user": "U0465JR5LPN",
            "ts": "1666028344.000000"
          },
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U045VNN3MPY"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "4AZv",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "im hoping to enable more ML into my arkit apps. how about a wall/window detection that's accurate"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "425ff83f-3473-4ce3-83ac-12e41df42e42",
          "type": "message",
          "user": "U046S70K9A4",
          "text": "I noticed that a small subset of the Sound Analysis framework is exposed in Shortcuts as an automation trigger. Got me thinking how it would be really cool for developers to be able to contribute custom triggers similar how we can contribute actions",
          "ts": "1666028663.102179",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fzqKg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I noticed that a small subset of the Sound Analysis framework is exposed in Shortcuts as an automation trigger. Got me thinking how it would be really cool for developers to be able to contribute custom triggers similar how we can contribute actions"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0c5f5abd-1dff-46b5-b2bd-f419c9b3f39b",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "\u003c@U0465JR5LPN\u003e Are you hoping for detection that's more accurate than what's already built in to ARKit? That could be done with a custom model which takes ARFrames as input. That said, I'd love to hear more about your specific need and use case.",
          "ts": "1666028864.035289",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/Qn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U0465JR5LPN"
                    },
                    {
                      "type": "text",
                      "text": " Are you hoping for detection that's more accurate than what's already built in to ARKit? That could be done with a custom model which takes ARFrames as input. That said, I'd love to hear more about your specific need and use case."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "51c2e641-b64d-4531-b91a-a302ab9f3ee6",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "\u003c@U046S70K9A4\u003e Great suggestion! Have you filed a Feedback Assistant report on this feature request for Shortcuts?",
          "ts": "1666028907.224749",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "sV+N",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U046S70K9A4"
                    },
                    {
                      "type": "text",
                      "text": " Great suggestion! Have you filed a Feedback Assistant report on this feature request for Shortcuts?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fe540b30-0f5f-4f5b-be44-0ea8615e9f38",
          "type": "message",
          "user": "U046S70K9A4",
          "text": "\u003c@U0455QUCD9N\u003e thanks! not yet but just added to my list",
          "ts": "1666028992.184459",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "oed7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U0455QUCD9N"
                    },
                    {
                      "type": "text",
                      "text": " thanks! not yet but just added to my list"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "7ea3bb8c-f7c9-481e-b1e0-e637376cc3c3",
          "type": "message",
          "user": "U04656DU0BX",
          "text": "For features like RoomPlan, it would be great if we could add our own custom detectable objects/architectural features.",
          "ts": "1666029104.967359",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "NI6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "For features like RoomPlan, it would be great if we could add our own custom detectable objects/architectural features."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4cdd0433-f792-4257-bc26-ae85423c1e8e",
          "type": "message",
          "user": "U04656DU0BX",
          "text": "I also wish Hand Pose Detection included depth data.",
          "ts": "1666029294.587139",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2uP65",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I also wish Hand Pose Detection included depth data."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "707bddbd-0204-4802-ba49-60c10b80004d",
          "type": "message",
          "user": "U0465JR5LPN",
          "text": "\u003c@U0455QUCD9N\u003e yes for example making a model that can run longer than roomPlan. ARKit scene understanding is too inaccurate.  models for detecting objects and their respective 3D pose.\nthose M1 iPads look powerful enough",
          "ts": "1666029374.706919",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ta3j",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U0455QUCD9N"
                    },
                    {
                      "type": "text",
                      "text": " yes for example making a model that can run longer than roomPlan. ARKit scene understanding is too inaccurate.  models for detecting objects and their respective 3D pose.\nthose M1 iPads look powerful enough"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b1992703-3848-456b-b1d8-ee2a896e8b99",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "Thanks \u003c@U04656DU0BX\u003e and \u003c@U0465JR5LPN\u003e for more detail around the ARKit needs!",
          "ts": "1666029660.636749",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5TaP",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks "
                    },
                    {
                      "type": "user",
                      "user_id": "U04656DU0BX"
                    },
                    {
                      "type": "text",
                      "text": " and "
                    },
                    {
                      "type": "user",
                      "user_id": "U0465JR5LPN"
                    },
                    {
                      "type": "text",
                      "text": " for more detail around the ARKit needs!"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "92bdf1e8-6add-45a3-bba2-25188521b6b7",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "\u003c@U04656DU0BX\u003e I'd love to hear more about your specific Hand Pose use case. For example, is about localizing to hand closest to camera or something similar?",
          "ts": "1666029717.778879",
          "thread_ts": "1666027935.177119",
          "edited": {
            "user": "U0455QUCD9N",
            "ts": "1666029734.000000"
          },
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "wcj2",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U04656DU0BX"
                    },
                    {
                      "type": "text",
                      "text": " I'd love to hear more about your specific Hand Pose use case. For example, is about localizing to hand closest to camera or something similar?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a920bf4b-dbd2-4b7f-875f-7d28e9e46b76",
          "type": "message",
          "user": "U04656DU0BX",
          "text": "When using Hand Pose in an AR setting, you want to be able to detect collisions with objects like tables, walls or even virtual objects.",
          "ts": "1666029871.806559",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "100",
              "count": 4,
              "users": [
                "U0455QUCD9N",
                "U045VNN3MPY",
                "U045Z8S5KE1",
                "U0470SJ7DFE"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "iPl1S",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "When using Hand Pose in an AR setting, you want to be able to detect collisions with objects like tables, walls or even virtual objects."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "2063FE1C-2AAA-4006-A952-F76B4FFFB48A",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "We run 3D body tracking and segmentation for our virtual clothing app",
          "ts": "1666030133.355059",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "heart",
              "count": 2,
              "users": [
                "U0455QUCD9N",
                "U045VNN3MPY"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "XMh1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We run 3D body tracking and segmentation for our virtual clothing app"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "A497C516-2460-43A9-8484-CC02C31C4BDD",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "I’d really want to run stable diffusion locally, optimized for ANE for custom garment generation from text prompt",
          "ts": "1666031153.262459",
          "thread_ts": "1666027935.177119",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "nZ1",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I’d really want to run stable diffusion locally, optimized for ANE for custom garment generation from text prompt"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U04625ZF30T\u003e asked\n\u0026gt; Hi! I've started to play around with CreateML and have trained a model using the \"Text Classifier\". The problem I have is that the trained model only gives me a Class Label. I would like to also receive the confidence as a Double or Float. Is this possible?\n\u0026gt; \n\u0026gt; If not, can you point me in the direction on how I can train a model that gives me all class labels associated with their confidence?",
      "ts": "1666028009.961689",
      "thread_ts": "1666028009.961689",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 4,
      "latest_reply": "1666028697.888099",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "fxefo",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U04625ZF30T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi! I've started to play around with CreateML and have trained a model using the \\\"Text Classifier\\\". The problem I have is that the trained model only gives me a Class Label. I would like to also receive the confidence as a Double or Float. Is this possible?\\n\\nIf not, can you point me in the direction on how I can train a model that gives me all class labels associated with their confidence?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "6d691bb9-861e-4c08-807a-d4e6fcbec4d6",
          "type": "message",
          "user": "U04656DU0BX",
          "text": "I don’t work at Apple, but I recently tackled this. The `predictedLabelHypotheses(for:)` should provide the label and confidence: \u003chttps://developer.apple.com/documentation/naturallanguage/nlmodel/3547997-predictedlabelhypotheses\u003e",
          "ts": "1666028258.547509",
          "thread_ts": "1666028009.961689",
          "edited": {
            "user": "U04656DU0BX",
            "ts": "1666028317.000000"
          },
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U0457S2646Q"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "ahQY",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I don’t work at Apple, but I recently tackled this. The "
                    },
                    {
                      "type": "text",
                      "text": "predictedLabelHypotheses(for:)",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " should provide the label and confidence: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/naturallanguage/nlmodel/3547997-predictedlabelhypotheses",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "84d937ff-4b7a-4be0-b5e5-aa7fa759ca50",
          "type": "message",
          "user": "U0457S2646Q",
          "text": "Text Classifier does support that. If you are using CreateML app, in the Preview pane, it should already give you a list of labels together with their confidences. If you are looking for APIs to get that from the exported CoreML model (.mlmodel), these two API give you that\n\u003chttps://developer.apple.com/documentation/naturallanguage/nlmodel/3013804-init\u003e\n\u003chttps://developer.apple.com/documentation/naturallanguage/nlmodel/3547997-predictedlabelhypotheses\u003e",
          "ts": "1666028375.430909",
          "thread_ts": "1666028009.961689",
          "edited": {
            "user": "U0457S2646Q",
            "ts": "1666028408.000000"
          },
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "PZp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Text Classifier does support that. If you are using CreateML app, in the Preview pane, it should already give you a list of labels together with their confidences. If you are looking for APIs to get that from the exported CoreML model (.mlmodel), these two API give you that\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/naturallanguage/nlmodel/3013804-init",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/naturallanguage/nlmodel/3547997-predictedlabelhypotheses",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "91ce24ca-b5bc-4e3a-92ed-3d5241848410",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "Great question, \u003c@U04656DU0BX\u003e. If you weren't already aware, we have a dedicated hour tomorrow in this same channel where we'll be joined by Engineers from the Create ML team. We can dive deeper into this and any other Create ML related question then as well.",
          "ts": "1666028640.326019",
          "thread_ts": "1666028009.961689",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U04625ZF30T"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "A0P",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Great question, "
                    },
                    {
                      "type": "user",
                      "user_id": "U04656DU0BX"
                    },
                    {
                      "type": "text",
                      "text": ". If you weren't already aware, we have a dedicated hour tomorrow in this same channel where we'll be joined by Engineers from the Create ML team. We can dive deeper into this and any other Create ML related question then as well."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "294d6cba-38ad-4dbb-9df1-02ec423e306f",
          "type": "message",
          "user": "U04625ZF30T",
          "text": "Thank you \u003c@U0457S2646Q\u003e \u0026amp; \u003c@U04656DU0BX\u003e. This is exactly what I was looking for. Sorry if I was unclear, I want to get the confidence at runtime in an app :slightly_smiling_face:",
          "ts": "1666028697.888099",
          "thread_ts": "1666028009.961689",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U0457S2646Q"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "3/lI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you "
                    },
                    {
                      "type": "user",
                      "user_id": "U0457S2646Q"
                    },
                    {
                      "type": "text",
                      "text": " \u0026 "
                    },
                    {
                      "type": "user",
                      "user_id": "U04656DU0BX"
                    },
                    {
                      "type": "text",
                      "text": ". This is exactly what I was looking for. Sorry if I was unclear, I want to get the confidence at runtime in an app "
                    },
                    {
                      "type": "emoji",
                      "name": "slightly_smiling_face",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U0468FNLBEX\u003e asked\n\u0026gt; Hi! I have two questions: \n\u0026gt; Up to how many classes can a model support? and as the number of classes is growing, the number of samples per class in the dataset must be as well?, is there a proportion or formula on how many samples per class should exist in the dataset optimally, depending on the number of classes?",
      "ts": "1666028482.507969",
      "thread_ts": "1666028482.507969",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 2,
      "latest_reply": "1666028717.966759",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "juQtw",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U0468FNLBEX"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi! I have two questions: \\nUp to how many classes can a model support? and as the number of classes is growing, the number of samples per class in the dataset must be as well?, is there a proportion or formula on how many samples per class should exist in the dataset optimally, depending on the number of classes?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "78c1821d-e23a-405e-bb88-2f8ba1559426",
          "type": "message",
          "user": "U0452E161DK",
          "text": "There are some representation limits on the number of classes a model can support; usually 2^32 or 2^16 depending on the type of the model.\n\nHowever, as you noted, before hitting these numeric limits you are more likely to hit challenges in terms of the complexity of training a model which can accurately discriminate between a large number of classes.\n\nWhile some model types and tasks can provide some rules of thumb around the number of training samples per class, there is no set formula and each use case varies in how much data is required to sufficiently represent the variation / differences in each class.",
          "ts": "1666028514.743649",
          "thread_ts": "1666028482.507969",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U0468FNLBEX"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Z1=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There are some representation limits on the number of classes a model can support; usually 2^32 or 2^16 depending on the type of the model.\n\nHowever, as you noted, before hitting these numeric limits you are more likely to hit challenges in terms of the complexity of training a model which can accurately discriminate between a large number of classes.\n\nWhile some model types and tasks can provide some rules of thumb around the number of training samples per class, there is no set formula and each use case varies in how much data is required to sufficiently represent the variation / differences in each class."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fdd4b72d-8975-42a8-b609-e2cb53c4e9cb",
          "type": "message",
          "user": "U0452E161DK",
          "text": "You may want to explore alternative ways to represent the problem such as considering a hierarchical approach to classification.",
          "ts": "1666028717.966759",
          "thread_ts": "1666028482.507969",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "eyes",
              "count": 1,
              "users": [
                "U0468FNLBEX"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "AL1J",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You may want to explore alternative ways to represent the problem such as considering a hierarchical approach to classification."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045WJTP0BG\u003e asked\n\u0026gt; A lot of our models have inputs and outputs that need to be metal textures or buffers. Are there any plans on the roadmap to add features to CoreML to allow Metal texture/buffer input to submit metal shaders to do some preprocessing, run a CoreML model, then submit metal shaders to some postprocessing?",
      "ts": "1666029239.119869",
      "thread_ts": "1666029239.119869",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 3,
      "latest_reply": "1666029971.513639",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "IB5L",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045WJTP0BG"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"A lot of our models have inputs and outputs that need to be metal textures or buffers. Are there any plans on the roadmap to add features to CoreML to allow Metal texture\\/buffer input to submit metal shaders to do some preprocessing, run a CoreML model, then submit metal shaders to some postprocessing?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "eae6b691-8ccd-43fd-9795-a53f0f3d7a62",
          "type": "message",
          "user": "U044JHA4KJ8",
          "text": "Hi Aaron, you can create a MTLTexture as a view into a CVPixelBuffer by using \u003chttps://developer.apple.com/documentation/corevideo/1456754-cvmetaltexturecachecreatetexture?language=objc\u003e. You can give the CVPixelBuffer to CoreML directly, then use the MTLTexture on GPU.",
          "ts": "1666029339.085049",
          "thread_ts": "1666029239.119869",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "INj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi Aaron, you can create a MTLTexture as a view into a CVPixelBuffer by using "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/corevideo/1456754-cvmetaltexturecachecreatetexture?language=objc",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": ". You can give the CVPixelBuffer to CoreML directly, then use the MTLTexture on GPU."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8509d343-6745-4fdd-b9f4-882aa6d982b9",
          "type": "message",
          "user": "U044JHA4KJ8",
          "text": "One further note, this still will require a buffer copy in CoreML if the pixel type is not `OneComponent16Half` .",
          "ts": "1666029536.394349",
          "thread_ts": "1666029239.119869",
          "edited": {
            "user": "U044JHA4KJ8",
            "ts": "1666029570.000000"
          },
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "azXa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "One further note, this still will require a buffer copy in CoreML if the pixel type is not "
                    },
                    {
                      "type": "text",
                      "text": "OneComponent16Half",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " ."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0e4eb06c-b5d2-4cd0-916a-702aa773b24a",
          "type": "message",
          "user": "U044JHA4KJ8",
          "text": "One final note here, submitting a feedback request describing your use case for CoreML+Metal and the support you're looking for would be very helpful! \u003chttps://feedbackassistant.apple.com\u003e",
          "ts": "1666029971.513639",
          "thread_ts": "1666029239.119869",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Se4t",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "One final note here, submitting a feedback request describing your use case for CoreML+Metal and the support you're looking for would be very helpful! "
                    },
                    {
                      "type": "link",
                      "url": "https://feedbackassistant.apple.com",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "8533a495-4dce-4dea-8d9a-42c3603fd67c",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "*Let's talk about model types!*\nWe're seeing questions from some of you regarding conversion and/or training of specific model types on Apple Silicon. We'd love to hear from all of you regarding various model types you'd like to leverage within Core ML. What are they? How would you envision using them in your apps? What kinds of features would they unlock? :thread:",
      "ts": "1666029352.928559",
      "thread_ts": "1666029352.928559",
      "reply_count": 4,
      "latest_reply": "1666087174.469359",
      "team": "T03U5MWB2FN",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "puJ",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Let's talk about model types!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\nWe're seeing questions from some of you regarding conversion and/or training of specific model types on Apple Silicon. We'd love to hear from all of you regarding various model types you'd like to leverage within Core ML. What are they? How would you envision using them in your apps? What kinds of features would they unlock? "
                },
                {
                  "type": "emoji",
                  "name": "thread",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "915DD3EC-14D3-48AE-869C-CB4C6F26C508",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "I would really love to be able to train NeRFs on Apple silicon.\n\nI've made first steps in that direction in my metal-nerf-2d repo, but it's really hard to move it to 3D space",
          "ts": "1666030327.626119",
          "thread_ts": "1666029352.928559",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "2e5rE",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I would really love to be able to train NeRFs on Apple silicon."
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "text",
                      "text": "\nI've made first steps in that direction in my metal-nerf-2d repo, but it's really hard to move it to 3D space"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "4215ffd7-32f4-419c-b199-eeb75aa39001",
          "type": "message",
          "user": "U046963BN78",
          "text": "Image segmentation. It would be great if we could have a create ML model to train image segmentation.",
          "ts": "1666042884.682579",
          "thread_ts": "1666029352.928559",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "tlZ",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Image segmentation. It would be great if we could have a create ML model to train image segmentation."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "F15AD413-4990-4527-B202-C0C4416C65FD",
          "type": "message",
          "user": "U04656V5MGU",
          "text": "Sorry late to the party :see_no_evil: I want to utilize a drawing classifier in my app, actually already am with Turi Create but it’s seemingly eol so I’m in need of a Create ML solution :grin: ",
          "ts": "1666068371.968079",
          "thread_ts": "1666029352.928559",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "yR2HR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sorry"
                    },
                    {
                      "type": "text",
                      "text": " late to the party "
                    },
                    {
                      "type": "emoji",
                      "name": "see_no_evil",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " I want to utilize a drawing classifier in my app, actually already am with Turi Create"
                    },
                    {
                      "type": "text",
                      "text": " "
                    },
                    {
                      "type": "text",
                      "text": "but it’s seemingly eol so I’m in need of a Create ML solution "
                    },
                    {
                      "type": "emoji",
                      "name": "grin",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " "
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "33096a83-508b-4729-85ac-e50618eaf43b",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "I’d really love stable diffusion version with ANE optimisations. I think it should cut down inference to mere seconds, and will open a whole bunch of possibilities for the creative apps",
          "ts": "1666087174.469359",
          "thread_ts": "1666029352.928559",
          "parent_user_id": "U0455QUCD9N",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mR9o",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I’d really love stable diffusion version with ANE optimisations. I think it should cut down inference to mere seconds, and will open a whole bunch of possibilities for the creative apps"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U04625ZF30T\u003e asked\n\u0026gt; I'm running an Apple Silicon Mac, but have two old eGPUs I used to use on my old Intel Mac to improve 3d render times.\n\u0026gt; \n\u0026gt; I have a vague memory these eGPUs could be used to improve the time it takes to train a machine learning model. Is this still something supported on Apple Silicon hardware?",
      "ts": "1666029739.500399",
      "thread_ts": "1666029739.500399",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 6,
      "latest_reply": "1666087077.369549",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "Y/02",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U04625ZF30T"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I'm running an Apple Silicon Mac, but have two old eGPUs I used to use on my old Intel Mac to improve 3d render times.\\n\\nI have a vague memory these eGPUs could be used to improve the time it takes to train a machine learning model. Is this still something supported on Apple Silicon hardware?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "dc78f936-2197-47da-9c6d-e55a3288a45b",
          "type": "message",
          "user": "U0457S2646Q",
          "text": "If you are using CreateML on Apple Silicon (AS) Mac, training with eGPU is not supported. For eGPU training support on AS in general, I’d recommend check out MPS APIs",
          "ts": "1666030014.770929",
          "thread_ts": "1666029739.500399",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "GPI/J",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you are using CreateML on Apple Silicon (AS) Mac, training with eGPU is not supported. For eGPU training support on AS in general, I’d recommend check out MPS APIs"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "48a40405-71cb-48bb-b4e3-1f8658433acd",
          "type": "message",
          "user": "U04625ZF30T",
          "text": "MPS? I’m new to machine learning so don’t know all the abbreviations yet :sweat_smile:",
          "ts": "1666030125.827989",
          "thread_ts": "1666029739.500399",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "m+wrg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "MPS? I’m new to machine learning so don’t know all the abbreviations yet "
                    },
                    {
                      "type": "emoji",
                      "name": "sweat_smile",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "767edd87-eec4-4310-9408-ce8835d45e8a",
          "type": "message",
          "user": "U0457S2646Q",
          "text": "\u003chttps://developer.apple.com/documentation/metalperformanceshaders/|metal performance shader\u003e",
          "ts": "1666030164.734979",
          "thread_ts": "1666029739.500399",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "=8OH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/metalperformanceshaders/",
                      "text": "metal performance shader"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "bdae3196-72c4-424e-ba61-66152b091938",
          "type": "message",
          "user": "U04625ZF30T",
          "text": "Got it. Thanks! :+1:",
          "ts": "1666030193.082139",
          "thread_ts": "1666029739.500399",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Hn7",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Got it. Thanks! "
                    },
                    {
                      "type": "emoji",
                      "name": "+1",
                      "skin_tone": 0
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8e36b25a-ff7f-4268-b4ed-613b5544e937",
          "type": "message",
          "user": "U045ZS8FJN9",
          "text": "Beware, there is no support for eGPU on Apple Silicon",
          "ts": "1666087032.056319",
          "thread_ts": "1666029739.500399",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fQ+F",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Beware, there is no support for eGPU on Apple Silicon"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "943a225c-6ce4-4703-88d1-7728bf56d9a1",
          "type": "message",
          "user": "U04625ZF30T",
          "text": "Thanks for the heads up \u003c@U045ZS8FJN9\u003e",
          "ts": "1666087077.369549",
          "thread_ts": "1666029739.500399",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "7KGfA",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks for the heads up "
                    },
                    {
                      "type": "user",
                      "user_id": "U045ZS8FJN9"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "f93e4ce2-2f72-4102-bf09-81682750cacc",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "\u003c!here\u003e\n*Thank You!*\nWe're officially at time for the Core ML Q\u0026amp;A activity, however, please don't hesitate to keep asking your questions today and throughout the week. We'll be keeping the workflow active and the team will be answering any and all questions as they can before or after other scheduled activities in this channel.",
      "ts": "1666029966.277439",
      "team": "T03U5MWB2FN",
      "reactions": [
        {
          "name": "raised_hands",
          "count": 1,
          "users": [
            "U0462538BS7"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "3Au",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "broadcast",
                  "range": "here"
                },
                {
                  "type": "text",
                  "text": "\n"
                },
                {
                  "type": "text",
                  "text": "Thank You!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\nWe're officially at time for the Core ML Q\u0026A activity, however, please don't hesitate to keep asking your questions today and throughout the week. We'll be keeping the workflow active and the team will be answering any and all questions as they can before or after other scheduled activities in this channel."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "fdc4415c-3c86-411f-bb87-cd5cdfa8f037",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "*It's Q\u0026amp;A Time!*\nHello everyone! Welcome to Day 2 of \"Ask Apple.\" We have a great team of engineers here to answer any and all of your questions regarding Create ML! If you haven't already, submit your questions using the workflow and we'll kick off discussion threads with initial answers. We're looking forward to a fun and engaging hour.",
      "ts": "1666112543.583729",
      "team": "T03U5MWB2FN",
      "reactions": [
        {
          "name": "gratitude-thank-you",
          "count": 1,
          "users": [
            "U045LLLQHLM"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "stsqY",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "It's Q\u0026A Time!",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\nHello everyone! Welcome to Day 2 of \"Ask Apple.\" We have a great team of engineers here to answer any and all of your questions regarding Create ML! If you haven't already, submit your questions using the workflow and we'll kick off discussion threads with initial answers. We're looking forward to a fun and engaging hour."
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U0463MF1YK0\u003e asked\n\u0026gt; We can use ML model to answer question of flat price nowadays. What project should we use in Create ML for that purpose ?",
      "ts": "1666112556.325199",
      "thread_ts": "1666112556.325199",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 3,
      "latest_reply": "1666116809.541239",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "pI70",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U0463MF1YK0"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"We can use ML model to answer question of flat price nowadays. What project should we use in Create ML for that purpose ?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "034f9d62-57eb-4f14-85ad-5d803c1cc14a",
          "type": "message",
          "user": "U044QRXNB4P",
          "text": "I want to make sure I understand your question correctly. You're looking to predict something like property values, correct? Regardless, if you're looking to predict the price of something, the tabular regression tasks can help achieve this.\n\nIn the Create ML App this is the Tabular Regressor template. In the framework API, you can look at \u003chttps://developer.apple.com/documentation/createml/mlregressor|MLRegressor\u003e and the supported algorithms.",
          "ts": "1666112713.208659",
          "thread_ts": "1666112556.325199",
          "edited": {
            "user": "U044QRXNB4P",
            "ts": "1666112743.000000"
          },
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "white_check_mark",
              "count": 1,
              "users": [
                "U044JH9PSJ0"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hrvmp",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I want to make sure I understand your question correctly. You're looking to predict something like property values, correct? Regardless, if you're looking to predict the price of something, the tabular regression tasks can help achieve this.\n\nIn the Create ML App this is the Tabular Regressor template. In the framework API, you can look at "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createml/mlregressor",
                      "text": "MLRegressor"
                    },
                    {
                      "type": "text",
                      "text": " and the supported algorithms."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0e90257c-3e6e-462e-ba14-d700e3541816",
          "type": "message",
          "user": "U044JH9PSJ0",
          "text": "There's some great documentation for exactly this case - house pricing using a *Regressor* on the developer website:\n\n\u003chttps://developer.apple.com/documentation/createml/mlregressor\u003e",
          "ts": "1666116466.905209",
          "thread_ts": "1666112556.325199",
          "edited": {
            "user": "U044JH9PSJ0",
            "ts": "1666116496.000000"
          },
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ut4HD",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "There's some great documentation for exactly this case - house pricing using a "
                    },
                    {
                      "type": "text",
                      "text": "Regressor",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " on the developer website:\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createml/mlregressor",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "663a1ff5-e09b-4f9f-9a69-0a3b859916f0",
          "type": "message",
          "user": "U044JH9PSJ0",
          "text": "We also covered the house pricing case with the CreateML app in the (now a little old) WWDC video on Tabular Regressors\n\n\u003chttps://developer.apple.com/wwdc18/703|Introducing Create ML - WWDC18 - Videos - Apple Developer\u003e",
          "ts": "1666116809.541239",
          "thread_ts": "1666112556.325199",
          "attachments": [
            {
              "fallback": "Apple Developer: Introducing Create ML - WWDC18 - Videos - Apple Developer",
              "id": 1,
              "title": "Introducing Create ML - WWDC18 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/wwdc18/703",
              "text": "Create ML is a new framework designed to help you easily build machine learning models using Swift and Xcode. Designed for Simplicity and...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/42/2081/2081_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/wwdc18/703",
              "original_url": "https://developer.apple.com/wwdc18/703",
              "blocks": null
            }
          ],
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "3fbZ8",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "We also covered the house pricing case with the CreateML app in the (now a little old) WWDC video on Tabular Regressors\n\n"
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/wwdc18/703",
                      "text": "Introducing Create ML - WWDC18 - Videos - Apple Developer"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U0463MF1YK0\u003e asked\n\u0026gt; Apple has pre-trained image segmentation model (DeeplabV3), can it further train by a Create ML project?",
      "ts": "1666112589.262029",
      "thread_ts": "1666112589.262029",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 1,
      "latest_reply": "1666112923.807369",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "di60p",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U0463MF1YK0"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Apple has pre-trained image segmentation model (DeeplabV3), can it further train by a Create ML project?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a79c56c1-9076-4f36-8184-f90969730c54",
          "type": "message",
          "user": "U0457S2646Q",
          "text": "Retraining of DeeplabV3 or general neural network models is not supported by CreateML. Are you looking at specific use cases. Keep in mind you’d need to annotated images for training your own image segmentation model.",
          "ts": "1666112923.807369",
          "thread_ts": "1666112589.262029",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "dC+ea",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Retraining of DeeplabV3 or general neural network models is not supported by CreateML. Are you looking at specific use cases. Keep in mind you’d need to annotated images for training your own image segmentation model."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZG1KGBF\u003e asked\n\u0026gt; Do you have a suggestions for training an Activity Classifier when classifying actions in a team sporting event (ie... during a basketball game, classifying a shot or a pass)",
      "ts": "1666113083.301189",
      "thread_ts": "1666113083.301189",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 8,
      "latest_reply": "1666115094.801839",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "EaM+",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZG1KGBF"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Do you have a suggestions for training an Activity Classifier when classifying actions in a team sporting event (ie... during a basketball game, classifying a shot or a pass)\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "36913182-182b-4b3d-a27d-45d631a64864",
          "type": "message",
          "user": "U0457S2646Q",
          "text": "are you looking at use case to use motion data (accelerometer/gyro, eg.) or video frames, to classify these activities?",
          "ts": "1666113129.732149",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "u=tGO",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "are you looking at use case to use motion data (accelerometer/gyro, eg.) or video frames, to classify these activities?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b5cb7c17-2b6b-4510-9920-1a4e91ff2c54",
          "type": "message",
          "user": "U045ZG1KGBF",
          "text": "Video frames",
          "ts": "1666113161.364509",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kfc",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Video frames"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "647a86bd-4f88-4d12-9edd-67aa395861a7",
          "type": "message",
          "user": "U046V2T2W30",
          "text": "Here is an example of how you can get started with an Action Classifier in Create ML: \u003chttps://developer.apple.com/documentation/createml/creating-an-action-classifier-model\u003e",
          "ts": "1666113591.723179",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "jouw",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Here is an example of how you can get started with an Action Classifier in Create ML: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/documentation/createml/creating-an-action-classifier-model",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0613beeb-9a5b-4c37-9fce-7446de400507",
          "type": "message",
          "user": "U045ZG1KGBF",
          "text": "Thank You. So I was looking to be able to train an Action Classifier on actions that have more than a single person in the frame. The documentation states \"Make sure each example video clearly shows a single person performing the action\" , is there a way to train a model for such actions?",
          "ts": "1666113916.144849",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "oTvqU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank You. So I was looking to be able to train an Action Classifier on actions that have more than a single person in the frame. The documentation states \"Make sure each example video clearly shows a single person performing the action\" , is there a way to train a model for such actions?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "49372dbf-0683-4912-bffc-6499902e86cf",
          "type": "message",
          "user": "U046V2T2W30",
          "text": "For videos of multiple people, please ensure that the individual performing the action (that you want to classify) is the largest and most dominant person in the frame. That will enable you to train a model with input data that has more that a single person.",
          "ts": "1666114094.667719",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KLt/",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "For videos of multiple people, please ensure that the individual performing the action (that you want to classify) is the largest and most dominant person in the frame. That will enable you to train a model with input data that has more that a single person."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "cbcfa58b-49e9-4bfd-a8df-4640d2c078b8",
          "type": "message",
          "user": "U045ZG1KGBF",
          "text": "Thank you, just a quick follow up to that. so in my basketball example would  you crop the training videos to accomplish this or might there be a better way?",
          "ts": "1666114341.645119",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "zExX",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thank you, just a quick follow up to that. so in my basketball example would  you crop the training videos to accomplish this or might there be a better way?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "57d3b994-5f0b-4fde-a2c8-ab71fffc1088",
          "type": "message",
          "user": "U046V2T2W30",
          "text": "Create ML preprocesses each video frame to automatically detect the dominant individual. You may need to edit your training videos to ensure that the correct individual is in focus.",
          "ts": "1666114628.112609",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "5YR",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Create ML preprocesses each video frame to automatically detect the dominant individual. You may need to edit your training videos to ensure that the correct individual is in focus."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ddfd3891-a8ef-4654-a8f1-390d7d8dba9d",
          "type": "message",
          "user": "U046V2T2W30",
          "text": "It's also worth mentioning that the same preprocessing step happens at inference time.",
          "ts": "1666115094.801839",
          "thread_ts": "1666113083.301189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "KLU",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "It's also worth mentioning that the same preprocessing step happens at inference time."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZG1KGBF\u003e asked\n\u0026gt; Are there tradeoffs to increasing the grid size when training a object detection model for video ( 1080p Video so a grid size of 19 x10)",
      "ts": "1666113343.640849",
      "thread_ts": "1666113343.640849",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 5,
      "latest_reply": "1666114732.973869",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "C5yHi",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZG1KGBF"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Are there tradeoffs to increasing the grid size when training a object detection model for video ( 1080p Video so a grid size of 19 x10)\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "4f9c51f9-c15c-4a20-916f-518c1598f30a",
          "type": "message",
          "user": "U044QRV704X",
          "text": "Yes, the larger the grid size is - you are treating a bigger portion of a frame as one box. So, there is an accuracy trade off while getting a faster training and inference, if you increase the grid size.",
          "ts": "1666113373.659409",
          "thread_ts": "1666113343.640849",
          "edited": {
            "user": "U044QRV704X",
            "ts": "1666113478.000000"
          },
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "YVJx",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes, the larger the grid size is - you are treating a bigger portion of a frame as one box. So, there is an accuracy trade off while getting a faster training and inference, if you increase the grid size."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d97f5765-0f77-4fdd-a8c0-487a75e2f170",
          "type": "message",
          "user": "U044QRV704X",
          "text": "In essence, a coarser grid (fewer cells, but each of these cells are larger in size) will reduce accuracy at the benefit of faster inference. A denser grid (more cells, but each cells are smaller in size) will have the inverse effect.",
          "ts": "1666113855.096319",
          "thread_ts": "1666113343.640849",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "cUro=",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "In essence, a coarser grid (fewer cells, but each of these cells are larger in size) will reduce accuracy at the benefit of faster inference. A denser grid (more cells, but each cells are smaller in size) will have the inverse effect."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "bd9a5b6a-2d4a-474c-89cc-85d2e132a90d",
          "type": "message",
          "user": "U044QRV704X",
          "text": "So when you say \"increase the grid size\" - do you mean increasing the cells size in the grid or increasing the number of cells in the grid?",
          "ts": "1666113948.488969",
          "thread_ts": "1666113343.640849",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "WbCI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "So when you say \"increase the grid size\" - do you mean increasing the cells size in the grid or increasing the number of cells in the grid?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "39da0e31-0a54-43f8-80e2-934444353aaf",
          "type": "message",
          "user": "U045ZG1KGBF",
          "text": "I was referring to Grid Size parameter in CreateML, I would guess that parameter is increasing the number of cells in the grid but I am not sure",
          "ts": "1666114148.465689",
          "thread_ts": "1666113343.640849",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "/xxB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I was referring to Grid Size parameter in CreateML, I would guess that parameter is increasing the number of cells in the grid but I am not sure"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "a53cb3a0-c2c8-4490-a723-b20cd3241df5",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "\u003c@U045ZG1KGBF\u003e The grid size control in Create ML is setting the number of cells wide x number of cells high. So, higher numbers equate to more cells of smaller size. There's a great discussion of this in a video we dropped a couple of years ago if you didn't see it: \u003chttps://developer.apple.com/videos/play/tech-talks/10155/\u003e",
          "ts": "1666114732.973869",
          "thread_ts": "1666113343.640849",
          "attachments": [
            {
              "fallback": "Apple Developer: Improve Object Detection models in Create ML - Tech Talks - Videos - Apple Developer",
              "id": 1,
              "title": "Improve Object Detection models in Create ML - Tech Talks - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/tech-talks/10155/",
              "text": "When you train custom Core ML models for object detection in Create ML, you can bring image understanding to your app. Discover how...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/8/3559/3559_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/tech-talks/10155/",
              "original_url": "https://developer.apple.com/videos/play/tech-talks/10155/",
              "blocks": null
            }
          ],
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "08ez0",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U045ZG1KGBF"
                    },
                    {
                      "type": "text",
                      "text": " The grid size control in Create ML is setting the number of cells wide x number of cells high. So, higher numbers equate to more cells of smaller size. There's a great discussion of this in a video we dropped a couple of years ago if you didn't see it: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/tech-talks/10155/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U046TKW3B2L\u003e asked\n\u0026gt; Hello, I'm using ML in my iOS project. There's a sub-thread keep working to recogize image by MLModel when app running in background. It works well before in iOS15. But in iOS16, cpu rate always keeping about 300% when app running in background. Set computeUnit to cpuAndNeuralEngine, no improvement. Maybe iOS16 doesn't switch to neural-engine automatically. So how to reduce cpu rate, like iOS15, about 20%?\n\u0026gt; (Env: macOS11, Xcode13.2.1)",
      "ts": "1666113691.258349",
      "thread_ts": "1666113691.258349",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 3,
      "latest_reply": "1666142808.196349",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "YR9",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U046TKW3B2L"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hello, I'm using ML in my iOS project. There's a sub-thread keep working to recogize image by MLModel when app running in background. It works well before in iOS15. But in iOS16, cpu rate always keeping about 300% when app running in background. Set computeUnit to cpuAndNeuralEngine, no improvement. Maybe iOS16 doesn't switch to neural-engine automatically. So how to reduce cpu rate, like iOS15, about 20%?\\n(Env: macOS11, Xcode13.2.1)\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "d67fdbac-536e-450d-b102-0f0a10c7dbcd",
          "type": "message",
          "user": "U044JH9PSJ0",
          "text": "Hi, sounds like something changed, and now it's behaving in an undesirable way - but why?\n\nThere's a great way to investigate performance issues with your models. In 2022 we introduced the new CoreML Instrument.  You should be able to clearly see where the 300% is coming from, and hopefully get insight into how to fix it.\n\n\u0026gt; Profile your app to view Core ML API calls and associated models using the Core ML instrument. Find out where and when Core ML dispatches work to the hardware and gain more visibility with the Metal and new Neural Engine instruments.\n\nYou can find out more in the WWDC video \u003chttps://developer.apple.com/videos/play/wwdc2022/10027/\u003e",
          "ts": "1666113772.908109",
          "thread_ts": "1666113691.258349",
          "attachments": [
            {
              "fallback": "Apple Developer: Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Optimize your Core ML usage - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "text": "Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6520/6520_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
              "blocks": null
            }
          ],
          "files": [
            {
              "id": "F046M8NC9FZ",
              "created": 1666113765,
              "timestamp": 1666113765,
              "name": "Screenshot 2022-10-18 at 10.21.15.png",
              "title": "Screenshot 2022-10-18 at 10.21.15.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U044JH9PSJ0",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 1060702,
              "url": "",
              "url_download": "",
              "url_private": "C042X5L3V7X/F046M8NC9FZ-Screenshot 2022-10-18 at 10.21.15.png",
              "url_private_download": "C042X5L3V7X/F046M8NC9FZ-Screenshot 2022-10-18 at 10.21.15.png",
              "original_h": 672,
              "original_w": 2572,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 360,
              "thumb_360_h": 94,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_480.png",
              "thumb_480_w": 480,
              "thumb_480_h": 125,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_720.png",
              "thumb_720_w": 720,
              "thumb_720_h": 188,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_960.png",
              "thumb_960_w": 960,
              "thumb_960_h": 251,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F046M8NC9FZ-870e469987/screenshot_2022-10-18_at_10.21.15_1024.png",
              "thumb_1024_w": 1024,
              "thumb_1024_h": 268,
              "permalink": "https://appleevents.enterprise.slack.com/files/U044JH9PSJ0/F046M8NC9FZ/screenshot_2022-10-18_at_10.21.15.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F046M8NC9FZ-9598fbbf3f",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "AZfX6",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Hi, sounds like something changed, and now it's behaving in an undesirable way - but why?\n\nThere's a great way to investigate performance issues with your models. In 2022 we introduced the new CoreML Instrument.  You should be able to clearly see where the 300% is coming from, and hopefully get insight into how to fix it.\n\n\u003e Profile your app to view Core ML API calls and associated models using the Core ML instrument. Find out where and when Core ML dispatches work to the hardware and gain more visibility with the Metal and new Neural Engine instruments.\n\nYou can find out more in the WWDC video "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10027/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "717ed902-b5ad-4c4b-9d2a-0849cec568c5",
          "type": "message",
          "user": "U044JH9PSJ0",
          "text": "Instruments can help you find the issue, but it seems like you have found a bug, and we would like to take a closer look to make sure you and others have a great experience with performance.\n\nI would appreciate it if you can file a feedback bug report to \u003chttp://feedbackassistant.apple.com|feedbackassistant.apple.com\u003e or using the feedback assistant app on your phone.\n\nTo help us diagnose the issue feedback reports should include:\n• what you expected, and what actually happened\n• steps to reproduce\n• Use feedback assistant on your phone while the 300% is occurring.  This will gather relevant logs and record hardware and software version \nThe very best feedback reports would also include the following details (if you feel comfortable sharing)\n• include a code sample of how you invoke the model \n• include the model you have built\n• include the instruments trace you capture from above.",
          "ts": "1666114885.328979",
          "thread_ts": "1666113691.258349",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "0MOEW",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Instruments can help you find the issue, but it seems like you have found a bug, and we would like to take a closer look to make sure you and others have a great experience with performance.\n\nI would appreciate it if you can file a feedback bug report to "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": "feedbackassistant.apple.com"
                    },
                    {
                      "type": "text",
                      "text": " or using the feedback assistant app on your phone.\n\nTo help us diagnose the issue feedback reports should include:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"what you expected, and what actually happened\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"steps to reproduce\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"Use feedback assistant on your phone while the 300% is occurring.  This will gather relevant logs and record hardware and software version \"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "\nThe very best feedback reports would also include the following details (if you feel comfortable sharing)\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"include a code sample of how you invoke the model \"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"include the model you have built\"}]},{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"text\",\"text\":\"include the instruments trace you capture from above.\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "b1f4fa30-052d-483b-8616-755a949e6e2a",
          "type": "message",
          "user": "U046TKW3B2L",
          "text": "Thanks, I'll try it.",
          "ts": "1666142808.196349",
          "thread_ts": "1666113691.258349",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "fZb/G",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks, I'll try it."
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "f0d7e482-2ad9-467b-a3d0-65b762fc7772",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "*Keep the questions coming!* :speech_balloon:\nThanks for all the great questions and feedback that you've already submitted. Keep them coming! While we may not get to everything in the few minutes we have remaining in this activity, the workflow will stay open throughout the rest of the day. You can ask your questions at anytime. Our team will pick those up and provide answers during our next activity in the channel tomorrow: _Vision Q\u0026amp;A_ from 10:00-11:00 AM Cupertino Time. :pray:",
      "ts": "1666115571.755719",
      "team": "T03U5MWB2FN",
      "reactions": [
        {
          "name": "+1",
          "count": 2,
          "users": [
            "U0455CXHDNW",
            "U045PGRV08K"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "imr",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "Keep the questions coming! ",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "emoji",
                  "name": "speech_balloon",
                  "skin_tone": 0,
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\nThanks for all the great questions and feedback that you've already submitted. Keep them coming! While we may not get to everything in the few minutes we have remaining in this activity, the workflow will stay open throughout the rest of the day. You can ask your questions at anytime. Our team will pick those up and provide answers during our next activity in the channel tomorrow: "
                },
                {
                  "type": "text",
                  "text": "Vision Q\u0026A ",
                  "style": {
                    "italic": true
                  }
                },
                {
                  "type": "text",
                  "text": "from 10:00-11:00 AM Cupertino Time. "
                },
                {
                  "type": "emoji",
                  "name": "pray",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U046S70K9A4\u003e asked\n\u0026gt; I just created a sample audio classifier in Create ML. When I drag an audio file to the training data box, it highlights like it's going to accept it. But then nothing happens after I drop the file there.",
      "ts": "1666116032.658899",
      "thread_ts": "1666116032.658899",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 6,
      "latest_reply": "1666116391.768609",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "SP=",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U046S70K9A4"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I just created a sample audio classifier in Create ML. When I drag an audio file to the training data box, it highlights like it's going to accept it. But then nothing happens after I drop the file there.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "b9033346-00fc-4fd7-beb9-79bee2e2b7c4",
          "type": "message",
          "user": "U0455CXHDNW",
          "text": "The accepted dataset format for sound classification is a folder with labeled classes. Each folder will contain sound files.",
          "ts": "1666116126.187309",
          "thread_ts": "1666116032.658899",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U046S70K9A4"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EXxeb",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The accepted dataset format for sound classification is a folder with labeled classes. Each folder will contain sound files."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dd859dfd-bce1-4c8a-a9ae-ccef27e4471c",
          "type": "message",
          "user": "U0455CXHDNW",
          "text": "If you drag it over Training Data, it will enable the Train button and you will be able to train a Sound Classifier.",
          "ts": "1666116159.291229",
          "thread_ts": "1666116032.658899",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ai+4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "If you drag it over Training Data, it will enable the Train button and you will be able to train a Sound Classifier."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "0469244d-8845-4ff7-b115-9dad62b6855d",
          "type": "message",
          "user": "U0455CXHDNW",
          "text": "I would appreciate it if you can file a feedback bug report to \u003chttp://feedbackassistant.apple.com|feedbackassistant.apple.com\u003e or using the feedback assistant app on your phone. We would like to improve your experience.",
          "ts": "1666116199.873509",
          "thread_ts": "1666116032.658899",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "cnH1y",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "I would appreciate it if you can file a feedback bug report to "
                    },
                    {
                      "type": "link",
                      "url": "http://feedbackassistant.apple.com",
                      "text": "feedbackassistant.apple.com"
                    },
                    {
                      "type": "text",
                      "text": " or using the feedback assistant app on your phone. We would like to improve your experience."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "23e4abc2-6f5d-4362-8598-0dc09061879d",
          "type": "message",
          "user": "U046S70K9A4",
          "text": ":pray: thanks for taking my question, will do !",
          "ts": "1666116305.666929",
          "thread_ts": "1666116032.658899",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lf=3",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "emoji",
                      "name": "pray",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " thanks for taking my question, will do !"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dcb25ba1-8908-479e-8b2f-0e3d07cea4a4",
          "type": "message",
          "user": "U044JH9PSJ0",
          "text": "Yeah, for example, a good data set would be a set of folders and files like this:\n\n:file_folder: My Training Data _(drag this_ :point_left: _to CreateML app)_\n  :file_folder: Bark  \n    :loud_sound: Woof1.aiff\n    :loud_sound: _Woof2.aiff_  \n    :loud_sound: _Woof3.aiff_  \n    _…_\n  :file_folder: _Meow_  \n    :loud_sound: _Meow1.aiff_\n    :loud_sound: _Meow2.aiff_  \n    :loud_sound: _Purr1.aiff_  \n    …",
          "ts": "1666116357.507429",
          "thread_ts": "1666116032.658899",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "raised_hands",
              "count": 1,
              "users": [
                "U046S70K9A4"
              ]
            },
            {
              "name": "heart",
              "count": 1,
              "users": [
                "U044QRXNB4P"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "09L",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yeah, for example, a good data set would be a set of folders and files like this:\n\n"
                    },
                    {
                      "type": "emoji",
                      "name": "file_folder",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " My Training Data "
                    },
                    {
                      "type": "text",
                      "text": "(drag this ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "emoji",
                      "name": "point_left",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " to CreateML app)",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "text",
                      "text": "  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "emoji",
                      "name": "file_folder",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "Bark"
                    },
                    {
                      "type": "text",
                      "text": "  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n    "
                    },
                    {
                      "type": "emoji",
                      "name": "loud_sound",
                      "skin_tone": 0
                    },
                    {
                      "type": "text",
                      "text": " Woof1.aiff\n"
                    },
                    {
                      "type": "text",
                      "text": "    ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "emoji",
                      "name": "loud_sound",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " Woof2.aiff  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "text",
                      "text": "    ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "emoji",
                      "name": "loud_sound",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " Woof3.aiff  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "text",
                      "text": "    …",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n"
                    },
                    {
                      "type": "text",
                      "text": "  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "emoji",
                      "name": "file_folder",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " Meow  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n    "
                    },
                    {
                      "type": "emoji",
                      "name": "loud_sound",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " Meow1.aiff",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n    "
                    },
                    {
                      "type": "emoji",
                      "name": "loud_sound",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " Meow2.aiff  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n    "
                    },
                    {
                      "type": "emoji",
                      "name": "loud_sound",
                      "skin_tone": 0,
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " Purr1.aiff  ",
                      "style": {
                        "italic": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\n    …"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "83187580-26e9-47f6-b3f1-c54ae981cde8",
          "type": "message",
          "user": "U046S70K9A4",
          "text": "amazing, that’s so helpful !",
          "ts": "1666116391.768609",
          "thread_ts": "1666116032.658899",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "csrNa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "amazing, that’s so helpful !"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U046T9VPPAL\u003e asked\n\u0026gt; Is it possible to have a user-trained model? Not a model that I have data on beforehand, and then input into the app, but a model that gets trained as the user interacts with the app, and adjusts real-time? Assuming it would be stored in the user's DocumentsDirectory. Is this a thing?",
      "ts": "1666116790.294189",
      "thread_ts": "1666116790.294189",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 3,
      "latest_reply": "1666193669.034839",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "vfe",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U046T9VPPAL"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Is it possible to have a user-trained model? Not a model that I have data on beforehand, and then input into the app, but a model that gets trained as the user interacts with the app, and adjusts real-time? Assuming it would be stored in the user's DocumentsDirectory. Is this a thing?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "a436122d-ee3c-42d0-a5a5-3264236bdad8",
          "type": "message",
          "user": "U0452E161DK",
          "text": "Yes. It is possible. You can use the Create ML or Create ML components framework to do so. You may find this WWDC session helpful:\n• \u003chttps://developer.apple.com/videos/play/wwdc2021/10037/\u003e",
          "ts": "1666116862.642729",
          "thread_ts": "1666116790.294189",
          "attachments": [
            {
              "fallback": "Apple Developer: Build dynamic iOS apps with the Create ML framework - WWDC21 - Videos - Apple Developer",
              "id": 1,
              "title": "Build dynamic iOS apps with the Create ML framework - WWDC21 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2021/10037/",
              "text": "Discover how your app can train Core ML models fully on device with the Create ML framework, enabling adaptive and customized app...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/119/4927/4927_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2021/10037/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2021/10037/",
              "blocks": null
            }
          ],
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "hIg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Yes. It is possible. You can use the Create ML or Create ML components framework to do so. You may find this WWDC session helpful:\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_list",
                  "Raw": "{\"type\":\"rich_text_list\",\"elements\":[{\"type\":\"rich_text_section\",\"elements\":[{\"type\":\"link\",\"url\":\"https:\\/\\/developer.apple.com\\/videos\\/play\\/wwdc2021\\/10037\\/\"}]}],\"style\":\"bullet\",\"indent\":0,\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "336d6b30-d233-4071-87f7-86fd6fa95753",
          "type": "message",
          "user": "U0452E161DK",
          "text": "That session shows examples with the Create ML task level APIs but you can also use the Create ML Components framework if you need more flexibility and customization: \u003chttps://developer.apple.com/videos/play/wwdc2022/10019/\u003e",
          "ts": "1666116901.397429",
          "thread_ts": "1666116790.294189",
          "attachments": [
            {
              "fallback": "Apple Developer: Get to know Create ML Components - WWDC22 - Videos - Apple Developer",
              "id": 1,
              "title": "Get to know Create ML Components - WWDC22 - Videos - Apple Developer",
              "title_link": "https://developer.apple.com/videos/play/wwdc2022/10019/",
              "text": "Create ML makes it easy to build custom machine learning models for image classification, object detection, sound classification, hand...",
              "image_url": "https://devimages-cdn.apple.com/wwdc-services/images/124/6512/6512_wide_250x141_2x.jpg",
              "service_name": "Apple Developer",
              "service_icon": "https://developer.apple.com/favicon.ico",
              "from_url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
              "original_url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
              "blocks": null
            }
          ],
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Voj",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "That session shows examples with the Create ML task level APIs but you can also use the Create ML Components framework if you need more flexibility and customization: "
                    },
                    {
                      "type": "link",
                      "url": "https://developer.apple.com/videos/play/wwdc2022/10019/",
                      "text": ""
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "8c24bc94-4a3b-4fb9-a59c-fc6d1c560cbf",
          "type": "message",
          "user": "U046T9VPPAL",
          "text": "Awesome! thanks for the resources!",
          "ts": "1666193669.034839",
          "thread_ts": "1666116790.294189",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "QViHa",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Awesome! thanks for the resources!"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045Y19K154\u003e asked\n\u0026gt; I'm not positive that this question pertains to Vision, I have a question about the new subject removal feature in ios 16 (see e.g. \u003chttps://www.macrumors.com/how-to/isolate-copy-share-subjects-photos/|https://www.macrumors.com/how-to/isolate-copy-share-subjects-photos/\u003e ). It works great in Photos etc., and I've built a shortcut to call it, but I can't see how to call it directly from Swift - is this possible? Thanks",
      "ts": "1666198956.467119",
      "thread_ts": "1666198956.467119",
      "attachments": [
        {
          "fallback": "MacRumors: iOS 16: How to Isolate, Copy, and Share Subjects From Photos",
          "id": 1,
          "title": "iOS 16: How to Isolate, Copy, and Share Subjects From Photos",
          "title_link": "https://www.macrumors.com/how-to/isolate-copy-share-subjects-photos/",
          "text": "In iOS 16, Apple has enhanced its Visual Look Up capability for photos by making it possible to isolate the subject of a picture from its background...",
          "image_url": "https://images.macrumors.com/t/3YcgikpAi6WlY9K5WA5FOt-jnfk=/1600x/article-new/2022/06/isolate-subject-ios-16.jpg",
          "service_name": "MacRumors",
          "service_icon": "https://images.macrumors.com/images-new/apple-touch-icon.png?v=01",
          "from_url": "https://www.macrumors.com/how-to/isolate-copy-share-subjects-photos/",
          "original_url": "https://www.macrumors.com/how-to/isolate-copy-share-subjects-photos/",
          "blocks": null
        }
      ],
      "edited": {
        "user": "B043H8DCKQ9",
        "ts": "1666198957.000000"
      },
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 8,
      "latest_reply": "1666203701.983889",
      "reactions": [
        {
          "name": "+1",
          "count": 1,
          "users": [
            "U045VNN3MPY"
          ]
        }
      ],
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "EOU",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045Y19K154"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I'm not positive that this question pertains to Vision, I have a question about the new subject removal feature in ios 16 (see e.g. \"},{\"type\":\"link\",\"url\":\"https:\\/\\/www.macrumors.com\\/how-to\\/isolate-copy-share-subjects-photos\\/\",\"text\":\"https:\\/\\/www.macrumors.com\\/how-to\\/isolate-copy-share-subjects-photos\\/\"},{\"type\":\"text\",\"text\":\" ). It works great in Photos etc., and I've built a shortcut to call it, but I can't see how to call it directly from Swift - is this possible? Thanks\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "8d6fb81d-7b27-4034-9d24-d6185a6e2dfd",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "Thanks Derick for the interest in this feature. There is no API for Copy Subject or Remove Background unfortunately .",
          "ts": "1666199118.884059",
          "thread_ts": "1666198956.467119",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "e=J4",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks Derick for the interest in this feature. There is no API for Copy Subject or Remove Background unfortunately ."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "1e4a2434-2086-48fe-afac-7b90079f2887",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "You might want to look at the VNPersonSegmentationRequest as an alternative that allows you to do Copy Subject for people.",
          "ts": "1666199335.888569",
          "thread_ts": "1666198956.467119",
          "team": "T03U5MWB2FN",
          "reactions": [
            {
              "name": "+1",
              "count": 1,
              "users": [
                "U045VNN3MPY"
              ]
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "LPI",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You might want to look at the VNPersonSegmentationRequest as an alternative that allows you to do Copy Subject for people."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d17d8e11-71a3-4fd0-a2af-c7664c0585d4",
          "type": "message",
          "user": "U045Y19K154",
          "text": "Ooooh really?!? It’s so wonderful - I’d love to add it to my app…..This is getting off-topic but is it possible to have Swift call a Shortcut (i.e. from the Shortcuts app) and capture the result?",
          "ts": "1666199369.923449",
          "thread_ts": "1666198956.467119",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "elH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Ooooh really?!? It’s so wonderful - I’d love to add it to my app…..This is getting off-topic but is it possible to have Swift call a Shortcut (i.e. from the Shortcuts app) and capture the result?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "d185f0a3-e4ca-4781-ac7b-a37bc0bd8bd0",
          "type": "message",
          "user": "U045Y19K154",
          "text": "Thanks - I really want to use it for clothes - e.g.",
          "ts": "1666199534.014239",
          "thread_ts": "1666198956.467119",
          "files": [
            {
              "id": "F047598C9T7",
              "created": 1666199506,
              "timestamp": 1666199506,
              "name": "IMG_8126 Background Removed.png",
              "title": "IMG_8126 Background Removed.png",
              "mimetype": "image/png",
              "image_exif_rotation": 1,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U045Y19K154",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 1854829,
              "url": "",
              "url_download": "",
              "url_private": "C042X5L3V7X/F047598C9T7-IMG_8126 Background Removed.png",
              "url_private_download": "C042X5L3V7X/F047598C9T7-IMG_8126 Background Removed.png",
              "original_h": 2049,
              "original_w": 1536,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 270,
              "thumb_360_h": 360,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_480.png",
              "thumb_480_w": 360,
              "thumb_480_h": 480,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_720.png",
              "thumb_720_w": 540,
              "thumb_720_h": 720,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_960.png",
              "thumb_960_w": 720,
              "thumb_960_h": 960,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047598C9T7-56934bafae/img_8126_background_removed_1024.png",
              "thumb_1024_w": 768,
              "thumb_1024_h": 1024,
              "permalink": "https://appleevents.enterprise.slack.com/files/U045Y19K154/F047598C9T7/img_8126_background_removed.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F047598C9T7-8bdd0a454b",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            },
            {
              "id": "F047AMC9W4C",
              "created": 1666199512,
              "timestamp": 1666199512,
              "name": "IMG_8126 pmp.png",
              "title": "IMG_8126 pmp.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U045Y19K154",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 4144934,
              "url": "",
              "url_download": "",
              "url_private": "C042X5L3V7X/F047AMC9W4C-IMG_8126 pmp.png",
              "url_private_download": "C042X5L3V7X/F047AMC9W4C-IMG_8126 pmp.png",
              "original_h": 2049,
              "original_w": 1536,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 270,
              "thumb_360_h": 360,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_480.png",
              "thumb_480_w": 360,
              "thumb_480_h": 480,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_720.png",
              "thumb_720_w": 540,
              "thumb_720_h": 720,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_960.png",
              "thumb_960_w": 720,
              "thumb_960_h": 960,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F047AMC9W4C-f4d8248d00/img_8126_pmp_1024.png",
              "thumb_1024_w": 768,
              "thumb_1024_h": 1024,
              "permalink": "https://appleevents.enterprise.slack.com/files/U045Y19K154/F047AMC9W4C/img_8126_pmp.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F047AMC9W4C-3352a1cc90",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "g+G",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks - I really want to use it for clothes - e.g."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "fc05414b-69e6-43f9-8aa6-f9d9ef813b37",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "Sorry that is something I am not familiar with. But person segmentation is easy to use it is just limited to persons not clothes.",
          "ts": "1666199547.931859",
          "thread_ts": "1666198956.467119",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "aIT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Sorry that is something I am not familiar with. But person segmentation is easy to use it is just limited to persons not clothes."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "551c61f4-8148-4cd2-a426-c9f2049dd17c",
          "type": "message",
          "user": "U0455QUCD9N",
          "text": "\u003c@U045Y19K154\u003e Regarding shortcuts, I encourage you to drop that question into the \u003c#C042ZRZM21Z|\u003e channel (if you signed up for it). We have Engineers from that team on deck to discuss all things related to Shortcuts this afternoon.",
          "ts": "1666200285.407729",
          "thread_ts": "1666198956.467119",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "n90oG",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "user",
                      "user_id": "U045Y19K154"
                    },
                    {
                      "type": "text",
                      "text": " Regarding shortcuts, I encourage you to drop that question into the "
                    },
                    {
                      "type": "channel",
                      "channel_id": "C042ZRZM21Z"
                    },
                    {
                      "type": "text",
                      "text": " channel (if you signed up for it). We have Engineers from that team on deck to discuss all things related to Shortcuts this afternoon."
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "ccfe97be-37e6-4043-8841-0e5dd3eb9283",
          "type": "message",
          "user": "U045Y19K154",
          "text": "Thanks - I also have a working model in Create ML that can identify clothing (it can tell this is a pair of pants with 99%% confidence) - is there a way to extract just the identified image?",
          "ts": "1666200863.246589",
          "thread_ts": "1666198956.467119",
          "files": [
            {
              "id": "F0471NS8CJJ",
              "created": 1666200686,
              "timestamp": 1666200686,
              "name": "IMG_8125.png",
              "title": "IMG_8125.png",
              "mimetype": "image/png",
              "image_exif_rotation": 0,
              "filetype": "png",
              "pretty_type": "PNG",
              "user": "U045Y19K154",
              "mode": "hosted",
              "editable": false,
              "is_external": false,
              "external_type": "",
              "size": 4126762,
              "url": "",
              "url_download": "",
              "url_private": "C042X5L3V7X/F0471NS8CJJ-IMG_8125.png",
              "url_private_download": "C042X5L3V7X/F0471NS8CJJ-IMG_8125.png",
              "original_h": 2049,
              "original_w": 1536,
              "thumb_64": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_64.png",
              "thumb_80": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_80.png",
              "thumb_160": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_160.png",
              "thumb_360": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_360.png",
              "thumb_360_gif": "",
              "thumb_360_w": 270,
              "thumb_360_h": 360,
              "thumb_480": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_480.png",
              "thumb_480_w": 360,
              "thumb_480_h": 480,
              "thumb_720": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_720.png",
              "thumb_720_w": 540,
              "thumb_720_h": 720,
              "thumb_960": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_960.png",
              "thumb_960_w": 720,
              "thumb_960_h": 960,
              "thumb_1024": "https://files.slack.com/files-tmb/T01PTBJ95PS-F0471NS8CJJ-0ef346679b/img_8125_1024.png",
              "thumb_1024_w": 768,
              "thumb_1024_h": 1024,
              "permalink": "https://appleevents.enterprise.slack.com/files/U045Y19K154/F0471NS8CJJ/img_8125.png",
              "permalink_public": "https://slack-files.com/T01PTBJ95PS-F0471NS8CJJ-936d51bdb4",
              "edit_link": "",
              "preview": "",
              "preview_highlight": "",
              "lines": 0,
              "lines_more": 0,
              "is_public": false,
              "public_url_shared": false,
              "channels": null,
              "groups": null,
              "ims": null,
              "initial_comment": {},
              "comments_count": 0,
              "num_stars": 0,
              "is_starred": false,
              "shares": {
                "public": null,
                "private": null
              }
            }
          ],
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "Ny6OB",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Thanks - I also have a working model in Create ML that can identify clothing (it can tell this is a pair of pants with 99%% confidence) - is there a way to extract just the identified image?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "17b2bac9-6c44-4fd1-9759-6f91c6d23075",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "Detectors and segmentation models are different. You could try the VNGenerateObjectnessBasedSaliencyRequest but you would need to upsample the mask",
          "ts": "1666203701.983889",
          "thread_ts": "1666198956.467119",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "C/fn",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Detectors and segmentation models are different. You could try the VNGenerateObjectnessBasedSaliencyRequest but you would need to upsample the mask"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "client_msg_id": "42f2ac98-d52d-4e6f-9844-2e70221609b3",
      "type": "message",
      "user": "U0455QUCD9N",
      "text": "*It's Q\u0026amp;A Time! Let's talk about Vision.*\nWelcome all to Day 3 of Ask Apple. For the next hour we've got a team of Engineers from the Vision team here to answer all your questions around this broad and powerful framework. Use the :heavy_plus_sign: button to kick off the workflow. And don't hesitate to follow-up in discussion using the threads that are created. We're looking forward to a lively conversation. :tada:",
      "ts": "1666199011.967139",
      "team": "T03U5MWB2FN",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "=cFzT",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "text",
                  "text": "It's Q\u0026A Time! Let's talk about Vision.",
                  "style": {
                    "bold": true
                  }
                },
                {
                  "type": "text",
                  "text": "\nWelcome all to Day 3 of Ask Apple. For the next hour we've got a team of Engineers from the Vision team here to answer all your questions around this broad and powerful framework. Use the "
                },
                {
                  "type": "emoji",
                  "name": "heavy_plus_sign",
                  "skin_tone": 0
                },
                {
                  "type": "text",
                  "text": " button to kick off the workflow. And don't hesitate to follow-up in discussion using the threads that are created. We're looking forward to a lively conversation. "
                },
                {
                  "type": "emoji",
                  "name": "tada",
                  "skin_tone": 0
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045YRUL42J\u003e asked\n\u0026gt; What are these types of operations in an MLProgram? `input_3_cast__@casted_to__@1`\n\u0026gt; \n\u0026gt; The sometimes appear in the operations list of the Performance report in XCode and I couldn't quite figure out under what circumstances they appear. Seems to be some type of casting, probably from `float16` to `float32` or the other way round. But why are these casts necessary? Do they hurt model performance? \n\u0026gt; \n\u0026gt; Could you shed some light on this?",
      "ts": "1666199460.031689",
      "thread_ts": "1666199460.031689",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 1,
      "latest_reply": "1666199594.805109",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "41I",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045YRUL42J"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"What are these types of operations in an MLProgram? `input_3_cast__@casted_to__@1`\\n\\nThe sometimes appear in the operations list of the Performance report in XCode and I couldn't quite figure out under what circumstances they appear. Seems to be some type of casting, probably from `float16` to `float32` or the other way round. But why are these casts necessary? Do they hurt model performance? \\n\\nCould you shed some light on this?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "dc90190e-654a-4123-a2b1-52367de4b6ff",
          "type": "message",
          "user": "U044G434P4J",
          "text": "These could be happening due to the float32 to float16 casting of intermediate tensors that happens by default.  This is done by default to increase the residency of the model on the neural engine, to improve performance. You can see this page for more details: \u003chttps://coremltools.readme.io/docs/typed-execution\u003e\nIf you convert the model with `compute_precision=ct.precision.FLOAT32` most likely you wont see many casts.\n(some time the casts maybe inserted during conversion to map to the dtypes that are supported by CoreML layers)",
          "ts": "1666199594.805109",
          "thread_ts": "1666199460.031689",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "mdV",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "These could be happening due to the float32 to float16 casting of intermediate tensors that happens by default.  This is done by default to increase the residency of the model on the neural engine, to improve performance. You can see this page for more details: "
                    },
                    {
                      "type": "link",
                      "url": "https://coremltools.readme.io/docs/typed-execution",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": "\nIf you convert the model with "
                    },
                    {
                      "type": "text",
                      "text": "compute_precision=ct.precision.FLOAT32",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " most likely you wont see many casts.\n(some time the casts maybe inserted during conversion to map to the dtypes that are supported by CoreML layers)"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045ZG1KGBF\u003e asked\n\u0026gt; I am using VNDetectTrajectoryRequest to track a hockey puck being shot but I am having trouble getting observations that go from the center of the screen to center-left. I seem to get the observations if the puck travels from center of the screen to center-right, the trajectory length is set to 5 and I have tried it with and without any type of filtering and still do not get any observations in  the center to center-left direction. Any suggestions on things I might be able to try?",
      "ts": "1666199670.073659",
      "thread_ts": "1666199670.073659",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 3,
      "latest_reply": "1666200698.192099",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "YNycm",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045ZG1KGBF"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"I am using VNDetectTrajectoryRequest to track a hockey puck being shot but I am having trouble getting observations that go from the center of the screen to center-left. I seem to get the observations if the puck travels from center of the screen to center-right, the trajectory length is set to 5 and I have tried it with and without any type of filtering and still do not get any observations in  the center to center-left direction. Any suggestions on things I might be able to try?\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "4e86f3a4-0f2f-44e0-88d4-16fe0fdcd9a4",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "The direction should not matter as long as the trajectories are not too short and they follow any trajectory",
          "ts": "1666199783.718849",
          "thread_ts": "1666199670.073659",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "uPySg",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The direction should not matter as long as the trajectories are not too short and they follow any trajectory"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "dd8343f6-9d05-4a61-9282-3d9eb0b61963",
          "type": "message",
          "user": "U045ZG1KGBF",
          "text": "Are there ways to understand what may be too short or is it something that is just trial and error?",
          "ts": "1666200499.438779",
          "thread_ts": "1666199670.073659",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "lUH",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Are there ways to understand what may be too short or is it something that is just trial and error?"
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "653a27c8-616d-4307-81c7-f4ae3477d111",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "The length of for instance 5 means that the object could be tracked over 5 consecutive frames. If it is less than that, the trajectory will not be returned",
          "ts": "1666200698.192099",
          "thread_ts": "1666199670.073659",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "EVT",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "The length of for instance 5 means that the object could be tracked over 5 consecutive frames. If it is less than that, the trajectory will not be returned"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U045YRUL42J\u003e asked\n\u0026gt; How do I properly create a CoreML model with multiple outputs in PyTorch?\n\u0026gt; \n\u0026gt; When I just return a tuple of tensors in my last PyTorch `forward` call then I receive a warning `WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.`.\n\u0026gt; \n\u0026gt; When I view the model in Netron it correctly displays two outputs. However, if I run the model performance test in the \"Performance\" tab in XCode it fails with \"Error on model declaration\".\n\u0026gt; \n\u0026gt; (The same model passes the test successfully if I concatenate the two output tensors and make them available through a single output.)",
      "ts": "1666202704.182499",
      "thread_ts": "1666202704.182499",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 2,
      "latest_reply": "1666203022.612399",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "xnNRh",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U045YRUL42J"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"How do I properly create a CoreML model with multiple outputs in PyTorch?\\n\\nWhen I just return a tuple of tensors in my last PyTorch `forward` call then I receive a warning `WARNING\"},{\"type\":\"emoji\",\"name\":\"root\"},{\"type\":\"text\",\"text\":\"Tuple detected at graph output. This will be flattened in the converted model.`.\\n\\nWhen I view the model in Netron it correctly displays two outputs. However, if I run the model performance test in the \\\"Performance\\\" tab in XCode it fails with \\\"Error on model declaration\\\".\\n\\n(The same model passes the test successfully if I concatenate the two output tensors and make them available through a single output.)\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "feb61af6-d078-4df6-b6a9-e78c7a5fc409",
          "type": "message",
          "user": "U044G434P4J",
          "text": "Instead of concatenating, have you tried the pytorch module return the outputs as is, instead of wrapping them in a tuple?\nThat is, doing\n```def forward():\n     ....\n     return x, y```\nInstead of\n```def forward():\n     ....\n     return (x, y)```",
          "ts": "1666202782.625579",
          "thread_ts": "1666202704.182499",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "1Tm",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Instead of concatenating, have you tried the pytorch module return the outputs as is, instead of wrapping them in a tuple?\nThat is, doing\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"def forward():\\n     ....\\n     return x, y\"}],\"border\":0}"
                },
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Instead of\n"
                    }
                  ]
                },
                {
                  "Type": "rich_text_preformatted",
                  "Raw": "{\"type\":\"rich_text_preformatted\",\"elements\":[{\"type\":\"text\",\"text\":\"def forward():\\n     ....\\n     return (x, y)\"}],\"border\":0}"
                }
              ]
            }
          ]
        },
        {
          "client_msg_id": "171140f2-4c44-442b-a61f-b656b0a98f23",
          "type": "message",
          "user": "U044G434P4J",
          "text": "Actually if it gave the warning, it `This will be flattened in the converted model.` it should have just worked correctly instead of giving the error `Error on model declaration`\nCan you file an issue please at \u003chttps://github.com/apple/coremltools/issues\u003e, if you can attach your code to reproduce the error, would be great (the code that works with output concat but does not without it).\nBtw, do you see the “Error on model declaration” error on Xcode perf tab and not on calling predict from coremltools? Or is that error present in both cases?",
          "ts": "1666203022.612399",
          "thread_ts": "1666202704.182499",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "kUo",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "Actually if it gave the warning, it "
                    },
                    {
                      "type": "text",
                      "text": "This will be flattened in the converted model.",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " it should have just worked correctly instead of giving the error "
                    },
                    {
                      "type": "text",
                      "text": "Error on model declaration",
                      "style": {
                        "code": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "\nCan you file an issue please at "
                    },
                    {
                      "type": "link",
                      "url": "https://github.com/apple/coremltools/issues",
                      "text": ""
                    },
                    {
                      "type": "text",
                      "text": ", if you can attach your code to reproduce the error, would be great (the code that works with output concat but does not without it).\nBtw, do you see the “Error on model declaration” error on Xcode perf tab and not on calling predict from coremltools? Or is that error present in both cases?"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "type": "message",
      "text": "\u003c@U046GJ0FC6M\u003e asked\n\u0026gt; Hi.\n\u0026gt; When you run a VNRecognizeTextRequest with recognitionLevel “accurate” Vison returns an array of VNRecognizedTextObservation, each with the corresponding String and BoundingBox. However when you browse through each character of the String and try to get their BoundingBox, Vision only returns the BoundingBox of the whole String. \n\u0026gt; Is there a way to get the BoundingBox for each character of the String?\n\u0026gt; If not do you intend to add this in a future release?\n\u0026gt; \n\u0026gt; Thanks.",
      "ts": "1666203356.811059",
      "thread_ts": "1666203356.811059",
      "subtype": "bot_message",
      "bot_id": "B043H8DCKQ9",
      "username": "Ask Apple - machine-learning",
      "reply_count": 1,
      "latest_reply": "1666203452.753129",
      "replace_original": false,
      "delete_original": false,
      "blocks": [
        {
          "type": "rich_text",
          "block_id": "sJrfR",
          "elements": [
            {
              "type": "rich_text_section",
              "elements": [
                {
                  "type": "user",
                  "user_id": "U046GJ0FC6M"
                },
                {
                  "type": "text",
                  "text": " asked\n"
                }
              ]
            },
            {
              "Type": "rich_text_quote",
              "Raw": "{\"type\":\"rich_text_quote\",\"elements\":[{\"type\":\"text\",\"text\":\"Hi.\\nWhen you run a VNRecognizeTextRequest with recognitionLevel \\u201caccurate\\u201d Vison returns an array of VNRecognizedTextObservation, each with the corresponding String and BoundingBox. However when you browse through each character of the String and try to get their BoundingBox, Vision only returns the BoundingBox of the whole String. \\nIs there a way to get the BoundingBox for each character of the String?\\nIf not do you intend to add this in a future release?\\n\\nThanks.\"}]}"
            }
          ]
        }
      ],
      "slackdump_thread_replies": [
        {
          "client_msg_id": "6aee9629-0d91-4dc4-8b76-d2443951755e",
          "type": "message",
          "user": "U045ULYCZFX",
          "text": "You have the following on the VNRecognizedText:/*!\n * *@brief* Calculate the bounding box around the characters in the range of the string.\n * *@discussion* The bounding boxes are not guaranteed to be an exact fit around the characters and are purely meant for UI purposes and not for image processing.\n */\n- (*nullable* VNRectangleObservation *)boundingBoxForRange:(NSRange)range error:(NSError**)error;",
          "ts": "1666203452.753129",
          "thread_ts": "1666203356.811059",
          "team": "T03U5MWB2FN",
          "replace_original": false,
          "delete_original": false,
          "blocks": [
            {
              "type": "rich_text",
              "block_id": "poD",
              "elements": [
                {
                  "type": "rich_text_section",
                  "elements": [
                    {
                      "type": "text",
                      "text": "You have the following on the VNRecognizedText:/*!\n * "
                    },
                    {
                      "type": "text",
                      "text": "@brief ",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "Calculate the bounding box around the characters in the range of the string.\n * "
                    },
                    {
                      "type": "text",
                      "text": "@discussion ",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": "The bounding boxes are not guaranteed to be an exact fit around the characters and are purely meant for UI purposes and not for image processing.\n */\n- ("
                    },
                    {
                      "type": "text",
                      "text": "nullable",
                      "style": {
                        "bold": true
                      }
                    },
                    {
                      "type": "text",
                      "text": " VNRectangleObservation *)boundingBoxForRange:(NSRange)range error:(NSError**)error;"
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    }
  ],
  "channel_id": "C042X5L3V7X"
}
